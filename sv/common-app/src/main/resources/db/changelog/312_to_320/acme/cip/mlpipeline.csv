created_by,created_date,deleted,description,job_id,json_content,lastmodifiedby,alias,lastmodifieddate,name,organization,type,version,tags,interfacetype,pipeline_metadata,is_template,is_app
"admin","2025-01-27 11:37:00.079000","\0","","NULL","{""elements"":[{""id"":""vGkxS"",""alias"":""Dataset Extractor"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractor"",""category"":""Extractor"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-04-10 14:16:38"",""alias"":""Tickets"",""id"":1,""name"":""Tickets"",""description"":""Tickets data"",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 12:16:34"",""alias"":""Tickets Schema"",""id"":1,""name"":""TicketsSchema"",""description"":null,""schemavalue"":""[{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 1,\\""recordcolumnname\\"": \\""number\\"",\\""recordcolumndisplayname\\"": \\""Number\\"",\\""isprimarykey\\"": true,\\""isunique\\"": true,\\""isrequired\\"": true},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 2,\\""recordcolumnname\\"": \\""shortdescription\\"",\\""recordcolumndisplayname\\"": \\""Short Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 3,\\""recordcolumnname\\"": \\""priority\\"",\\""recordcolumndisplayname\\"": \\""Priority\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 4,\\""recordcolumnname\\"": \\""type\\"",\\""recordcolumndisplayname\\"": \\""Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 5,\\""recordcolumnname\\"": \\""createdDate\\"",\\""recordcolumndisplayname\\"": \\""Created Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 6,\\""recordcolumnname\\"": \\""approval\\"",\\""recordcolumndisplayname\\"": \\""Approval\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 7,\\""recordcolumnname\\"": \\""assignedDate\\"",\\""recordcolumndisplayname\\"": \\""Assigned Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 8,\\""recordcolumnname\\"": \\""assignedto\\"",\\""recordcolumndisplayname\\"": \\""Assigned To\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 9,\\""recordcolumnname\\"": \\""assignmentgroup\\"",\\""recordcolumndisplayname\\"": \\""Assignment Group\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 10,\\""recordcolumnname\\"": \\""business_service\\"",\\""recordcolumndisplayname\\"": \\""Business Service\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 11,\\""recordcolumnname\\"": \\""caller\\"",\\""recordcolumndisplayname\\"": \\""Caller\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 12,\\""recordcolumnname\\"": \\""category\\"",\\""recordcolumndisplayname\\"": \\""Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 13,\\""recordcolumnname\\"": \\""closecode\\"",\\""recordcolumndisplayname\\"": \\""Close Code\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 14,\\""recordcolumnname\\"": \\""closedby\\"",\\""recordcolumndisplayname\\"": \\""Closed By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 15,\\""recordcolumnname\\"": \\""closedDate\\"",\\""recordcolumndisplayname\\"": \\""Closed Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 16,\\""recordcolumnname\\"": \\""closenotes\\"",\\""recordcolumndisplayname\\"": \\""Close Notes\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 17,\\""recordcolumnname\\"": \\""comments\\"",\\""recordcolumndisplayname\\"": \\""Comments\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 18,\\""recordcolumnname\\"": \\""configurationItem\\"",\\""recordcolumndisplayname\\"": \\""Configuration Item\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 19,\\""recordcolumnname\\"": \\""createdby\\"",\\""recordcolumndisplayname\\"": \\""Created By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 20,\\""recordcolumnname\\"": \\""description\\"",\\""recordcolumndisplayname\\"": \\""Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 21,\\""recordcolumnname\\"": \\""duedate\\"",\\""recordcolumndisplayname\\"": \\""Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 22,\\""recordcolumnname\\"": \\""impact\\"",\\""recordcolumndisplayname\\"": \\""Impact\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 23,\\""recordcolumnname\\"": \\""last_updated_by\\"",\\""recordcolumndisplayname\\"": \\""Last Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 24,\\""recordcolumnname\\"": \\""lastUpdated\\"",\\""recordcolumndisplayname\\"": \\""Last Updated\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 25,\\""recordcolumnname\\"": \\""location\\"",\\""recordcolumndisplayname\\"": \\""Location\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 26,\\""recordcolumnname\\"": \\""openedDate\\"",\\""recordcolumndisplayname\\"": \\""Opened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 27,\\""recordcolumnname\\"": \\""price\\"",\\""recordcolumndisplayname\\"": \\""Price\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 28,\\""recordcolumnname\\"": \\""reopenedDate\\"",\\""recordcolumndisplayname\\"": \\""Reopened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 29,\\""recordcolumnname\\"": \\""request_state\\"",\\""recordcolumndisplayname\\"": \\""Request State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 30,\\""recordcolumnname\\"": \\""requested_by\\"",\\""recordcolumndisplayname\\"": \\""Requested By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 31,\\""recordcolumnname\\"": \\""requested_for\\"",\\""recordcolumndisplayname\\"": \\""Requested For\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 32,\\""recordcolumnname\\"": \\""resolvedby\\"",\\""recordcolumndisplayname\\"": \\""Resolved By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 33,\\""recordcolumnname\\"": \\""resolvedDate\\"",\\""recordcolumndisplayname\\"": \\""Resolved Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 34,\\""recordcolumnname\\"": \\""risk\\"",\\""recordcolumndisplayname\\"": \\""Risk\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 35,\\""recordcolumnname\\"": \\""severity\\"",\\""recordcolumndisplayname\\"": \\""Severity\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 36,\\""recordcolumnname\\"": \\""sladueDate\\"",\\""recordcolumndisplayname\\"": \\""SLA Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 37,\\""recordcolumnname\\"": \\""special_instructions\\"",\\""recordcolumndisplayname\\"": \\""Special Instructions\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 38,\\""recordcolumnname\\"": \\""resolution_Steps\\"",\\""recordcolumndisplayname\\"": \\""Resolution Steps\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 39,\\""recordcolumnname\\"": \\""state\\"",\\""recordcolumndisplayname\\"": \\""State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 40,\\""recordcolumnname\\"": \\""sysId\\"",\\""recordcolumndisplayname\\"": \\""Sys Id\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 41,\\""recordcolumnname\\"": \\""taskType\\"",\\""recordcolumndisplayname\\"": \\""Task Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 42,\\""recordcolumnname\\"": \\""updatedby\\"",\\""recordcolumndisplayname\\"": \\""Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 43,\\""recordcolumnname\\"": \\""updatedDate\\"",\\""recordcolumndisplayname\\"": \\""Updated Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 44,\\""recordcolumnname\\"": \\""sop\\"",\\""recordcolumndisplayname\\"": \\""SOP\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 45,\\""recordcolumnname\\"": \\""tags\\"",\\""recordcolumndisplayname\\"": \\""Tags\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 46,\\""recordcolumnname\\"": \\""source\\"",\\""recordcolumndisplayname\\"": \\""Source\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 47,\\""recordcolumnname\\"": \\""resolutionCategory\\"",\\""recordcolumndisplayname\\"": \\""Resolution Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""workflow\\"",\\""recordcolumndisplayname\\"": \\""Workflow\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""resolutionStepsClusterName\\"",\\""recordcolumndisplayname\\"": \\""resolutionStepsClusterName\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false}]"",""organization"":""leo1311"",""type"":null,""capability"":null},""schemajson"":""[{\\""type\\"":\\""object\\"",\\""templateName\\"":\\""Form1\\"",\\""components\\"":[{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""R\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-refresh\\"",\\""tooltip\\"":\\""Refresh\\"",\\""tableView\\"":false,\\""key\\"":\\""refresh\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountrefresh = Number(document.getElementById('formio-btnclk-refresh').innerHTML); document.getElementById('formio-btnclk-refresh').innerHTML=(clickCountrefresh+1);\\"",\\""input\\"":true}],\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""offset\\"":6,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""TE\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-play\\"",\\""tooltip\\"":\\""Trigger Event\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""eventTrigger\\"",\\""properties\\"":{\\""event1\\"":\\""refreshTickets\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCounteventTrigger = Number(document.getElementById('formio-btnclk-eventTrigger').innerHTML); document.getElementById('formio-btnclk-eventTrigger').innerHTML=(clickCounteventTrigger+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""IN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cog\\"",\\""tooltip\\"":\\""Internal Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""internalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""landing/iamp-usm/dashconstant/{number}/false\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountinternalNavigation = Number(document.getElementById('formio-btnclk-internalNavigation').innerHTML); document.getElementById('formio-btnclk-internalNavigation').innerHTML=(clickCountinternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""EN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-google\\"",\\""tooltip\\"":\\""External Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""externalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""https://www.google.com\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountexternalNavigation = Number(document.getElementById('formio-btnclk-externalNavigation').innerHTML); document.getElementById('formio-btnclk-externalNavigation').innerHTML=(clickCountexternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""MA\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cogs\\"",\\""tooltip\\"":\\""Multiple Actions\\"",\\""tableView\\"":false,\\""key\\"":\\""multipleActions\\"",\\""properties\\"":{\\""submit\\"":\\""submit\\"",\\""reset\\"":\\""reset\\"",\\""refresh\\"":\\""refresh\\"",\\""eventTrigger\\"":\\""createIncident\\"",\\""externalNavigation1\\"":\\""https://www.google.com\\"",\\""externalNavigation2\\"":\\""https://www.youtube.com\\"",\\""internalNavigation\\"":\\""landing/iamp-usm/dashconstant\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountmultipleActions = Number(document.getElementById('formio-btnclk-multipleActions').innerHTML); document.getElementById('formio-btnclk-multipleActions').innerHTML=(clickCountmultipleActions+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1}],\\""key\\"":\\""columns1\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false},{\\""label\\"":\\""Table\\"",\\""cellAlignment\\"":\\""left\\"",\\""key\\"":\\""table\\"",\\""type\\"":\\""table\\"",\\""input\\"":false,\\""tableView\\"":false,\\""rows\\"":[[{\\""components\\"":[{\\""label\\"":\\""Number\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""number\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Short Description\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""shortdescription\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Priority\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""1 - Critical\\"",\\""value\\"":\\""1 - Critical\\""},{\\""label\\"":\\""2 - High\\"",\\""value\\"":\\""2 - High\\""},{\\""label\\"":\\""3 - Medium\\"",\\""value\\"":\\""3 - Medium\\""},{\\""label\\"":\\""4 - Low\\"",\\""value\\"":\\""4 - Low\\""},{\\""label\\"":\\""5 - Very Low\\"",\\""value\\"":\\""5 - Very Low\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""priority\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true}]}],[{\\""components\\"":[{\\""label\\"":\\""State\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""New\\"",\\""value\\"":\\""new\\""},{\\""label\\"":\\""In Progress\\"",\\""value\\"":\\""inProgress\\""},{\\""label\\"":\\""On Hold\\"",\\""value\\"":\\""onHold\\""},{\\""label\\"":\\""Resolved\\"",\\""value\\"":\\""resolved\\""},{\\""label\\"":\\""Closed\\"",\\""value\\"":\\""closed\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""state\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true,\\""defaultValue\\"":\\""new\\""}]},{\\""components\\"":[{\\""label\\"":\\""Configuration Item\\"",\\""tableView\\"":true,\\""key\\"":\\""configurationItem\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Assignment Group\\"",\\""tableView\\"":true,\\""key\\"":\\""assignmentgroup\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]}]],\\""numRows\\"":2},{\\""label\\"":\\""Description\\"",\\""autoExpand\\"":false,\\""tableView\\"":true,\\""key\\"":\\""description\\"",\\""type\\"":\\""textarea\\"",\\""input\\"":true},{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""Submit\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""success\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""submit\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountsubmit = Number(document.getElementById('formio-btnclk-submit').innerHTML); document.getElementById('formio-btnclk-submit').innerHTML=(clickCountsubmit+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""Reset\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""danger\\"",\\""tableView\\"":false,\\""key\\"":\\""reset\\"",\\""type\\"":\\""button\\"",\\""input\\"":true,\\""custom\\"":\\""let clickCountreset = Number(document.getElementById('formio-btnclk-reset').innerHTML); document.getElementById('formio-btnclk-reset').innerHTML=(clickCountreset+1);\\""}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1}],\\""key\\"":\\""columns\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false}]}]"",""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT * from @projectname_tickets\\"",\\""Cacheable\\"":\\""false\\"",\\""isStreaming\\"":\\""false\\"",\\""defaultValues\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_tickets\\"",\\""uniqueIdentifier\\"":\\""number\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":4,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""179"",""position_y"":""116"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""uNpuH"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""requirements"":[],""servicenow"":{},""imports"":[],""MYSQL"":{},""w"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r    import importlib.util\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'extractors', f'{extractortype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Extractor', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Extractor'] = module\\r    spec.loader.exec_module(module)\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""context"":[]},{""id"":""uNpuH"",""alias"":""Python Script"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""detect_language"",""requirements"":""translatepy"",""params"":[],""script"":[""import pandas as pd\\rfrom translatepy import Translator\\rfrom translatepy.exceptions import TranslatepyException\\r\\rdef language(text): \\r    translator = Translator()\\r    language_map = {\\r        'eng': 'English',\\r        'hin': 'Hindi',\\r        'spa': 'Spanish',\\r        'fra': 'French',\\r        'deu': 'German',\\r        'ita': 'Italian',\\r        'por': 'Portuguese',\\r        'rus': 'Russian',\\r        'ara': 'Arabic',\\r        'ben': 'Bengali',\\r        'mar': 'Marathi',\\r        'tam': 'Tamil',\\r        'tel': 'Telugu',\\r        'kan': 'Kannada',\\r        'mal': 'Malayalam',\\r        'guj': 'Gujarati',\\r        'urd': 'Urdu',\\r        'chi': 'Chinese',\\r        'jpn': 'Japanese',\\r        'kor': 'Korean'\\r    }\\r    if isinstance(text, str) and text.strip():\\r        try:\\r            language = translator.language(text)\\r            lang_code= str(language.result)\\r            return language_map.get(lang_code, lang_code)\\r        except TranslatepyException:\\r            return \\""Unknown\\""\\r    return \\""Unknown\\""\\r\\rdef detect_language(dataset):\\r    dataset=pd.DataFrame(dataset)\\r    dataset=dataset.head(10)\\r    \\r    try:\\r        \\r        dataset['DetectedLanguage'] = dataset['shortdescription'].apply(\\r            lambda x: language(x)\\r        )\\r        dataset= dataset[['number','shortdescription', 'DetectedLanguage']]\\r        print(dataset, len(dataset))\\r        return dataset.to_dict('records')\\r        \\r    except Exception as e:\\r        print(f\\""Error: {str(e)}\\"")\\r        return dataset\\r    ""]},""position_x"":""406"",""position_y"":""125"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""vGkxS"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""BNUGR"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-04-10 14:16:38"",""alias"":""Tickets"",""id"":1,""name"":""Tickets"",""description"":""Tickets data"",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 12:16:34"",""alias"":""Tickets Schema"",""id"":1,""name"":""TicketsSchema"",""description"":null,""schemavalue"":""[{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 1,\\""recordcolumnname\\"": \\""number\\"",\\""recordcolumndisplayname\\"": \\""Number\\"",\\""isprimarykey\\"": true,\\""isunique\\"": true,\\""isrequired\\"": true},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 2,\\""recordcolumnname\\"": \\""shortdescription\\"",\\""recordcolumndisplayname\\"": \\""Short Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 3,\\""recordcolumnname\\"": \\""priority\\"",\\""recordcolumndisplayname\\"": \\""Priority\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 4,\\""recordcolumnname\\"": \\""type\\"",\\""recordcolumndisplayname\\"": \\""Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 5,\\""recordcolumnname\\"": \\""createdDate\\"",\\""recordcolumndisplayname\\"": \\""Created Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 6,\\""recordcolumnname\\"": \\""approval\\"",\\""recordcolumndisplayname\\"": \\""Approval\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 7,\\""recordcolumnname\\"": \\""assignedDate\\"",\\""recordcolumndisplayname\\"": \\""Assigned Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 8,\\""recordcolumnname\\"": \\""assignedto\\"",\\""recordcolumndisplayname\\"": \\""Assigned To\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 9,\\""recordcolumnname\\"": \\""assignmentgroup\\"",\\""recordcolumndisplayname\\"": \\""Assignment Group\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 10,\\""recordcolumnname\\"": \\""business_service\\"",\\""recordcolumndisplayname\\"": \\""Business Service\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 11,\\""recordcolumnname\\"": \\""caller\\"",\\""recordcolumndisplayname\\"": \\""Caller\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 12,\\""recordcolumnname\\"": \\""category\\"",\\""recordcolumndisplayname\\"": \\""Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 13,\\""recordcolumnname\\"": \\""closecode\\"",\\""recordcolumndisplayname\\"": \\""Close Code\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 14,\\""recordcolumnname\\"": \\""closedby\\"",\\""recordcolumndisplayname\\"": \\""Closed By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 15,\\""recordcolumnname\\"": \\""closedDate\\"",\\""recordcolumndisplayname\\"": \\""Closed Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 16,\\""recordcolumnname\\"": \\""closenotes\\"",\\""recordcolumndisplayname\\"": \\""Close Notes\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 17,\\""recordcolumnname\\"": \\""comments\\"",\\""recordcolumndisplayname\\"": \\""Comments\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 18,\\""recordcolumnname\\"": \\""configurationItem\\"",\\""recordcolumndisplayname\\"": \\""Configuration Item\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 19,\\""recordcolumnname\\"": \\""createdby\\"",\\""recordcolumndisplayname\\"": \\""Created By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 20,\\""recordcolumnname\\"": \\""description\\"",\\""recordcolumndisplayname\\"": \\""Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 21,\\""recordcolumnname\\"": \\""duedate\\"",\\""recordcolumndisplayname\\"": \\""Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 22,\\""recordcolumnname\\"": \\""impact\\"",\\""recordcolumndisplayname\\"": \\""Impact\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 23,\\""recordcolumnname\\"": \\""last_updated_by\\"",\\""recordcolumndisplayname\\"": \\""Last Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 24,\\""recordcolumnname\\"": \\""lastUpdated\\"",\\""recordcolumndisplayname\\"": \\""Last Updated\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 25,\\""recordcolumnname\\"": \\""location\\"",\\""recordcolumndisplayname\\"": \\""Location\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 26,\\""recordcolumnname\\"": \\""openedDate\\"",\\""recordcolumndisplayname\\"": \\""Opened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 27,\\""recordcolumnname\\"": \\""price\\"",\\""recordcolumndisplayname\\"": \\""Price\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 28,\\""recordcolumnname\\"": \\""reopenedDate\\"",\\""recordcolumndisplayname\\"": \\""Reopened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 29,\\""recordcolumnname\\"": \\""request_state\\"",\\""recordcolumndisplayname\\"": \\""Request State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 30,\\""recordcolumnname\\"": \\""requested_by\\"",\\""recordcolumndisplayname\\"": \\""Requested By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 31,\\""recordcolumnname\\"": \\""requested_for\\"",\\""recordcolumndisplayname\\"": \\""Requested For\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 32,\\""recordcolumnname\\"": \\""resolvedby\\"",\\""recordcolumndisplayname\\"": \\""Resolved By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 33,\\""recordcolumnname\\"": \\""resolvedDate\\"",\\""recordcolumndisplayname\\"": \\""Resolved Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 34,\\""recordcolumnname\\"": \\""risk\\"",\\""recordcolumndisplayname\\"": \\""Risk\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 35,\\""recordcolumnname\\"": \\""severity\\"",\\""recordcolumndisplayname\\"": \\""Severity\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 36,\\""recordcolumnname\\"": \\""sladueDate\\"",\\""recordcolumndisplayname\\"": \\""SLA Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 37,\\""recordcolumnname\\"": \\""special_instructions\\"",\\""recordcolumndisplayname\\"": \\""Special Instructions\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 38,\\""recordcolumnname\\"": \\""resolution_Steps\\"",\\""recordcolumndisplayname\\"": \\""Resolution Steps\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 39,\\""recordcolumnname\\"": \\""state\\"",\\""recordcolumndisplayname\\"": \\""State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 40,\\""recordcolumnname\\"": \\""sysId\\"",\\""recordcolumndisplayname\\"": \\""Sys Id\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 41,\\""recordcolumnname\\"": \\""taskType\\"",\\""recordcolumndisplayname\\"": \\""Task Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 42,\\""recordcolumnname\\"": \\""updatedby\\"",\\""recordcolumndisplayname\\"": \\""Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 43,\\""recordcolumnname\\"": \\""updatedDate\\"",\\""recordcolumndisplayname\\"": \\""Updated Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 44,\\""recordcolumnname\\"": \\""sop\\"",\\""recordcolumndisplayname\\"": \\""SOP\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 45,\\""recordcolumnname\\"": \\""tags\\"",\\""recordcolumndisplayname\\"": \\""Tags\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 46,\\""recordcolumnname\\"": \\""source\\"",\\""recordcolumndisplayname\\"": \\""Source\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 47,\\""recordcolumnname\\"": \\""resolutionCategory\\"",\\""recordcolumndisplayname\\"": \\""Resolution Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""workflow\\"",\\""recordcolumndisplayname\\"": \\""Workflow\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""resolutionStepsClusterName\\"",\\""recordcolumndisplayname\\"": \\""resolutionStepsClusterName\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false}]"",""organization"":""leo1311"",""type"":null,""capability"":null},""schemajson"":""[{\\""type\\"":\\""object\\"",\\""templateName\\"":\\""Form1\\"",\\""components\\"":[{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""R\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-refresh\\"",\\""tooltip\\"":\\""Refresh\\"",\\""tableView\\"":false,\\""key\\"":\\""refresh\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountrefresh = Number(document.getElementById('formio-btnclk-refresh').innerHTML); document.getElementById('formio-btnclk-refresh').innerHTML=(clickCountrefresh+1);\\"",\\""input\\"":true}],\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""offset\\"":6,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""TE\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-play\\"",\\""tooltip\\"":\\""Trigger Event\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""eventTrigger\\"",\\""properties\\"":{\\""event1\\"":\\""refreshTickets\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCounteventTrigger = Number(document.getElementById('formio-btnclk-eventTrigger').innerHTML); document.getElementById('formio-btnclk-eventTrigger').innerHTML=(clickCounteventTrigger+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""IN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cog\\"",\\""tooltip\\"":\\""Internal Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""internalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""landing/iamp-usm/dashconstant/{number}/false\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountinternalNavigation = Number(document.getElementById('formio-btnclk-internalNavigation').innerHTML); document.getElementById('formio-btnclk-internalNavigation').innerHTML=(clickCountinternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""EN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-google\\"",\\""tooltip\\"":\\""External Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""externalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""https://www.google.com\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountexternalNavigation = Number(document.getElementById('formio-btnclk-externalNavigation').innerHTML); document.getElementById('formio-btnclk-externalNavigation').innerHTML=(clickCountexternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""MA\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cogs\\"",\\""tooltip\\"":\\""Multiple Actions\\"",\\""tableView\\"":false,\\""key\\"":\\""multipleActions\\"",\\""properties\\"":{\\""submit\\"":\\""submit\\"",\\""reset\\"":\\""reset\\"",\\""refresh\\"":\\""refresh\\"",\\""eventTrigger\\"":\\""createIncident\\"",\\""externalNavigation1\\"":\\""https://www.google.com\\"",\\""externalNavigation2\\"":\\""https://www.youtube.com\\"",\\""internalNavigation\\"":\\""landing/iamp-usm/dashconstant\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountmultipleActions = Number(document.getElementById('formio-btnclk-multipleActions').innerHTML); document.getElementById('formio-btnclk-multipleActions').innerHTML=(clickCountmultipleActions+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1}],\\""key\\"":\\""columns1\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false},{\\""label\\"":\\""Table\\"",\\""cellAlignment\\"":\\""left\\"",\\""key\\"":\\""table\\"",\\""type\\"":\\""table\\"",\\""input\\"":false,\\""tableView\\"":false,\\""rows\\"":[[{\\""components\\"":[{\\""label\\"":\\""Number\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""number\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Short Description\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""shortdescription\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Priority\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""1 - Critical\\"",\\""value\\"":\\""1 - Critical\\""},{\\""label\\"":\\""2 - High\\"",\\""value\\"":\\""2 - High\\""},{\\""label\\"":\\""3 - Medium\\"",\\""value\\"":\\""3 - Medium\\""},{\\""label\\"":\\""4 - Low\\"",\\""value\\"":\\""4 - Low\\""},{\\""label\\"":\\""5 - Very Low\\"",\\""value\\"":\\""5 - Very Low\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""priority\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true}]}],[{\\""components\\"":[{\\""label\\"":\\""State\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""New\\"",\\""value\\"":\\""new\\""},{\\""label\\"":\\""In Progress\\"",\\""value\\"":\\""inProgress\\""},{\\""label\\"":\\""On Hold\\"",\\""value\\"":\\""onHold\\""},{\\""label\\"":\\""Resolved\\"",\\""value\\"":\\""resolved\\""},{\\""label\\"":\\""Closed\\"",\\""value\\"":\\""closed\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""state\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true,\\""defaultValue\\"":\\""new\\""}]},{\\""components\\"":[{\\""label\\"":\\""Configuration Item\\"",\\""tableView\\"":true,\\""key\\"":\\""configurationItem\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Assignment Group\\"",\\""tableView\\"":true,\\""key\\"":\\""assignmentgroup\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]}]],\\""numRows\\"":2},{\\""label\\"":\\""Description\\"",\\""autoExpand\\"":false,\\""tableView\\"":true,\\""key\\"":\\""description\\"",\\""type\\"":\\""textarea\\"",\\""input\\"":true},{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""Submit\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""success\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""submit\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountsubmit = Number(document.getElementById('formio-btnclk-submit').innerHTML); document.getElementById('formio-btnclk-submit').innerHTML=(clickCountsubmit+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""Reset\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""danger\\"",\\""tableView\\"":false,\\""key\\"":\\""reset\\"",\\""type\\"":\\""button\\"",\\""input\\"":true,\\""custom\\"":\\""let clickCountreset = Number(document.getElementById('formio-btnclk-reset').innerHTML); document.getElementById('formio-btnclk-reset').innerHTML=(clickCountreset+1);\\""}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1}],\\""key\\"":\\""columns\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false}]}]"",""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT * from @projectname_tickets\\"",\\""Cacheable\\"":\\""false\\"",\\""isStreaming\\"":\\""false\\"",\\""defaultValues\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_tickets\\"",\\""uniqueIdentifier\\"":\\""number\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":4,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}}]},{""id"":""BNUGR"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoader"",""category"":""Loader"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-27 11:40:10"",""alias"":""Detected Language"",""id"":14328,""name"":""LEODTCTD81889"",""description"":null,""schema"":null,""schemajson"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, shortdescription, DetectedLanguage from @projectname_tickets_language\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""overwrite\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_tickets_language\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""660"",""position_y"":""124"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""uNpuH"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""requirements"":[],""imports"":[""import importlib""],""MYSQL"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r    import importlib.util\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'loaders', f'{loadertype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Loader', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Loader'] = module\\r    spec.loader.exec_module(module)\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\n""}}],""pipeline_attributes"":[],""environment"":[],""default_runtime"":""{\\""dsAlias\\"":\\""LocalCluster\\"",\\""dsName\\"":\\""LEALCLCL12132\\"",\\""type\\"":\\""REMOTE\\""}""}","admin","Language Detection","2025-01-31 04:52:51","LEOLNGDT92586","leo1311","DragNDropLite","7","NULL","pipeline","NULL","\0","0"
"admin","2025-01-28 07:33:11.719000","\0","","NULL","{""elements"":[{""id"":""OVsqJ"",""alias"":""Ease Mappings"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractor"",""category"":""Extractor"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:55:16"",""alias"":""EASE Mapping"",""id"":288,""name"":""ACMESMPN85731"",""description"":"""",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:49:12"",""alias"":""EASE Mapping"",""id"":14,""name"":""ACMESMPN94605"",""description"":null,""schemavalue"":""[{\\""columntype\\"":\\""int\\"",\\""columnorder\\"":1,\\""recordcolumnname\\"":\\""ID\\"",\\""recordcolumndisplayname\\"":\\""ID\\"",\\""isunique\\"":true,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":2,\\""recordcolumnname\\"":\\""Key_Word\\"",\\""recordcolumndisplayname\\"":\\""Key_Word\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":3,\\""recordcolumnname\\"":\\""Category\\"",\\""recordcolumndisplayname\\"":\\""Category\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":4,\\""recordcolumnname\\"":\\""Ease\\"",\\""recordcolumndisplayname\\"":\\""Ease\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":5,\\""recordcolumnname\\"":\\""Support_Level\\"",\\""recordcolumndisplayname\\"":\\""Support_Level\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":6,\\""recordcolumnname\\"":\\""Ranks\\"",\\""recordcolumndisplayname\\"":\\""Ranks\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":7,\\""recordcolumnname\\"":\\""Business_Area\\"",\\""recordcolumndisplayname\\"":\\""Business_Area\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":8,\\""recordcolumnname\\"":\\""Typical_Resolution\\"",\\""recordcolumndisplayname\\"":\\""Typical_Resolution\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":9,\\""recordcolumnname\\"":\\""Business_Impact\\"",\\""recordcolumndisplayname\\"":\\""Business_Impact\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":10,\\""recordcolumnname\\"":\\""Account\\"",\\""recordcolumndisplayname\\"":\\""Account\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":11,\\""recordcolumnname\\"":\\""BotName\\"",\\""recordcolumndisplayname\\"":\\""BotName\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false}]"",""organization"":""leo1311""},""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from @projectname_EASEMapping\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_EASEMapping\\"",\\""uniqueIdentifier\\"":\\""ID\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null}},""position_x"":""53"",""position_y"":""163"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""bWxAP"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""requirements"":[],""servicenow"":{},""imports"":[],""MYSQL"":{},""w"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r    import importlib.util\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'extractors', f'{extractortype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Extractor', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Extractor'] = module\\r    spec.loader.exec_module(module)\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""context"":[]},{""id"":""yeSbs"",""alias"":""Clusters"",""name"":""Dataset Loader"",""classname"":""DatasetLoader"",""category"":""Loader"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2024-10-23 11:04:06"",""alias"":""TicketsEnriched"",""id"":1073,""name"":""LEOTCKTS70205"",""description"":null,""schema"":null,""schemajson"":""\\""\\\\\\""\\\\\\\\\\\\\\""[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""id\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":6,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""TicketsEnriched\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""alias\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Tickets Enriched\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""organization\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leo1311\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""schemaname\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""ACMTCKTS40780\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""formtemplate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Table\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""cellAlignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""left\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""table\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""table\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""numRows\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":4,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""numCols\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":4,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""rows\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Short Description Based Cluster Name(Auto)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""cluster_classification_label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Apply\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""action\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""showValidations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leftIcon\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""fa fa-check-circle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customClass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""bottom-alignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""eventTrigger3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""event\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""updateIncident\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""body\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""incidentPayload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""number\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$number$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""shortDescription\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$cluster_classification_label$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customConditional\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""show=!!data.cluster_classification_label;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""button\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""let clickCounteventTrigger3 = Number(document.getElementById('formio-btnclk-eventTrigger3').innerHTML); document.getElementById('formio-btnclk-eventTrigger3').innerHTML=(clickCounteventTrigger3+1);\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Resolution Steps Based Cluster Name(Auto)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""resolution_steps_cluster\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Apply\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""action\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""showValidations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leftIcon\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""fa fa-check-circle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customClass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""bottom-alignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""eventTrigger4\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""event\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""updateIncident\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""body\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""incidentPayload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""number\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$number$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""resolution_steps\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$resolution_steps_cluster$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customConditional\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""show = !!data.resolution_steps_cluster;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""button\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""let clickCounteventTrigger4 = Number(document.getElementById('formio-btnclk-eventTrigger4').innerHTML); document.getElementById('formio-btnclk-eventTrigger4').innerHTML=(clickCounteventTrigger4+1);\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]}],[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Recommended Assignment Group\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""predicted_assignment_group\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Apply\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""action\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""showValidations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leftIcon\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""fa fa-check-circle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customClass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""bottom-alignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""eventTrigger5\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""event\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""updateIncident\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""body\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""incidentPayload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""number\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$number$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""assignmentGroup\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$predicted_assignment_group$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customConditional\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""show = !!data.predicted_assignment_group;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""button\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""let clickCounteventTrigger5 = Number(document.getElementById('formio-btnclk-eventTrigger5').innerHTML); document.getElementById('formio-btnclk-eventTrigger5').innerHTML=(clickCounteventTrigger5+1);\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Recommended Assignee\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""predicted_assignee\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Apply\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""action\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""showValidations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leftIcon\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""fa fa-check-circle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customClass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""bottom-alignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""eventTrigger6\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""event\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""updateIncident\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""body\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""incidentPayload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""number\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$number$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""assignee\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$predicted_assignee$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customConditional\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""show=!!data.predicted_assignee;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""button\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""let clickCounteventTrigger6 = Number(document.getElementById('formio-btnclk-eventTrigger6').innerHTML); document.getElementById('formio-btnclk-eventTrigger6').innerHTML=(clickCounteventTrigger6+1);\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]}],[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Predicted SOP\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""clearOnHide\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sop\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Apply\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""action\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""showValidations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leftIcon\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""fa fa-check-circle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customClass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""bottom-alignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""eventTrigger7\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""event\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""updateIncident\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""body\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""incidentPayload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""number\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$number$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sop\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$sop$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customConditional\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""show=!!data.sop\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""button\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""let clickCounteventTrigger7 = Number(document.getElementById('formio-btnclk-eventTrigger7').innerHTML); document.getElementById('formio-btnclk-eventTrigger7').innerHTML=(clickCounteventTrigger7+1);\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Predicted Workflow\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""workflow\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Apply\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""action\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""showValidations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""sm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""leftIcon\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""fa fa-check-circle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customClass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""bottom-alignment\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":false,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""eventTrigger8\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""event\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""updateIncident\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""body\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""incidentPayload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""number\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$number$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""workflow\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""$workflow$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""customConditional\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""show=!!data.workflow\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""button\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""custom\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""let clickCounteventTrigger8 = Number(document.getElementById('formio-btnclk-eventTrigger8').innerHTML); document.getElementById('formio-btnclk-eventTrigger8').innerHTML=(clickCounteventTrigger8+1);\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]}],[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Recommended Response SLA Date\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""response_SLA\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""label\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""Recommended Resolution SLA Date\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""disabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""tableView\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""key\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""resolution_SLA\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""textfield\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"",\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":true}]},{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""components\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"":[]}]]}]}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\""}]\\\\\\\\\\\\\\""\\\\\\""\\"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select * from @projectname_tickets_enriched"",""Cacheable"":false,""isStreaming"":""false"",""Headers"":"""",""defaultValues"":"""",""QueryParams"":"""",""writeMode"":""update"",""params"":""{}"",""tableName"":""@projectname_tickets_enriched"",""uniqueIdentifier"":""number""},""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""context"":null,""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""dashboard"":null,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""870"",""position_y"":""234"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""FZLGS"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""requirements"":[],""imports"":[""import importlib""],""MYSQL"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r    import importlib.util\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'loaders', f'{loadertype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Loader', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Loader'] = module\\r    spec.loader.exec_module(module)\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\n""},""context"":[{""FunctionName"":""prioritization"",""requirements"":"""",""params"":[],""script"":[""from concurrent.futures import ProcessPoolExecutor\\r\\rfrom datetime import datetime\\r\\rdef prioritization(dataset):\\r\\r    def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r\\r    # def getCluster(ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r\\r        cluster = ''\\r\\r        if tags is not None and tags != '':\\r\\r            cluster = tags\\r\\r        elif ngram is not None and ngram != '':\\r\\r            cluster = ngram\\r\\r        elif soundex_cluster is not None and soundex_cluster != '':\\r\\r            cluster = soundex_cluster\\r\\r        elif mapped_phrase is not None and mapped_phrase != '':\\r\\r            cluster = mapped_phrase\\r\\r        elif lda_cluster is not None and lda_cluster != '':\\r\\r            cluster = lda_cluster\\r\\r        return cluster\\r\\r    logger.info('Prioritizing cluster for tickets..')\\r\\r    try:\\r\\r        dataset = dataset.replace(np.nan,'')\\r\\r        dataset = dataset.to_dict('records')\\r\\r        \\r\\r        for row in dataset:\\r\\r            row['post_ranking_cluster'] = getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r\\r            # row['post_ranking_cluster'] = getCluster(row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r\\r\\r\\r        dataset = pd.DataFrame(dataset)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        dataset = dataset[['number','clean_text','ngram','soundex_cluster','lda_cluster','extracted_phrase','mapped_phrase','mapped_phrase_confidennce','last_updated','post_ranking_cluster']]\\r\\r        dataset = dataset.to_dict('records')\\r\\r        logger.info('Total tickets after clustering {0}'.format(len(dataset)))\\r\\r    except Exception as ex:\\r\\r        logger.error('error in cluster Prioritizing')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info('Saving Data')\\r\\r    end_time = datetime.now()\\r\\r    \\r\\r    ended_time = datetime.now()\\r\\r    Total_time = ended_time - started_time\\r\\r    print(Total_time)\\r\\r    return dataset""]},{""FunctionName"":""lda"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer\\r\\rfrom sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\rfrom functools import partial\\r\\rfrom datetime import datetime\\r\\r\\r\\rdef count_vectorize(dataset):\\r\\r    count_vectorizer = CountVectorizer(stop_words='english')\\r\\r    count_data = count_vectorizer.fit_transform(dataset['clean_text'].to_list())\\r\\r    words = count_vectorizer.get_feature_names_out()\\r\\r    return {'data': count_data, 'words': words, 'number': dataset['number'].to_list()}\\r\\r\\r\\rdef lda_process_group(group_data, clustercount_param):\\r\\r    number_topics = clustercount_param\\r\\r    count_data, words, number = group_data['data'], group_data['words'], group_data['number']\\r\\r    \\r\\r    lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\r    lda.fit(count_data)\\r\\r    documents = lda.transform(count_data)\\r\\r    \\r\\r    argmax_values = np.argmax(documents, axis=1)\\r\\r    \\r\\r    def wordsWithWeights(termIndices, termWeights, index):\\r\\r        terms = [words[i] for i in termIndices]\\r\\r        topic = [index]*len(terms)\\r\\r        return list(zip(terms, topic, termIndices, termWeights))\\r\\r    \\r\\r    topics = [wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) \\r\\r              for index, topic in enumerate(lda.components_)]\\r\\r    \\r\\r    topicWords = [item for topic in topics for item in topic]\\r\\r    finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\r    finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\r    finalTopic.columns = ['topic', 'word', 'weight']\\r\\r    \\r\\r    newdocuments = pd.DataFrame({'cluster_Id': argmax_values, 'number': number})\\r\\r    topicwordsdf = pd.DataFrame({'topic': range(number_topics), \\r\\r                                 'cluster_Name': [\\r\\r                                     list(finalTopic[finalTopic['topic'] == i]['word']) \\r\\r                                     for i in range(number_topics)\\r\\r                                 ]})\\r\\r    \\r\\r    newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\r    newdocuments = newdocuments.drop(columns='topic')\\r\\r    newdocuments['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r    \\r\\r    return newdocuments, finalTopic\\r\\r\\r\\rdef lda(dataset, clustercount_param=5, uniqueidcolumn_param=''):\\r    \\r    if os.environ.get(\\""lda\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r        try:\\r            logger.info(\\""lda started...\\"")\\r    \\r            dataset = pd.DataFrame(dataset)\\r    \\r            originaldataset = dataset\\r    \\r            \\r    \\r            # Group by field processing\\r    \\r            grouped = dataset.groupby('group_by_field')\\r    \\r            grouped_df = {name: group for name, group in grouped}\\r    \\r            \\r    \\r            # Prepare data for processing\\r    \\r            vectorized_data = {name: count_vectorize(group) for name, group in grouped_df.items()}\\r    \\r            \\r            num_cores = multiprocessing.cpu_count()\\r            # Parallel LDA processing\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                lda_func = partial(lda_process_group, clustercount_param=clustercount_param)\\r    \\r                results = pool.map(lda_func, vectorized_data.values())\\r    \\r            \\r    \\r            # Separate results\\r    \\r            newdocuments_result = []\\r    \\r            finalTopic_result = []\\r    \\r            for (newdocuments, finalTopic), (name, _) in zip(results, vectorized_data.items()):\\r    \\r                finalTopic['group_by_field'] = name\\r    \\r                newdocuments_result.append(newdocuments)\\r    \\r                finalTopic_result.append(finalTopic)\\r    \\r            \\r    \\r            # Combine results\\r    \\r            resultsdf = pd.concat(newdocuments_result, ignore_index=True)\\r    \\r            topicsdf = pd.concat(finalTopic_result, ignore_index=True)\\r    \\r            \\r    \\r            # Process topics\\r    \\r            topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({\\r    \\r                'word': list, \\r    \\r                'weight': list\\r    \\r            }).reset_index()\\r    \\r            topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r    \\r            topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(map(str, x)))\\r    \\r            topicsdf['alias'] = topicsdf['words']\\r    \\r            topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r    \\r            \\r    \\r            # Process results\\r    \\r            resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r    \\r            resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r    \\r            resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']]\\r    \\r            \\r    \\r            # Merge with original dataset\\r    \\r            dataset = pd.merge(originaldataset, resultsdf, on='number', how='left')\\r    \\r            logger.info(\\""Lda is done...\\"")\\r            return dataset\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in LDA')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise""]},{""FunctionName"":""map_phrases"",""requirements"":"""",""params"":[],""script"":[""import numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rfrom functools import partial\\r\\rdef getSimilar(sentences, keywords):\\r    corpus = sentences + keywords\\r    keywordstartIndex = len(sentences)\\r    tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r    pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r    arr = pairwise_similarity.toarray()\\r    np.fill_diagonal(arr, np.nan)\\r    results = {}\\r    for s in sentences:\\r        input_idx = sentences.index(s)\\r        result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r        match = arr[input_idx][keywordstartIndex + result_idx]\\r        r = keywords[result_idx]\\r        results[s] = r + ':' + str(match) if match > 0 else 'NO MATCH:0'\\r    return results\\rdef process_chunk(chunk, keywords):\\r    results = getSimilar(chunk, keywords)\\r    mappings = {}\\r    for pattern, result in results.items():\\r        kw = result.split(':')[0]\\r        score = result.split(':')[-1]\\r        if kw != 'NO MATCH':\\r            mappings[pattern] = {'keyword': kw, 'score': score}\\r    return mappings\\rdef map_phrases(dataset, ease):\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        return pd.DataFrame(dataset)\\r    else:\\r        \\r        try:\\r            logger.info(\\""map_phrases started ...\\"")\\r            # Extract unique keywords and phrases\\r            keywords = list(set(item['Key_Word'] for item in ease))\\r            phrases = list(set(item['extracted_phrase'] for item in dataset))\\r            # Multiprocessing setup\\r            num_cores = multiprocessing.cpu_count()\\r            chunk_size = max(1, len(phrases) // num_cores)\\r            # Split phrases into chunks\\r            phrase_chunks = [phrases[i:i+chunk_size] for i in range(0, len(phrases), chunk_size)]\\r            # Partial function to pass keywords\\r            process_func = partial(process_chunk, keywords=keywords)\\r            # Parallel processing\\r            with multiprocessing.Pool(num_cores) as pool:\\r                chunk_results = pool.map(process_func, phrase_chunks)\\r            # Combine results\\r            mappings = {}\\r            for chunk_mapping in chunk_results:\\r                mappings.update(chunk_mapping)\\r            # Update dataset with mappings\\r            for row in dataset:\\r                phrase = row['extracted_phrase']\\r                if phrase in mappings:\\r                    row['mapped_phrase'] = mappings[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round(float(mappings[phrase]['score']), 4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r            logger.info(\\""map_phrases is done ...\\"")\\r            return dataset\\r        except Exception as ex:\\r            logger.error('Error in Map phrase')\\r            raise ex\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:55:16"",""alias"":""EASE Mapping"",""id"":288,""name"":""ACMESMPN85731"",""description"":"""",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:49:12"",""alias"":""EASE Mapping"",""id"":14,""name"":""ACMESMPN94605"",""description"":null,""schemavalue"":""[{\\""columntype\\"":\\""int\\"",\\""columnorder\\"":1,\\""recordcolumnname\\"":\\""ID\\"",\\""recordcolumndisplayname\\"":\\""ID\\"",\\""isunique\\"":true,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":2,\\""recordcolumnname\\"":\\""Key_Word\\"",\\""recordcolumndisplayname\\"":\\""Key_Word\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":3,\\""recordcolumnname\\"":\\""Category\\"",\\""recordcolumndisplayname\\"":\\""Category\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":4,\\""recordcolumnname\\"":\\""Ease\\"",\\""recordcolumndisplayname\\"":\\""Ease\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":5,\\""recordcolumnname\\"":\\""Support_Level\\"",\\""recordcolumndisplayname\\"":\\""Support_Level\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":6,\\""recordcolumnname\\"":\\""Ranks\\"",\\""recordcolumndisplayname\\"":\\""Ranks\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":7,\\""recordcolumnname\\"":\\""Business_Area\\"",\\""recordcolumndisplayname\\"":\\""Business_Area\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":8,\\""recordcolumnname\\"":\\""Typical_Resolution\\"",\\""recordcolumndisplayname\\"":\\""Typical_Resolution\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":9,\\""recordcolumnname\\"":\\""Business_Impact\\"",\\""recordcolumndisplayname\\"":\\""Business_Impact\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":10,\\""recordcolumnname\\"":\\""Account\\"",\\""recordcolumndisplayname\\"":\\""Account\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":11,\\""recordcolumnname\\"":\\""BotName\\"",\\""recordcolumndisplayname\\"":\\""BotName\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false}]"",""organization"":""leo1311""},""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from @projectname_EASEMapping\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_EASEMapping\\"",\\""uniqueIdentifier\\"":\\""ID\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null}},{""FunctionName"":""extract_phrases_parallel"",""requirements"":"""",""params"":[],""script"":[""\\r\\rimport logging\\rimport pytextrank\\rimport spacy\\r# from spacy.language import Language\\r\\rdef extract_phrases(dataset):\\r\\r    logger.info('Extracting phrases....')\\r\\r    try:\\r        dataset = dataset.to_dict('records')\\r        \\r        nlp = spacy.load('en_core_web_sm')\\r        \\r        nlp.add_pipe('textrank')\\r\\r        timenow = datetime.now()\\r\\r        totalRecords = len(dataset)\\r        \\r        count =0\\r\\r        textPhraseMappings = {}\\r        \\r\\r        for row in dataset:\\r\\r            try:\\r\\r                text = row['clean_text']\\r                \\r                if textPhraseMappings.get(text,'') != '':\\r\\r                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\r                    break\\r\\r                doc = nlp(text)\\r\\r                phrase =''\\r\\r                if len(doc._.phrases)>0:\\r\\r                    for item in doc._.phrases:\\r\\r                        withoutSpace = item.text.replace(' ' ,'')\\r\\r                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\r                            phrase = item.text\\r\\r                            break\\r\\r                if phrase != '':\\r\\r                    row['extracted_phrase'] = phrase\\r\\r                else:\\r\\r                    row['extracted_phrase'] = text\\r\\r            except Exception as ex:\\r\\r                logging.info(ex)\\r\\r                row['extracted_phrase'] = text\\r\\r    except Exception as ex:\\r\\r        logger.error('error in Extract Phrases')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info(\\""phrase Extraction is done...\\"")\\r\\r    # dataset = pd.DataFrame(dataset)\\r\\r    return dataset\\r\\r\\r\\r\\r\\rdef extract_phrases_parallel(dataset, num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Extract phrases from ticket descriptions using parallel processing\\r\\r    \\""\\""\\""\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        df = pd.DataFrame(dataset)\\r        return df\\r    \\r    else:\\r        start_time = datetime.now()\\r    \\r        \\r    \\r        # Convert to DataFrame if not already\\r    \\r        df = pd.DataFrame(dataset)\\r    \\r        total_records = len(df)\\r    \\r        \\r    \\r        if num_cores is None:\\r    \\r            num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r        \\r    \\r        # Split the dataframe into chunks\\r    \\r        chunks = np.array_split(df, num_cores)\\r    \\r        \\r    \\r        # Process chunks in parallel using ProcessPoolExecutor\\r    \\r        processed_chunks = []\\r    \\r        exctract_phrases_chunk = partial(extract_phrases)\\r    \\r        with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r            # Submit all chunks for processing\\r            futures = [executor.submit(exctract_phrases_chunk, chunk) for chunk in chunks]\\r    \\r            # Collect results as they complete\\r    \\r            for future in concurrent.futures.as_completed(futures):\\r    \\r                try:\\r    \\r                    result = future.result()\\r    \\r                    if result is not None and len(result) > 0:\\r    \\r                        processed_chunks.append(result)\\r    \\r                except Exception as e:\\r    \\r                    logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r    \\r                \\r        if not processed_chunks:\\r    \\r            logger.warning(\\""No chunks were successfully processed\\"")\\r    \\r            return pd.DataFrame()\\r    \\r        # Combine all processed chunks\\r    \\r        final_df = pd.concat([pd.DataFrame(chunk) for chunk in processed_chunks], ignore_index=True)\\r    \\r        # Print processing summary\\r        end_time = datetime.now()\\r        final_results = final_df.to_dict('records')\\r        processing_time = end_time - start_time\\r    \\r        print(f'Processing Summary:')\\r        print(f'Total time required: {processing_time}')\\r    \\r        return final_results""]},{""FunctionName"":""ngram"",""requirements"":"""",""params"":[],""script"":[""import sklearn\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef process_ngram_chunk(chunk, top50):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of data to map text to top ngrams\\r\\r    \\""\\""\\""\\r\\r    ngramDict = {}\\r\\r    for item in chunk:\\r\\r        if item != '' and len(item) > 2:\\r\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r\\r            if len(matchingGrams) > 0:\\r\\r                ngramDict[item] = matchingGrams[0]\\r\\r    return ngramDict\\r\\r\\r\\rdef ngram(dataset, num_cores=None):\\r    if os.environ.get(\\""NGRAM\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r        \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Generate Ngram clusters using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating Ngram clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # Prepare text data\\r    \\r            txt1 = [row['clean_text'] for index, row in dataset.iterrows() \\r    \\r                    if row['clean_text'] != '' and len(row['clean_text']) > 2]\\r    \\r           \\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r    \\r    \\r            # Getting trigrams \\r    \\r            count_vectorizer = CountVectorizer(ngram_range=(3,3))\\r    \\r            X1 = count_vectorizer.fit_transform(txt1) \\r    \\r            features = count_vectorizer.get_feature_names_out()\\r    \\r    \\r    \\r            # Applying TFIDF\\r    \\r            tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3))\\r    \\r            X2 = tfidf_vectorizer.fit_transform(txt1)\\r    \\r            \\r    \\r            # Getting top ranking features\\r    \\r            logger.info('Getting Top 50 Grams')\\r    \\r            sums = X2.sum(axis=0)\\r    \\r            data1 = []\\r    \\r            for col, term in enumerate(features):\\r    \\r                data1.append((term, sums[0,col]))\\r    \\r            \\r    \\r            ranking = pd.DataFrame(data1, columns=['term','rank'])\\r    \\r            words = ranking.sort_values('rank', ascending=False)\\r    \\r            top50df = words.nlargest(50,'rank')\\r    \\r            top50 = words['term'].tolist()\\r    \\r            \\r    \\r            # Get unique texts\\r    \\r            distinctText = list(set(txt1))\\r    \\r            \\r    \\r            # Split work across cores\\r    \\r            chunks = np.array_split(distinctText, num_cores)\\r    \\r            \\r    \\r            # Prepare partial function with top50 grams\\r    \\r            process_chunk_with_top50 = partial(process_ngram_chunk, top50=top50)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                ngram_results = pool.map(process_chunk_with_top50, chunks)\\r    \\r            \\r    \\r            # Combine results from all chunks\\r    \\r            ngramDict = {}\\r    \\r            for chunk_dict in ngram_results:\\r    \\r                ngramDict.update(chunk_dict)\\r    \\r            \\r    \\r            # Map ngrams to original dataset\\r    \\r            dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x, ''))\\r    \\r            \\r    \\r            logger.info('Ngram mapping completed')\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in Ngram')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            dataset['ngram'] = ''\\r\\r\\r    return dataset\\r""]},{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""jtTZa"",""alias"":""Clean tickets"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},""position_x"":""441"",""position_y"":""4"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""ddKCY"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""emiga"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""ddKCY"",""alias"":""soundex"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},""position_x"":""637"",""position_y"":""5"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""jtTZa"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""HspNm"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""HspNm"",""alias"":""ngram"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""ngram"",""requirements"":"""",""params"":[],""script"":[""import sklearn\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef process_ngram_chunk(chunk, top50):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of data to map text to top ngrams\\r\\r    \\""\\""\\""\\r\\r    ngramDict = {}\\r\\r    for item in chunk:\\r\\r        if item != '' and len(item) > 2:\\r\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r\\r            if len(matchingGrams) > 0:\\r\\r                ngramDict[item] = matchingGrams[0]\\r\\r    return ngramDict\\r\\r\\r\\rdef ngram(dataset, num_cores=None):\\r    if os.environ.get(\\""NGRAM\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r        \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Generate Ngram clusters using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating Ngram clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # Prepare text data\\r    \\r            txt1 = [row['clean_text'] for index, row in dataset.iterrows() \\r    \\r                    if row['clean_text'] != '' and len(row['clean_text']) > 2]\\r    \\r           \\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r    \\r    \\r            # Getting trigrams \\r    \\r            count_vectorizer = CountVectorizer(ngram_range=(3,3))\\r    \\r            X1 = count_vectorizer.fit_transform(txt1) \\r    \\r            features = count_vectorizer.get_feature_names_out()\\r    \\r    \\r    \\r            # Applying TFIDF\\r    \\r            tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3))\\r    \\r            X2 = tfidf_vectorizer.fit_transform(txt1)\\r    \\r            \\r    \\r            # Getting top ranking features\\r    \\r            logger.info('Getting Top 50 Grams')\\r    \\r            sums = X2.sum(axis=0)\\r    \\r            data1 = []\\r    \\r            for col, term in enumerate(features):\\r    \\r                data1.append((term, sums[0,col]))\\r    \\r            \\r    \\r            ranking = pd.DataFrame(data1, columns=['term','rank'])\\r    \\r            words = ranking.sort_values('rank', ascending=False)\\r    \\r            top50df = words.nlargest(50,'rank')\\r    \\r            top50 = words['term'].tolist()\\r    \\r            \\r    \\r            # Get unique texts\\r    \\r            distinctText = list(set(txt1))\\r    \\r            \\r    \\r            # Split work across cores\\r    \\r            chunks = np.array_split(distinctText, num_cores)\\r    \\r            \\r    \\r            # Prepare partial function with top50 grams\\r    \\r            process_chunk_with_top50 = partial(process_ngram_chunk, top50=top50)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                ngram_results = pool.map(process_chunk_with_top50, chunks)\\r    \\r            \\r    \\r            # Combine results from all chunks\\r    \\r            ngramDict = {}\\r    \\r            for chunk_dict in ngram_results:\\r    \\r                ngramDict.update(chunk_dict)\\r    \\r            \\r    \\r            # Map ngrams to original dataset\\r    \\r            dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x, ''))\\r    \\r            \\r    \\r            logger.info('Ngram mapping completed')\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in Ngram')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            dataset['ngram'] = ''\\r\\r\\r    return dataset\\r""]},""position_x"":""838"",""position_y"":""5"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""ddKCY"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""XjOcS"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""bWxAP"",""alias"":""Map Phrases"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""map_phrases"",""requirements"":"""",""params"":[],""script"":[""import numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rfrom functools import partial\\r\\rdef getSimilar(sentences, keywords):\\r    corpus = sentences + keywords\\r    keywordstartIndex = len(sentences)\\r    tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r    pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r    arr = pairwise_similarity.toarray()\\r    np.fill_diagonal(arr, np.nan)\\r    results = {}\\r    for s in sentences:\\r        input_idx = sentences.index(s)\\r        result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r        match = arr[input_idx][keywordstartIndex + result_idx]\\r        r = keywords[result_idx]\\r        results[s] = r + ':' + str(match) if match > 0 else 'NO MATCH:0'\\r    return results\\rdef process_chunk(chunk, keywords):\\r    results = getSimilar(chunk, keywords)\\r    mappings = {}\\r    for pattern, result in results.items():\\r        kw = result.split(':')[0]\\r        score = result.split(':')[-1]\\r        if kw != 'NO MATCH':\\r            mappings[pattern] = {'keyword': kw, 'score': score}\\r    return mappings\\rdef map_phrases(dataset, ease):\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        return pd.DataFrame(dataset)\\r    else:\\r        \\r        try:\\r            logger.info(\\""map_phrases started ...\\"")\\r            # Extract unique keywords and phrases\\r            keywords = list(set(item['Key_Word'] for item in ease))\\r            phrases = list(set(item['extracted_phrase'] for item in dataset))\\r            # Multiprocessing setup\\r            num_cores = multiprocessing.cpu_count()\\r            chunk_size = max(1, len(phrases) // num_cores)\\r            # Split phrases into chunks\\r            phrase_chunks = [phrases[i:i+chunk_size] for i in range(0, len(phrases), chunk_size)]\\r            # Partial function to pass keywords\\r            process_func = partial(process_chunk, keywords=keywords)\\r            # Parallel processing\\r            with multiprocessing.Pool(num_cores) as pool:\\r                chunk_results = pool.map(process_func, phrase_chunks)\\r            # Combine results\\r            mappings = {}\\r            for chunk_mapping in chunk_results:\\r                mappings.update(chunk_mapping)\\r            # Update dataset with mappings\\r            for row in dataset:\\r                phrase = row['extracted_phrase']\\r                if phrase in mappings:\\r                    row['mapped_phrase'] = mappings[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round(float(mappings[phrase]['score']), 4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r            logger.info(\\""map_phrases is done ...\\"")\\r            return dataset\\r        except Exception as ex:\\r            logger.error('Error in Map phrase')\\r            raise ex\\r""]},""position_x"":""253"",""position_y"":""235"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""OVsqJ"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""WkoEz"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""XjOcS"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:55:16"",""alias"":""EASE Mapping"",""id"":288,""name"":""ACMESMPN85731"",""description"":"""",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:49:12"",""alias"":""EASE Mapping"",""id"":14,""name"":""ACMESMPN94605"",""description"":null,""schemavalue"":""[{\\""columntype\\"":\\""int\\"",\\""columnorder\\"":1,\\""recordcolumnname\\"":\\""ID\\"",\\""recordcolumndisplayname\\"":\\""ID\\"",\\""isunique\\"":true,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":2,\\""recordcolumnname\\"":\\""Key_Word\\"",\\""recordcolumndisplayname\\"":\\""Key_Word\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":3,\\""recordcolumnname\\"":\\""Category\\"",\\""recordcolumndisplayname\\"":\\""Category\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":4,\\""recordcolumnname\\"":\\""Ease\\"",\\""recordcolumndisplayname\\"":\\""Ease\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":5,\\""recordcolumnname\\"":\\""Support_Level\\"",\\""recordcolumndisplayname\\"":\\""Support_Level\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":6,\\""recordcolumnname\\"":\\""Ranks\\"",\\""recordcolumndisplayname\\"":\\""Ranks\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":7,\\""recordcolumnname\\"":\\""Business_Area\\"",\\""recordcolumndisplayname\\"":\\""Business_Area\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":8,\\""recordcolumnname\\"":\\""Typical_Resolution\\"",\\""recordcolumndisplayname\\"":\\""Typical_Resolution\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":9,\\""recordcolumnname\\"":\\""Business_Impact\\"",\\""recordcolumndisplayname\\"":\\""Business_Impact\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":10,\\""recordcolumnname\\"":\\""Account\\"",\\""recordcolumndisplayname\\"":\\""Account\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":11,\\""recordcolumnname\\"":\\""BotName\\"",\\""recordcolumndisplayname\\"":\\""BotName\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false}]"",""organization"":""leo1311""},""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from @projectname_EASEMapping\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_EASEMapping\\"",\\""uniqueIdentifier\\"":\\""ID\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null}},{""FunctionName"":""extract_phrases_parallel"",""requirements"":"""",""params"":[],""script"":[""\\r\\rimport logging\\rimport pytextrank\\rimport spacy\\r# from spacy.language import Language\\r\\rdef extract_phrases(dataset):\\r\\r    logger.info('Extracting phrases....')\\r\\r    try:\\r        dataset = dataset.to_dict('records')\\r        \\r        nlp = spacy.load('en_core_web_sm')\\r        \\r        nlp.add_pipe('textrank')\\r\\r        timenow = datetime.now()\\r\\r        totalRecords = len(dataset)\\r        \\r        count =0\\r\\r        textPhraseMappings = {}\\r        \\r\\r        for row in dataset:\\r\\r            try:\\r\\r                text = row['clean_text']\\r                \\r                if textPhraseMappings.get(text,'') != '':\\r\\r                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\r                    break\\r\\r                doc = nlp(text)\\r\\r                phrase =''\\r\\r                if len(doc._.phrases)>0:\\r\\r                    for item in doc._.phrases:\\r\\r                        withoutSpace = item.text.replace(' ' ,'')\\r\\r                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\r                            phrase = item.text\\r\\r                            break\\r\\r                if phrase != '':\\r\\r                    row['extracted_phrase'] = phrase\\r\\r                else:\\r\\r                    row['extracted_phrase'] = text\\r\\r            except Exception as ex:\\r\\r                logging.info(ex)\\r\\r                row['extracted_phrase'] = text\\r\\r    except Exception as ex:\\r\\r        logger.error('error in Extract Phrases')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info(\\""phrase Extraction is done...\\"")\\r\\r    # dataset = pd.DataFrame(dataset)\\r\\r    return dataset\\r\\r\\r\\r\\r\\rdef extract_phrases_parallel(dataset, num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Extract phrases from ticket descriptions using parallel processing\\r\\r    \\""\\""\\""\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        df = pd.DataFrame(dataset)\\r        return df\\r    \\r    else:\\r        start_time = datetime.now()\\r    \\r        \\r    \\r        # Convert to DataFrame if not already\\r    \\r        df = pd.DataFrame(dataset)\\r    \\r        total_records = len(df)\\r    \\r        \\r    \\r        if num_cores is None:\\r    \\r            num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r        \\r    \\r        # Split the dataframe into chunks\\r    \\r        chunks = np.array_split(df, num_cores)\\r    \\r        \\r    \\r        # Process chunks in parallel using ProcessPoolExecutor\\r    \\r        processed_chunks = []\\r    \\r        exctract_phrases_chunk = partial(extract_phrases)\\r    \\r        with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r            # Submit all chunks for processing\\r            futures = [executor.submit(exctract_phrases_chunk, chunk) for chunk in chunks]\\r    \\r            # Collect results as they complete\\r    \\r            for future in concurrent.futures.as_completed(futures):\\r    \\r                try:\\r    \\r                    result = future.result()\\r    \\r                    if result is not None and len(result) > 0:\\r    \\r                        processed_chunks.append(result)\\r    \\r                except Exception as e:\\r    \\r                    logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r    \\r                \\r        if not processed_chunks:\\r    \\r            logger.warning(\\""No chunks were successfully processed\\"")\\r    \\r            return pd.DataFrame()\\r    \\r        # Combine all processed chunks\\r    \\r        final_df = pd.concat([pd.DataFrame(chunk) for chunk in processed_chunks], ignore_index=True)\\r    \\r        # Print processing summary\\r        end_time = datetime.now()\\r        final_results = final_df.to_dict('records')\\r        processing_time = end_time - start_time\\r    \\r        print(f'Processing Summary:')\\r        print(f'Total time required: {processing_time}')\\r    \\r        return final_results""]},{""FunctionName"":""ngram"",""requirements"":"""",""params"":[],""script"":[""import sklearn\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef process_ngram_chunk(chunk, top50):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of data to map text to top ngrams\\r\\r    \\""\\""\\""\\r\\r    ngramDict = {}\\r\\r    for item in chunk:\\r\\r        if item != '' and len(item) > 2:\\r\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r\\r            if len(matchingGrams) > 0:\\r\\r                ngramDict[item] = matchingGrams[0]\\r\\r    return ngramDict\\r\\r\\r\\rdef ngram(dataset, num_cores=None):\\r    if os.environ.get(\\""NGRAM\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r        \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Generate Ngram clusters using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating Ngram clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # Prepare text data\\r    \\r            txt1 = [row['clean_text'] for index, row in dataset.iterrows() \\r    \\r                    if row['clean_text'] != '' and len(row['clean_text']) > 2]\\r    \\r           \\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r    \\r    \\r            # Getting trigrams \\r    \\r            count_vectorizer = CountVectorizer(ngram_range=(3,3))\\r    \\r            X1 = count_vectorizer.fit_transform(txt1) \\r    \\r            features = count_vectorizer.get_feature_names_out()\\r    \\r    \\r    \\r            # Applying TFIDF\\r    \\r            tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3))\\r    \\r            X2 = tfidf_vectorizer.fit_transform(txt1)\\r    \\r            \\r    \\r            # Getting top ranking features\\r    \\r            logger.info('Getting Top 50 Grams')\\r    \\r            sums = X2.sum(axis=0)\\r    \\r            data1 = []\\r    \\r            for col, term in enumerate(features):\\r    \\r                data1.append((term, sums[0,col]))\\r    \\r            \\r    \\r            ranking = pd.DataFrame(data1, columns=['term','rank'])\\r    \\r            words = ranking.sort_values('rank', ascending=False)\\r    \\r            top50df = words.nlargest(50,'rank')\\r    \\r            top50 = words['term'].tolist()\\r    \\r            \\r    \\r            # Get unique texts\\r    \\r            distinctText = list(set(txt1))\\r    \\r            \\r    \\r            # Split work across cores\\r    \\r            chunks = np.array_split(distinctText, num_cores)\\r    \\r            \\r    \\r            # Prepare partial function with top50 grams\\r    \\r            process_chunk_with_top50 = partial(process_ngram_chunk, top50=top50)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                ngram_results = pool.map(process_chunk_with_top50, chunks)\\r    \\r            \\r    \\r            # Combine results from all chunks\\r    \\r            ngramDict = {}\\r    \\r            for chunk_dict in ngram_results:\\r    \\r                ngramDict.update(chunk_dict)\\r    \\r            \\r    \\r            # Map ngrams to original dataset\\r    \\r            dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x, ''))\\r    \\r            \\r    \\r            logger.info('Ngram mapping completed')\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in Ngram')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            dataset['ngram'] = ''\\r\\r\\r    return dataset\\r""]},{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""WkoEz"",""alias"":""LDA"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""lda"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer\\r\\rfrom sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\rfrom functools import partial\\r\\rfrom datetime import datetime\\r\\r\\r\\rdef count_vectorize(dataset):\\r\\r    count_vectorizer = CountVectorizer(stop_words='english')\\r\\r    count_data = count_vectorizer.fit_transform(dataset['clean_text'].to_list())\\r\\r    words = count_vectorizer.get_feature_names_out()\\r\\r    return {'data': count_data, 'words': words, 'number': dataset['number'].to_list()}\\r\\r\\r\\rdef lda_process_group(group_data, clustercount_param):\\r\\r    number_topics = clustercount_param\\r\\r    count_data, words, number = group_data['data'], group_data['words'], group_data['number']\\r\\r    \\r\\r    lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\r    lda.fit(count_data)\\r\\r    documents = lda.transform(count_data)\\r\\r    \\r\\r    argmax_values = np.argmax(documents, axis=1)\\r\\r    \\r\\r    def wordsWithWeights(termIndices, termWeights, index):\\r\\r        terms = [words[i] for i in termIndices]\\r\\r        topic = [index]*len(terms)\\r\\r        return list(zip(terms, topic, termIndices, termWeights))\\r\\r    \\r\\r    topics = [wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) \\r\\r              for index, topic in enumerate(lda.components_)]\\r\\r    \\r\\r    topicWords = [item for topic in topics for item in topic]\\r\\r    finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\r    finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\r    finalTopic.columns = ['topic', 'word', 'weight']\\r\\r    \\r\\r    newdocuments = pd.DataFrame({'cluster_Id': argmax_values, 'number': number})\\r\\r    topicwordsdf = pd.DataFrame({'topic': range(number_topics), \\r\\r                                 'cluster_Name': [\\r\\r                                     list(finalTopic[finalTopic['topic'] == i]['word']) \\r\\r                                     for i in range(number_topics)\\r\\r                                 ]})\\r\\r    \\r\\r    newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\r    newdocuments = newdocuments.drop(columns='topic')\\r\\r    newdocuments['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r    \\r\\r    return newdocuments, finalTopic\\r\\r\\r\\rdef lda(dataset, clustercount_param=5, uniqueidcolumn_param=''):\\r    \\r    if os.environ.get(\\""lda\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r        try:\\r            logger.info(\\""lda started...\\"")\\r    \\r            dataset = pd.DataFrame(dataset)\\r    \\r            originaldataset = dataset\\r    \\r            \\r    \\r            # Group by field processing\\r    \\r            grouped = dataset.groupby('group_by_field')\\r    \\r            grouped_df = {name: group for name, group in grouped}\\r    \\r            \\r    \\r            # Prepare data for processing\\r    \\r            vectorized_data = {name: count_vectorize(group) for name, group in grouped_df.items()}\\r    \\r            \\r            num_cores = multiprocessing.cpu_count()\\r            # Parallel LDA processing\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                lda_func = partial(lda_process_group, clustercount_param=clustercount_param)\\r    \\r                results = pool.map(lda_func, vectorized_data.values())\\r    \\r            \\r    \\r            # Separate results\\r    \\r            newdocuments_result = []\\r    \\r            finalTopic_result = []\\r    \\r            for (newdocuments, finalTopic), (name, _) in zip(results, vectorized_data.items()):\\r    \\r                finalTopic['group_by_field'] = name\\r    \\r                newdocuments_result.append(newdocuments)\\r    \\r                finalTopic_result.append(finalTopic)\\r    \\r            \\r    \\r            # Combine results\\r    \\r            resultsdf = pd.concat(newdocuments_result, ignore_index=True)\\r    \\r            topicsdf = pd.concat(finalTopic_result, ignore_index=True)\\r    \\r            \\r    \\r            # Process topics\\r    \\r            topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({\\r    \\r                'word': list, \\r    \\r                'weight': list\\r    \\r            }).reset_index()\\r    \\r            topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r    \\r            topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(map(str, x)))\\r    \\r            topicsdf['alias'] = topicsdf['words']\\r    \\r            topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r    \\r            \\r    \\r            # Process results\\r    \\r            resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r    \\r            resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r    \\r            resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']]\\r    \\r            \\r    \\r            # Merge with original dataset\\r    \\r            dataset = pd.merge(originaldataset, resultsdf, on='number', how='left')\\r    \\r            logger.info(\\""Lda is done...\\"")\\r            return dataset\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in LDA')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise""]},""position_x"":""454"",""position_y"":""236"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""bWxAP"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""FZLGS"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""map_phrases"",""requirements"":"""",""params"":[],""script"":[""import numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rfrom functools import partial\\r\\rdef getSimilar(sentences, keywords):\\r    corpus = sentences + keywords\\r    keywordstartIndex = len(sentences)\\r    tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r    pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r    arr = pairwise_similarity.toarray()\\r    np.fill_diagonal(arr, np.nan)\\r    results = {}\\r    for s in sentences:\\r        input_idx = sentences.index(s)\\r        result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r        match = arr[input_idx][keywordstartIndex + result_idx]\\r        r = keywords[result_idx]\\r        results[s] = r + ':' + str(match) if match > 0 else 'NO MATCH:0'\\r    return results\\rdef process_chunk(chunk, keywords):\\r    results = getSimilar(chunk, keywords)\\r    mappings = {}\\r    for pattern, result in results.items():\\r        kw = result.split(':')[0]\\r        score = result.split(':')[-1]\\r        if kw != 'NO MATCH':\\r            mappings[pattern] = {'keyword': kw, 'score': score}\\r    return mappings\\rdef map_phrases(dataset, ease):\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        return pd.DataFrame(dataset)\\r    else:\\r        \\r        try:\\r            logger.info(\\""map_phrases started ...\\"")\\r            # Extract unique keywords and phrases\\r            keywords = list(set(item['Key_Word'] for item in ease))\\r            phrases = list(set(item['extracted_phrase'] for item in dataset))\\r            # Multiprocessing setup\\r            num_cores = multiprocessing.cpu_count()\\r            chunk_size = max(1, len(phrases) // num_cores)\\r            # Split phrases into chunks\\r            phrase_chunks = [phrases[i:i+chunk_size] for i in range(0, len(phrases), chunk_size)]\\r            # Partial function to pass keywords\\r            process_func = partial(process_chunk, keywords=keywords)\\r            # Parallel processing\\r            with multiprocessing.Pool(num_cores) as pool:\\r                chunk_results = pool.map(process_func, phrase_chunks)\\r            # Combine results\\r            mappings = {}\\r            for chunk_mapping in chunk_results:\\r                mappings.update(chunk_mapping)\\r            # Update dataset with mappings\\r            for row in dataset:\\r                phrase = row['extracted_phrase']\\r                if phrase in mappings:\\r                    row['mapped_phrase'] = mappings[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round(float(mappings[phrase]['score']), 4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r            logger.info(\\""map_phrases is done ...\\"")\\r            return dataset\\r        except Exception as ex:\\r            logger.error('Error in Map phrase')\\r            raise ex\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:55:16"",""alias"":""EASE Mapping"",""id"":288,""name"":""ACMESMPN85731"",""description"":"""",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:49:12"",""alias"":""EASE Mapping"",""id"":14,""name"":""ACMESMPN94605"",""description"":null,""schemavalue"":""[{\\""columntype\\"":\\""int\\"",\\""columnorder\\"":1,\\""recordcolumnname\\"":\\""ID\\"",\\""recordcolumndisplayname\\"":\\""ID\\"",\\""isunique\\"":true,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":2,\\""recordcolumnname\\"":\\""Key_Word\\"",\\""recordcolumndisplayname\\"":\\""Key_Word\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":3,\\""recordcolumnname\\"":\\""Category\\"",\\""recordcolumndisplayname\\"":\\""Category\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":4,\\""recordcolumnname\\"":\\""Ease\\"",\\""recordcolumndisplayname\\"":\\""Ease\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":5,\\""recordcolumnname\\"":\\""Support_Level\\"",\\""recordcolumndisplayname\\"":\\""Support_Level\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":6,\\""recordcolumnname\\"":\\""Ranks\\"",\\""recordcolumndisplayname\\"":\\""Ranks\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":7,\\""recordcolumnname\\"":\\""Business_Area\\"",\\""recordcolumndisplayname\\"":\\""Business_Area\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":8,\\""recordcolumnname\\"":\\""Typical_Resolution\\"",\\""recordcolumndisplayname\\"":\\""Typical_Resolution\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":9,\\""recordcolumnname\\"":\\""Business_Impact\\"",\\""recordcolumndisplayname\\"":\\""Business_Impact\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":10,\\""recordcolumnname\\"":\\""Account\\"",\\""recordcolumndisplayname\\"":\\""Account\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":11,\\""recordcolumnname\\"":\\""BotName\\"",\\""recordcolumndisplayname\\"":\\""BotName\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false}]"",""organization"":""leo1311""},""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from @projectname_EASEMapping\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_EASEMapping\\"",\\""uniqueIdentifier\\"":\\""ID\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null}},{""FunctionName"":""extract_phrases_parallel"",""requirements"":"""",""params"":[],""script"":[""\\r\\rimport logging\\rimport pytextrank\\rimport spacy\\r# from spacy.language import Language\\r\\rdef extract_phrases(dataset):\\r\\r    logger.info('Extracting phrases....')\\r\\r    try:\\r        dataset = dataset.to_dict('records')\\r        \\r        nlp = spacy.load('en_core_web_sm')\\r        \\r        nlp.add_pipe('textrank')\\r\\r        timenow = datetime.now()\\r\\r        totalRecords = len(dataset)\\r        \\r        count =0\\r\\r        textPhraseMappings = {}\\r        \\r\\r        for row in dataset:\\r\\r            try:\\r\\r                text = row['clean_text']\\r                \\r                if textPhraseMappings.get(text,'') != '':\\r\\r                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\r                    break\\r\\r                doc = nlp(text)\\r\\r                phrase =''\\r\\r                if len(doc._.phrases)>0:\\r\\r                    for item in doc._.phrases:\\r\\r                        withoutSpace = item.text.replace(' ' ,'')\\r\\r                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\r                            phrase = item.text\\r\\r                            break\\r\\r                if phrase != '':\\r\\r                    row['extracted_phrase'] = phrase\\r\\r                else:\\r\\r                    row['extracted_phrase'] = text\\r\\r            except Exception as ex:\\r\\r                logging.info(ex)\\r\\r                row['extracted_phrase'] = text\\r\\r    except Exception as ex:\\r\\r        logger.error('error in Extract Phrases')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info(\\""phrase Extraction is done...\\"")\\r\\r    # dataset = pd.DataFrame(dataset)\\r\\r    return dataset\\r\\r\\r\\r\\r\\rdef extract_phrases_parallel(dataset, num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Extract phrases from ticket descriptions using parallel processing\\r\\r    \\""\\""\\""\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        df = pd.DataFrame(dataset)\\r        return df\\r    \\r    else:\\r        start_time = datetime.now()\\r    \\r        \\r    \\r        # Convert to DataFrame if not already\\r    \\r        df = pd.DataFrame(dataset)\\r    \\r        total_records = len(df)\\r    \\r        \\r    \\r        if num_cores is None:\\r    \\r            num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r        \\r    \\r        # Split the dataframe into chunks\\r    \\r        chunks = np.array_split(df, num_cores)\\r    \\r        \\r    \\r        # Process chunks in parallel using ProcessPoolExecutor\\r    \\r        processed_chunks = []\\r    \\r        exctract_phrases_chunk = partial(extract_phrases)\\r    \\r        with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r            # Submit all chunks for processing\\r            futures = [executor.submit(exctract_phrases_chunk, chunk) for chunk in chunks]\\r    \\r            # Collect results as they complete\\r    \\r            for future in concurrent.futures.as_completed(futures):\\r    \\r                try:\\r    \\r                    result = future.result()\\r    \\r                    if result is not None and len(result) > 0:\\r    \\r                        processed_chunks.append(result)\\r    \\r                except Exception as e:\\r    \\r                    logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r    \\r                \\r        if not processed_chunks:\\r    \\r            logger.warning(\\""No chunks were successfully processed\\"")\\r    \\r            return pd.DataFrame()\\r    \\r        # Combine all processed chunks\\r    \\r        final_df = pd.concat([pd.DataFrame(chunk) for chunk in processed_chunks], ignore_index=True)\\r    \\r        # Print processing summary\\r        end_time = datetime.now()\\r        final_results = final_df.to_dict('records')\\r        processing_time = end_time - start_time\\r    \\r        print(f'Processing Summary:')\\r        print(f'Total time required: {processing_time}')\\r    \\r        return final_results""]},{""FunctionName"":""ngram"",""requirements"":"""",""params"":[],""script"":[""import sklearn\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef process_ngram_chunk(chunk, top50):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of data to map text to top ngrams\\r\\r    \\""\\""\\""\\r\\r    ngramDict = {}\\r\\r    for item in chunk:\\r\\r        if item != '' and len(item) > 2:\\r\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r\\r            if len(matchingGrams) > 0:\\r\\r                ngramDict[item] = matchingGrams[0]\\r\\r    return ngramDict\\r\\r\\r\\rdef ngram(dataset, num_cores=None):\\r    if os.environ.get(\\""NGRAM\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r        \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Generate Ngram clusters using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating Ngram clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # Prepare text data\\r    \\r            txt1 = [row['clean_text'] for index, row in dataset.iterrows() \\r    \\r                    if row['clean_text'] != '' and len(row['clean_text']) > 2]\\r    \\r           \\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r    \\r    \\r            # Getting trigrams \\r    \\r            count_vectorizer = CountVectorizer(ngram_range=(3,3))\\r    \\r            X1 = count_vectorizer.fit_transform(txt1) \\r    \\r            features = count_vectorizer.get_feature_names_out()\\r    \\r    \\r    \\r            # Applying TFIDF\\r    \\r            tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3))\\r    \\r            X2 = tfidf_vectorizer.fit_transform(txt1)\\r    \\r            \\r    \\r            # Getting top ranking features\\r    \\r            logger.info('Getting Top 50 Grams')\\r    \\r            sums = X2.sum(axis=0)\\r    \\r            data1 = []\\r    \\r            for col, term in enumerate(features):\\r    \\r                data1.append((term, sums[0,col]))\\r    \\r            \\r    \\r            ranking = pd.DataFrame(data1, columns=['term','rank'])\\r    \\r            words = ranking.sort_values('rank', ascending=False)\\r    \\r            top50df = words.nlargest(50,'rank')\\r    \\r            top50 = words['term'].tolist()\\r    \\r            \\r    \\r            # Get unique texts\\r    \\r            distinctText = list(set(txt1))\\r    \\r            \\r    \\r            # Split work across cores\\r    \\r            chunks = np.array_split(distinctText, num_cores)\\r    \\r            \\r    \\r            # Prepare partial function with top50 grams\\r    \\r            process_chunk_with_top50 = partial(process_ngram_chunk, top50=top50)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                ngram_results = pool.map(process_chunk_with_top50, chunks)\\r    \\r            \\r    \\r            # Combine results from all chunks\\r    \\r            ngramDict = {}\\r    \\r            for chunk_dict in ngram_results:\\r    \\r                ngramDict.update(chunk_dict)\\r    \\r            \\r    \\r            # Map ngrams to original dataset\\r    \\r            dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x, ''))\\r    \\r            \\r    \\r            logger.info('Ngram mapping completed')\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in Ngram')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            dataset['ngram'] = ''\\r\\r\\r    return dataset\\r""]},{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""FZLGS"",""alias"":""Prioritization"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""prioritization"",""requirements"":"""",""params"":[],""script"":[""from concurrent.futures import ProcessPoolExecutor\\r\\rfrom datetime import datetime\\r\\rdef prioritization(dataset):\\r\\r    def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r\\r    # def getCluster(ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r\\r        cluster = ''\\r\\r        if tags is not None and tags != '':\\r\\r            cluster = tags\\r\\r        elif ngram is not None and ngram != '':\\r\\r            cluster = ngram\\r\\r        elif soundex_cluster is not None and soundex_cluster != '':\\r\\r            cluster = soundex_cluster\\r\\r        elif mapped_phrase is not None and mapped_phrase != '':\\r\\r            cluster = mapped_phrase\\r\\r        elif lda_cluster is not None and lda_cluster != '':\\r\\r            cluster = lda_cluster\\r\\r        return cluster\\r\\r    logger.info('Prioritizing cluster for tickets..')\\r\\r    try:\\r\\r        dataset = dataset.replace(np.nan,'')\\r\\r        dataset = dataset.to_dict('records')\\r\\r        \\r\\r        for row in dataset:\\r\\r            row['post_ranking_cluster'] = getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r\\r            # row['post_ranking_cluster'] = getCluster(row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r\\r\\r\\r        dataset = pd.DataFrame(dataset)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        dataset = dataset[['number','clean_text','ngram','soundex_cluster','lda_cluster','extracted_phrase','mapped_phrase','mapped_phrase_confidennce','last_updated','post_ranking_cluster']]\\r\\r        dataset = dataset.to_dict('records')\\r\\r        logger.info('Total tickets after clustering {0}'.format(len(dataset)))\\r\\r    except Exception as ex:\\r\\r        logger.error('error in cluster Prioritizing')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info('Saving Data')\\r\\r    end_time = datetime.now()\\r\\r    \\r\\r    ended_time = datetime.now()\\r\\r    Total_time = ended_time - started_time\\r\\r    print(Total_time)\\r\\r    return dataset""]},""position_x"":""660"",""position_y"":""234"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""WkoEz"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""yeSbs"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""lda"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer\\r\\rfrom sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\rfrom functools import partial\\r\\rfrom datetime import datetime\\r\\r\\r\\rdef count_vectorize(dataset):\\r\\r    count_vectorizer = CountVectorizer(stop_words='english')\\r\\r    count_data = count_vectorizer.fit_transform(dataset['clean_text'].to_list())\\r\\r    words = count_vectorizer.get_feature_names_out()\\r\\r    return {'data': count_data, 'words': words, 'number': dataset['number'].to_list()}\\r\\r\\r\\rdef lda_process_group(group_data, clustercount_param):\\r\\r    number_topics = clustercount_param\\r\\r    count_data, words, number = group_data['data'], group_data['words'], group_data['number']\\r\\r    \\r\\r    lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\r    lda.fit(count_data)\\r\\r    documents = lda.transform(count_data)\\r\\r    \\r\\r    argmax_values = np.argmax(documents, axis=1)\\r\\r    \\r\\r    def wordsWithWeights(termIndices, termWeights, index):\\r\\r        terms = [words[i] for i in termIndices]\\r\\r        topic = [index]*len(terms)\\r\\r        return list(zip(terms, topic, termIndices, termWeights))\\r\\r    \\r\\r    topics = [wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) \\r\\r              for index, topic in enumerate(lda.components_)]\\r\\r    \\r\\r    topicWords = [item for topic in topics for item in topic]\\r\\r    finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\r    finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\r    finalTopic.columns = ['topic', 'word', 'weight']\\r\\r    \\r\\r    newdocuments = pd.DataFrame({'cluster_Id': argmax_values, 'number': number})\\r\\r    topicwordsdf = pd.DataFrame({'topic': range(number_topics), \\r\\r                                 'cluster_Name': [\\r\\r                                     list(finalTopic[finalTopic['topic'] == i]['word']) \\r\\r                                     for i in range(number_topics)\\r\\r                                 ]})\\r\\r    \\r\\r    newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\r    newdocuments = newdocuments.drop(columns='topic')\\r\\r    newdocuments['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r    \\r\\r    return newdocuments, finalTopic\\r\\r\\r\\rdef lda(dataset, clustercount_param=5, uniqueidcolumn_param=''):\\r    \\r    if os.environ.get(\\""lda\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r        try:\\r            logger.info(\\""lda started...\\"")\\r    \\r            dataset = pd.DataFrame(dataset)\\r    \\r            originaldataset = dataset\\r    \\r            \\r    \\r            # Group by field processing\\r    \\r            grouped = dataset.groupby('group_by_field')\\r    \\r            grouped_df = {name: group for name, group in grouped}\\r    \\r            \\r    \\r            # Prepare data for processing\\r    \\r            vectorized_data = {name: count_vectorize(group) for name, group in grouped_df.items()}\\r    \\r            \\r            num_cores = multiprocessing.cpu_count()\\r            # Parallel LDA processing\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                lda_func = partial(lda_process_group, clustercount_param=clustercount_param)\\r    \\r                results = pool.map(lda_func, vectorized_data.values())\\r    \\r            \\r    \\r            # Separate results\\r    \\r            newdocuments_result = []\\r    \\r            finalTopic_result = []\\r    \\r            for (newdocuments, finalTopic), (name, _) in zip(results, vectorized_data.items()):\\r    \\r                finalTopic['group_by_field'] = name\\r    \\r                newdocuments_result.append(newdocuments)\\r    \\r                finalTopic_result.append(finalTopic)\\r    \\r            \\r    \\r            # Combine results\\r    \\r            resultsdf = pd.concat(newdocuments_result, ignore_index=True)\\r    \\r            topicsdf = pd.concat(finalTopic_result, ignore_index=True)\\r    \\r            \\r    \\r            # Process topics\\r    \\r            topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({\\r    \\r                'word': list, \\r    \\r                'weight': list\\r    \\r            }).reset_index()\\r    \\r            topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r    \\r            topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(map(str, x)))\\r    \\r            topicsdf['alias'] = topicsdf['words']\\r    \\r            topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r    \\r            \\r    \\r            # Process results\\r    \\r            resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r    \\r            resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r    \\r            resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']]\\r    \\r            \\r    \\r            # Merge with original dataset\\r    \\r            dataset = pd.merge(originaldataset, resultsdf, on='number', how='left')\\r    \\r            logger.info(\\""Lda is done...\\"")\\r            return dataset\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in LDA')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise""]},{""FunctionName"":""map_phrases"",""requirements"":"""",""params"":[],""script"":[""import numpy as np\\r\\rimport multiprocessing\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rfrom functools import partial\\r\\rdef getSimilar(sentences, keywords):\\r    corpus = sentences + keywords\\r    keywordstartIndex = len(sentences)\\r    tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r    pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r    arr = pairwise_similarity.toarray()\\r    np.fill_diagonal(arr, np.nan)\\r    results = {}\\r    for s in sentences:\\r        input_idx = sentences.index(s)\\r        result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r        match = arr[input_idx][keywordstartIndex + result_idx]\\r        r = keywords[result_idx]\\r        results[s] = r + ':' + str(match) if match > 0 else 'NO MATCH:0'\\r    return results\\rdef process_chunk(chunk, keywords):\\r    results = getSimilar(chunk, keywords)\\r    mappings = {}\\r    for pattern, result in results.items():\\r        kw = result.split(':')[0]\\r        score = result.split(':')[-1]\\r        if kw != 'NO MATCH':\\r            mappings[pattern] = {'keyword': kw, 'score': score}\\r    return mappings\\rdef map_phrases(dataset, ease):\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        return pd.DataFrame(dataset)\\r    else:\\r        \\r        try:\\r            logger.info(\\""map_phrases started ...\\"")\\r            # Extract unique keywords and phrases\\r            keywords = list(set(item['Key_Word'] for item in ease))\\r            phrases = list(set(item['extracted_phrase'] for item in dataset))\\r            # Multiprocessing setup\\r            num_cores = multiprocessing.cpu_count()\\r            chunk_size = max(1, len(phrases) // num_cores)\\r            # Split phrases into chunks\\r            phrase_chunks = [phrases[i:i+chunk_size] for i in range(0, len(phrases), chunk_size)]\\r            # Partial function to pass keywords\\r            process_func = partial(process_chunk, keywords=keywords)\\r            # Parallel processing\\r            with multiprocessing.Pool(num_cores) as pool:\\r                chunk_results = pool.map(process_func, phrase_chunks)\\r            # Combine results\\r            mappings = {}\\r            for chunk_mapping in chunk_results:\\r                mappings.update(chunk_mapping)\\r            # Update dataset with mappings\\r            for row in dataset:\\r                phrase = row['extracted_phrase']\\r                if phrase in mappings:\\r                    row['mapped_phrase'] = mappings[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round(float(mappings[phrase]['score']), 4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r            logger.info(\\""map_phrases is done ...\\"")\\r            return dataset\\r        except Exception as ex:\\r            logger.error('Error in Map phrase')\\r            raise ex\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:55:16"",""alias"":""EASE Mapping"",""id"":288,""name"":""ACMESMPN85731"",""description"":"""",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 11:49:12"",""alias"":""EASE Mapping"",""id"":14,""name"":""ACMESMPN94605"",""description"":null,""schemavalue"":""[{\\""columntype\\"":\\""int\\"",\\""columnorder\\"":1,\\""recordcolumnname\\"":\\""ID\\"",\\""recordcolumndisplayname\\"":\\""ID\\"",\\""isunique\\"":true,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":2,\\""recordcolumnname\\"":\\""Key_Word\\"",\\""recordcolumndisplayname\\"":\\""Key_Word\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":3,\\""recordcolumnname\\"":\\""Category\\"",\\""recordcolumndisplayname\\"":\\""Category\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":4,\\""recordcolumnname\\"":\\""Ease\\"",\\""recordcolumndisplayname\\"":\\""Ease\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":5,\\""recordcolumnname\\"":\\""Support_Level\\"",\\""recordcolumndisplayname\\"":\\""Support_Level\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":6,\\""recordcolumnname\\"":\\""Ranks\\"",\\""recordcolumndisplayname\\"":\\""Ranks\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":true},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":7,\\""recordcolumnname\\"":\\""Business_Area\\"",\\""recordcolumndisplayname\\"":\\""Business_Area\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":8,\\""recordcolumnname\\"":\\""Typical_Resolution\\"",\\""recordcolumndisplayname\\"":\\""Typical_Resolution\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""text\\"",\\""columnorder\\"":9,\\""recordcolumnname\\"":\\""Business_Impact\\"",\\""recordcolumndisplayname\\"":\\""Business_Impact\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":10,\\""recordcolumnname\\"":\\""Account\\"",\\""recordcolumndisplayname\\"":\\""Account\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false},{\\""columntype\\"":\\""varchar\\"",\\""columnorder\\"":11,\\""recordcolumnname\\"":\\""BotName\\"",\\""recordcolumndisplayname\\"":\\""BotName\\"",\\""isprimarykey\\"":false,\\""isunique\\"":false,\\""isrequired\\"":false}]"",""organization"":""leo1311""},""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from @projectname_EASEMapping\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_EASEMapping\\"",\\""uniqueIdentifier\\"":\\""ID\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null}},{""FunctionName"":""extract_phrases_parallel"",""requirements"":"""",""params"":[],""script"":[""\\r\\rimport logging\\rimport pytextrank\\rimport spacy\\r# from spacy.language import Language\\r\\rdef extract_phrases(dataset):\\r\\r    logger.info('Extracting phrases....')\\r\\r    try:\\r        dataset = dataset.to_dict('records')\\r        \\r        nlp = spacy.load('en_core_web_sm')\\r        \\r        nlp.add_pipe('textrank')\\r\\r        timenow = datetime.now()\\r\\r        totalRecords = len(dataset)\\r        \\r        count =0\\r\\r        textPhraseMappings = {}\\r        \\r\\r        for row in dataset:\\r\\r            try:\\r\\r                text = row['clean_text']\\r                \\r                if textPhraseMappings.get(text,'') != '':\\r\\r                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\r                    break\\r\\r                doc = nlp(text)\\r\\r                phrase =''\\r\\r                if len(doc._.phrases)>0:\\r\\r                    for item in doc._.phrases:\\r\\r                        withoutSpace = item.text.replace(' ' ,'')\\r\\r                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\r                            phrase = item.text\\r\\r                            break\\r\\r                if phrase != '':\\r\\r                    row['extracted_phrase'] = phrase\\r\\r                else:\\r\\r                    row['extracted_phrase'] = text\\r\\r            except Exception as ex:\\r\\r                logging.info(ex)\\r\\r                row['extracted_phrase'] = text\\r\\r    except Exception as ex:\\r\\r        logger.error('error in Extract Phrases')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info(\\""phrase Extraction is done...\\"")\\r\\r    # dataset = pd.DataFrame(dataset)\\r\\r    return dataset\\r\\r\\r\\r\\r\\rdef extract_phrases_parallel(dataset, num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Extract phrases from ticket descriptions using parallel processing\\r\\r    \\""\\""\\""\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        df = pd.DataFrame(dataset)\\r        return df\\r    \\r    else:\\r        start_time = datetime.now()\\r    \\r        \\r    \\r        # Convert to DataFrame if not already\\r    \\r        df = pd.DataFrame(dataset)\\r    \\r        total_records = len(df)\\r    \\r        \\r    \\r        if num_cores is None:\\r    \\r            num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r        \\r    \\r        # Split the dataframe into chunks\\r    \\r        chunks = np.array_split(df, num_cores)\\r    \\r        \\r    \\r        # Process chunks in parallel using ProcessPoolExecutor\\r    \\r        processed_chunks = []\\r    \\r        exctract_phrases_chunk = partial(extract_phrases)\\r    \\r        with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r            # Submit all chunks for processing\\r            futures = [executor.submit(exctract_phrases_chunk, chunk) for chunk in chunks]\\r    \\r            # Collect results as they complete\\r    \\r            for future in concurrent.futures.as_completed(futures):\\r    \\r                try:\\r    \\r                    result = future.result()\\r    \\r                    if result is not None and len(result) > 0:\\r    \\r                        processed_chunks.append(result)\\r    \\r                except Exception as e:\\r    \\r                    logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r    \\r                \\r        if not processed_chunks:\\r    \\r            logger.warning(\\""No chunks were successfully processed\\"")\\r    \\r            return pd.DataFrame()\\r    \\r        # Combine all processed chunks\\r    \\r        final_df = pd.concat([pd.DataFrame(chunk) for chunk in processed_chunks], ignore_index=True)\\r    \\r        # Print processing summary\\r        end_time = datetime.now()\\r        final_results = final_df.to_dict('records')\\r        processing_time = end_time - start_time\\r    \\r        print(f'Processing Summary:')\\r        print(f'Total time required: {processing_time}')\\r    \\r        return final_results""]},{""FunctionName"":""ngram"",""requirements"":"""",""params"":[],""script"":[""import sklearn\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef process_ngram_chunk(chunk, top50):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of data to map text to top ngrams\\r\\r    \\""\\""\\""\\r\\r    ngramDict = {}\\r\\r    for item in chunk:\\r\\r        if item != '' and len(item) > 2:\\r\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r\\r            if len(matchingGrams) > 0:\\r\\r                ngramDict[item] = matchingGrams[0]\\r\\r    return ngramDict\\r\\r\\r\\rdef ngram(dataset, num_cores=None):\\r    if os.environ.get(\\""NGRAM\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r        \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Generate Ngram clusters using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating Ngram clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # Prepare text data\\r    \\r            txt1 = [row['clean_text'] for index, row in dataset.iterrows() \\r    \\r                    if row['clean_text'] != '' and len(row['clean_text']) > 2]\\r    \\r           \\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r    \\r    \\r            # Getting trigrams \\r    \\r            count_vectorizer = CountVectorizer(ngram_range=(3,3))\\r    \\r            X1 = count_vectorizer.fit_transform(txt1) \\r    \\r            features = count_vectorizer.get_feature_names_out()\\r    \\r    \\r    \\r            # Applying TFIDF\\r    \\r            tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3))\\r    \\r            X2 = tfidf_vectorizer.fit_transform(txt1)\\r    \\r            \\r    \\r            # Getting top ranking features\\r    \\r            logger.info('Getting Top 50 Grams')\\r    \\r            sums = X2.sum(axis=0)\\r    \\r            data1 = []\\r    \\r            for col, term in enumerate(features):\\r    \\r                data1.append((term, sums[0,col]))\\r    \\r            \\r    \\r            ranking = pd.DataFrame(data1, columns=['term','rank'])\\r    \\r            words = ranking.sort_values('rank', ascending=False)\\r    \\r            top50df = words.nlargest(50,'rank')\\r    \\r            top50 = words['term'].tolist()\\r    \\r            \\r    \\r            # Get unique texts\\r    \\r            distinctText = list(set(txt1))\\r    \\r            \\r    \\r            # Split work across cores\\r    \\r            chunks = np.array_split(distinctText, num_cores)\\r    \\r            \\r    \\r            # Prepare partial function with top50 grams\\r    \\r            process_chunk_with_top50 = partial(process_ngram_chunk, top50=top50)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                ngram_results = pool.map(process_chunk_with_top50, chunks)\\r    \\r            \\r    \\r            # Combine results from all chunks\\r    \\r            ngramDict = {}\\r    \\r            for chunk_dict in ngram_results:\\r    \\r                ngramDict.update(chunk_dict)\\r    \\r            \\r    \\r            # Map ngrams to original dataset\\r    \\r            dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x, ''))\\r    \\r            \\r    \\r            logger.info('Ngram mapping completed')\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in Ngram')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            dataset['ngram'] = ''\\r\\r\\r    return dataset\\r""]},{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""emiga"",""alias"":""Filter data"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},""position_x"":""238"",""position_y"":""4"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""jtTZa"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""CNyte"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""XjOcS"",""alias"":""Extract Phrases"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""extract_phrases_parallel"",""requirements"":"""",""params"":[],""script"":[""\\r\\rimport logging\\rimport pytextrank\\rimport spacy\\r# from spacy.language import Language\\r\\rdef extract_phrases(dataset):\\r\\r    logger.info('Extracting phrases....')\\r\\r    try:\\r        dataset = dataset.to_dict('records')\\r        \\r        nlp = spacy.load('en_core_web_sm')\\r        \\r        nlp.add_pipe('textrank')\\r\\r        timenow = datetime.now()\\r\\r        totalRecords = len(dataset)\\r        \\r        count =0\\r\\r        textPhraseMappings = {}\\r        \\r\\r        for row in dataset:\\r\\r            try:\\r\\r                text = row['clean_text']\\r                \\r                if textPhraseMappings.get(text,'') != '':\\r\\r                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\r                    break\\r\\r                doc = nlp(text)\\r\\r                phrase =''\\r\\r                if len(doc._.phrases)>0:\\r\\r                    for item in doc._.phrases:\\r\\r                        withoutSpace = item.text.replace(' ' ,'')\\r\\r                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\r                            phrase = item.text\\r\\r                            break\\r\\r                if phrase != '':\\r\\r                    row['extracted_phrase'] = phrase\\r\\r                else:\\r\\r                    row['extracted_phrase'] = text\\r\\r            except Exception as ex:\\r\\r                logging.info(ex)\\r\\r                row['extracted_phrase'] = text\\r\\r    except Exception as ex:\\r\\r        logger.error('error in Extract Phrases')\\r\\r        logger.error(traceback.format_exc())\\r\\r        exit()\\r\\r    logger.info(\\""phrase Extraction is done...\\"")\\r\\r    # dataset = pd.DataFrame(dataset)\\r\\r    return dataset\\r\\r\\r\\r\\r\\rdef extract_phrases_parallel(dataset, num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Extract phrases from ticket descriptions using parallel processing\\r\\r    \\""\\""\\""\\r    if os.environ.get(\\""EASE\\"",\\""\\"") ==\\""False\\"":\\r        df = pd.DataFrame(dataset)\\r        return df\\r    \\r    else:\\r        start_time = datetime.now()\\r    \\r        \\r    \\r        # Convert to DataFrame if not already\\r    \\r        df = pd.DataFrame(dataset)\\r    \\r        total_records = len(df)\\r    \\r        \\r    \\r        if num_cores is None:\\r    \\r            num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r        \\r    \\r        # Split the dataframe into chunks\\r    \\r        chunks = np.array_split(df, num_cores)\\r    \\r        \\r    \\r        # Process chunks in parallel using ProcessPoolExecutor\\r    \\r        processed_chunks = []\\r    \\r        exctract_phrases_chunk = partial(extract_phrases)\\r    \\r        with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r            # Submit all chunks for processing\\r            futures = [executor.submit(exctract_phrases_chunk, chunk) for chunk in chunks]\\r    \\r            # Collect results as they complete\\r    \\r            for future in concurrent.futures.as_completed(futures):\\r    \\r                try:\\r    \\r                    result = future.result()\\r    \\r                    if result is not None and len(result) > 0:\\r    \\r                        processed_chunks.append(result)\\r    \\r                except Exception as e:\\r    \\r                    logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r    \\r                \\r        if not processed_chunks:\\r    \\r            logger.warning(\\""No chunks were successfully processed\\"")\\r    \\r            return pd.DataFrame()\\r    \\r        # Combine all processed chunks\\r    \\r        final_df = pd.concat([pd.DataFrame(chunk) for chunk in processed_chunks], ignore_index=True)\\r    \\r        # Print processing summary\\r        end_time = datetime.now()\\r        final_results = final_df.to_dict('records')\\r        processing_time = end_time - start_time\\r    \\r        print(f'Processing Summary:')\\r        print(f'Total time required: {processing_time}')\\r    \\r        return final_results""]},""position_x"":""256"",""position_y"":""106"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""HspNm"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""bWxAP"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""ngram"",""requirements"":"""",""params"":[],""script"":[""import sklearn\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport pandas as pd\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef process_ngram_chunk(chunk, top50):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of data to map text to top ngrams\\r\\r    \\""\\""\\""\\r\\r    ngramDict = {}\\r\\r    for item in chunk:\\r\\r        if item != '' and len(item) > 2:\\r\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r\\r            if len(matchingGrams) > 0:\\r\\r                ngramDict[item] = matchingGrams[0]\\r\\r    return ngramDict\\r\\r\\r\\rdef ngram(dataset, num_cores=None):\\r    if os.environ.get(\\""NGRAM\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r        \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Generate Ngram clusters using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating Ngram clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # Prepare text data\\r    \\r            txt1 = [row['clean_text'] for index, row in dataset.iterrows() \\r    \\r                    if row['clean_text'] != '' and len(row['clean_text']) > 2]\\r    \\r           \\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r    \\r    \\r            # Getting trigrams \\r    \\r            count_vectorizer = CountVectorizer(ngram_range=(3,3))\\r    \\r            X1 = count_vectorizer.fit_transform(txt1) \\r    \\r            features = count_vectorizer.get_feature_names_out()\\r    \\r    \\r    \\r            # Applying TFIDF\\r    \\r            tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3))\\r    \\r            X2 = tfidf_vectorizer.fit_transform(txt1)\\r    \\r            \\r    \\r            # Getting top ranking features\\r    \\r            logger.info('Getting Top 50 Grams')\\r    \\r            sums = X2.sum(axis=0)\\r    \\r            data1 = []\\r    \\r            for col, term in enumerate(features):\\r    \\r                data1.append((term, sums[0,col]))\\r    \\r            \\r    \\r            ranking = pd.DataFrame(data1, columns=['term','rank'])\\r    \\r            words = ranking.sort_values('rank', ascending=False)\\r    \\r            top50df = words.nlargest(50,'rank')\\r    \\r            top50 = words['term'].tolist()\\r    \\r            \\r    \\r            # Get unique texts\\r    \\r            distinctText = list(set(txt1))\\r    \\r            \\r    \\r            # Split work across cores\\r    \\r            chunks = np.array_split(distinctText, num_cores)\\r    \\r            \\r    \\r            # Prepare partial function with top50 grams\\r    \\r            process_chunk_with_top50 = partial(process_ngram_chunk, top50=top50)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                ngram_results = pool.map(process_chunk_with_top50, chunks)\\r    \\r            \\r    \\r            # Combine results from all chunks\\r    \\r            ngramDict = {}\\r    \\r            for chunk_dict in ngram_results:\\r    \\r                ngramDict.update(chunk_dict)\\r    \\r            \\r    \\r            # Map ngrams to original dataset\\r    \\r            dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x, ''))\\r    \\r            \\r    \\r            logger.info('Ngram mapping completed')\\r    \\r        \\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in Ngram')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            dataset['ngram'] = ''\\r\\r\\r    return dataset\\r""]},{""FunctionName"":""soundex"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport multiprocessing\\r\\rfrom functools import partial\\r\\r\\r\\rdef soundex_generator(token):\\r\\r    if token == '':\\r\\r        return ''\\r\\r    # Convert the word to upper \\r\\r    # case for uniformity\\r\\r    token = token.upper()\\r\\r    soundex = ''\\r\\r    # Retain the First Letter\\r\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r\\r                  'DT': '3',\\r\\r                  'L': '4', 'MN': '5', 'R': '6',\\r\\r                  'AEIOUHWY': '.'}\\r\\r \\r\\r    # Encode as per the dictionary\\r\\r    for char in token[1:]:\\r\\r        for key in dictionary.keys():\\r\\r            if char in key:\\r\\r                code = dictionary[key]\\r\\r                if code != '.':\\r\\r                    if code != soundex[-1]:\\r\\r                        soundex += code\\r\\r \\r\\r    return soundex\\r\\r\\r\\rdef process_soundex_chunk(chunk):\\r\\r    \\""\\""\\""\\r\\r    Process a chunk of the dataset for soundex generation\\r\\r    \\""\\""\\""\\r\\r    chunk['sound'] = chunk['clean_text'].apply(soundex_generator)\\r\\r    return chunk\\r\\r\\r\\rdef soundex(dataset, num_cores=None):\\r    \\r    if os.environ.get(\\""SOUNDEX\\"",\\""\\"") ==\\""False\\"":\\r        pass;\\r    \\r    else:\\r        \\r\\r        \\""\\""\\""\\r    \\r        Apply soundex processing using multiprocessing\\r    \\r        \\""\\""\\""\\r    \\r        logger.info('Generating soundex clusters...')\\r    \\r        dataset=pd.DataFrame(dataset)\\r    \\r        try:\\r    \\r            # If num_cores not specified, use all available cores\\r    \\r            if num_cores is None:\\r    \\r                num_cores = max(1, multiprocessing.cpu_count() - 1)\\r    \\r            \\r    \\r            logger.info(f'Using {num_cores} cores for processing')\\r    \\r            \\r    \\r            # Split the dataset into chunks\\r    \\r            chunks = np.array_split(dataset, num_cores)\\r    \\r            \\r    \\r            # Use multiprocessing to process chunks\\r    \\r            with multiprocessing.Pool(num_cores) as pool:\\r    \\r                processed_chunks = pool.map(process_soundex_chunk, chunks)\\r    \\r            \\r    \\r            # Combine processed chunks\\r    \\r            dataset = pd.concat(processed_chunks, ignore_index=True)\\r    \\r            \\r    \\r            # Generate sound clusters\\r    \\r            sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    \\r                numberList = pd.NamedAgg(column='number', aggfunc=list),\\r    \\r                textList = pd.NamedAgg(column='clean_text', aggfunc=list)\\r    \\r            ).reset_index()\\r    \\r            \\r    \\r            sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    \\r            sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x: len(x) >= 5)]\\r    \\r            \\r    \\r            sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    \\r            sound_Df = sound_Df.drop(columns=['textList'])\\r    \\r            \\r    \\r            sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    \\r            sound_Df = sound_Df.rename(columns={'numberList': 'number', 'cluster': 'soundex_cluster'})\\r    \\r            \\r    \\r            # Merge back to original dataset\\r    \\r            dataset = pd.merge(dataset, sound_Df, on=['number', 'group_by_field', 'sound'], how='left')\\r    \\r            \\r    \\r            logger.info(f'Total tickets after soundex {len(dataset.index)}')\\r    \\r        except Exception as ex:\\r    \\r            logger.error('Error in soundex')\\r    \\r            logger.error(traceback.format_exc())\\r    \\r            raise  # Re-raise the exception instead of exiting\\r\\r    \\r\\r    return dataset\\r\\r""]},{""FunctionName"":""clean_tickets"",""requirements"":""langid"",""params"":[{""name"":""custom_stopwords"",""value"":""test,id"",""type"":""Text"",""alias"":""test,id"",""index"":""1""}],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\rfrom translatepy import Translator\\rimport langid\\r\\r\\r\\rdef alphaNum(text: str) -> str:   \\r\\r    try:\\r\\r        if pd.isna(text):\\r\\r            return ''\\r\\r        alphanumeric = ''\\r\\r        for character in str(text):\\r\\r            if character.isalnum():            \\r\\r                alphanumeric += character        \\r\\r            else:            \\r\\r                alphanumeric += ' '    \\r\\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\\r        return ' '.join(finalTokens)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in alphaNum for text: {text}\\"")\\r\\r        return ''\\r\\r\\r\\rdef stopword_remover(tokens: str, custom_stopwords_param: str = '') -> str:\\r\\r    try:\\r\\r        if pd.isna(tokens) or tokens == '':\\r\\r            return ''\\r\\r        custom_stopwords_param = custom_stopwords_param.split(',')\\r\\r        stopwords_nltk = set(stopwords.words('english'))\\r\\r        stop_words = stopwords_nltk.union(custom_stopwords_param)\\r\\r        word = word_tokenize(str(tokens))\\r\\r        words = [token for token in word if token.lower() not in stop_words]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error in stopword_remover for tokens: {tokens}\\"")\\r\\r        return ''\\r\\r\\r\\rdef lematize(text: str) -> str:\\r\\r    try:\\r\\r        if pd.isna(text) or text == '':\\r\\r            return ''\\r\\r        w_tokenizer = word_tokenize(str(text))\\r\\r        lemmatizer = nltk.stem.WordNetLemmatizer()\\r\\r        words = [lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r\\r        return ' '.join(words)\\r\\r    except Exception as e:\\r\\r        print(f\\""Error: {str(e)}\\"")\\r\\r        return ''\\r\\r\\r\\rdef tokenize(dataset):\\r\\r    try:\\r\\r        dataset = dataset.copy()\\r\\r        group_by_field_values = dataset['group_by_field'].copy()\\r\\r        \\r\\r        grouped = dataset.groupby('group_by_field')\\r\\r        grouped_df = {}\\r\\r        \\r\\r        for name, group in grouped:\\r\\r            group = group.copy()\\r\\r            group['tokens'] = group['clean_text'].apply(lambda input: word_tokenize(input))\\r\\r            grouped_df[name] = group\\r\\r            \\r\\r        if grouped_df:\\r\\r            dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\r            \\r\\r            if 'group_by_field' not in dataset.columns:\\r\\r                dataset['group_by_field'] = group_by_field_values\\r\\r                \\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('error in tokenizer')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r\\r\\rdef process_group(group_data: pd.DataFrame, custom_stopwords_param: str = '') -> pd.DataFrame:\\r\\r    try:\\r\\r        if 'group_by_field' not in group_data.columns:\\r\\r            raise ValueError(f\\""Input data missing group_by_field column. Available columns: {group_data.columns}\\"")\\r\\r            \\r\\r        dataset = group_data.copy()\\r            \\r\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover, custom_stopwords_param=custom_stopwords_param)\\r\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r\\r        dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\r        \\r\\r        tokenize_df = tokenize(dataset)\\r\\r        \\r\\r        final_result = pd.DataFrame.from_dict(tokenize_df)\\r\\r                \\r        return final_result[['number', 'clean_text', 'last_updated', 'group_by_field', 'tags']]\\r        # return final_result[['number', 'clean_text', 'last_updated', 'group_by_field']]\\r\\r        \\r\\r    except Exception as e:\\r\\r        logger.error(f\\""Error processing group: {str(e)}\\"")\\r\\r        raise e\\r\\r\\r\\rdef clean_tickets(dataset, custom_stopwords_param: str = '', num_cores=None):\\r\\r    \\""\\""\\""\\r\\r    Clean ticket descriptions using parallel processing based on unique values in group_column\\r\\r    \\""\\""\\""\\r\\r    start_time = datetime.now()\\r\\r    \\r\\r    # Convert to DataFrame if not already\\r\\r    df = pd.DataFrame(dataset)\\r\\r    total_records = len(df)\\r\\r    \\r\\r    # Initial data preparation\\r\\r    df = df[~df.shortdescription.isnull()]\\r    \\r    # Detecting language.\\r    shortdescriptionDF = df['shortdescription'].head()\\r    top_5records = [sample for sample in shortdescriptionDF]\\r    top_5records_joined = ' '.join(top_5records)\\r    lang, confidence = langid.classify(top_5records_joined)\\r    \\r    translator = Translator()\\r    translated = translator.translate(text=custom_stopwords_param, destination_language=lang)\\r    translated_text = str(translated)\\r    \\r    if num_cores is None:\\r\\r        num_cores = max(1, multiprocessing.cpu_count() - 1)\\r\\r    # Split the dataframe into chunks\\r\\r    chunks = np.array_split(df, num_cores)\\r\\r    \\r\\r    # Process chunks in parallel using ProcessPoolExecutor\\r\\r    processed_groups = []\\r\\r    \\r\\r    # Create a partial function with the custom_stopwords_param\\r\\r    process_group_partial = partial(process_group, custom_stopwords_param=translated_text)\\r\\r    with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\\r\\r        # Submit all chunks for processing\\r\\r        futures = [executor.submit(process_group_partial, chunk) for chunk in chunks]\\r\\r        \\r\\r        # Collect results as they complete\\r\\r        for future in concurrent.futures.as_completed(futures):\\r\\r            try:\\r\\r                result = future.result()\\r\\r                if result is not None and len(result) > 0:\\r\\r                    processed_groups.append(result)\\r\\r            except Exception as e:\\r\\r                logger.error(f\\""Error processing chunk: {str(e)}\\"")\\r\\r    if not processed_groups:\\r\\r        logger.warning(\\""No groups were successfully processed\\"")\\r\\r        return pd.DataFrame()\\r    # Combine all processed groups\\r\\r    final_df = pd.concat(processed_groups, ignore_index=True)\\r\\r    return final_df""]},{""FunctionName"":""filter_data"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rimport nltk\\r\\rfrom nltk.corpus import stopwords\\r\\rfrom nltk.tokenize import word_tokenize\\r\\rfrom datetime import datetime\\r\\rimport concurrent.futures\\r\\rimport numpy as np\\r\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\rimport spacy\\r\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\r\\rimport multiprocessing\\r\\rimport logging as logger\\r\\rfrom functools import partial\\r\\rimport traceback\\r\\rimport importlib\\rimport os\\r\\r\\rstarted_time = datetime.now()\\r\\rdef filter_data(dataset):    \\r\\r    # logger.info('Fetched {0} tickets'.format(len(dataset)))\\r\\r    try:\\r        dataset_id = os.environ.get(\\""dataset_id\\"")\\r\\r        dataset = pd.DataFrame(dataset).copy()\\r        data_count = dataset.count()\\r        logger.info(\\""Count------------>\\"", data_count)\\r\\r        logger.info(f\\""Columns before filtering: {dataset.columns.tolist()}\\"")\\r        filter_column = \\""category\\""\\r        \\r        if dataset_id == \\""LEOTRNSL80208\\"":\\r            pass;\\r        else:\\r            dataset = dataset[['number', 'shortdescription', \\""category\\"", \\""tags\\""]].copy()\\r        \\r        # dataset = dataset[['number', 'shortdescription', \\""category\\""]].copy()\\r\\r            dataset = dataset.rename(columns={\\""category\\"": 'group_by_field'})\\r    \\r            dataset['shortdescription'].replace('', np.nan, inplace=True)\\r    \\r            dataset.dropna(subset=['shortdescription'], inplace=True)\\r    \\r            filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r    \\r            ciList = filteredCIs[filteredCIs['count'] >= 10]['group_by_field'].tolist()\\r    \\r            dataset = dataset[dataset['group_by_field'].isin(ciList)].copy()\\r    \\r            logger.info('Tickets for clustering {0}'.format(len(dataset.index)))\\r\\r        return dataset\\r\\r    except Exception as ex:\\r\\r        logger.error('Error in Filter Data')\\r\\r        logger.error(traceback.format_exc())\\r\\r        raise ex\\r\\r    ""]},{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]}]},{""id"":""CNyte"",""alias"":""DatasetExtractor"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""Extract_data"",""requirements"":"""",""params"":[],""script"":[""import os\\r\\rimport logging as logger\\r\\rimport requests\\r\\rimport json\\r\\rimport boto3\\r\\rimport os\\r\\rimport shutil\\r\\rfrom leaputils import Security\\r\\rimport ast\\rfrom urllib.parse import urlparse\\rfrom leaputils import Vault\\rfrom leaputils import Security\\rimport pandas as pd\\rimport mysql.connector\\r\\rdef getdatasetconfig(dataset_id: str, organization: str):\\r\\r    print(dataset_id)\\r\\r    print(organization)\\r    \\r    #os.environ['AIPlatform_Referred']='https://leap2:7000'\\r    os.environ['NO_PROXY']= f'victlpth5-04,10.82.53.110,infyaiplat.ad.infosys.com,{referrer}'\\r\\r    referrer = os.environ.get('referrer')\\r\\r    '''\\r\\r    call ai-plat api to get dataset config\\r\\r    return {dataset_config}\\r\\r    '''\\r    url = f\\""https://{referrer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}\\""\\r\\r    headers = {\\r\\r        'access-token': os.environ.get('access-token'),\\r\\r        'Project': os.environ.get('project_id'),\\r\\r        'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B'\\r\\r    }\\r\\r    try:\\r\\r        response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r        print(\\""status code was :\\"",response.status_code)\\r\\r        response.raise_for_status()\\r\\r        dataset_config = json.loads(response.text)\\r\\r        print('dataset_details', dataset_config)\\r\\r    except json.JSONDecodeError:\\r\\r        print(f\\""Error: Received invalid JSON response: {response.text}\\"")\\r\\r        dataset_config = None\\r\\r    except requests.exceptions.HTTPError as err:\\r\\r        print(f\\""HTTP error occurred: {err}\\"")\\r\\r        dataset_config = None\\r\\r    except Exception as e:\\r\\r        print(f\\""An error occurred: {e}\\"")\\r\\r        dataset_config = None\\r\\r    return dataset_config\\r\\r    \\r\\rdef s3_download_data(end_point_url: str = '', access_key: str = '', secret_key: str = '', bucket: str = '',\\r\\r                     obj_key: str = '', local_path: str = '/data'):\\r\\r    '''\\r\\r    download data from s3\\r\\r    return local file path\\r\\r    '''\\r\\r    s3_client = boto3.resource(service_name='s3',\\r\\r                               endpoint_url=end_point_url,\\r\\r                               aws_access_key_id=access_key,\\r\\r                               aws_secret_access_key=secret_key,\\r\\r                               verify=False)\\r\\r    bucket_object = s3_client.Bucket(bucket)\\r\\r    print(bucket_object.objects.filter(Prefix=obj_key))\\r\\r    if os.path.exists(local_path):\\r\\r        shutil.rmtree(local_path)\\r\\r    os.makedirs(local_path)\\r\\r    model_path = os.path.join(local_path)\\r\\r    for obj in bucket_object.objects.filter(Prefix=obj_key):\\r\\r        print((obj.key))\\r\\r        if obj.key.endswith('/'):\\r\\r            if not os.path.exists(f\\""{model_path}/{obj.key}\\""):\\r\\r                os.makedirs(f\\""{model_path}/{obj.key}\\"")\\r\\r        else:\\r\\r            os.makedirs(os.path.dirname(f\\""{model_path}/{obj.key}\\""), exist_ok=True)\\r\\r            res = bucket_object.download_file(obj.key, f\\""{model_path}/{obj.key}\\"")\\r\\r    return model_path\\r\\r    \\r\\r    \\r\\rdef Extract_data():  # python-script Data\\r\\r\\r    # get dataset configurations\\r    datasetid_param = os.environ['dataset_id']\\r    print(\\""dstId###\\"",datasetid_param)\\r    org_param = os.environ['org']\\r    print(\\""organization###\\"",org_param)\\r\\r    datasetcofig = getdatasetconfig(dataset_id=datasetid_param, organization=org_param)\\r    \\r    print(datasetcofig)\\r\\r    dataset_type = datasetcofig['datasource']['type']\\r    \\r    try: \\r        \\r\\r        if dataset_type == 'S3':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r    \\r            print(connection_dict)\\r    \\r            print(\\""Fetched Connection Details\\"")\\r    \\r            s3_access_key = connection_dict['accessKey']\\r    \\r            s3_secret_key = connection_dict['secretKey']\\r    \\r            s3_end_point_url = connection_dict['url']\\r    \\r            attribute = json.loads(datasetcofig['attributes'])\\r    \\r            bucket = attribute['bucket']\\r    \\r            path = attribute['path']\\r    \\r            #obj_key = attribute['object']\\r    \\r            #key = f'{path}/{obj_key}'\\r            key = f'{path}/'\\r    \\r            local_path = 'dataset_file' + '_' + datasetid_param\\r    \\r            file_path = s3_download_data(end_point_url=s3_end_point_url, access_key=s3_access_key, secret_key=s3_secret_key,\\r    \\r                                         bucket=bucket, obj_key=key, local_path=local_path)\\r    \\r            print(file_path)\\r    \\r            print(\\""LOCALPATH\\"", local_path)\\r    \\r        elif dataset_type == 'MYSQL':   \\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']  \\r            \\r    \\r            url = connection_dict['url']\\r            db_password = connection_dict['password']   \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            query = attributes['Query'] \\r            password = Security.decrypt(db_password,salt)\\r            host = urlparse(url[5:]).hostname\\r            port =urlparse(url[5:]).port\\r            database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r            connection = mysql.connector.connect(user=db_user_name, password=password, host=host, database=database, port = port)\\r            cursor = connection.cursor(dictionary=True)\\r            cursor.execute(query)\\r            results = cursor.fetchall()\\r            df= pd.DataFrame(results)  \\r            \\r            #connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r    \\r            #self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r    \\r            return df            \\r    \\r        elif dataset_type == 'POSTGRESQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata         \\r    \\r        elif dataset_type == 'MSSQL':\\r    \\r            connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r    \\r            attributes = json.loads(datasetcofig['attributes'])            \\r    \\r            db_user_name = connection_dict['userName']           \\r    \\r            db_url = connection_dict['url']\\r    \\r            db_password = connection_dict['password']   \\r    \\r            salt = datasetcofig['datasource']['salt']\\r    \\r            sql_query = attributes['Query'] \\r    \\r            connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r    \\r            self.store_connection_details(connection_details, connection_string, config.index_search)\\r    \\r            return metadata \\r\\r    except Exception as e:\\r\\r        logger.info('Exception in DatasetExtractor as: ',e)\\r    \\r        return e\\r\\r""]},""position_x"":""52"",""position_y"":""4"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""emiga"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[]}],""pipeline_attributes"":[{""name"":""storageType"",""value"":""s3""}],""environment"":[{""name"":""access-token"",""value"":""aec127c2-c984-33f6-9a3a-355xd1dof097""},{""name"":""project_id"",""value"":""2""},{""name"":""referrer"",""value"":""leap2:8000""}],""default_runtime"":""{\\""dsAlias\\"":\\""LocalCluster\\"",\\""dsName\\"":\\""LEALCLCL12132\\"",\\""type\\"":\\""REMOTE\\""}""}","admin","Clustering 2.0","2025-01-31 04:57:32","LEOCLSTR48373","leo1311","DragNDropLite","23","NULL","pipeline","NULL","\0","0"
"admin","2025-01-27 11:37:32.872000","\0","","NULL","{""elements"":[{""id"":""vGkxS"",""alias"":""Dataset Extractor"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractor"",""category"":""Extractor"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-04-10 14:16:38"",""alias"":""Tickets"",""id"":1,""name"":""Tickets"",""description"":""Tickets data"",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 12:16:34"",""alias"":""Tickets Schema"",""id"":1,""name"":""TicketsSchema"",""description"":null,""schemavalue"":""[{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 1,\\""recordcolumnname\\"": \\""number\\"",\\""recordcolumndisplayname\\"": \\""Number\\"",\\""isprimarykey\\"": true,\\""isunique\\"": true,\\""isrequired\\"": true},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 2,\\""recordcolumnname\\"": \\""shortdescription\\"",\\""recordcolumndisplayname\\"": \\""Short Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 3,\\""recordcolumnname\\"": \\""priority\\"",\\""recordcolumndisplayname\\"": \\""Priority\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 4,\\""recordcolumnname\\"": \\""type\\"",\\""recordcolumndisplayname\\"": \\""Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 5,\\""recordcolumnname\\"": \\""createdDate\\"",\\""recordcolumndisplayname\\"": \\""Created Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 6,\\""recordcolumnname\\"": \\""approval\\"",\\""recordcolumndisplayname\\"": \\""Approval\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 7,\\""recordcolumnname\\"": \\""assignedDate\\"",\\""recordcolumndisplayname\\"": \\""Assigned Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 8,\\""recordcolumnname\\"": \\""assignedto\\"",\\""recordcolumndisplayname\\"": \\""Assigned To\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 9,\\""recordcolumnname\\"": \\""assignmentgroup\\"",\\""recordcolumndisplayname\\"": \\""Assignment Group\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 10,\\""recordcolumnname\\"": \\""business_service\\"",\\""recordcolumndisplayname\\"": \\""Business Service\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 11,\\""recordcolumnname\\"": \\""caller\\"",\\""recordcolumndisplayname\\"": \\""Caller\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 12,\\""recordcolumnname\\"": \\""category\\"",\\""recordcolumndisplayname\\"": \\""Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 13,\\""recordcolumnname\\"": \\""closecode\\"",\\""recordcolumndisplayname\\"": \\""Close Code\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 14,\\""recordcolumnname\\"": \\""closedby\\"",\\""recordcolumndisplayname\\"": \\""Closed By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 15,\\""recordcolumnname\\"": \\""closedDate\\"",\\""recordcolumndisplayname\\"": \\""Closed Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 16,\\""recordcolumnname\\"": \\""closenotes\\"",\\""recordcolumndisplayname\\"": \\""Close Notes\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 17,\\""recordcolumnname\\"": \\""comments\\"",\\""recordcolumndisplayname\\"": \\""Comments\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 18,\\""recordcolumnname\\"": \\""configurationItem\\"",\\""recordcolumndisplayname\\"": \\""Configuration Item\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 19,\\""recordcolumnname\\"": \\""createdby\\"",\\""recordcolumndisplayname\\"": \\""Created By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 20,\\""recordcolumnname\\"": \\""description\\"",\\""recordcolumndisplayname\\"": \\""Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 21,\\""recordcolumnname\\"": \\""duedate\\"",\\""recordcolumndisplayname\\"": \\""Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 22,\\""recordcolumnname\\"": \\""impact\\"",\\""recordcolumndisplayname\\"": \\""Impact\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 23,\\""recordcolumnname\\"": \\""last_updated_by\\"",\\""recordcolumndisplayname\\"": \\""Last Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 24,\\""recordcolumnname\\"": \\""lastUpdated\\"",\\""recordcolumndisplayname\\"": \\""Last Updated\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 25,\\""recordcolumnname\\"": \\""location\\"",\\""recordcolumndisplayname\\"": \\""Location\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 26,\\""recordcolumnname\\"": \\""openedDate\\"",\\""recordcolumndisplayname\\"": \\""Opened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 27,\\""recordcolumnname\\"": \\""price\\"",\\""recordcolumndisplayname\\"": \\""Price\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 28,\\""recordcolumnname\\"": \\""reopenedDate\\"",\\""recordcolumndisplayname\\"": \\""Reopened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 29,\\""recordcolumnname\\"": \\""request_state\\"",\\""recordcolumndisplayname\\"": \\""Request State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 30,\\""recordcolumnname\\"": \\""requested_by\\"",\\""recordcolumndisplayname\\"": \\""Requested By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 31,\\""recordcolumnname\\"": \\""requested_for\\"",\\""recordcolumndisplayname\\"": \\""Requested For\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 32,\\""recordcolumnname\\"": \\""resolvedby\\"",\\""recordcolumndisplayname\\"": \\""Resolved By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 33,\\""recordcolumnname\\"": \\""resolvedDate\\"",\\""recordcolumndisplayname\\"": \\""Resolved Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 34,\\""recordcolumnname\\"": \\""risk\\"",\\""recordcolumndisplayname\\"": \\""Risk\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 35,\\""recordcolumnname\\"": \\""severity\\"",\\""recordcolumndisplayname\\"": \\""Severity\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 36,\\""recordcolumnname\\"": \\""sladueDate\\"",\\""recordcolumndisplayname\\"": \\""SLA Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 37,\\""recordcolumnname\\"": \\""special_instructions\\"",\\""recordcolumndisplayname\\"": \\""Special Instructions\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 38,\\""recordcolumnname\\"": \\""resolution_Steps\\"",\\""recordcolumndisplayname\\"": \\""Resolution Steps\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 39,\\""recordcolumnname\\"": \\""state\\"",\\""recordcolumndisplayname\\"": \\""State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 40,\\""recordcolumnname\\"": \\""sysId\\"",\\""recordcolumndisplayname\\"": \\""Sys Id\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 41,\\""recordcolumnname\\"": \\""taskType\\"",\\""recordcolumndisplayname\\"": \\""Task Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 42,\\""recordcolumnname\\"": \\""updatedby\\"",\\""recordcolumndisplayname\\"": \\""Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 43,\\""recordcolumnname\\"": \\""updatedDate\\"",\\""recordcolumndisplayname\\"": \\""Updated Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 44,\\""recordcolumnname\\"": \\""sop\\"",\\""recordcolumndisplayname\\"": \\""SOP\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 45,\\""recordcolumnname\\"": \\""tags\\"",\\""recordcolumndisplayname\\"": \\""Tags\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 46,\\""recordcolumnname\\"": \\""source\\"",\\""recordcolumndisplayname\\"": \\""Source\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 47,\\""recordcolumnname\\"": \\""resolutionCategory\\"",\\""recordcolumndisplayname\\"": \\""Resolution Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""workflow\\"",\\""recordcolumndisplayname\\"": \\""Workflow\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""resolutionStepsClusterName\\"",\\""recordcolumndisplayname\\"": \\""resolutionStepsClusterName\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false}]"",""organization"":""leo1311"",""type"":null,""capability"":null},""schemajson"":""[{\\""type\\"":\\""object\\"",\\""templateName\\"":\\""Form1\\"",\\""components\\"":[{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""R\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-refresh\\"",\\""tooltip\\"":\\""Refresh\\"",\\""tableView\\"":false,\\""key\\"":\\""refresh\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountrefresh = Number(document.getElementById('formio-btnclk-refresh').innerHTML); document.getElementById('formio-btnclk-refresh').innerHTML=(clickCountrefresh+1);\\"",\\""input\\"":true}],\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""offset\\"":6,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""TE\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-play\\"",\\""tooltip\\"":\\""Trigger Event\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""eventTrigger\\"",\\""properties\\"":{\\""event1\\"":\\""refreshTickets\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCounteventTrigger = Number(document.getElementById('formio-btnclk-eventTrigger').innerHTML); document.getElementById('formio-btnclk-eventTrigger').innerHTML=(clickCounteventTrigger+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""IN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cog\\"",\\""tooltip\\"":\\""Internal Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""internalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""landing/iamp-usm/dashconstant/{number}/false\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountinternalNavigation = Number(document.getElementById('formio-btnclk-internalNavigation').innerHTML); document.getElementById('formio-btnclk-internalNavigation').innerHTML=(clickCountinternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""EN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-google\\"",\\""tooltip\\"":\\""External Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""externalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""https://www.google.com\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountexternalNavigation = Number(document.getElementById('formio-btnclk-externalNavigation').innerHTML); document.getElementById('formio-btnclk-externalNavigation').innerHTML=(clickCountexternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""MA\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cogs\\"",\\""tooltip\\"":\\""Multiple Actions\\"",\\""tableView\\"":false,\\""key\\"":\\""multipleActions\\"",\\""properties\\"":{\\""submit\\"":\\""submit\\"",\\""reset\\"":\\""reset\\"",\\""refresh\\"":\\""refresh\\"",\\""eventTrigger\\"":\\""createIncident\\"",\\""externalNavigation1\\"":\\""https://www.google.com\\"",\\""externalNavigation2\\"":\\""https://www.youtube.com\\"",\\""internalNavigation\\"":\\""landing/iamp-usm/dashconstant\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountmultipleActions = Number(document.getElementById('formio-btnclk-multipleActions').innerHTML); document.getElementById('formio-btnclk-multipleActions').innerHTML=(clickCountmultipleActions+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1}],\\""key\\"":\\""columns1\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false},{\\""label\\"":\\""Table\\"",\\""cellAlignment\\"":\\""left\\"",\\""key\\"":\\""table\\"",\\""type\\"":\\""table\\"",\\""input\\"":false,\\""tableView\\"":false,\\""rows\\"":[[{\\""components\\"":[{\\""label\\"":\\""Number\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""number\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Short Description\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""shortdescription\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Priority\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""1 - Critical\\"",\\""value\\"":\\""1 - Critical\\""},{\\""label\\"":\\""2 - High\\"",\\""value\\"":\\""2 - High\\""},{\\""label\\"":\\""3 - Medium\\"",\\""value\\"":\\""3 - Medium\\""},{\\""label\\"":\\""4 - Low\\"",\\""value\\"":\\""4 - Low\\""},{\\""label\\"":\\""5 - Very Low\\"",\\""value\\"":\\""5 - Very Low\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""priority\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true}]}],[{\\""components\\"":[{\\""label\\"":\\""State\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""New\\"",\\""value\\"":\\""new\\""},{\\""label\\"":\\""In Progress\\"",\\""value\\"":\\""inProgress\\""},{\\""label\\"":\\""On Hold\\"",\\""value\\"":\\""onHold\\""},{\\""label\\"":\\""Resolved\\"",\\""value\\"":\\""resolved\\""},{\\""label\\"":\\""Closed\\"",\\""value\\"":\\""closed\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""state\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true,\\""defaultValue\\"":\\""new\\""}]},{\\""components\\"":[{\\""label\\"":\\""Configuration Item\\"",\\""tableView\\"":true,\\""key\\"":\\""configurationItem\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Assignment Group\\"",\\""tableView\\"":true,\\""key\\"":\\""assignmentgroup\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]}]],\\""numRows\\"":2},{\\""label\\"":\\""Description\\"",\\""autoExpand\\"":false,\\""tableView\\"":true,\\""key\\"":\\""description\\"",\\""type\\"":\\""textarea\\"",\\""input\\"":true},{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""Submit\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""success\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""submit\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountsubmit = Number(document.getElementById('formio-btnclk-submit').innerHTML); document.getElementById('formio-btnclk-submit').innerHTML=(clickCountsubmit+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""Reset\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""danger\\"",\\""tableView\\"":false,\\""key\\"":\\""reset\\"",\\""type\\"":\\""button\\"",\\""input\\"":true,\\""custom\\"":\\""let clickCountreset = Number(document.getElementById('formio-btnclk-reset').innerHTML); document.getElementById('formio-btnclk-reset').innerHTML=(clickCountreset+1);\\""}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1}],\\""key\\"":\\""columns\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false}]}]"",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT * from @projectname_tickets"",""Cacheable"":""false"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""@projectname_tickets"",""uniqueIdentifier"":""number""},""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":4,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""172"",""position_y"":""126"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""uNpuH"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""requirements"":[],""servicenow"":{},""imports"":[],""MYSQL"":{},""w"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r    import importlib.util\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'extractors', f'{extractortype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Extractor', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Extractor'] = module\\r    spec.loader.exec_module(module)\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""context"":[]},{""id"":""uNpuH"",""alias"":""Translate Language"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""translate_language"",""requirements"":""translatepy"",""params"":[],""script"":[""import pandas as pd\\rfrom translatepy import Translator\\rfrom translatepy.exceptions import TranslatepyException\\rfrom concurrent.futures import ProcessPoolExecutor, as_completed\\rfrom datetime import datetime\\rimport multiprocessing\\rimport numpy as np\\r\\r\\rdef translate_text(text):\\r    \\""\\""\\""Translate a single text string\\""\\""\\""\\r    if not isinstance(text, str) or not text.strip():\\r        return \\""Unknown\\""\\r    try:\\r        translator = Translator()\\r        # translated = translator.translate(text, os.environ.get(\\""Taget_language\\""))\\r        translated = translator.translate(text, os.environ('Target_Language'))\\r        return str(translated)\\r    except Exception as e:\\r        logger.error(f\\""Translation error for '{text[:50]}...': {str(e)}\\"")\\r        return \\""Unknown\\""\\r\\rdef process_chunk(texts):\\r    \\""\\""\\""Process a chunk of unique texts\\""\\""\\""\\r\\r    logger.info(\\""Process chunk has started...\\"")\\r    translations = {}\\r    for text in texts:\\r        translations[text] = translate_text(text)\\r    logger.info(\\""Process chunk has ended...\\"")\\r    return translations\\r\\rdef translate_language(dataset):\\r    try:\\r        start_time = datetime.now()\\r        logger.info(\\""Starting translation process...\\"")\\r        df=pd.DataFrame(dataset)\\r        df = df[['number',  'shortdescription', 'category','tags']]\\r        df['shortdescription'].replace('', np.nan, inplace=True)\\r        df.dropna(subset=['shortdescription'], inplace=True)\\r        df = df.rename(columns={'category':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = df.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = df['group_by_field'].isin(ciList)\\r        df['include'] = filtereddf\\r        df = df[df['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(df.index)))\\r        # df=df.dropna()\\r        total_records = len(df)\\r        logger.info(f\\""Total records: {total_records}\\"")        \\r        unique_texts = df['shortdescription'].dropna().unique()\\r        logger.info(f\\""Unique values to translate: {len(unique_texts)}\\"")\\r        num_cores = multiprocessing.cpu_count()\\r        chunks = np.array_split(unique_texts, num_cores)\\r        translation_dict = {}\\r        # Process chunks in parallel\\r        with ProcessPoolExecutor(max_workers=num_cores) as executor:\\r            futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\\r            for i, future in enumerate(as_completed(futures)):\\r                try:\\r                    chunk_translations = future.result()\\r                    translation_dict.update(chunk_translations)\\r                    logger.info(f\\""Completed chunk {i + 1}/{len(chunks)}\\"")\\r                except Exception as e:\\r                    logger.error(f\\""Error in chunk {i}: {str(e)}\\"")\\r\\r        logger.info(\\""Translations completed. Mapping back to original positions...\\"")\\r        df['shortdescription'] = df['shortdescription'].map(lambda x: translation_dict.get(x, \\""Unknown\\""))\\r        \\r        end_time = datetime.now()\\r        logger.info(f\\""Total processing time: {end_time - start_time}\\"")\\r        logger.info(f\\""Successfully processed {len(df)} records\\"")\\r        final_data = df.to_dict('records')\\r        return final_data\\r\\r    except Exception as e:\\r        logger.error(f\\""Error in main process: {str(e)}\\"")\\r        final_data = df.to_dict('records')\\r        return final_data\\r""]},""position_x"":""406"",""position_y"":""125"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""vGkxS"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""RajeH"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-04-10 14:16:38"",""alias"":""Tickets"",""id"":1,""name"":""Tickets"",""description"":""Tickets data"",""schema"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2021-06-14 12:16:34"",""alias"":""Tickets Schema"",""id"":1,""name"":""TicketsSchema"",""description"":null,""schemavalue"":""[{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 1,\\""recordcolumnname\\"": \\""number\\"",\\""recordcolumndisplayname\\"": \\""Number\\"",\\""isprimarykey\\"": true,\\""isunique\\"": true,\\""isrequired\\"": true},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 2,\\""recordcolumnname\\"": \\""shortdescription\\"",\\""recordcolumndisplayname\\"": \\""Short Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 3,\\""recordcolumnname\\"": \\""priority\\"",\\""recordcolumndisplayname\\"": \\""Priority\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 4,\\""recordcolumnname\\"": \\""type\\"",\\""recordcolumndisplayname\\"": \\""Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 5,\\""recordcolumnname\\"": \\""createdDate\\"",\\""recordcolumndisplayname\\"": \\""Created Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": true,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 6,\\""recordcolumnname\\"": \\""approval\\"",\\""recordcolumndisplayname\\"": \\""Approval\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 7,\\""recordcolumnname\\"": \\""assignedDate\\"",\\""recordcolumndisplayname\\"": \\""Assigned Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 8,\\""recordcolumnname\\"": \\""assignedto\\"",\\""recordcolumndisplayname\\"": \\""Assigned To\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 9,\\""recordcolumnname\\"": \\""assignmentgroup\\"",\\""recordcolumndisplayname\\"": \\""Assignment Group\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 10,\\""recordcolumnname\\"": \\""business_service\\"",\\""recordcolumndisplayname\\"": \\""Business Service\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 11,\\""recordcolumnname\\"": \\""caller\\"",\\""recordcolumndisplayname\\"": \\""Caller\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 12,\\""recordcolumnname\\"": \\""category\\"",\\""recordcolumndisplayname\\"": \\""Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 13,\\""recordcolumnname\\"": \\""closecode\\"",\\""recordcolumndisplayname\\"": \\""Close Code\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 14,\\""recordcolumnname\\"": \\""closedby\\"",\\""recordcolumndisplayname\\"": \\""Closed By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 15,\\""recordcolumnname\\"": \\""closedDate\\"",\\""recordcolumndisplayname\\"": \\""Closed Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false,\\""isautoincrement\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 16,\\""recordcolumnname\\"": \\""closenotes\\"",\\""recordcolumndisplayname\\"": \\""Close Notes\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 17,\\""recordcolumnname\\"": \\""comments\\"",\\""recordcolumndisplayname\\"": \\""Comments\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 18,\\""recordcolumnname\\"": \\""configurationItem\\"",\\""recordcolumndisplayname\\"": \\""Configuration Item\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 19,\\""recordcolumnname\\"": \\""createdby\\"",\\""recordcolumndisplayname\\"": \\""Created By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 20,\\""recordcolumnname\\"": \\""description\\"",\\""recordcolumndisplayname\\"": \\""Description\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 21,\\""recordcolumnname\\"": \\""duedate\\"",\\""recordcolumndisplayname\\"": \\""Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 22,\\""recordcolumnname\\"": \\""impact\\"",\\""recordcolumndisplayname\\"": \\""Impact\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 23,\\""recordcolumnname\\"": \\""last_updated_by\\"",\\""recordcolumndisplayname\\"": \\""Last Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 24,\\""recordcolumnname\\"": \\""lastUpdated\\"",\\""recordcolumndisplayname\\"": \\""Last Updated\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 25,\\""recordcolumnname\\"": \\""location\\"",\\""recordcolumndisplayname\\"": \\""Location\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 26,\\""recordcolumnname\\"": \\""openedDate\\"",\\""recordcolumndisplayname\\"": \\""Opened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 27,\\""recordcolumnname\\"": \\""price\\"",\\""recordcolumndisplayname\\"": \\""Price\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 28,\\""recordcolumnname\\"": \\""reopenedDate\\"",\\""recordcolumndisplayname\\"": \\""Reopened Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 29,\\""recordcolumnname\\"": \\""request_state\\"",\\""recordcolumndisplayname\\"": \\""Request State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 30,\\""recordcolumnname\\"": \\""requested_by\\"",\\""recordcolumndisplayname\\"": \\""Requested By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 31,\\""recordcolumnname\\"": \\""requested_for\\"",\\""recordcolumndisplayname\\"": \\""Requested For\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 32,\\""recordcolumnname\\"": \\""resolvedby\\"",\\""recordcolumndisplayname\\"": \\""Resolved By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 33,\\""recordcolumnname\\"": \\""resolvedDate\\"",\\""recordcolumndisplayname\\"": \\""Resolved Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 34,\\""recordcolumnname\\"": \\""risk\\"",\\""recordcolumndisplayname\\"": \\""Risk\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 35,\\""recordcolumnname\\"": \\""severity\\"",\\""recordcolumndisplayname\\"": \\""Severity\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 36,\\""recordcolumnname\\"": \\""sladueDate\\"",\\""recordcolumndisplayname\\"": \\""SLA Due Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 37,\\""recordcolumnname\\"": \\""special_instructions\\"",\\""recordcolumndisplayname\\"": \\""Special Instructions\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""text\\"",\\""columnorder\\"": 38,\\""recordcolumnname\\"": \\""resolution_Steps\\"",\\""recordcolumndisplayname\\"": \\""Resolution Steps\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 39,\\""recordcolumnname\\"": \\""state\\"",\\""recordcolumndisplayname\\"": \\""State\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 40,\\""recordcolumnname\\"": \\""sysId\\"",\\""recordcolumndisplayname\\"": \\""Sys Id\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 41,\\""recordcolumnname\\"": \\""taskType\\"",\\""recordcolumndisplayname\\"": \\""Task Type\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 42,\\""recordcolumnname\\"": \\""updatedby\\"",\\""recordcolumndisplayname\\"": \\""Updated By\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""datetime\\"",\\""columnorder\\"": 43,\\""recordcolumnname\\"": \\""updatedDate\\"",\\""recordcolumndisplayname\\"": \\""Updated Date\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 44,\\""recordcolumnname\\"": \\""sop\\"",\\""recordcolumndisplayname\\"": \\""SOP\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 45,\\""recordcolumnname\\"": \\""tags\\"",\\""recordcolumndisplayname\\"": \\""Tags\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 46,\\""recordcolumnname\\"": \\""source\\"",\\""recordcolumndisplayname\\"": \\""Source\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 47,\\""recordcolumnname\\"": \\""resolutionCategory\\"",\\""recordcolumndisplayname\\"": \\""Resolution Category\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""workflow\\"",\\""recordcolumndisplayname\\"": \\""Workflow\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false},{\\""columntype\\"": \\""string\\"",\\""columnorder\\"": 48,\\""recordcolumnname\\"": \\""resolutionStepsClusterName\\"",\\""recordcolumndisplayname\\"": \\""resolutionStepsClusterName\\"",\\""isprimarykey\\"": false,\\""isunique\\"": false,\\""isrequired\\"": false}]"",""organization"":""leo1311"",""type"":null,""capability"":null},""schemajson"":""[{\\""type\\"":\\""object\\"",\\""templateName\\"":\\""Form1\\"",\\""components\\"":[{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""R\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-refresh\\"",\\""tooltip\\"":\\""Refresh\\"",\\""tableView\\"":false,\\""key\\"":\\""refresh\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountrefresh = Number(document.getElementById('formio-btnclk-refresh').innerHTML); document.getElementById('formio-btnclk-refresh').innerHTML=(clickCountrefresh+1);\\"",\\""input\\"":true}],\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""offset\\"":6,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""TE\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-play\\"",\\""tooltip\\"":\\""Trigger Event\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""eventTrigger\\"",\\""properties\\"":{\\""event1\\"":\\""refreshTickets\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCounteventTrigger = Number(document.getElementById('formio-btnclk-eventTrigger').innerHTML); document.getElementById('formio-btnclk-eventTrigger').innerHTML=(clickCounteventTrigger+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""IN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cog\\"",\\""tooltip\\"":\\""Internal Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""internalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""landing/iamp-usm/dashconstant/{number}/false\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountinternalNavigation = Number(document.getElementById('formio-btnclk-internalNavigation').innerHTML); document.getElementById('formio-btnclk-internalNavigation').innerHTML=(clickCountinternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""EN\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-google\\"",\\""tooltip\\"":\\""External Navigation\\"",\\""tableView\\"":false,\\""key\\"":\\""externalNavigation\\"",\\""properties\\"":{\\""url\\"":\\""https://www.google.com\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountexternalNavigation = Number(document.getElementById('formio-btnclk-externalNavigation').innerHTML); document.getElementById('formio-btnclk-externalNavigation').innerHTML=(clickCountexternalNavigation+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1},{\\""components\\"":[{\\""label\\"":\\""MA\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""leftIcon\\"":\\""fa fa-cogs\\"",\\""tooltip\\"":\\""Multiple Actions\\"",\\""tableView\\"":false,\\""key\\"":\\""multipleActions\\"",\\""properties\\"":{\\""submit\\"":\\""submit\\"",\\""reset\\"":\\""reset\\"",\\""refresh\\"":\\""refresh\\"",\\""eventTrigger\\"":\\""createIncident\\"",\\""externalNavigation1\\"":\\""https://www.google.com\\"",\\""externalNavigation2\\"":\\""https://www.youtube.com\\"",\\""internalNavigation\\"":\\""landing/iamp-usm/dashconstant\\""},\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountmultipleActions = Number(document.getElementById('formio-btnclk-multipleActions').innerHTML); document.getElementById('formio-btnclk-multipleActions').innerHTML=(clickCountmultipleActions+1);\\"",\\""input\\"":true}],\\""size\\"":\\""md\\"",\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""width\\"":1,\\""currentWidth\\"":1}],\\""key\\"":\\""columns1\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false},{\\""label\\"":\\""Table\\"",\\""cellAlignment\\"":\\""left\\"",\\""key\\"":\\""table\\"",\\""type\\"":\\""table\\"",\\""input\\"":false,\\""tableView\\"":false,\\""rows\\"":[[{\\""components\\"":[{\\""label\\"":\\""Number\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""number\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Short Description\\"",\\""tableView\\"":true,\\""validate\\"":{\\""required\\"":true},\\""key\\"":\\""shortdescription\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Priority\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""1 - Critical\\"",\\""value\\"":\\""1 - Critical\\""},{\\""label\\"":\\""2 - High\\"",\\""value\\"":\\""2 - High\\""},{\\""label\\"":\\""3 - Medium\\"",\\""value\\"":\\""3 - Medium\\""},{\\""label\\"":\\""4 - Low\\"",\\""value\\"":\\""4 - Low\\""},{\\""label\\"":\\""5 - Very Low\\"",\\""value\\"":\\""5 - Very Low\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""priority\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true}]}],[{\\""components\\"":[{\\""label\\"":\\""State\\"",\\""tableView\\"":true,\\""data\\"":{\\""values\\"":[{\\""label\\"":\\""New\\"",\\""value\\"":\\""new\\""},{\\""label\\"":\\""In Progress\\"",\\""value\\"":\\""inProgress\\""},{\\""label\\"":\\""On Hold\\"",\\""value\\"":\\""onHold\\""},{\\""label\\"":\\""Resolved\\"",\\""value\\"":\\""resolved\\""},{\\""label\\"":\\""Closed\\"",\\""value\\"":\\""closed\\""}]},\\""selectThreshold\\"":0.3,\\""validate\\"":{\\""onlyAvailableItems\\"":false},\\""key\\"":\\""state\\"",\\""type\\"":\\""select\\"",\\""indexeddb\\"":{\\""filter\\"":{}},\\""input\\"":true,\\""defaultValue\\"":\\""new\\""}]},{\\""components\\"":[{\\""label\\"":\\""Configuration Item\\"",\\""tableView\\"":true,\\""key\\"":\\""configurationItem\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]},{\\""components\\"":[{\\""label\\"":\\""Assignment Group\\"",\\""tableView\\"":true,\\""key\\"":\\""assignmentgroup\\"",\\""type\\"":\\""textfield\\"",\\""input\\"":true}]}]],\\""numRows\\"":2},{\\""label\\"":\\""Description\\"",\\""autoExpand\\"":false,\\""tableView\\"":true,\\""key\\"":\\""description\\"",\\""type\\"":\\""textarea\\"",\\""input\\"":true},{\\""label\\"":\\""Columns\\"",\\""columns\\"":[{\\""components\\"":[{\\""label\\"":\\""Submit\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""success\\"",\\""disableOnInvalid\\"":true,\\""tableView\\"":false,\\""key\\"":\\""submit\\"",\\""type\\"":\\""button\\"",\\""custom\\"":\\""let clickCountsubmit = Number(document.getElementById('formio-btnclk-submit').innerHTML); document.getElementById('formio-btnclk-submit').innerHTML=(clickCountsubmit+1);\\"",\\""input\\"":true}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1},{\\""components\\"":[{\\""label\\"":\\""Reset\\"",\\""action\\"":\\""custom\\"",\\""showValidations\\"":false,\\""theme\\"":\\""danger\\"",\\""tableView\\"":false,\\""key\\"":\\""reset\\"",\\""type\\"":\\""button\\"",\\""input\\"":true,\\""custom\\"":\\""let clickCountreset = Number(document.getElementById('formio-btnclk-reset').innerHTML); document.getElementById('formio-btnclk-reset').innerHTML=(clickCountreset+1);\\""}],\\""offset\\"":0,\\""push\\"":0,\\""pull\\"":0,\\""size\\"":\\""md\\"",\\""currentWidth\\"":1,\\""width\\"":1}],\\""key\\"":\\""columns\\"",\\""type\\"":\\""columns\\"",\\""input\\"":false,\\""tableView\\"":false}]}]"",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT * from @projectname_tickets"",""Cacheable"":""false"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""@projectname_tickets"",""uniqueIdentifier"":""number""},""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":4,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":false,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":null,""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}}]},{""id"":""RajeH"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoader"",""category"":""Loader"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-27 11:41:52"",""alias"":""Translated Tickets"",""id"":14329,""name"":""LEOTRNSL80208"",""description"":null,""schema"":null,""schemajson"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from @projectname_translated_tickets\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""overwrite\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""@projectname_translated_tickets\\"",\\""uniqueIdentifier\\"":\\""number\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-20 05:48:32"",""alias"":""leo1311"",""id"":1,""name"":""leo1311"",""description"":""Local data for setup"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enc2iXYbQsvZpkec0R8Py0pW0VEoEnCOUCA\\"",\\""datasource\\"":\\""\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/leap_8000_refdb\\""}"",""salt"":""Tb7eXrN4zxUO26FnNKM3XynB+6Gd/ee723tThnHQGbyMdEL2GD1kEuCwHoJstMCqaKo2QF6BtFiAxFCPmjnHgA=="",""organization"":""leo1311"",""dshashcode"":""260dd5aad04a2c2bda900c08f45fabeacc92363b0296fbab9dc46be477976a7f"",""activetime"":""2025-01-20 05:48:31"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":false,""forruntime"":false,""foradapter"":false,""formodel"":false,""forpromptprovider"":false,""forendpoint"":false},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""654"",""position_y"":""123"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""uNpuH"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""requirements"":[],""imports"":[""import importlib""],""MYSQL"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r    import importlib.util\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'loaders', f'{loadertype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Loader', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Loader'] = module\\r    spec.loader.exec_module(module)\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\n""}}],""pipeline_attributes"":[],""environment"":[],""default_runtime"":""{\\""dsAlias\\"":\\""LocalCluster\\"",\\""dsName\\"":\\""LEALCLCL12132\\"",\\""type\\"":\\""REMOTE\\""}""}","admin","Language Translation","2025-01-31 04:49:25","LEOLNGTR30094","leo1311","DragNDropLite","31","NULL","pipeline","NULL","\0","0"
"admin","2025-01-17 08:22:11.107000","\0","open telemetry",NULL,"{""elements"":[{""id"":""cIjkW"",""alias"":""Dataset Extractor"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractor"",""category"":""Extractor"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:38:31"",""alias"":""telemetry new dataset"",""id"":13336,""name"":""LEOTLMTR57504"",""description"":null,""schema"":null,""schemajson"":""\\""null\\"""",""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from open_telemetry\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""open_telemetry\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""type"":""r"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:06:40"",""alias"":""TelemetryDB"",""id"":6147,""name"":""LEOTLMTR19390"",""description"":"""",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""encLkcb27L/zuAk1JyL/ZRkK+OtD+xHAnPc\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/telemetrydb\\""}"",""salt"":""MXqgjgoT80rHK7Iws+9JVbQDtx1z30l0YwsukVbKWah17uos5iceLGfHIOqKpoxngkuFjtAJRKWZZG/SCbmd6w=="",""organization"":""leo1311"",""dshashcode"":""43cc308507c43465d602b076401eb02932457a977861b740e25d52d67955ddbf"",""activetime"":""2025-01-17 08:06:40"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":null,""forruntime"":null,""foradapter"":null,""formodel"":null,""forpromptprovider"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""context"":null,""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""dashboard"":null,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""63"",""position_y"":""246"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""DiUAa"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""requirements"":[],""servicenow"":{},""imports"":[],""MYSQL"":{},""w"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r    import importlib.util\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'extractors', f'{extractortype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Extractor', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Extractor'] = module\\r    spec.loader.exec_module(module)\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""context"":[]},{""id"":""yYaHE"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoader"",""category"":""Loader"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:39:11"",""alias"":""snapshot"",""id"":13341,""name"":""LEOSNPSH41456"",""description"":null,""schema"":null,""schemajson"":""\\""null\\"""",""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from dailysnapshot\\"",\\""isStreaming\\"":\\""false\\"",\\""defaultValues\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""dailysnapshot\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""type"":""r"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:06:40"",""alias"":""TelemetryDB"",""id"":6147,""name"":""LEOTLMTR19390"",""description"":"""",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""encLkcb27L/zuAk1JyL/ZRkK+OtD+xHAnPc\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/telemetrydb\\""}"",""salt"":""MXqgjgoT80rHK7Iws+9JVbQDtx1z30l0YwsukVbKWah17uos5iceLGfHIOqKpoxngkuFjtAJRKWZZG/SCbmd6w=="",""organization"":""leo1311"",""dshashcode"":""43cc308507c43465d602b076401eb02932457a977861b740e25d52d67955ddbf"",""activetime"":""2025-01-17 08:06:40"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":null,""forruntime"":null,""foradapter"":null,""formodel"":null,""forpromptprovider"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""context"":null,""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""dashboard"":null,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}},""position_x"":""740"",""position_y"":""244"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""DiUAa"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""requirements"":[],""imports"":[""import importlib""],""MYSQL"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r    import importlib.util\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'loaders', f'{loadertype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Loader', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Loader'] = module\\r    spec.loader.exec_module(module)\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\n""},""context"":[{""FunctionName"":""Start_Process"",""requirements"":"""",""params"":[],""script"":[""import multiprocessing \\r\\r\\rdef Start_Process( raw_data):    #python-script Data\\r\\r    queue = multiprocessing.Queue()\\r    p1 = multiprocessing.Process(target=ivm, args=( raw_data,queue)) \\r    # starting process \\r    p1.start() \\r    # wait until process is finished \\r    p1.join() \\r    \\r    p2 = multiprocessing.Process(target=dbs, args=( raw_data,queue)) \\r    # starting process \\r    p2.start() \\r    # wait until process is finished \\r    p2.join() \\r    \\r    p3 = multiprocessing.Process(target=usm, args=( raw_data,queue)) \\r    # starting process \\r    p3.start() \\r    # wait until process is finished \\r    p3.join()\\r    \\r    p4 = multiprocessing.Process(target=eda, args=( raw_data,queue)) \\r    # starting process \\r    p4.start() \\r    # wait until process is finished \\r    p4.join()\\r    \\r    p5 = multiprocessing.Process(target=sre, args=( raw_data,queue)) \\r    # starting process \\r    p5.start() \\r    # wait until process is finished \\r    p5.join()\\r    \\r    p6 = multiprocessing.Process(target=iegp, args=( raw_data,queue))\\r    # starting Process\\r    p6.start()\\r    #wait until process is finished\\r    p6.join()\\r    \\r    p7 = multiprocessing.Process(target=grh, args=( raw_data,queue))\\r    # starting Process\\r    p7.start()\\r    #wait until process is finished\\r    p7.join()\\r    \\r    p8 = multiprocessing.Process(target=cap, args=( raw_data,queue))\\r    # starting Process\\r    p8.start()\\r    #wait until process is finished\\r    p8.join()\\r    \\r    p9 = multiprocessing.Process(target=cas, args=( raw_data,queue))\\r    # # starting Process\\r    p9.start()\\r     #wait until process is finished\\r    p9.join()\\r    \\r    p10 = multiprocessing.Process(target=aip_app, args=( raw_data,queue))\\r    # # starting Process\\r    p10.start()\\r     #wait until process is finished\\r    p10.join()\\r\\r    results = []\\r    while not queue.empty():\\r        results = results + queue.get()\\r\\r    print(results)\\r    return results""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:38:31"",""alias"":""telemetry new dataset"",""id"":13336,""name"":""LEOTLMTR57504"",""description"":null,""schema"":null,""schemajson"":""\\""null\\"""",""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from open_telemetry\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""open_telemetry\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""type"":""r"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:06:40"",""alias"":""TelemetryDB"",""id"":6147,""name"":""LEOTLMTR19390"",""description"":"""",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""encLkcb27L/zuAk1JyL/ZRkK+OtD+xHAnPc\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/telemetrydb\\""}"",""salt"":""MXqgjgoT80rHK7Iws+9JVbQDtx1z30l0YwsukVbKWah17uos5iceLGfHIOqKpoxngkuFjtAJRKWZZG/SCbmd6w=="",""organization"":""leo1311"",""dshashcode"":""43cc308507c43465d602b076401eb02932457a977861b740e25d52d67955ddbf"",""activetime"":""2025-01-17 08:06:40"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":null,""forruntime"":null,""foradapter"":null,""formodel"":null,""forpromptprovider"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""context"":null,""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""dashboard"":null,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}}]},{""id"":""DiUAa"",""alias"":""Process"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""Start_Process"",""requirements"":"""",""params"":[],""script"":[""import multiprocessing \\r\\r\\rdef Start_Process( raw_data):    #python-script Data\\r\\r    queue = multiprocessing.Queue()\\r    p1 = multiprocessing.Process(target=ivm, args=( raw_data,queue)) \\r    # starting process \\r    p1.start() \\r    # wait until process is finished \\r    p1.join() \\r    \\r    p2 = multiprocessing.Process(target=dbs, args=( raw_data,queue)) \\r    # starting process \\r    p2.start() \\r    # wait until process is finished \\r    p2.join() \\r    \\r    p3 = multiprocessing.Process(target=usm, args=( raw_data,queue)) \\r    # starting process \\r    p3.start() \\r    # wait until process is finished \\r    p3.join()\\r    \\r    p4 = multiprocessing.Process(target=eda, args=( raw_data,queue)) \\r    # starting process \\r    p4.start() \\r    # wait until process is finished \\r    p4.join()\\r    \\r    p5 = multiprocessing.Process(target=sre, args=( raw_data,queue)) \\r    # starting process \\r    p5.start() \\r    # wait until process is finished \\r    p5.join()\\r    \\r    p6 = multiprocessing.Process(target=iegp, args=( raw_data,queue))\\r    # starting Process\\r    p6.start()\\r    #wait until process is finished\\r    p6.join()\\r    \\r    p7 = multiprocessing.Process(target=grh, args=( raw_data,queue))\\r    # starting Process\\r    p7.start()\\r    #wait until process is finished\\r    p7.join()\\r    \\r    p8 = multiprocessing.Process(target=cap, args=( raw_data,queue))\\r    # starting Process\\r    p8.start()\\r    #wait until process is finished\\r    p8.join()\\r    \\r    p9 = multiprocessing.Process(target=cas, args=( raw_data,queue))\\r    # # starting Process\\r    p9.start()\\r     #wait until process is finished\\r    p9.join()\\r    \\r    p10 = multiprocessing.Process(target=aip_app, args=( raw_data,queue))\\r    # # starting Process\\r    p10.start()\\r     #wait until process is finished\\r    p10.join()\\r\\r    results = []\\r    while not queue.empty():\\r        results = results + queue.get()\\r\\r    print(results)\\r    return results""]},""position_x"":""453"",""position_y"":""244"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""cIjkW"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""yYaHE"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:38:31"",""alias"":""telemetry new dataset"",""id"":13336,""name"":""LEOTLMTR57504"",""description"":null,""schema"":null,""schemajson"":""\\""null\\"""",""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from open_telemetry\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""open_telemetry\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""type"":""r"",""datasource"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2025-01-17 08:06:40"",""alias"":""TelemetryDB"",""id"":6147,""name"":""LEOTLMTR19390"",""description"":"""",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""encLkcb27L/zuAk1JyL/ZRkK+OtD+xHAnPc\\"",\\""userName\\"":\\""leapadm\\"",\\""url\\"":\\""jdbc:mysql://10.67.9.46:3307/telemetrydb\\""}"",""salt"":""MXqgjgoT80rHK7Iws+9JVbQDtx1z30l0YwsukVbKWah17uos5iceLGfHIOqKpoxngkuFjtAJRKWZZG/SCbmd6w=="",""organization"":""leo1311"",""dshashcode"":""43cc308507c43465d602b076401eb02932457a977861b740e25d52d67955ddbf"",""activetime"":""2025-01-17 08:06:40"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null,""fordataset"":null,""forruntime"":null,""foradapter"":null,""formodel"":null,""forpromptprovider"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":null,""views"":""Table View"",""context"":null,""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""dashboard"":null,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""[]"",""interfacetype"":null,""adaptername"":null,""isadapteractive"":null,""indexname"":null,""summary"":null,""event_details"":null}}]},{""id"":""DGrdK"",""alias"":""ivm"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""ivm"",""requirements"":""pandasql"",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""import pandas as pd\\rfrom pandasql import sqldf\\rimport sqlalchemy\\rfrom datetime import datetime\\rdef ivm(raw_data_param,result_param):    #python-script Data\\r    global result\\r    if raw_data_param !='': \\r        global datetime\\r        now=datetime.now()\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r        raw_data=raw_data_param\\r        print(type(raw_data))\\r        snapshot =[]\\r        # print(raw_data)\\r        df=pd.DataFrame(raw_data);\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r        # engine.create_all();\\r        print(type(df))\\r        q=\\""SELECT component,COUNT(*) AS value FROM df WHERE module ='ivm' GROUP BY component\\""\\r        result_df = sqldf(q, locals())\\r        totalusers = result_df.to_dict('records')\\r        metrictopComponent={}\\r        metrictopComponent['module_name']='ivm'\\r        metrictopComponent['metric_name']='top component'\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r        metrictopComponent['DateTime']=datetime\\r        snapshot.append(metrictopComponent)\\r        q1=\\""SELECT context,COUNT(*) AS value FROM df WHERE module ='ivm' GROUP BY context\\""\\r        result_df = sqldf(q1, locals())\\r        totalusers = result_df.to_dict('records')\\r        metrictopComponent={}\\r        metrictopComponent['module_name']='ivm'\\r        metrictopComponent['metric_name']='top questionnaire'\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r        metrictopComponent['DateTime']=datetime\\r        snapshot.append(metrictopComponent)\\r        q2=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='ivm' AND events LIKE '%Survey Saved%' GROUP BY context\\""\\r        result_df = sqldf(q2, locals())\\r        totalusers = result_df.to_dict('records')\\r        metrictopComponent={}\\r        metrictopComponent['module_name']='ivm'\\r        metrictopComponent['metric_name']='questionnaires saved'\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r        metrictopComponent['DateTime']=datetime\\r        snapshot.append(metrictopComponent)\\r        q3=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='ivm' AND events LIKE '%Survey Submitted%' GROUP BY context\\""\\r        result_df = sqldf(q3, locals())\\r        totalusers = result_df.to_dict('records')\\r        metrictopComponent={}\\r        metrictopComponent['module_name']='ivm'\\r        metrictopComponent['metric_name']='questionnaires submitted'\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r        metrictopComponent['DateTime']=datetime\\r        snapshot.append(metrictopComponent)\\r        result = {'raw_data':raw_data, \\""snapshot\\"" : snapshot}    \\r        \\r        result_param.put(snapshot)\\r        return result_param\\r ""]},""position_x"":""71"",""position_y"":""0"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""XXgCG"",""alias"":""dbs"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""dbs"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""from datetime import datetime\\rdef dbs( raw_data_param,result_param):    #python-script Data\\r\\r    print(\\""from aip\\"")\\r    if raw_data_param !='':\\r        global datetime\\r        now=datetime.now()\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r        raw_data =raw_data_param\\r        snapshot = []\\r        df=pd.DataFrame(raw_data);\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r        q=\\""SELECT 'dbs' AS module_name, 'total_dashboards' AS metric_name, COUNT(`component`) AS metric_value ,CURRENT_TIMESTAMP AS DATETIME  FROM df WHERE module='iamp-dbs'\\""\\r        result_df = sqldf(q, locals())\\r        totaldashboard= result_df.to_dict('records')[0]\\r        metrictop3DB={}\\r        metrictop3DB['module_name']='dbs'\\r        metrictop3DB['metric_name']='total_dashboards'\\r        metrictop3DB['metric_value']=totaldashboard['metric_value']\\r        metrictop3DB['DateTime']=datetime\\r        print(\\""totaldashboard\\"")\\r        print(metrictop3DB)\\r        snapshot.append(metrictop3DB)\\r        q1= \\""SELECT `component` AS Dashboard_Name, COUNT(`component`) AS counts FROM df WHERE module='iamp-dbs' GROUP BY `component` ORDER BY counts DESC LIMIT 3\\""\\r        result_df = sqldf(q1, locals())\\r        top3DB = result_df.to_dict('records')\\r        metrictop3DB={}\\r        metrictop3DB['module_name']='dbs'\\r        metrictop3DB['metric_name']='top 3 dashboard'\\r        metrictop3DB['metric_value']=json.dumps(top3DB)\\r        metrictop3DB['DateTime']=datetime\\r        snapshot.append(metrictop3DB);\\r        q3=\\""SELECT project, `component` AS Dashboard_Name, cnt FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY project ORDER BY cnt DESC) AS row_num FROM (SELECT project, `component`, COUNT(*) AS cnt FROM df WHERE module='iamp-dbs' GROUP BY project, `component`) mytab) hell WHERE row_num <= 3\\""\\r        metrictop3DB_project={}\\r        result_df = sqldf(q3, locals())\\r        top3DBbyProj = result_df.to_dict('records')\\r        metrictop3DB_project['module_name']='dbs'\\r        metrictop3DB_project['metric_name']='top 3 dashboard by project'\\r        metrictop3DB_project['metric_value']=json.dumps(top3DBbyProj)\\r        metrictop3DB_project['DateTime']=datetime\\r        snapshot.append(metrictop3DB_project);\\r        q4= \\""SELECT `component` AS Dashboard_Name, COUNT(`component`) AS counts FROM df WHERE module='iamp-dbs' GROUP BY `component` ORDER BY counts DESC\\""\\r        result_df = sqldf(q4, locals())\\r        allDB = result_df.to_dict('records')\\r        allDBs={}\\r        allDBs['module_name']='dbs'\\r        allDBs['metric_name']='all dashboards count'\\r        allDBs['metric_value']=json.dumps(allDB)\\r        allDBs['DateTime']=datetime\\r        snapshot.append(allDBs);\\r        q5= \\""SELECT `component` AS Dashboard_Name, COUNT(*) AS counts FROM df WHERE module='iamp-dbs'\\""\\r        result_df = sqldf(q5, locals())\\r        totalImpressionDBS = result_df.to_dict('records')[0]['counts']\\r        totalImpressionsDBS={}\\r        totalImpressionsDBS['module_name']='dbs'\\r        totalImpressionsDBS['metric_name']='totalImpressionsDBS'\\r        totalImpressionsDBS['metric_value']=json.dumps(totalImpressionDBS)\\r        totalImpressionsDBS['DateTime']=datetime\\r        snapshot.append(totalImpressionsDBS);\\r        result = {'rawdata':raw_data, \\""snapshot\\"" : snapshot}\\r        result_param.put(snapshot)\\r\\r        return result_param""]},""position_x"":""322"",""position_y"":""4"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""WdqUt"",""alias"":""usm"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""usm"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""\\rdef usm( raw_data_param,result_param):    #python-script Data\\r    global result\\r    print(\\""from usm\\"")\\r    if raw_data_param !='': \\r        global datetime\\r        now=datetime.now()\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r        raw_data=raw_data_param\\r        print(type(raw_data))\\r        snapshot =[]\\r        # print(raw_data)\\r        df=pd.DataFrame(raw_data);\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r        # engine.create_all();\\r        print(type(df))\\r        q=\\""SELECT 'usm' as module_name,'total_active_users' AS metric_name, COUNT(DISTINCT(user)) AS metric_value,CURRENT_TIMESTAMP as DateTime FROM df\\""\\r        result_df = sqldf(q, locals())\\r        totalusers = result_df.to_dict('records')[0]\\r        snapshot.append(totalusers)\\r        result = {'raw_data':raw_data, \\""snapshot\\"" : snapshot}    \\r        \\r        result_param.put(snapshot)\\r        return result_param\\r ""]},""position_x"":""507"",""position_y"":""2"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""iEEVq"",""alias"":""iegp"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""iegp"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""from datetime import datetime\\r\\rimport pandas as pd\\r\\rimport json\\r\\rimport sqlalchemy\\r\\rfrom pandasql import sqldf\\r\\r\\r\\rdef iegp(raw_data_param,result_param):    #python-script Data\\r\\r \\r\\r    print(\\""from iegp\\"")\\r\\r    if raw_data_param !='':\\r\\r        global datetime\\r\\r        now=datetime.now()\\r\\r        datetime_str=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r        raw_data =raw_data_param\\r\\r        snapshot = []\\r\\r        df=pd.DataFrame(raw_data)\\r        \\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r\\r\\r        # Execute First SQL query(Count Of Badge Creation)\\r\\r        q = \\""SELECT 'iegp' AS module_name, 'Badge Creation Count' AS metric_name, COUNT(`component`) AS metric_value, CURRENT_TIMESTAMP AS DATETIME FROM df WHERE module='iamp-iegp' AND context='Badge Creation'\\""\\r\\r        result_df = sqldf(q, locals())\\r\\r        Badge_creation = result_df.to_dict('records')[0]\\r\\r        metrictop3DB = {}\\r\\r        metrictop3DB['module_name'] = 'iegp'\\r\\r        metrictop3DB['metric_name'] = 'Badge Creation Count'\\r\\r        metrictop3DB['metric_value'] = Badge_creation['metric_value']\\r\\r        metrictop3DB['DateTime'] = datetime_str\\r\\r        print(\\""Badge_creation\\"")\\r\\r        print(metrictop3DB)\\r\\r        snapshot.append(metrictop3DB)\\r\\r\\r\\r        #Execute second SQL query(Top 3 component)\\r\\r        q1 = \\""SELECT `component` AS Component_Name, COUNT(`component`) AS counts FROM df WHERE module='iamp-iegp' GROUP BY `component` ORDER BY counts DESC LIMIT 3\\""\\r\\r        result_df = sqldf(q1, locals())\\r\\r        top3DB = result_df.to_dict('records')\\r\\r        metrictop3DB = {}\\r\\r        metrictop3DB['module_name'] = 'iegp'\\r\\r        metrictop3DB['metric_name'] = 'top 3 component'\\r\\r        metrictop3DB['metric_value'] = json.dumps(top3DB)\\r\\r        metrictop3DB['DateTime'] = datetime_str\\r\\r        snapshot.append(metrictop3DB)\\r\\r\\r\\r         # Execute third SQL query ( Rule Creation Count)\\r\\r        q2 = \\""SELECT 'iegp' AS module_name, 'Rule Creation Count' AS metric_name, COUNT(`component`) AS metric_value, CURRENT_TIMESTAMP AS DATETIME FROM df WHERE module='iamp-iegp' AND context='Rule Creation'\\""\\r\\r        result_df = sqldf(q2, locals())\\r\\r        Rule_creation = result_df.to_dict('records')[0]\\r\\r\\r\\r        metrictop3DB = {}\\r\\r        metrictop3DB['module_name'] = 'iegp'\\r\\r        metrictop3DB['metric_name'] = 'Rule Creation Count'\\r\\r        metrictop3DB['metric_value'] = Rule_creation['metric_value']\\r\\r        metrictop3DB['DateTime'] = datetime_str\\r\\r        print(\\""Rule_creation\\"")\\r\\r        print(metrictop3DB)\\r\\r        snapshot.append(metrictop3DB)\\r\\r\\r\\r         # Execute fourth SQL query ( Total Dashboard)\\r\\r        q3 = \\""SELECT 'iegp' AS module_name, 'Total_Dashboard' AS metric_name, COUNT(`component`) AS metric_value, CURRENT_TIMESTAMP AS DATETIME FROM df WHERE module='iamp-iegp' AND context LIKE '%Dashboard%'\\""\\r\\r    \\r\\r        result_df = sqldf(q3, locals())\\r\\r        Total_Dashboard = result_df.to_dict('records')[0]\\r\\r    \\r\\r        metrictop3DB = {}\\r\\r        metrictop3DB['module_name'] = 'iegp'\\r\\r        metrictop3DB['metric_name'] = 'Total Dashboard'\\r\\r        metrictop3DB['metric_value'] = Total_Dashboard['metric_value']\\r\\r        metrictop3DB['DateTime'] = datetime_str\\r\\r        print(\\""Total_Dashboard\\"")\\r\\r        print(metrictop3DB)\\r\\r        snapshot.append(metrictop3DB)\\r\\r\\r\\r        # Execute fifth SQL query ( Total Component)\\r\\r        q4=\\""SELECT component,COUNT(*) AS value FROM df  WHERE module ='iamp-iegp' GROUP BY component\\""\\r\\r        result_df = sqldf(q4, locals())\\r\\r        \\r\\r        Total_Users = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='iegp'\\r\\r        metrictopComponent['metric_name']='total component'\\r\\r        metrictopComponent['metric_value']=json.dumps(Total_Users)\\r\\r        metrictopComponent['DateTime']=datetime_str\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        result = {'rawdata':raw_data, \\""snapshot\\"" : snapshot}\\r\\r        result_param.put(snapshot)\\r\\r \\r\\r        return result_param""]},""position_x"":""74"",""position_y"":""110"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""wNaAi"",""alias"":""AIP_APP"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""aip_app"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""import json\\r\\rimport pandas as pd\\r\\rfrom pandasql import sqldf\\r\\rimport sqlalchemy\\r\\rfrom datetime import datetime\\r\\rdef aip_app( raw_data_param,result_param):    #python-script Data\\r\\r    print(\\""from aip-app\\"")\\r\\r    if raw_data_param !='':\\r\\r        global datetime\\r\\r        now=datetime.now()\\r\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r        raw_data =raw_data_param\\r\\r        snapshot = []\\r\\r        df=pd.DataFrame(raw_data)\\r        \\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r        #.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r\\r\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r\\r\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r      # total no of adapters\\r\\r        q = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Adapters' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'AdapterCreateEditComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%Implementation Created%'  \\r\\r            \\""\\""\\"" \\r\\r        result_df = sqldf(q, locals())  \\r\\r        totalAdapters = result_df.to_dict('records')[0]  \\r\\r        metricAdapter = {}  \\r\\r        metricAdapter['module_name'] = 'aip-app'  \\r\\r        metricAdapter['metric_name'] = 'Total Adapters'  \\r\\r        metricAdapter['metric_value'] = totalAdapters['metric_value']  \\r\\r        metricAdapter['DateTime'] = datetime  \\r\\r        print(\\""Total Adapters\\"")\\r\\r        print(metricAdapter)\\r\\r        snapshot.append(metricAdapter)\\r\\r        # total no of pipelines\\r\\r        q1 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Pipelines' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'PipelineCreateComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%pipeline created%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q1, locals())  \\r        \\r        totalPipelines = result_df.to_dict('records')[0] \\r\\r        metricPipelines = {}  \\r\\r        metricPipelines['module_name'] = 'aip-app'  \\r\\r        metricPipelines['metric_name'] = 'Total Pipelines'  \\r\\r        metricPipelines['metric_value'] = totalPipelines['metric_value']  \\r\\r        metricPipelines['DateTime'] = datetime     \\r        print(\\""Total Pipelines\\"")\\r\\r        print(metricPipelines)\\r\\r        snapshot.append(metricPipelines)\\r\\r        # total no of apps\\r\\r        q2 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Apps' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'CreateAppComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%App created%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q2, locals())  \\r\\r        totalApps = result_df.to_dict('records')[0]  \\r\\r        metricApps = {}  \\r\\r        metricApps['module_name'] = 'aip-app'  \\r\\r        metricApps['metric_name'] = 'Total Apps'  \\r\\r        metricApps['metric_value'] = totalApps['metric_value']  \\r\\r        metricApps['DateTime'] = datetime        \\r\\r        print(\\""Total Apps\\"")\\r\\r        print(metricApps)\\r\\r        snapshot.append(metricApps)\\r\\r        # total pipeline runs\\r\\r        q3 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Executed Pipelines' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'PipelineDescriptionComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%pipeline started running%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q3, locals())  \\r\\r        totalExecutedPipelines = result_df.to_dict('records')[0]  \\r\\r        metricExecutedPipelines = {}  \\r\\r        metricExecutedPipelines['module_name'] = 'aip-app'  \\r\\r        metricExecutedPipelines['metric_name'] = 'Total Executed Pipelines'  \\r\\r        metricExecutedPipelines['metric_value'] = totalExecutedPipelines['metric_value']  \\r\\r        metricExecutedPipelines['DateTime'] = datetime        \\r\\r        print(\\""Total Executed Pipelines\\"")\\r\\r        print(metricExecutedPipelines)\\r\\r        snapshot.append(metricExecutedPipelines)       \\r\\r        #total no of connections\\r\\r        q4 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Connections' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'DatasourceConfigComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%connection created successfully%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q4, locals())  \\r\\r        totalConnections = result_df.to_dict('records')[0]  \\r\\r        metricConnections = {}  \\r\\r        metricConnections['module_name'] = 'aip-app'  \\r\\r        metricConnections['metric_name'] = 'Total Connections'  \\r\\r        metricConnections['metric_value'] = totalConnections['metric_value']  \\r\\r        metricConnections['DateTime'] = datetime      \\r\\r        print(\\""Total Connections\\"")\\r\\r        print(metricConnections)\\r        \\r        snapshot.append(metricConnections)\\r\\r        # TOTAL NO OF PROMPTS\\r\\r        q5 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Prompts' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'PromptCreateComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%Prompt created%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q5, locals())  \\r\\r        totalPrompts = result_df.to_dict('records')[0]  \\r\\r        metricPrompts = {}  \\r\\r        metricPrompts['module_name'] = 'aip-app'  \\r\\r        metricPrompts['metric_name'] = 'Total Prompts'  \\r\\r        metricPrompts['metric_value'] = totalPrompts['metric_value']  \\r\\r        metricPrompts['DateTime'] = datetime     \\r\\r        print(\\""Total Prompts\\"")\\r\\r        print(metricPrompts)\\r\\r        snapshot.append(metricPrompts)\\r\\r    \\r\\r        # TOTAL no of datasets\\r\\r        q6 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Datasets' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'ModalConfigDatasetComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%Dataset Created%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q6, locals())  \\r\\r        totalDatasets = result_df.to_dict('records')[0]  \\r\\r        metricDatasets = {}  \\r\\r        metricDatasets['module_name'] = 'aip-app'  \\r\\r        metricDatasets['metric_name'] = 'Total Datasets'  \\r\\r        metricDatasets['metric_value'] = totalDatasets['metric_value']  \\r\\r        metricDatasets['DateTime'] = datetime      \\r\\r        print(\\""Total Datasets\\"")\\r\\r        print(metricDatasets)\\r\\r        snapshot.append(metricDatasets)\\r\\r        # total executed prompts\\r\\r        q7 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Executed Prompts' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'PromptEditComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%prompts started%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q7, locals())  \\r\\r        totalExecutedPrompts = result_df.to_dict('records')[0]  \\r\\r        metricExecutedPrompts = {}  \\r\\r        metricExecutedPrompts['module_name'] = 'aip-app'  \\r\\r        metricExecutedPrompts['metric_name'] = 'Total Executed Prompts'  \\r\\r        metricExecutedPrompts['metric_value'] = totalExecutedPrompts['metric_value']  \\r\\r        metricExecutedPrompts['DateTime'] = datetime      \\r\\r        print(\\""Total Executed Prompts\\"")\\r\\r        print(metricExecutedPrompts)\\r\\r        snapshot.append(metricExecutedPrompts)\\r\\r        #  pipelines count\\r\\r        q8 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                TRIM(SUBSTR(EVENTS, (INSTR(EVENTS, 'name=')+5), INSTR(EVENTS, 'pipeline updated')-(INSTR(EVENTS, 'name=')+5))) AS Pipeline_Name,  \\r\\r                COUNT(*) AS counts   \\r\\r            FROM df   \\r\\r            WHERE module='aip-app'   \\r\\r            AND component = 'PipelineDescriptionComponent' \\r\\r            AND events LIKE '%pipeline started running%'   \\r\\r            GROUP BY Pipeline_Name   \\r\\r            ORDER BY counts DESC     \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q8, locals())  \\r\\r        top3pipelines = result_df.to_dict('records') \\r\\r        metrictoppipelines={}\\r\\r        metrictoppipelines['module_name']='aip-app'\\r\\r        metrictoppipelines['metric_name']='Pipeline Execution Summary'\\r\\r        metrictoppipelines['metric_value']=json.dumps(top3pipelines)\\r\\r        metrictoppipelines['DateTime']=datetime\\r\\r        snapshot.append(metrictoppipelines)\\r\\r    # total impressions AIP\\r        q9= \\""SELECT `component` AS Component_Name, COUNT(*) AS counts FROM df WHERE module='aip-app'\\""\\r        result_df = sqldf(q9, locals())\\r        totalImpressionsAIP = result_df.to_dict('records')[0]['counts']\\r        totalImpressionsAIP={}\\r        totalImpressionsAIP['module_name']='aip-app'\\r        totalImpressionsAIP['metric_name']='Total Impressions AIP'\\r        totalImpressionsAIP['metric_value']=json.dumps(totalImpressionsAIP)\\r        totalImpressionsAIP['DateTime']=datetime\\r        snapshot.append(totalImpressionsAIP);\\r        \\r    # total scheduled jobs\\r        q10 = \\""\\""\\""  \\r\\r            SELECT   \\r\\r                'aip-app' AS module_name,   \\r\\r                'Total Scheduled Jobs' AS metric_name,   \\r\\r                COUNT(`component`) AS metric_value,   \\r\\r                CURRENT_TIMESTAMP AS DATETIME  \\r\\r            FROM   \\r\\r                df   \\r\\r            WHERE   \\r\\r                component = 'SchedulerComponent'   \\r\\r                AND module = 'aip-app'   \\r\\r                AND events LIKE '%Scheduled job%'  \\r\\r            \\""\\""\\""\\r\\r        result_df = sqldf(q10, locals())  \\r\\r        totalScheduledJobs = result_df.to_dict('records')[0]  \\r\\r        metricScheduledJobs = {}  \\r\\r        metricScheduledJobs['module_name'] = 'aip-app'  \\r\\r        metricScheduledJobs['metric_name'] = 'Total Scheduled Jobs'  \\r\\r        metricScheduledJobs['metric_value'] = totalScheduledJobs['metric_value']  \\r\\r        metricScheduledJobs['DateTime'] = datetime      \\r\\r        print(\\""Total Scheduled Jobs\\"")\\r\\r        print(metricScheduledJobs)\\r\\r        snapshot.append(metricScheduledJobs)\\r\\r        result = {'rawdata':raw_data, \\""snapshot\\"" : snapshot}\\r\\r        result_param.put(snapshot)\\r\\r        return result_param""]},""position_x"":""267"",""position_y"":""112"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""TdrKl"",""alias"":""cap"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""cap"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""import json\\r\\r\\r\\rimport pandas as pd\\r\\r\\r\\rfrom pandasql import sqldf\\r\\r\\r\\rimport sqlalchemy\\r\\r\\r\\rfrom datetime import datetime\\r\\r\\r\\rdef cap(raw_data_param, result_param):\\r\\r\\r\\r    # Python-script Data\\r\\r\\r\\r    print(\\""from cap\\"")\\r\\r\\r\\r    if raw_data_param !='': \\r\\r\\r\\r        global datetime\\r\\r\\r\\r        now = datetime.now()\\r\\r\\r\\r        datetime = now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r\\r\\r        raw_data = raw_data_param\\r\\r\\r\\r        snapshot =[]\\r\\r\\r\\r        # print(raw_data)\\r\\r\\r\\r        df=pd.DataFrame(raw_data)\\r\\r\\r\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r\\r\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r\\r\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r\\r\\r        # engine.create_all();\\r\\r\\r\\r        q = \\""SELECT CONTEXT, COUNT(*) AS metric_value  FROM df  WHERE module ='iamp-cap' AND EVENTS LIKE '%Create Node%' GROUP BY CONTEXT;\\""\\r\\r\\r\\r        result = sqldf(q, locals())  \\r\\r\\r\\r        totalnodes = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name']='iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name']='Total CAP Nodes'\\r\\r\\r\\r        metrictopComponent['metric_value']=totalnodes['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r\\r\\r        print(totalnodes,\\""printed nodes\\"")\\r\\r\\r\\r        print(result,\\""printed nodes result\\"")\\r\\r\\r\\r        print(\\""No of Nodes\\"")\\r\\r\\r\\r        print(metrictopComponent)\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r\\r\\r\\r\\r        # Execute second SQL query (Bots)\\r\\r\\r\\r        q1 = \\""SELECT CONTEXT, COUNT(*) AS metric_value FROM df  WHERE module ='iamp-cap' AND EVENTS LIKE '%Created Bot For%' GROUP BY CONTEXT;\\""\\r\\r\\r\\r        result = sqldf(q1, locals())     \\r\\r\\r\\r        totalbots = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name'] = 'iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name'] = 'Total CAP Bots'\\r\\r\\r\\r        metrictopComponent['metric_value'] = totalbots['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r\\r\\r        print(totalbots,\\""printed bots\\"")\\r\\r\\r\\r        print(result,\\""printred bots result\\"")\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        # Execute third SQL query (Execute)\\r\\r\\r\\r        q2 = \\""SELECT CONTEXT, COUNT(*) AS metric_value FROM df WHERE module ='iamp-cap' AND EVENTS LIKE '%Executed%' GROUP BY CONTEXT;\\""\\r\\r\\r\\r\\r\\r\\r\\r        result = sqldf(q2, locals())\\r\\r\\r\\r        totalprocesswf = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name'] = 'iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name'] = 'Workflow Execution'\\r\\r\\r\\r        metrictopComponent['metric_value'] = totalprocesswf['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r\\r\\r        print(\\""Cap process workflow\\"")\\r\\r\\r\\r        print(totalprocesswf,\\""printed cap process wf\\"")\\r\\r\\r\\r        print(result,\\""printed wf cap result\\"")\\r\\r\\r\\r        print(metrictopComponent)\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        # Execute fourth SQL query ( Total List)\\r\\r\\r\\r        q3 =  \\""SELECT CONTEXT, COUNT(*) AS metric_value FROM df WHERE module ='iamp-cap' AND EVENTS LIKE '%Create workflow%' GROUP BY CONTEXT;\\""\\r\\r        #q3 = \\""SELECT CONTEXT,COUNT(*) AS num_workflows_created FROM df WHERE module = 'iamp-cap' AND EVENTS LIKE '%Create workflow%' GROUP BY CONTEXT,strftime(lastUpdatedDate, '%Y-%m-01') ORDER BY strftime(lastUpdatedDate, '%Y-%m-01');\\""\\r\\r\\r\\r       \\r\\r\\r\\r        result = sqldf(q3, locals())  \\r\\r\\r\\r        totalworkflows = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name'] = 'iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name'] = 'Total Workflows'\\r\\r\\r\\r        metrictopComponent['metric_value'] = totalworkflows['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r\\r\\r        print(\\""Total no of cap workflows\\"")\\r\\r\\r\\r        print(totalworkflows,\\""printed cap workflows\\"")\\r\\r\\r\\r        print(result,\\""printed cap wf result\\"")\\r\\r\\r\\r        print(metrictopComponent)\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        q4 = \\""SELECT CONTEXT, COUNT(*) AS metric_value FROM df WHERE module ='iamp-cap' AND EVENTS LIKE '%RESOLVER%' GROUP BY CONTEXT;\\""\\r\\r        \\r\\r        result = sqldf(q4, locals())  \\r\\r\\r\\r        totalresolverworkflows = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name'] = 'iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name'] = 'Resolver Workflows'\\r\\r\\r\\r        metrictopComponent['metric_value'] = totalresolverworkflows['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r\\r\\r        print(\\""Total no of resolver cap workflows\\"")\\r\\r\\r\\r        print(totalresolverworkflows,\\""printed res cap workflows\\"")\\r\\r\\r\\r        print(result,\\""printed cap res wf result\\"")\\r\\r\\r\\r        print(metrictopComponent)\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        q5 = \\""SELECT CONTEXT, COUNT(*) AS metric_value FROM df WHERE module ='iamp-cap' AND EVENTS LIKE '%EXTRACTOR%' GROUP BY CONTEXT;\\""\\r\\r        \\r\\r        result = sqldf(q5, locals())  \\r\\r\\r\\r        totalextractorworkflows = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name'] = 'iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name'] = 'Extractor Workflows'\\r\\r\\r\\r        metrictopComponent['metric_value'] = totalextractorworkflows['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r\\r\\r        print(\\""Total no of extractor cap workflows\\"")\\r\\r\\r\\r        print(totalresolverworkflows,\\""printed extractor cap workflows\\"")\\r\\r\\r\\r        print(result,\\""printed cap extractor wf result\\"")\\r\\r\\r\\r        print(metrictopComponent)\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        q6 = \\""SELECT CONTEXT, COUNT(*) AS metric_value  FROM df WHERE module ='iamp-cap' AND EVENTS LIKE '%CLASSIFIER%' GROUP BY CONTEXT;\\""\\r\\r        \\r\\r        result = sqldf(q6, locals())  \\r\\r\\r\\r        totalclassifierworkflows = result.to_dict('records')[0]\\r\\r\\r\\r        metrictopComponent = {}\\r\\r\\r\\r        metrictopComponent['module_name'] = 'iamp-cap'\\r\\r\\r\\r        metrictopComponent['metric_name'] = 'Classifier Workflows'\\r\\r\\r\\r        metrictopComponent['metric_value'] = totalclassifierworkflows['metric_value']\\r\\r\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r\\r\\r        print(\\""Total no of classifier cap workflows\\"")\\r\\r\\r\\r        print(totalclassifierworkflows,\\""printed classifier cap workflows\\"")\\r\\r\\r\\r        print(result,\\""printed cap classifier wf result\\"")\\r\\r\\r\\r        print(metrictopComponent)\\r\\r\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        result = {'rawdata': raw_data, \\""snapshot\\"": snapshot}\\r\\r\\r\\r        result_param.put(snapshot)\\r\\r\\r\\r        return result_param\\r\\r\\r\\r\\r\\r""]},""position_x"":""455"",""position_y"":""120"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""TywMc"",""alias"":""grh"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""grh"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""def grh( raw_data_param,result_param):    #python-script Data\\r\\r\\r\\r    global result\\r\\r\\r\\r    print(\\""from grh\\"")\\r\\r\\r\\r    if raw_data_param !='': \\r\\r\\r\\r        global datetime\\r\\r\\r\\r        now=datetime.now()\\r\\r\\r\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r\\r\\r        raw_data=raw_data_param\\r\\r\\r\\r        print(type(raw_data))\\r\\r\\r\\r        snapshot =[]\\r\\r\\r\\r        # print(raw_data)\\r\\r\\r\\r        df=pd.DataFrame(raw_data);\\r\\r\\r\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r\\r\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r\\r\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r\\r\\r        # engine.create_all();\\r\\r\\r\\r        print(type(df))\\r\\r\\r\\r        # q=\\""SELECT 'grh' as module_name,'total_active_users' AS metric_name, COUNT(DISTINCT(user)) AS metric_value,CURRENT_TIMESTAMP as DateTime FROM df\\""\\r\\r\\r\\r        # result_df = sqldf(q, locals())\\r\\r\\r\\r        # totalusers = result_df.to_dict('records')[0]\\r\\r\\r\\r        # snapshot.append(totalusers);\\r\\r        \\r\\r        q= \\""SELECT COUNT(`component`) AS metric_value FROM df WHERE module='iamp-grh' AND COMPONENT='ActiveNode'\\""\\r\\r        result_df = sqldf(q, locals())\\r\\r        totalImpressions = result_df.to_dict('records')[0]\\r\\r        print(\\""kg\\"",totalImpressions)\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='grh'\\r\\r        metrictopComponent['metric_name']='GRH Daily Impressions'\\r\\r        # metrictopComponent['metric_value']=json.dumps(totalImpressions)\\r\\r        metrictopComponent['metric_value']=totalImpressions['metric_value']\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        \\r\\r        \\r\\r        q=\\""SELECT COUNT(`component`) as metric_value FROM df WHERE module = 'iamp-grh' AND component = 'Nodes'\\""\\r\\r        result_df = sqldf(q, locals())\\r\\r        totalusers = result_df.to_dict('records')[0]\\r\\r        print(\\""totalnodesexecuted\\"",totalusers)\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='grh'\\r\\r        metrictopComponent['metric_name']='TotalNodesExecuted'\\r\\r        metrictopComponent['metric_value']=totalusers['metric_value']\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r        result = {'raw_data':raw_data, \\""snapshot\\"" : snapshot}    \\r\\r\\r\\r        \\r\\r\\r\\r        result_param.put(snapshot)\\r\\r\\r\\r        return result_param""]},""position_x"":""695"",""position_y"":""0"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""vXAMj"",""alias"":""eda"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""eda"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""import pandas as pd\\r\\rfrom pandasql import sqldf\\r\\rimport sqlalchemy\\r\\rfrom datetime import datetime\\r\\rdef eda(raw_data_param,result_param):    #python-script Data\\r\\r    global result\\r\\r    if raw_data_param !='': \\r\\r        global datetime\\r\\r        now=datetime.now()\\r\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r        raw_data=raw_data_param\\r\\r        print(type(raw_data))\\r\\r        snapshot =[]\\r\\r        # print(raw_data)\\r\\r        df=pd.DataFrame(raw_data);\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r        # engine.create_all();\\r\\r        print(type(df))\\r\\r        q=\\""SELECT component,COUNT(*) AS value FROM df WHERE module ='eda' GROUP BY component\\""\\r\\r        result_df = sqldf(q, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='eda'\\r\\r        metrictopComponent['metric_name']='top component'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q1=\\""SELECT context,COUNT(*) AS value FROM df WHERE module ='eda' GROUP BY context\\""\\r\\r        result_df = sqldf(q1, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='eda'\\r\\r        metrictopComponent['metric_name']='top context'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q2=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='eda' AND events LIKE '%Event Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q2, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='eda'\\r\\r        metrictopComponent['metric_name']='event created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q3=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='eda' AND events LIKE '%Workflows Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q3, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='eda'\\r\\r        metrictopComponent['metric_name']='workflow created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q4=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='eda' AND events LIKE '%Tryout action is success%' GROUP BY context\\""\\r\\r        result_df = sqldf(q4, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='eda'\\r\\r        metrictopComponent['metric_name']='Tryout action is success'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        result = {'raw_data':raw_data, \\""snapshot\\"" : snapshot}    \\r\\r        \\r\\r        result_param.put(snapshot)\\r\\r        return result_param\\r\\r ""]},""position_x"":""645"",""position_y"":""123"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""xLkLn"",""alias"":""sre"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""sre"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""import pandas as pd\\r\\rfrom pandasql import sqldf\\r\\rimport sqlalchemy\\r\\rfrom datetime import datetime\\r\\rdef sre(raw_data_param,result_param):    #python-script Data\\r\\r    global result\\r\\r    if raw_data_param !='': \\r\\r        global datetime\\r\\r        now=datetime.now()\\r\\r        datetime=now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r        raw_data=raw_data_param\\r\\r        print(type(raw_data))\\r\\r        snapshot =[]\\r\\r        # print(raw_data)\\r\\r        df=pd.DataFrame(raw_data);\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r        # engine.create_all();\\r\\r        print(type(df))\\r\\r        q=\\""SELECT component,COUNT(*) AS value FROM df WHERE module ='sre' GROUP BY component\\""\\r\\r        result_df = sqldf(q, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='top component'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q1=\\""SELECT context,COUNT(*) AS value FROM df WHERE module ='sre' GROUP BY context\\""\\r\\r        result_df = sqldf(q1, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='top context'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q2=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='sre' AND events LIKE '%SreProbeLocation Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q2, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='SreProbeLocation Created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q3=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='sre' AND events LIKE '%Probe Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q3, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='Probe Created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q4=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='sre' AND events LIKE '%SreLocation Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q4, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='SreLocation Created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q5=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='sre' AND events LIKE '%SreApp Downtime Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q5, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='SreApp Downtime Created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q6=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='sre' AND events LIKE '%App Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q6, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='App Created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        q7=\\""SELECT context, COUNT(*) as value FROM df WHERE module ='sre' AND events LIKE '%Application Group Created%' GROUP BY context\\""\\r\\r        result_df = sqldf(q7, locals())\\r\\r        totalusers = result_df.to_dict('records')\\r\\r        metrictopComponent={}\\r\\r        metrictopComponent['module_name']='sre'\\r\\r        metrictopComponent['metric_name']='Application Group Created'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalusers)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        snapshot.append(metrictopComponent)\\r\\r        result = {'raw_data':raw_data, \\""snapshot\\"" : snapshot}    \\r\\r        \\r\\r        result_param.put(snapshot)\\r\\r        return result_param\\r\\r ""]},""position_x"":""837"",""position_y"":""120"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]},{""id"":""WzLBn"",""alias"":""cas"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""cas"",""requirements"":"""",""params"":[{""name"":""raw_data"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""result"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""script"":[""import json\\r\\rimport pandas as pd\\r\\rfrom pandasql import sqldf\\r\\rimport sqlalchemy\\r\\rfrom datetime import datetime\\r\\r \\r\\rdef cas(raw_data_param, result_param): # Python-script Data\\r\\r \\r\\r    print(\\""from cas\\"")\\r\\r    if raw_data_param != '':\\r\\r        global datetime\\r\\r        now = datetime.now()\\r\\r        datetime = now.strftime(\\""%Y-%m-%d %H:%M:%S\\"")\\r\\r        raw_data = raw_data_param\\r\\r        snapshot =[]\\r\\r        # print(raw_data)\\r\\r        df=pd.DataFrame(raw_data);\\r        df.columns=['id','url','portfolioId','portfolio','projectId','project','userId','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r        #df.columns=['id','url','portfolio','project','user','role','version','module','component','context','events','startTime','endTime','duration','spanId','traceId','lastUpdatedDate']\\r\\r      #  df.columns=['id' ,'eid'  ,'ets' ,'ver'  ,'mid'  ,'actor_id'  ,'actor_type'  ,'context_channel'  ,'context_pdata_id'  ,'context_pdata_ver'  ,'context_pdata_pid'  ,'object_id'  ,'object_ver'  ,'edata_type'  ,'edata_id'  ,'edata_pageid'  ,'edata_subType'  ,'edata_stageto'  ,'edata_duration'  ,'ets_datetime'  ,'json_data' ,'edata_state_user_login'  ,'edata_state_user_email'  ,'edata_state_onboarded'  ,'edata_state_activated'  ,'edata_state_user_act_ind'  ,'edata_state_user_f_name'  ,'edata_state_user_l_name'  ,'edata_state_force_password_change'  ,'edata_state_country'  ,'edata_state_timezone'  ,'edata_state_contact_number'  ,'edata_state_isUiInactivityTracked'  ,'edata_prevstate']\\r\\r        engine = sqlalchemy.create_engine('sqlite:///:memory:')\\r\\r        # engine.create_all();\\r\\r        q = \\""SELECT CONTEXT, COUNT(*) AS VALUE FROM df WHERE module ='cas' AND EVENTS LIKE '%Savedwf%' GROUP BY CONTEXT\\""\\r\\r        result = sqldf(q, locals())\\r\\r        \\r\\r        totalbpmnwf = result.to_dict('records')\\r\\r        metrictopComponent = {}\\r\\r        metrictopComponent['module_name']='cas'\\r\\r        metrictopComponent['metric_name']='top bpmn workflows'\\r\\r        metrictopComponent['metric_value']=json.dumps(totalbpmnwf)\\r\\r        metrictopComponent['DateTime']=datetime\\r\\r        print(\\""BPMN workflow\\"")\\r\\r        print(metrictopComponent)\\r\\r        snapshot.append(metrictopComponent)\\r\\r \\r\\r        # Execute second SQL query (Context)\\r\\r        q1 = \\""SELECT CONTEXT, COUNT(*) AS VALUE FROM df WHERE module ='cas' AND EVENTS LIKE '%Saved Micro Bot%' GROUP BY CONTEXT\\""\\r\\r        result = sqldf(q1, locals())\\r\\r       \\r\\r        totalbots = result.to_dict('records')\\r\\r        metrictopComponent = {}\\r\\r        metrictopComponent['module_name'] = 'cas'\\r\\r        metrictopComponent['metric_name'] = 'top micro bots'\\r\\r        metrictopComponent['metric_value'] = json.dumps(totalbots)\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r        print(\\""Total num of bots\\"")\\r\\r        snapshot.append(metrictopComponent)\\r\\r \\r\\r        # Execute third SQL query ( Creation)\\r\\r        q2 = \\""SELECT context, COUNT(*) as value FROM df WHERE module ='cas' AND events LIKE '%Saved%' GROUP BY context\\""\\r\\r        result = sqldf(q2, locals())\\r\\r      \\r\\r        totalwf = result.to_dict('records')\\r\\r        metrictopComponent = {}\\r\\r        metrictopComponent['module_name'] = 'cas'\\r\\r        metrictopComponent['metric_name'] = 'Saved'\\r\\r        metrictopComponent['metric_value'] = json.dumps(totalwf)\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r        print(\\""saved workflows\\"")\\r\\r        print(metrictopComponent)\\r\\r        snapshot.append(metrictopComponent)\\r\\r \\r\\r        # Execute third SQL query ( Total List)\\r\\r        q3 = \\""SELECT CONTEXT, COUNT(*) AS VALUE FROM df WHERE module ='cas' AND EVENTS LIKE '%Triggered%' GROUP BY CONTEXT\\""\\r\\r        result = sqldf(q3, locals())\\r\\r     \\r\\r        totaltriggeredwf = result.to_dict('records')\\r\\r        metrictopComponent = {}\\r\\r        metrictopComponent['module_name'] = 'cas'\\r\\r        metrictopComponent['metric_name'] = 'Triggered'\\r\\r        metrictopComponent['metric_value'] = json.dumps(totaltriggeredwf)\\r\\r        metrictopComponent['DateTime'] = datetime\\r\\r        print(\\""Triggered workflow\\"")\\r\\r        print(metrictopComponent)\\r\\r        snapshot.append(metrictopComponent)\\r\\r\\r\\r         # Execute fourth SQL query (Edit)\\r\\r       # q2 = \\""SELECT context, COUNT(*) as value FROM df WHERE module ='cas' AND events LIKE '%Edited%' GROUP BY context\\""\\r\\r        # result = sqldf(q2, locals())\\r\\r      \\r\\r        # totalusers = result.to_dict('records')\\r\\r        # metrictopComponent = {}\\r\\r        # metrictopComponent['module_name'] = 'cas'\\r\\r        # metrictopComponent['metric_name'] = 'Saved'\\r\\r        # metrictopComponent['metric_value'] = json.dumps(totalusers)\\r\\r        # metrictopComponent['DateTime'] = datetime\\r\\r        # print(\\""Creation\\"")\\r\\r        # print(metrictopComponent)\\r\\r        # snapshot.append(metrictopComponent)\\r\\r \\r\\r        \\r\\r        result = {'rawdata': raw_data, \\""snapshot\\"": snapshot}\\r\\r        result_param.put(snapshot)\\r\\r \\r\\r        return result_param\\r\\r   \\r\\r \\r\\r""]},""position_x"":""870"",""position_y"":""8"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[]}],""pipeline_attributes"":[],""environment"":[],""default_runtime"":""{\\""dsAlias\\"":\\""LocalCluster\\"",\\""dsName\\"":\\""LEALCLCL12132\\"",\\""type\\"":\\""REMOTE\\""}""}","admin","Daily new snapshot- Open Telemetry","2025-02-17 14:53:59","LEONWSNP71149","leo1311","DragNDropLite","776",NULL,"pipeline","{""776"":{""taskId"":""a8ade220-c0e8-4bd6-9bbf-219313f8f530""}}","\0","0"
"admin","2025-02-03 10:18:27.255000","\0","",NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEOONCTS10308_leo1311.py""],""arguments"":[{""name"":""dataset"",""value"":""LEOSNPSH41456"",""type"":""Dataset"",""alias"":""LEOSNPSH41456"",""index"":""1""}],""dataset"":[]}}]}","admin","OneIcets Pipeline Montly Aggregation","2025-02-17 11:57:21","LEOONCTS10308","leo1311","NativeScript","89",NULL,"pipeline","{""86"":{""taskId"":""64eb9cc8-494f-4380-b003-fdb6ffdabe56""}}","\0","0"
demouser,"2025-05-26 09:03:03.389000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":["LEOLNRR86498_leo1311.py"],""arguments"":[{""name"":""dataset"",""value"":""LEOLNRRG50793"",""type"":""Dataset"",""alias"":""LEOLNRRG50793"",""index"":""1""}],""dataset"":[]}}]}",demouser,Sample-Linear-Regresssion,"2025-05-26 11:01:27",LEOLNR-R86498,leo1311,NativeScript,17,NULL,pipeline,"{"17":{}}",0,0
demouser,"2025-05-26 11:02:21.737000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":["LEOSMPL65915_leo1311.py"],""arguments"":[],""dataset"":[]}}]}",demouser,Sample-Infer,"2025-05-28 08:04:04",LEOSMPL-65915,leo1311,NativeScript,8,NULL,pipeline,"{"8":{}}",0,0
demouser,"2025-05-26 11:11:59.601000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":["LEOSMPL56635_leo1311.py"],""arguments"":[],""dataset"":[]}}]}",demouser,Sample-GradIO,"2025-05-28 07:54:49",LEOSMPL-56635,leo1311,NativeScript,7,NULL,pipeline,"{"7":{}}",0,0
demouser,"2025-05-26 11:21:39.428000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEOSMPLL19634_leo1311.py""],""arguments"":[{""name"":""App"",""value"":""Sample Linear Gradio""}],""dataset"":[]}}]}",demouser,"Sample Gradio","2025-05-28 08:15:25",LEOSMPLL19634,leo1311,App,1,NULL,App,NULL,0,0
demouser,"2025-05-28 07:46:16.626000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":["LEOSMPL68488_leo1311.py"],""arguments"":[{""name"":""dataset"",""value"":""LEOSMPL_14585"",""type"":""Dataset"",""alias"":""LEOSMPL_14585"",""index"":""1""}],""dataset"":[]}}]}",demouser,Sample-Classification-Training,"2025-05-28 07:47:14",LEOSMPL-68488,leo1311,NativeScript,0,NULL,pipeline,"{"0":{}}",0,0
demouser,"2025-05-28 08:07:05.625000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEOSMPL18776_leo1311.py""],""arguments"":[],""dataset"":[]}}]}",demouser,sample-streamlit,"2025-05-28 08:13:42",LEOSMPL-18776,leo1311,NativeScript,4,NULL,pipeline,"{"4":{}}",0,0
"demouser","2025-05-28 08:15:03.121000","0","","NULL","{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEOSMPL_87385_leo1311.py""],""arguments"":[{""name"":""App"",""value"":""Sample_streamlit""}],""dataset"":[]}}]}","demouser","Sample_streamlit","2025-05-28 08:15:03","LEOSMPL_87385","leo1311","App","0","NULL","App","NULL","0","0"
demouser,"2025-06-11 05:10:30.797000",0,,NULL,"{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEORGSCS58369_leo1311.py""],""arguments"":[{""name"":""dataset"",""value"":""LEORG-SC86772"",""type"":""Dataset"",""alias"":""LEORG-SC86772"",""index"":""1""}],""dataset"":[]}}]}",demouser,ragusecase,"2025-06-16 10:02:20",LEORGSCS58369,leo1311,NativeScript,46,NULL,pipeline,"{{""46"":{{}}}}",0,0
"demouser","2025-06-16 10:32:23.549000","0","","NULL","{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEORG_SC89080_leo1311.py""],""arguments"":[{""name"":""App"",""value"":""RAG_usecase_gradeio""}],""dataset"":[]}}]}","demouser","RAG_usecase_gradeio","2025-06-16 10:32:23","LEORG_SC89080","leo1311","App","1","NULL","App","NULL","0","0"





