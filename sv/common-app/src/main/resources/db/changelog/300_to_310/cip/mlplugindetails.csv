type,org,plugindetails,pluginname
"Chatbot","Core","{""formats"":{},""classname"":""Tools"",""name"":""Tools"",""parentCategory"":"""",""alias"":""Tools"",""attributes"":{},""id"":""LEOTLSBM21587"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOTLSBM21587"
"Chatbot","Core","{""formats"":{},""classname"":""Common Nodes"",""name"":""Common Nodes"",""parentCategory"":"""",""alias"":""Common Nodes"",""attributes"":{},""id"":""LEOCMNND34003"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Common Nodes"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOCMNND34003"
"Chatbot","Core","{""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""classname"":""PythonScriptConfig"",""name"":""Python Script"",""parentCategory"":""LEOCMNND34003"",""alias"":""Python Script"",""attributes"":{""FunctionName"":""PythonScript"",""requirements"":"""",""params"":"""",""script"":""def PythonScript():     return \\""Done\\""""},""id"":""LEOPYTHN77386"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""BaseConfig"",""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""]}","LEOPYTHN77386"
"Chatbot","Core","{""formats"":{""query"":""text""},""classname"":""tavily_tool"",""name"":""Trivaly"",""parentCategory"":""LEOSRCHY97728"",""alias"":""Trivaly"",""id"":""LEOTVLY_16784"",""codeGeneration"":{""requirements"":[""langchain==0.1.16""],""imports"":[""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""import requests, json, os""],""script"":""# Define a Pydantic model for the tool's input parameters\\r\\nclass SearchInput(BaseModel):\\r\\n    query: str = Field(description='should be a search query')\\r\\n\\r\\n@tool(\\""tavily_tool\\"", args_schema=SearchInput, return_direct=True)\\r\\ndef tavily_tool(query: str) -> str:\\r\\n    '''\\r\\n    Searches for information on the internet using Tavily's search API and returns the results.\\r\\n    Args:\\r\\n        query (str): The search query\\r\\n    return: The search results\\r\\n    '''\\r\\n    try:\\r\\n        logger.info(f'tavily_tool method called with query = {query}')\\r\\n        base_url = 'https://api.tavily.com/search'\\r\\n        api_key = os.environ.get('app_tavily_api_key')\\r\\n        headers = {'Content-Type': 'application/json'}\\r\\n        data = {\\r\\n                'query': query,\\r\\n                'search_depth': 'advanced',\\r\\n                'api_key': api_key,\\r\\n            }\\r\\n        response = requests.post(base_url, data=json.dumps(data), headers=headers,verify=False)\\r\\n        if response.status_code == 200:\\r\\n            status_message = response.text\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[]\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            status_message = \\""Some error occured while fetching the data\\""\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[]\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        #return response.text\\r\\n    except Exception as ex:\\r\\n        logger.info(f'Exception occured in Tavily Tool = {ex}')\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""None""}}","LEOTVLY_16784"
"Chatbot","Core","{""formats"":{""port"":""text"",""script"":""textarea""},""classname"":""FlaskApp"",""name"":""FlaskApp"",""parentCategory"":""LEOCMNND34003"",""alias"":""FlaskApp"",""id"":""LEOFLSKP57879"",""codeGeneration"":{""requirements"":[""flask""],""imports"":[""from flask import Flask"",""from flask_cors import CORS""],""script"":""def FlaskApp(port_param=5000,script_param=''):\\n    app.run(debug=False, host='0.0.0.0', port = port_param)\\n\\n""},""category"":""FlaskApp"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""port"":"""",""script"":""""}}","LEOFLSKP57879"
"Chatbot","Core","{""formats"":{""script"":""textarea""},""classname"":""PythonClass"",""name"":""PythonClass"",""parentCategory"":""LEOCMNND34003"",""alias"":""PythonClass"",""attributes"":{""script"":""""},""id"":""LEOPYTHN23924"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""PythonClass"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""]}","LEOPYTHN23924"
"Chatbot","Core","{""formats"":{""query"":""text""},""classname"":""web_scrapper"",""name"":""Web Scrapper"",""parentCategory"":""LEOSRCHY97728"",""alias"":""Web Scrapper"",""id"":""LEOWB_SC54827"",""codeGeneration"":{""requirements"":[""bs4"",""langchain"",""langchain-openai"",""flask_cors""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""import requests"",""from bs4 import BeautifulSoup"",""from langchain_text_splitters import RecursiveCharacterTextSplitter"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""import re""],""script"":""llm = AzureChatOpenAI(    \\n        deployment_name='gpt-4',\\n        model_name='gpt-4',\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\n        openai_api_version = '2023-03-15-preview',\\n        azure_endpoint='https://azureft.openai.azure.com/',\\n        openai_api_type='azure',        \\n        streaming=True,\\n        verbose=True\\n        )\\n\\nclass WebScrapperTool(BaseModel):\\n    query: str = Field(..., description='The query to ask the model from website content.')\\n\\n@tool('web_scrapper', args_schema=WebScrapperTool, return_direct=True)\\ndef web_scrapper(query: str):\\n    \\""\\""\\""\\n    This tool will be used to scrape the content from the top 3 websites and then summarize the content.\\n    parameters:\\n        - Topic: The topic of the websites to be scraped.\\n        - query: The query to ask the model.\\n    returns:\\n        - A summary based on the topic.\\n    \\""\\""\\""\\n    try:\\n        encoded_topic = \\""+\\"".join(query.split())\\n        url = f'https://www.google.com/search?q={encoded_topic}'\\n        response = requests.get(url, verify=False)\\n        if response.status_code == 200:\\n            soup = BeautifulSoup(response.text, 'html.parser')\\n            search_results = soup.find_all('a')\\n            top_urls = []\\n            for results in search_results:\\n                href = results.get('href')\\n                if href.startswith('/url?q='):\\n                    url = href.split('/url?q=')[-1].split('&')[0]\\n                    top_urls.append(url)\\n                    if len(top_urls) == 5:\\n                        break\\n            top_urls = top_urls[2:]\\n            \\n        logger.info(top_urls)\\n        all_scraped_text = \\""\\""\\n        for url in top_urls:\\n            response = requests.get(url, verify=False)\\n            soup = BeautifulSoup(response.text, 'html.parser')\\n            paragraph = ' '.join([re.sub(r'[^\\\\x00-\\\\x7F]+', ' ', p.text.strip()) for p in soup.find_all(['p'])])\\n            code_snippets = ' '.join([re.sub(r'[^\\\\x00-\\\\x7F]+', ' ', code.text.strip()) for code in soup.find_all(['code'])])\\n            all_text = paragraph + ' ' + code_snippets\\n            all_scraped_text += all_text\\n        logger.info(\\""All the text has been scrapped...\\"")\\n        return all_scraped_text\\n    except Exception as e:\\n        print('Got an exception in web_scrapper tool:', e)\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""None""}}","LEOWB_SC54827"
"Chatbot","Core","{""formats"":{""url"":""text"",""query"":""text"",""topic"":""text""},""classname"":""youtube_scrapper"",""name"":""YouTube Scrapper"",""parentCategory"":""LEOSRCHY97728"",""alias"":""YouTube Scrapper"",""id"":""LEOYTB_S41993"",""codeGeneration"":{""requirements"":[""youtube-transcript-api"",""langchain-openai""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""from langchain_community.document_loaders import YoutubeLoader"",""from langchain.text_splitter import CharacterTextSplitter"",""import requests"",""from langchain.tools import tool"",""from pydantic import BaseModel, Field""],""script"":""llm = AzureChatOpenAI(    \\r\\n        deployment_name='gpt-4',\\r\\n        model_name='gpt-4',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version = '2023-03-15-preview',\\r\\n        azure_endpoint='https://azureft.openai.azure.com/',\\r\\n        openai_api_type='azure',        \\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n\\r\\nclass YoutubeScrapperTool(BaseModel):\\r\\n    topic: str = Field(..., description='The Topic of the websites.')\\r\\n    query: str = Field(..., description='The query to ask the model')\\r\\n\\r\\n@tool('youtube_scrapper', args_schema=YoutubeScrapperTool, return_direct=True)\\r\\ndef youtube_scrapper(topic: str, query: str):\\r\\n    \\""\\""\\""\\r\\n    This tool will be used to scrape transcripts from all the videos related to the topic and then summarize the content.\\r\\n    parameters:\\r\\n        - Topic: The topic of the videos to be scraped.\\r\\n        - query: The query to ask the model.\\r\\n    returns:\\r\\n        - A summary based on the topic.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        url = chatbot_params[\\""youtube_scrapper_url\\""]\\r\\n        youtube_api_key = os.environ.get('youtube_api_key')\\r\\n        params = {\\r\\n            'part': 'snippet',\\r\\n            'q': topic,\\r\\n            'maxResults': 3,\\r\\n            'type': 'video',\\r\\n            'key': youtube_api_key\\r\\n        }\\r\\n        splitter = CharacterTextSplitter(\\r\\n                    chunk_size=2000, \\r\\n                    chunk_overlap=50,\\r\\n                    separator=' '\\r\\n                    )\\r\\n        response = requests.get(url, params=params, verify=False)\\r\\n        if response.status_code == 200:\\r\\n            video_urls = [f\\""https://www.youtube.com/watch?v={item['id']['videoId']}\\"" for item in response.json()['items']]\\r\\n            yt_data = []\\r\\n            for url in video_urls:\\r\\n                yt_loader = YoutubeLoader.from_youtube_url(youtube_url=url)\\r\\n                yt_data.append(yt_loader.load())\\r\\n            flat_list = [document for sublist in yt_data for document in sublist]\\r\\n            yt_data_split = splitter.split_documents(flat_list)\\r\\n            chain = load_qa_chain(llm=llm, chain_type='stuff')\\r\\n            response = chain.invoke(input={'input_documents':yt_data_split, 'question':query})\\r\\n            return response['output_text']\\r\\n        else:\\r\\n            print\\r\\n            \\r\\n    except Exception as e:\\r\\n        logger.info(f\\""@@@@@@@@@@@ Exception in youtube scrapper method: {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""url"":""https://www.googleapis.com/youtube/v3/search"",""query"":""None"",""topic"":""None""}}","LEOYTB_S41993"
"Chatbot","Core","{""formats"":{""query"":""text"",""id"":""text""},""classname"":""youtube_scrapper_with_id"",""name"":""YouTube Scrapper ID"",""parentCategory"":""LEOSRCHY97728"",""alias"":""YouTube Scrapper ID"",""id"":""LEOYTB_S92340"",""codeGeneration"":{""requirements"":[""youtube-transcript-api"",""langchain-openai""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""from langchain_community.document_loaders import YoutubeLoader"",""from langchain.text_splitter import CharacterTextSplitter"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""llm = AzureChatOpenAI(    \\r\\n        deployment_name='gpt-4',\\r\\n        model_name='gpt-4',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version = '2023-03-15-preview',\\r\\n        azure_endpoint='https://azureft.openai.azure.com/',\\r\\n        openai_api_type='azure',        \\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n\\r\\nclass YoutubeScrapperWithIdTool(BaseModel):\\r\\n    id: str = Field(..., description='ID of the youtube video to scrape.')\\r\\n    query: str = Field(..., description='The query to ask the model.')\\r\\n\\r\\n@tool('youtube_scrapper_with_id', args_schema=YoutubeScrapperWithIdTool, return_direct=True)\\r\\ndef youtube_scrapper_with_id(id: str, query: str):\\r\\n    \\""\\""\\""\\r\\n    This tool will be used to scrape transcripts from all the videos related to the given id and then summarize the content.\\r\\n    parameters:\\r\\n        - id: The id of the video to be scraped.\\r\\n        - query: The query to ask the model.\\r\\n    returns:\\r\\n        - A summary based on the topic.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        splitter = CharacterTextSplitter(\\r\\n                    chunk_size=2000, \\r\\n                    chunk_overlap=50,\\r\\n                    separator=\\"" \\""\\r\\n                    )\\r\\n        yt_loader = YoutubeLoader(id)\\r\\n        yt_data = yt_loader.load()\\r\\n        yt_data_split = splitter.split_documents(yt_data)\\r\\n        chain = load_qa_chain(llm=llm, chain_type='stuff')\\r\\n        response = chain.invoke(input={'input_documents':yt_data_split, 'question':query})\\r\\n        return response['output_text']\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""@@@@@@@@@@@ Exception in youtube scrapper with id method: {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""None"",""id"":""None""}}","LEOYTB_S92340"
"Chatbot","Core","{""formats"":{""git_url"":""text"",""query"":""text""},""classname"":""git_scrapper"",""name"":""GIT Scrapper"",""parentCategory"":""LEOSRCHY97728"",""alias"":""GIT Scrapper"",""id"":""LEOGT_SC22937"",""codeGeneration"":{""requirements"":[""GitPython"",""langchain_openai""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""from langchain_community.document_loaders import GitLoader"",""from langchain.text_splitter import CharacterTextSplitter"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""llm = AzureChatOpenAI(    \\r\\n        deployment_name='gpt-4',\\r\\n        model_name='gpt-4',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version = '2023-03-15-preview',\\r\\n        azure_endpoint='https://azureft.openai.azure.com/',\\r\\n        openai_api_type='azure',        \\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n\\r\\nclass GitHubScrapperTool(BaseModel):\\r\\n    git_url: str = Field(..., description='Local Path of the git repo.')\\r\\n    query: str = Field(..., description='The query to ask the model.')\\r\\n\\r\\n@tool('git_scrapper', args_schema=GitHubScrapperTool, return_direct=True)\\r\\ndef git_scrapper(git_url: str, query: str):\\r\\n    \\""\\""\\""Summarizes the content of a GitHub repository.\\r\\n    Parameters:\\r\\n    - url (str): The URL of the GitHub repository to summarize.\\r\\n    Returns:\\r\\n    - A summary of the repository content.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        splitter = CharacterTextSplitter(\\r\\n                    chunk_size=2000, \\r\\n                    chunk_overlap=50,\\r\\n                    separator=' '\\r\\n                    )\\r\\n        path = git_url.split(\\""/\\"")[-1]\\r\\n        repo_path = f'./repos/{path}'\\r\\n        if os.path.exists(repo_path):\\r\\n            os.remove('./repos')\\r\\n            loader = GitLoader(clone_url= git_url, repo_path= repo_path, branch='main', \\r\\n                           file_filter = lambda file_path: file_path.endswith('.md')\\r\\n                           )\\r\\n            git_data = loader.load()\\r\\n            git_data_split = splitter.split_documents(git_data)\\r\\n            chain = load_qa_chain(llm=llm, chain_type=\\""stuff\\"")\\r\\n            response = chain.invoke(input={'input_documents':git_data_split, 'question':query})\\r\\n            return response['output_text']\\r\\n        else:\\r\\n            loader = GitLoader(clone_url= git_url, repo_path= repo_path, branch='main', \\r\\n                           file_filter = lambda file_path: file_path.endswith('.md')\\r\\n                           )\\r\\n            git_data = loader.load()\\r\\n            git_data_split = splitter.split_documents(git_data)\\r\\n            chain = load_qa_chain(llm=llm, chain_type='stuff')\\r\\n            response = chain.invoke(input={'input_documents':git_data_split, 'question':query})\\r\\n            return response['output_text']\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""@@@@@@@@@@@ Exception in Git scrapper method: {e} \\"")\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""git_url"":""None"",""query"":""None""}}","LEOGT_SC22937"
"Chatbot","Core","{""formats"":{},""classname"":""fetch_details_from_database"",""name"":""NL to SQL"",""parentCategory"":""LEOTLSBM21587"",""alias"":""NL to SQL"",""id"":""LEONLP_T49952"",""codeGeneration"":{""requirements"":[""langchain"",""pydantic==1.10.10"",""mysql-connector-python"",""openai"",""pandas""],""imports"":[""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""import pandas as pd"",""import mysql.connector"",""from langchain.chat_models import AzureChatOpenAI"",""from langchain.utilities import SQLDatabase"",""from langchain.chains import create_sql_query_chain"",""from langchain_core.prompts import PromptTemplate""],""script"":""# class NLP_to_SQL(BaseModel):\\n#     query: str =Field(None,description=\\""Query passed by the user\\"")\\n\\n# NLP to SQL - to fetch details from db\\n# @tool(\\""fetch_details_from_database\\"",args_schema=NLP_to_SQL, return_direct=True)\\n@tool(\\""fetch_details_from_database\\"", return_direct=True)\\ndef fetch_details_from_database(query:str)->str:\\n    \\""\\""\\""\\n    this tool is used to answer all the queries related to database, or queries askinng to fetch something from database only.\\n    Parameters:\\n        query (str): A question in natural language form which is sent by user.\\n    Returns:\\n        A  dictionary containing the relevant details fetched from the database based on the SQL query.\\n    \\""\\""\\""\\n\\n    try:\\n        \\n        mydb = mysql.connector.connect(\\n            host=os.environ.get('app_host_name'),\\n            user=os.environ.get('app_mysql_user'),\\n            password=os.environ.get('app_mysql_password'),\\n            database=os.environ.get('database_name'),\\n            port= os.environ.get('port')\\n        )\\n        logger.info(f\\""Mysql db connected sussessfully \\"")\\n        \\n        mycursor = mydb.cursor(dictionary=True)\\n        # Initializing the LLM\\n        llm = AzureChatOpenAI(\\n            deployment_name='gtp35turbo',\\n            model_name='gpt-35-turbo',\\n            openai_api_key=os.environ.get('app_openai_api_key'),\\n            openai_api_version='2023-03-15-preview',\\n            openai_api_base=os.environ.get('app_openai_api_base'),\\n            openai_api_type='azure',\\n            streaming=True,\\n            verbose=True\\n        ) \\n        def LLMresponse(query):\\n            logger.debug(f\\"" LLMresponse method called with query parameter: {query} \\"")\\n            try:\\n                connection_string=os.environ.get('app_sql_connection')\\n                table_name=os.environ.get(\\""table_name\\"")\\n                db = SQLDatabase.from_uri(connection_string,include_tables=[table_name])\\n                template = '''\\n                    Given an input question, first create a syntactically correct query to run, then look at the results of the query and return the answer and it should be {top_k} and the sql query should include \\n                    shortdescription, priority, assignedto, number, createdDate, createdby and state columns for all the query unless and untill the query wants to have some specific columns then have those only.\\n                    Use the following format:\\n                    Question: \\""Question here\\"" SQLQuery: \\""SQL Query to run\\"" SQLResult: \\""Result of the SQLQuery\\"" Answer: \\""Final answer here\\""\\n                    Only use the following tables: {table_info}.\\n                    Question: {input}\\n                    '''\\n                prompt = PromptTemplate.from_template(template)\\n                chain = create_sql_query_chain(llm, db, prompt=prompt)\\n                response = chain.invoke({\\""question\\"": query})\\n                return response\\n                \\n            except Exception as e:\\n                logger.debug(f\\""Exception in LLMresponse Method : {e}\\"")\\n    \\n        response = LLMresponse(query)\\n        mycursor.execute(response)\\n        data = mycursor.fetchall()\\n        column_names = [desc[0] for desc in mycursor.description]\\n        data_df = pd.DataFrame(data, columns=column_names)\\n        result = data_df.to_dict('records')\\n        user_prompt = '''\\n            Generate summary for each of the tickets in the given data with ticket number as the heading after that there should summary of each fields of that ticket like Short Description,Created By,Created Date,Assigned To,State.\\n            {data}\\n            Generated summary for a ticket should look like: \\n\\n            Ticket Number: INC424244\\n            \\n            Ticket Details:\\n            This ticket has short description as Process_Invoice_Id:12345, the ticket was created by admin, on Thu, 01 Oct 2020 17:11:11 GMT with high priority and it's been assigned to DAS group and the ticket is currently in open state.\\n            '''\\n        answer = getFormatedResponse(result,user_prompt=user_prompt)\\n        return answer\\n\\n    except Exception as e:\\n            logger.debug(f\\""Exception in Nl2sql Method : {e}\\"")\\n\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","LEONLP_T49952"
"Chatbot","Core","{""formats"":{},""classname"":""Ticket_Summarization"",""name"":""Ticket Summarization"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Ticket Summarization"",""id"":""LEOTCKT_32789"",""codeGeneration"":{""requirements"":[""langchain==0.1.16"",""pydantic==1.10.10"",""openai==0.28.0"",""mysql-connector-python""],""imports"":[""import mysql.connector"",""import os"",""import json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from langchain.output_parsers import ResponseSchema, StructuredOutputParser"",""from langchain.chains import LLMChain"",""from langchain.chat_models import AzureChatOpenAI"",""from langchain_core.prompts import PromptTemplate""],""script"":""# class Ticket_Summarization(BaseModel):\\r\\n#     query: str =Field(None,description=\\""Ticket number passed by the user\\"")\\r\\n\\r\\n@tool(\\""summary_about_ticket\\"", return_direct=True)\\r\\ndef summary_about_ticket(query:str)->str:\\r\\n    \\""\\""\\""\\r\\n    this tool retrieves the complete details of the ticket based on the ticket number provided, if no ticket number is provided return no ticket found, Return the exact response.\\r\\n    \\r\\n    Parameters:\\r\\n        query (str): The unique identifier or ticket number.\\r\\n    \\r\\n    Returns:\\r\\n        str: A summary or brief description of the requested ticket, or \\""No ticket found\\"" if the ticket number is not provided. \\r\\n    \\""\\""\\""\\r\\n\\r\\n    try:\\r\\n        ticket = query\\r\\n        mydb = mysql.connector.connect(\\r\\n            host=os.environ.get('app_host_name'),\\r\\n            user=os.environ.get('app_mysql_user'),\\r\\n            password=os.environ.get('app_mysql_password'),\\r\\n            database=os.environ.get('database_name'),\\r\\n            port = os.environ.get('port')\\r\\n            )\\r\\n        logger.info(f\\""Mysql db connected sussessfully \\"")\\r\\n        mycursor = mydb.cursor(dictionary=True)\\r\\n        table_name = os.environ.get(\\""table_name\\"")\\r\\n        query = \\""SELECT * FROM {0} WHERE number in ('{1}')\\"".format(table_name, query)\\r\\n        mycursor.execute(query)\\r\\n        inc_dict = mycursor.fetchall()\\r\\n    \\r\\n        llm = AzureChatOpenAI(\\r\\n            deployment_name='gtp35turbo',\\r\\n            model_name='gpt-35-turbo',\\r\\n            openai_api_key=os.environ.get('app_openai_api_key'),\\r\\n            openai_api_version='2023-03-15-preview',\\r\\n            openai_api_base=os.environ.get('app_openai_api_base'),\\r\\n            openai_api_type='azure',\\r\\n            streaming=True,\\r\\n            verbose=True\\r\\n        )\\r\\n        logger.info(f\\""Got LLM \\"")\\r\\n    \\r\\n        def get_completion(number):\\r\\n            logger.debug(f\\"" Get Completion method called with parameter ticket number: {number}\\"")\\r\\n            summary_template = \\""\\""\\""\\r\\n    \\r\\n                        given the information {information} about a ticket from I want you to generate :\\r\\n    \\r\\n                        1. Summary basing on the whole information given.\\r\\n    \\r\\n                        2. A short summary on Short description\\r\\n    \\r\\n                        3. A short summary on Resolution and worknotes\\r\\n    \\r\\n                        4. A short summary on Priority and urgency\\r\\n    \\r\\n                        5. A short summary on created_by, assignmentgroup and assignedto\\r\\n    \\r\\n                        6. A short summary on Current Status\\r\\n                        \\r\\n                        7. A short summary on SLA Timeline \\r\\n    \\r\\n                        {format_instructions}\\r\\n    \\r\\n                    \\""\\""\\""\\r\\n    \\r\\n            response_schemas = [\\r\\n    \\r\\n                ResponseSchema(name=\\""Summary\\"", description=\\""Summary basing on the whole information given.\\""),\\r\\n                 \\r\\n                ResponseSchema(name=\\""Short description\\"", description=\\""A short summary on Short Discription\\""),\\r\\n    \\r\\n                ResponseSchema(name=\\""ResolutionNotes\\"", description=\\""A short summary on Resolution and worknotes\\""),\\r\\n    \\r\\n                ResponseSchema(name=\\""Priority and Urgency\\"", description=\\""A short summary on Priority and urgency\\""),\\r\\n    \\r\\n                ResponseSchema(name=\\""Owner\\"", description=\\""A short summary on created_by, assignmentgroup and assignedto\\""),\\r\\n    \\r\\n                ResponseSchema(name=\\""current_status\\"", description=\\""A short summary on Current Status\\""),\\r\\n                \\r\\n                ResponseSchema(name=\\""SLA Timeline\\"", description=\\""A short summary on SLA Timeline\\""),\\r\\n            ]\\r\\n            \\r\\n            try:\\r\\n                \\r\\n                output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\\r\\n        \\r\\n                format_instructions = output_parser.get_format_instructions()\\r\\n        \\r\\n                summary_prompt_template = PromptTemplate(input_variables=[\\""information\\""], template=summary_template,\\r\\n        \\r\\n                                                         partial_variables={\\""format_instructions\\"": format_instructions})\\r\\n        \\r\\n                response = LLMChain(llm=llm, prompt=summary_prompt_template)\\r\\n                \\r\\n                \\r\\n                result = response.run(information=inc_dict)\\r\\n        \\r\\n                title_data = f'''JSON :\\r\\n        \\r\\n                                {inc_dict}\\r\\n        \\r\\n                            '''\\r\\n        \\r\\n                result = result[7:]\\r\\n        \\r\\n                result = result[:-3]\\r\\n                \\r\\n                result = json.loads(result)\\r\\n                summary_result = f'''Ticket Number: {ticket}, {os.linesep} Ticket Summary: {os.linesep} {result['Short description']} {result['Owner']} {result[\\""SLA Timeline\\""]} {result['Priority and Urgency']} {result['current_status']} '''\\r\\n                return summary_result\\r\\n            except Exception as e:\\r\\n                logger.debug(f\\""Exception in get_completion Method : {e}\\"")\\r\\n    \\r\\n        output = get_completion(query)\\r\\n        return output\\r\\n    except Exception as e:\\r\\n        logger.debug(f\\""Exception in ticket Summarization method: {e}\\"")\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","LEOTCKT_32789"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""get_related_tickets"",""name"":""Related Tickets A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Related Tickets A"",""id"":""LEOGT_RL62937"",""codeGeneration"":{""requirements"":[],""imports"":[""import json, requests"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""class RelatedTickets(BaseModel):\\r\\n    short_description: str =Field(None,description=\\""short description passed by the user\\"")\\r\\n\\r\\n@tool(\\""get_related_Tickets\\"",args_schema=RelatedTickets, return_direct=True)\\r\\ndef get_related_tickets(short_description:str)->str:\\r\\n    \\""\\""\\""\\r\\n    This tool is use to fetch tickets with similar short description.\\r\\n    Parameters:\\r\\n        short_description (str): The short description of a ticket.\\r\\n    Returns:\\r\\n        A list of tickets with short descriptions similar to the input short description.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        logger.info(f\\""Related Tickets method called with short Description: {short_description}\\"")\\r\\n        # REGEX_pattern = '_(.*?):.*'\\r\\n        short_description = re.sub('_(.*?):.*', r\\"" \\\\1\\"", short_description)\\r\\n        url = chatbot_params[\\""get_related_tickets_url\\""]\\r\\n        headers = {\\r\\n                    'access-token': os.environ.get('ACCESS_TOKEN'),\\r\\n                    'accept': 'application/json',\\r\\n                    'Content-Type': 'application/json',\\r\\n                }\\r\\n        json_data = {\\r\\n        \\""config\\"": {\\r\\n            \\""VectordbConfig\\"": {\\r\\n                \\""DB_Type\\"": \\""Qdrant\\"",\\r\\n                \\""query\\"": short_description,\\r\\n                \\""index_name\\"": os.environ.get('INDEX_NAME'),\\r\\n                \\""qdrant_url\\"": os.environ.get('QDRANT_URL'),\\r\\n                \\""limit\\"": 10\\r\\n            },\\r\\n            \\""EmbeddingConfig\\"": {\\r\\n                \\""embedding_type\\"": \\""AzureOpenAI\\"",\\r\\n                \\""azure_openai_api_key\\"": os.environ.get('app_openai_api_key'),\\r\\n                \\""azure_api_version\\"": \\""2023-03-15-preview\\"",\\r\\n                \\""azure_openai_endpoint\\"": os.environ.get('app_openai_api_base'),\\r\\n                \\""azure_deployment\\"": \\""openaiada2\\"",\\r\\n                \\""model_name\\"": \\""text-embedding-ada-002\\"",\\r\\n                \\""openai_api_base\\"": os.environ.get('app_openai_api_base'),\\r\\n                \\""openai_api_type\\"": \\""azure\\""\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n\\r\\n        response = requests.post(url=url,json=json_data,headers=headers,verify=False)\\r\\n        logger.info(f\\""Response Status:  {response.status_code}\\"")\\r\\n        answer = json.loads(response.text)\\r\\n        answer_data = answer[0][\\""Answer\\""]\\r\\n        # answer_list = json.loads(answer_data)\\r\\n        answer_list = []\\r\\n        for key, value in answer_data.items():\\r\\n            response_dict = {\\r\\n                'Ticket Number':value.get('number',None),\\r\\n                'Description':value.get(\\""shortdescription\\"", None),\\r\\n                'Assigned To':value.get(\\""assignedto\\"", None),\\r\\n                'Assignment Group': value.get(\\""assignmentgroup\\"",None),\\r\\n                'Category': value.get(\\""category\\"",None),\\r\\n                'Created by': value.get(\\""createdby\\"",None),\\r\\n                'Created Date': value.get(\\""createdDate\\"",None),\\r\\n                'Priority': value.get(\\""priority\\"",None),\\r\\n                'State': value.get(\\""state\\"",None)                 \\r\\n            }\\r\\n            answer_list.append(response_dict)\\r\\n            \\r\\n        user_prompt = \\""\\""\\""\\r\\n            Generate summary for each of the tickets in the given data with ticket number as the heading then the summary of that ticket. \\r\\n            {data} \\r\\n            \\""\\""\\""\\r\\n        Answer = getFormatedResponse(answer_list,user_prompt)\\r\\n        global ticket_number\\r\\n        response = {\\r\\n            'chat_system_response':Answer,\\r\\n            'type': 'Text',\\r\\n            'chat_suggestions':None\\r\\n                }\\r\\n        return json.dumps(response)\\r\\n    except Exception as e:\\r\\n        return(f\\""Exception in Related_Tickets Method: {e}\\"")\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victadpst-38:6190/ticketsrecommendation""}}","LEOGT_RL62937"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""sop_recommender_tool"",""name"":""SOPRecommender"",""parentCategory"":""LEOITSMQ55029"",""alias"":""SOPRecommender"",""id"":""LEOSP_RC17321"",""codeGeneration"":{""requirements"":[""langchain"",""pydantic==1.10.10"",""requests""],""imports"":[""import ast"",""import json"",""import requests"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""class SOP_Recommender(BaseModel):\\n    query: str =Field(None,description=\\""query passed by the user\\"")\\n@tool(\\""sop_recommender_tool\\"",args_schema=SOP_Recommender, return_direct=True)\\ndef sop_recommender_tool(query:str)->str:\\n    \\""\\""\\""\\n    This tool is use to get the SOP recommendation of ticket.\\n    Parameters:\\n        query (str): The query for SOP recommendation.\\n    Returns:\\n        SOP for resolving the ticket.\\n    \\""\\""\\""\\n    try:\\n        url = chatbot_params['SOPRecommender_url']\\n        headers = {\\n            'access-token': os.environ.get('access_token'),\\n            'accept': 'application/json',\\n            'Content-Type': 'application/json',\\n            }\\n        json_data = {\\n                        \\""config\\"": {\\n                            \\""VectorStoreConfig\\"": {\\n                                \\""DB_Type\\"": \\""Faiss\\"",\\n                                \\""query\\"": query,\\n                                \\""index_name\\"": \\""default\\"",\\n                                \\""k\\"": 10\\n                            },\\n                            \\""LLMConfig\\"": {\\n                                \\""LLM_Type\\"": \\""bedrock\\"",\\n                                \\""query\\"": query,\\n                                \\""aws_access_key_id\\"": os.environ.get('aws_access_key_id'),\\n                                \\""aws_secret_access_key\\"":os.environ.get('aws_secret_access_key'),\\n                                \\""max_tokens\\"": 4000\\n                            },\\n                            \\""EmbeddingConfig\\"": {}\\n                        }\\n                    }\\n        response = requests.post(url=url,headers=headers,json=json_data,verify=False)\\n        answer = ast.literal_eval(response.text)[0].get('Answer')\\n        response = {'chat_system_response':answer,\\n                'type': 'Text',\\n                'chat_suggestions':None,\\n                'navigate': {}\\n                }\\n        return json.dumps(response)\\n    except Exception as e:\\n        return(f\\""Exception Inside semantic_search method  {e}\\"")\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://localhost:5000""}}","LEOSP_RC17321"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""ticket_Assignment"",""name"":""Ticket Assignment"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Ticket Assignment"",""id"":""LEOTCKT_54196"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""\\r\\n@tool(\\""ticket_Assignment\\"",return_direct=True)\\r\\ndef ticket_Assignment()->str:\\r\\n    \\""\\""\\""\\r\\n    Use this tool when the user query is something related to Assign the ticket or ticket assignment, this tool doesn't require any input parameters.\\r\\n\\r\\n    Returns:\\r\\n        Tickets Assigned succesfully or not \\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        url = chatbot_params[\\""ticket_Assignment_url\\""]\\r\\n        json_data = {}\\r\\n        response = requests.post(url=url,json=json_data,verify=False)\\r\\n        print(f\\""response status, {response.status_code}, {response.text}\\"")\\r\\n        if response.status_code == 200:\\r\\n            response = {\\r\\n                'chat_system_response':\\""Tickets Assigned succesfully\\"",\\r\\n                'type': 'Text',\\r\\n                'chat_suggestions':[]\\r\\n            }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            return \\""Error occured while assigning tickets\\""\\r\\n            \\r\\n    except Exception as e:\\r\\n        return(f\\""Exception Inside ticketAssignment method  {e}\\"")\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://victlpth5-04:9876/api/event/trigger/AssignTicket?org=leo1311&corelid=&datasourceName=LEALCLCL12132""}}","LEOTCKT_54196"
"Chatbot","Core","{""formats"":{},""classname"":""semantic_search"",""name"":""Semantic Search"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Semantic Search"",""id"":""LEOSMNTC84402"",""codeGeneration"":{""requirements"":[""langchain==0.1.16"",""pandas"",""pydantic==1.10.10"",""openai==0.28.0"",""boto3"",""requests"",""sentence-transformers"",""faiss-cpu""],""imports"":[""import openai"",""import requests"",""import boto3"",""import json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from langchain.chat_models import AzureChatOpenAI"",""from langchain.embeddings import HuggingFaceEmbeddings"",""from langchain.vectorstores import FAISS"",""from langchain_core.prompts import ChatPromptTemplate"",""from langchain_core.runnables import RunnablePassthrough"",""from langchain_core.output_parsers import StrOutputParser""],""script"":""\\r\\n@tool(\\""query_about_policy\\"", return_direct=True)\\r\\ndef query_about_policy(query: str)->str:\\r\\n    \\""\\""\\""\\r\\n    this tool is used to answer all the queries related to policies of adoption, leaves and employee related queries. Use this to answer any resolution related and process invoice types of queries as well.\\r\\n    Parameters:\\r\\n        query (str): A natural language question or statement from the user regarding employee policies, adoption procedures, or leave-related matters.\\r\\n    Returns:\\r\\n        str: A contextual response addressing the user's query, fetched from the appropriate Knowledge base.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        global flag   \\r\\n        flag = True   \\r\\n        model_type='azure'\\r\\n        retriever = getconext(query)\\r\\n        context = retriever.invoke(query)\\r\\n        lst2 = context\\r\\n        response_list= list()\\r\\n        if len(lst2)>0:\\r\\n            for source in lst2:\\r\\n                temp_dict = dict()   \\r\\n                temp_dict['dataset_id'] = source.metadata['dataset_id']\\r\\n                temp_dict['source'] = source.metadata['source']\\r\\n                temp_dict['context'] = source.page_content\\r\\n                response_list.append(temp_dict)\\r\\n        global references\\r\\n        references = response_list\\r\\n        prompt_template = 'Answer the query asked by user based on the given Context below.'\\r\\n        model_url = os.environ.get(\\""app_model_url\\"")\\r\\n        max_tokens = 2000\\r\\n        top_k =  5\\r\\n        top_p =  0.95\\r\\n        typical_p = 0.95\\r\\n        temperature = 0.1\\r\\n        repetition_penalty = 1\\r\\n        generated_answer = ''\\r\\n        if model_type == 'onprem':\\r\\n            try:\\r\\n                input_prompt = f'''{prompt_template} Context: '{context}'. Question: {query}. Answer:''' \\r\\n                payload = json.dumps({\\r\\n                'inputs': input_prompt,\\r\\n                'parameters': {\\r\\n                        'max_new_tokens': max_tokens,\\r\\n                        'top_k': top_k,\\r\\n                        'top_p': top_p,\\r\\n                        'typical_p': typical_p,\\r\\n                        'temperature': temperature,\\r\\n                        'repetition_penalty': repetition_penalty\\r\\n                    }\\r\\n                })\\r\\n                headers = {\\r\\n                    'Content-Type': 'application/json' \\r\\n                }\\r\\n                response = requests.request('POST', model_url, headers=headers, data=payload, verify=False)\\r\\n                generated_answer = json.loads(response.text)[0]['generated_text']\\r\\n                return generated_answer\\r\\n            except Exception as ex:\\r\\n                return ex\\r\\n        elif model_type == 'azure':\\r\\n            try:\\r\\n                api_type = 'azure'\\r\\n                api_base = os.environ.get(\\""app_openai_api_base\\"")\\r\\n                api_version = '2023-03-15-preview'\\r\\n                api_key = os.environ.get(\\""app_openai_api_key\\"")\\r\\n                stop = 'stop'\\r\\n                prompt_template = 'Answer the query asked by user based on the given Context below.'\\r\\n                engine='gtp35turbo'\\r\\n                openai.api_type = api_type\\r\\n                openai.api_base = api_base\\r\\n                openai.api_version = api_version\\r\\n                openai.api_key = api_key\\r\\n                llm = AzureChatOpenAI(    \\r\\n                            deployment_name='gtp35turbo', \\r\\n                            model_name='gpt-35-turbo', \\r\\n                            openai_api_key=api_key,\\r\\n                            openai_api_version = api_version, \\r\\n                            azure_endpoint= api_base,\\r\\n                            openai_api_type=api_type,\\r\\n                            streaming=True,\\r\\n                            verbose=True,\\r\\n                            temperature=0\\r\\n                        )\\r\\n                template = \\""\\""\\""Understand the question and Answer the question based only on the following context, find what is the best suitable from the context to generate the answer for the question and the response should be in points, If you don't know the answer, just say that you don't know, don't try to make up an answer.\\r\\n                            {context}\\r\\n                            Query: {query}\\r\\n                            \\""\\""\\""               \\r\\n                prompt = ChatPromptTemplate.from_template(template)  \\r\\n                chain = (\\r\\n                    {\\""context\\"": retriever, \\""query\\"": RunnablePassthrough()}\\r\\n                    | prompt\\r\\n                    | llm\\r\\n                    | StrOutputParser()\\r\\n                )\\r\\n                answer = chain.invoke(query)\\r\\n                return answer\\r\\n            except Exception as ex:\\r\\n                logger.info(f\\""Exception in Azure service : {ex}\\"")\\r\\n        elif model_type == 'bedrock':\\r\\n            service_name = 'bedrock-runtime'\\r\\n            aws_access_key_id = os.environ.get('app_aws_access_key_id')\\r\\n            aws_secret_access_key = os.environ.get('app_aws_secret_access_key')\\r\\n            region_name = 'us-east-1'\\r\\n            endpoint_url = os.environ.get(\\""app_bedrock_endpoint_url\\"")\\r\\n            model_id = 'anthropic.claude-v2'\\r\\n            bedrock = boto3.client(service_name=service_name,aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,region_name=region_name,endpoint_url=endpoint_url)    \\r\\n            prompt_data = f\\""\\""\\""Human: {prompt_template} \\r\\n    Context: '{context}'. \\r\\n    Question: {query}\\r\\n            Assistant:\\""\\""\\""\\r\\n            body = json.dumps({\\r\\n                \\""prompt\\"": prompt_data,\\r\\n                \\""max_tokens_to_sample\\"":max_tokens,\\r\\n                \\""temperature\\"":temperature,\\r\\n                \\""top_p\\"":top_p\\r\\n            })\\r\\n            modelId = model_id\\r\\n            response = bedrock.invoke_model(body=body, modelId=modelId)\\r\\n            response_body = json.loads(response.get('body').read())\\r\\n            generated_answer = response_body.get('completion')\\r\\n            return generated_answer\\r\\n    except Exception as e:\\r\\n        logger.debug(f\\""Exception in query_about_policy Method : {e}\\"")\\r\\n            \\r\\ndef getconext(query):\\r\\n    try:\\r\\n        model_name = \\""sentence-transformers/all-MiniLM-L6-v2\\""\\r\\n        persist_directory = '/Vector_Store'\\r\\n        embeddings = HuggingFaceEmbeddings(\\r\\n        model_name=model_name\\r\\n        )\\r\\n        db = FAISS.load_local(persist_directory, embeddings,allow_dangerous_deserialization=True)\\r\\n        retriever = db.as_retriever(search_type=\\""mmr\\"")\\r\\n        return retriever\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in getconext method: {e}\\"")\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","LEOSMNTC84402"
"Chatbot","Core","{""formats"":{""query"":""text""},""classname"":""scrape_medium"",""name"":""Scrape Medium"",""parentCategory"":""LEOSRCHY97728"",""alias"":""Scrape Medium"",""id"":""LEOSCRP_78502"",""codeGeneration"":{""requirements"":[""selenium"",""bs4""],""imports"":[""from selenium import webdriver"",""from selenium.webdriver.chrome.options import Options"",""from bs4 import BeautifulSoup"",""import logging, warnings, time, re, requests"",""from datetime import datetime"",""from pydantic.v1 import BaseModel, Field"",""from langchain.tools import tool""],""script"":""warnings.filterwarnings('ignore')\\r\\n\\r\\ndef find_links(driver, no_of_post):\\r\\n    scroll = 2000\\r\\n    link_dict = {}\\r\\n    while True:\\r\\n        logging.info(\\""Inside while loop\\"")\\r\\n        driver.execute_script(f'window.scrollBy(0, {scroll})')\\r\\n        # wait for page to load\\r\\n        time.sleep(15)\\r\\n        # check if at bottom of page\\r\\n        soup = BeautifulSoup(driver.page_source, 'html.parser')\\r\\n        # print(soup.prettify())\\r\\n        link = soup.find_all('div', class_=\\""l er lm\\"")\\r\\n        logging.info(f'No of link found: {len(link)}')\\r\\n        link_dict.update({add_prefix(s.find('a').get('href')): s.find('a').find('h2').text for s in link if not s.find('div', class_=\\""kt ab\\"")})\\r\\n        if len(link_dict) >= no_of_post:\\r\\n            logging.info(f'No of link found: {len(link_dict)}')\\r\\n            break\\r\\n        if driver.execute_script('return window.innerHeight + window.pageYOffset >= document.body.offsetHeight'):\\r\\n            break\\r\\n        scroll += 500\\r\\n    return link_dict\\r\\n\\r\\n\\r\\ndef add_prefix(h: str, prefix='https://medium.com'):\\r\\n    \\""\\""\\""\\r\\n    This function adds the prefix to the hrefs\\r\\n    :param h:\\r\\n    :param prefix:\\r\\n    :return:\\r\\n    \\""\\""\\""\\r\\n    if prefix not in h:\\r\\n        m = prefix + h\\r\\n        return m\\r\\n    else:\\r\\n        return h\\r\\n\\r\\n\\r\\ndef single_page_scraper(page_url):\\r\\n    response = requests.get(url=page_url, verify=False)\\r\\n    soup = BeautifulSoup(response.content, \\""html.parser\\"")\\r\\n    return soup\\r\\n\\r\\n# Define a Pydantic model for the tool's input parameters\\r\\nclass scrape_medium(BaseModel):\\r\\n    topic_name: str = Field(...,description=\\""should be a search query\\"")\\r\\n    no_of_post: int = Field(...,description=\\""should be a search query\\"")\\r\\n\\r\\n@tool(\\""scrape_medium\\"", args_schema=scrape_medium, return_direct=True)\\r\\ndef scrape_medium(topic_name, no_of_post):\\r\\n    \\""\\""\\""\\r\\n    Searches for information on the internet using Tavily's search API and returns the results.\\r\\n    Args:\\r\\n        query (str): The search query\\r\\n    return: The search results\\r\\n    \\""\\""\\""\\r\\n    years = list(range(datetime.now().year, 2000, -1))\\r\\n    # setting up the chrome options\\r\\n    option = Options()\\r\\n    # option.add_argument(\\""headless\\"")\\r\\n    option.add_argument(\\""InPrivate\\"")\\r\\n    option.binary_location = r\\""C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\\""\\r\\n    link_dict = {}\\r\\n    for year in years:\\r\\n        logging.info(f'Looking in records of: {year}')\\r\\n        base_url = 'https://medium.com/tag/{0}/archive/{1}'.format(topic_name, year)\\r\\n        driver = webdriver.Chrome(options=option)\\r\\n        driver.get(base_url)\\r\\n        link_dict = find_links(driver, no_of_post)\\r\\n        # soup = BeautifulSoup(driver.page_source, 'html.parser')\\r\\n        # link = soup.find_all('div', class_=\\""l er lh\\"")\\r\\n        # link_dict = {add_prefix(s.find('a').get('href')): s.find('a').find('h2').text for s in link if not s.find('div', class_=\\""kt ab\\"")}\\r\\n        if len(link_dict) >= no_of_post:\\r\\n            logging.info(f'No of link found: {len(link_dict)}')\\r\\n            driver.quit()\\r\\n            break\\r\\n    files_saved = 0\\r\\n    for count, (link, name) in enumerate(link_dict.items(), 1):\\r\\n        try:\\r\\n            soup = single_page_scraper(link)\\r\\n        except:\\r\\n            pass\\r\\n        # step1 = soup.find('div', class_=\\""fr gh gi gj gk gl\\"")\\r\\n        step2 = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'ol'])\\r\\n        step3 = [s.text for s in step2]\\r\\n        article = \\""\\\\n\\"".join(\\"" \\"".join(step3).strip().splitlines())\\r\\n        article = re.sub('\\\\s+', ' ', article)\\r\\n        try:\\r\\n            with open(r'D:\\\\gautamsumanyu.t\\\\OneDrive - Infosys Limited\\\\Project Tasks\\\\March_2024\\\\test_files\\\\{0}.txt'.format(\\r\\n                    re.sub('[^A-Za-z0-9]+', '', name)), 'w+',\\r\\n                      encoding='utf-8') as f:\\r\\n                f.write(article)\\r\\n                f.close()\\r\\n            logging.info(f'Article {count} saved successfully')\\r\\n            files_saved += 1\\r\\n            if files_saved == no_of_post:\\r\\n                break\\r\\n        except:\\r\\n            pass\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""None""}}","LEOSCRP_78502"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""create_ticket"",""name"":""Create Ticket"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Create Ticket"",""id"":""LEOCRT_T78045"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""class CreateTicket(BaseModel):\\r\\n    short_description: str =Field(None,description=\\""Short description provided by the user\\"")\\r\\n\\r\\n@tool(\\""create_ticket\\"",args_schema=CreateTicket, return_direct=True)\\r\\ndef create_ticket(short_description: str):\\r\\n    '''\\r\\n    use to create ticket with given short description\\r\\n    Parameters: Short description provided by the user\\r\\n    Returns: response includes whether the ticket was successfully created or not\\r\\n    '''\\r\\n    try:\\r\\n        logger.info(f\\""createTicket method called with short description: {short_description}\\"") \\r\\n        url =  chatbot_params[\\""create_ticket_url\\""]\\r\\n        json_data = {\\r\\n                        \\""environment\\"": [\\r\\n                            {\\r\\n                                \\""key\\"": \\""incidentPayload\\"",\\r\\n                                \\""value\\"": f\\""{{\\\\\\""short_description\\\\\\"":\\\\\\""{short_description}\\\\\\"",\\\\\\""state\\\\\\"":null,\\\\\\""pritory\\\\\\"":null}}\\""\\r\\n                            }\\r\\n                        ]\\r\\n                    }\\r\\n        response = requests.post(url=url,json=json_data,verify=False)\\r\\n        logger.info(f\\""response status  {response.status_code}\\"")\\r\\n        if response.status_code == 200:\\r\\n            response = {'chat_system_response':\\""Ticket created succesfully\\"",\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[]\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:        \\r\\n            response = {'chat_system_response':\\""Ticket not created, Please contact admin\\"",\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[]\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in createTicket method: {e}\\"")\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://victlpth5-04:8095/api/event/trigger/createIncident?org=leo1311&corelid=&datasourceName=LEALCLCL12132""}}","LEOCRT_T78045"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""userAssignment"",""name"":""User Assignment"",""parentCategory"":""LEOITSMQ55029"",""alias"":""User Assignment"",""id"":""LEOUSRSG67802"",""codeGeneration"":{""requirements"":[],""imports"":[""import logging as logger"",""import ast"",""import requests,json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from typing import Optional""],""script"":""class User_Assignment(BaseModel):\\r\\n    ticket_id: str =Field(None,description=\\""ticket ID passed by the user\\"")\\r\\n    invoice_id: str=Field(None,description=\\""Invoice ID passed by the user\\"")\\r\\n    customer_id: str=Field(None,description=\\""Customer ID passed by the user\\"")\\r\\n    requested_by: Optional[str]=None\\r\\n\\r\\n@tool(\\""userAssignment\\"",args_schema=User_Assignment, return_direct=True)\\r\\ndef userAssignment(ticket_id: str = None, invoice_id: str = None, customer_id: str = None, requested_by:str = None)-> str:\\r\\n    '''\\r\\n    User assignment, returns the form id for the user assignment. ticket_id, invoice_id, customer_id are not mandatory parameters.\\r\\n    parameters: \\r\\n    ticket_id(str): string representing ticket number\\r\\n    invoice_id (str):string representing the invoice id\\r\\n    customer_id (str): string representing the customer id\\r\\n    requested_id (str): string representing the requested id, which is optional\\r\\n    returns:\\r\\n     valid json type response contaning all the details\\r\\n    '''\\r\\n    try:\\r\\n        logger.info(f\\""userAssignment assignment method called with params: Ticket ID: {ticket_id}, Invoice ID: {invoice_id}, Customer ID: {customer_id}, REQUESTED BY: {requested_by}\\"")\\r\\n        cookies = {\\r\\n            'JSESSIONID': '205A48EA5D6FF43FFB3AC2C7962D55C3',\\r\\n            'XSRF-TOKEN': 'a2432b90-ae00-4717-8621-d4a4e4141f1f',\\r\\n        }\\r\\n        token = generateBearerToken()\\r\\n        headers = {\\r\\n            \\""Authorization\\"": f\\""Bearer {token}\\"",\\r\\n            'Cookie': \\""JSESSIONID = 205A48EA5D6FF43FFB3AC2C7962D55C3 ; XSRF-TOKEN = a2432b90-ae00-4717-8621-d4a4e4141f1f\\""\\r\\n        }\\r\\n        params = {\\r\\n            'pKey': 'jci_invoice_processing',\\r\\n            'org': 'jci',\\r\\n            'constant': 'forms',\\r\\n        }\\r\\n        url = chatbot_params[\\""userAssignment_url\\""]\\r\\n        response = requests.get(url, params=params, headers=headers, verify=False)\\r\\n        logger.info(f\\""Response status for get form ID: {response.status_code}\\"")\\r\\n        if response.status_code == 200:\\r\\n            response = ast.literal_eval(response.text)\\r\\n            form_id = response.get('completed')\\r\\n            response = {\\r\\n                        'chat_system_response':form_id,\\r\\n                        'type': 'Form',\\r\\n                        'chat_formName': form_id,\\r\\n                        'form_fields':{\\r\\n                                    'ticket_id':ticket_id,\\r\\n                                    'invoice_id':invoice_id,\\r\\n                                    'customer_id':customer_id,\\r\\n                                    'requested_by':requested_by\\r\\n                                  },\\r\\n                        'chat_suggestions':None\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            issue = \\""User Assignment unsuccessful\\""\\r\\n            form_id = response.get('completed')\\r\\n            response = {\\r\\n                        'chat_system_response': issue,\\r\\n                        'type': 'Form',\\r\\n                        'chat_formName': form_id,\\r\\n                        'form_fields':{\\r\\n                                    'ticket_id':ticket_id,\\r\\n                                    'invoice_id':invoice_id,\\r\\n                                    'customer_id':customer_id,\\r\\n                                    'requested_by':requested_by\\r\\n                                  },\\r\\n                        'chat_suggestions':None\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in userAssignment method: {e} \\"")\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://livebpm/api/inbox/constant/""}}","LEOUSRSG67802"
"Chatbot","Core","{""formats"":{},""classname"":""get_case_status"",""name"":""Case Status Tool"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Case Status Tool"",""id"":""LEOGTCSS91908"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests,json"",""from langchain.tools import tool"",""from pydantic import BaseModel, Field"",""import logging as logger""],""script"":""class Get_Case_Status(BaseModel):\\r\\n    case_id: str=Field(None,description=\\""case_id provided by the user\\"")\\r\\n@tool(\\""get_case_status\\"",args_schema=Get_Case_Status, return_direct=True)\\r\\ndef get_case_status(case_id: str):\\r\\n    \\r\\n    '''\\r\\n    use to get status of the case based on case ID provided, case ID is of the format inv:00000008.\\r\\n \\r\\n    '''\\r\\n    logger.info(f\\""getCaseStatus method called with caseID: {case_id}\\"")\\r\\n    token = generateBearerToken()\\r\\n    headers = {\\r\\n        \\""Authorization\\"": f\\""Bearer {token}\\"",\\r\\n        'accept': 'application/json',\\r\\n        'Content-Type': 'application/json',\\r\\n    }\\r\\n    url = f\\""https://livebpm/api/datasets/searchData/JCIINVCP12565/jci?page=0&size=10&sortEvent=BID&sortOrder=-1&searchParams=%7B'and':%5B%7B'or':%7B'property':'business_key_','equality':'=','value':{case_id}%7D%7D%5D%7D\\""\\r\\n    response = requests.get(url=url,headers=headers,verify=False)\\r\\n    logger.info(f\\""Response status code for GET CASE STATUS method: {response.status_code}\\"")\\r\\n    if response.status_code == 200:\\r\\n        response = json.loads(response.text)\\r\\n        status_message = response[0]\\r\\n        response_dict={\\r\\n            'Ticket Number':status_message['ticket_id'],\\r\\n            'Customer ID': status_message['customer_id'],\\r\\n            'Invoice ID':status_message['invoice_id'],\\r\\n            'Requested by': status_message['requested_by'],\\r\\n            'Created On': status_message['created_date_'],\\r\\n            'Status': status_message['status_'],\\r\\n            'State':status_message['state_'],\\r\\n            'Last Updated': status_message['last_updated_date_']\\r\\n            }\\r\\n        user_prompt = '''\\r\\n            Summarize the given data\\r\\n            {data}\\r\\n            '''\\r\\n        status_message = getFormatedResponse(response_dict,user_prompt)\\r\\n        response = {'chat_system_response':status_message,\\r\\n                    'type': 'Text',\\r\\n                    'chat_suggestions':None\\r\\n                    }\\r\\n        return json.dumps(response)\\r\\n    else:\\r\\n        status_message = \\""Error in Response\\""\\r\\n        response = {'chat_system_response':status_message,\\r\\n                    'type': 'Text',\\r\\n                    'chat_suggestions':None\\r\\n                    }\\r\\n \\r\\n        return json.dumps(response)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{}}","LEOGTCSS91908"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""create_case"",""name"":""Create Case Tool"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Create Case Tool"",""id"":""LEOCRT_C45541"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests,re,json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from typing import Optional""],""script"":""class CreateCase(BaseModel):\\r\\n    ticket_number: str= Field(\\""Ticket number expressed as a string\\"")\\r\\n    customer_id: str=Field(\\""Customer id expressed as a string\\"")\\r\\n    invoice_id: str=Field(\\""Invoice id expressed as a string\\"")\\r\\n    requested_by: Optional[str]=None\\r\\n\\r\\n@tool(\\""create_case\\"",args_schema=CreateCase, return_direct=True)\\r\\ndef create_case(ticket_number: str, customer_id: str, invoice_id: str, requested_by: str )->str:\\r\\n\\r\\n    \\""\\""\\""\\r\\n    This tool is use to create case with when all three parameters customer id, ticket number and invoice id are given.\\r\\n    Parameters:\\r\\n        ticket_number(str): string representing ticket number\\r\\n        invoice_id (str):string representing the invoice id\\r\\n        customer_id (str): string representing the customer id\\r\\n        requested_by (str): string representing the requested id, which is optional\\r\\n    Returns:\\r\\n        Returns the case id.\\r\\n    \\""\\""\\""\\r\\n    \\r\\n    try: \\r\\n        logger.info(f\\"" CASE method called with params: TICKET NUMBER: {ticket_number}, CUSTOMER ID: {customer_id}, INVOICE ID: {invoice_id}, REQUESTED BY: {requested_by}\\"")        \\r\\n        token = generateBearerToken() \\r\\n\\r\\n        headers = {\\r\\n            \\""Authorization\\"": f\\""Bearer {token}\\"",\\r\\n            'accept': 'application/json',\\r\\n            'Content-Type': 'application/json',\\r\\n        }\\r\\n        url = chatbot_params[\\""create_case_url\\""]\\r\\n        json_data = {\\r\\n            \\""ticket_id\\"":ticket_number,\\r\\n            \\""customer_id\\"":customer_id,\\r\\n            \\""invoice_id\\"":invoice_id,\\r\\n            \\""requested_by\\"":\\""thippesha.thippesha@infosys.com\\""}\\r\\n        response = requests.post(url=url,headers=headers, json=json_data,verify=False)\\r\\n        logger.info(f\\""Response Status: {response.status_code}\\"")\\r\\n        if response.status_code == 200 or response.status_code== 201:\\r\\n            status_message = response.text\\r\\n            match = re.search(r'inv:\\\\d+', response.text)\\r\\n            if match:\\r\\n                case_id = match.group()\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[f'get case status of {case_id}']\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            status_message = \\""Error in Response\\""\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':['get case status']\\r\\n                        }\\r\\n            return json.dumps(response) \\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception Inside createCase method  {e}\\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://livebpm/api/inbox/startProcess/jci/jci_invoice_processing/Manual?ocrIdList=""}}","LEOCRT_C45541"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""semantic_search_adapter"",""name"":""Semantic Search A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Semantic Search A"",""id"":""LEOSMNTC35379"",""codeGeneration"":{""requirements"":[],""imports"":[""from langchain.tools import tool"",""import json, requests""],""script"":""class SemanticSearch(BaseModel):\\r    query: str =Field(None,description=\\""query passed by the user\\"")\\r\\r@tool(\\""semantic_search_adapter\\"",args_schema=SemanticSearch,return_direct=True) \\rdef semantic_search_adapter(query: str):\\r\\r    '''\\r\\r    use to provide resolution, SOP for given ticket description\\r\\r    '''\\r    try:\\r        \\r        logger.info(f\\""Semantic Search method called with query: {query}\\"")\\r\\r        url = chatbot_params[\\""semantic_search_adapter_url\\""]\\r        index = chatbot_params[\\""semantic_search_adapter_index\\""]\\r    \\r        headers = {\\r    \\r            'access-token': os.environ.get(\\""ACCESS_TOKEN\\""),\\r        \\r            'accept': 'application/json',\\r\\r            'Content-Type': 'application/json',\\r            \\r            }\\r\\r        # json_data = {\\r        #                 \\""config\\"": {\\r        #                     \\""EmbeddingConfig\\"": {\\r        #                         \\""embedding_type\\"": \\""azureopenai\\""\\r        #                     },\\r        #                     \\""VectorStoreConfig\\"": {\\r        #                         \\""DB_Type\\"": \\""Faiss\\"",\\r        #                         \\""query\\"": query,\\r        #                         \\""index_name\\"": index\\r        #                     },\\r        #                     \\""LLMConfig\\"": {\\r        #                         \\""LLM_Type\\"": \\""azureopenai\\"",\\r        #                         \\""temperature\\"": 0.1,\\r        #                         \\""query\\"": query,\\r        #                         \\""prompt_template\\"": \\""Human: Use the following pieces of context to provide a concise answer to the question at the end. If you dont know the answer, just say that you dont know, dont try to make up an answer{context}.For the query, if it requires drawing a table give results as a table markdown format.Question: {question}Assistant: \\"",\\r        #                         \\""max_tokens\\"": 2000\\r        #                     }\\r        #                 }\\r        #             }\\r        json_data = {\\""query\\"":query,\\""index_name\\"":index}\\r        response = requests.post(url=url,headers=headers,json=json_data,verify=False)\\r    \\r        logger.info(f\\""Response Status:  {response.status_code}\\"")\\r\\r        answer = ast.literal_eval(response.text)[0].get('Answer')\\r    \\r        response = {'chat_system_response':answer,\\r    \\r                'type': 'Text',\\r    \\r                'chat_suggestions':None\\r    \\r                }\\r    \\r        return json.dumps(response)\\r    \\r    except Exception as e:\\r        logger.info(f\\""Exception in semantic_search method  {e}\\"" )\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victlptst-57.ad.infosys.com:9080/Infer""}}","LEOSMNTC35379"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""ticket_summarization_adapter"",""name"":""Summarization A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Summarization A"",""id"":""LEOTCKT_75963"",""codeGeneration"":{""requirements"":[],""imports"":[""import json, requests"",""from langchain.tools import tool""],""script"":""class TicketSummarization(BaseModel):\\r    ticketNumber: str =Field(None,description=\\""Ticket number provided by the user\\"")\\r\\r\\r@tool(\\""ticket_summarization_adapter\\"",args_schema=TicketSummarization,return_direct=True)\\rdef ticket_summarization_adapter(ticketNumber:str):\\r    '''\\r    use to get summary about a ticket, takes ticket number as parameter and returns the summary of the ticket\\r\\r    '''\\r    try:\\r\\r        logger.info(f\\""Summary About Ticket method called with ticket no {ticketNumber}\\"")\\r        \\r        global ticket_number\\r        \\r        ticket_number = ticketNumber\\r\\r        access_token = os.environ.get(\\""ACCESS_TOKEN\\"")\\r\\r        headers = {\\r\\r        'access-token': access_token,\\r\\r        'accept': 'application/json',\\r\\r        'Content-Type': 'application/json',\\r\\r        }\\r\\r        json_data = {\\""number\\"":ticketNumber}\\r\\r\\r        url = chatbot_params['ticket_summarization_adapter_url']\\r\\r        response = requests.post(url=url, headers=headers, json=json_data,verify=False)\\r\\r        logger.info(f\\""Response status code {response.status_code}\\"")\\r        \\r        user_prompt = '''\\r        Using the given data, create a summary of the ticket using all it's details and in the final response, the Ticket number should be the heading and then it's summary.\\r        {data}\\r        '''\\r        answer = getFormatedResponse(response.text,user_prompt=user_prompt)\\r\\r        response = {'chat_system_response':answer,\\r\\r            'type': 'Text',\\r\\r            'chat_suggestions':None\\r            \\r            }\\r\\r        return json.dumps(response)\\r\\r    except Exception as e:\\r\\r        logger.info(f\\""Exception in ticket_summarization_adapter method : {e} \\"")\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victlpth5-07.ad.infosys.com:2430/infer""}}","LEOTCKT_75963"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""nlp2Sql_adapter"",""name"":""NLP to SQL A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""NLP to SQL A"",""id"":""LEONLP2S39547"",""codeGeneration"":{""requirements"":[""pydantic==1.10.10""],""imports"":[""import logging as logger"",""import requests,json"",""from langchain.tools import tool"",""from pydantic import BaseModel, Field""],""script"":""class NLP_to_SQL_Adapter(BaseModel):\\r\\n    query: str =Field(None,description=\\""Query passed by the user\\"")\\r\\n\\r\\n# NLP to SQL - to fetch details from db\\r\\n@tool(\\""nlp2Sql_adapter\\"",args_schema=NLP_to_SQL_Adapter,return_direct=True)\\r\\ndef nlp2Sql_adapter(query:str)->str:\\r\\n    '''\\r\\n\\r\\n    use to fetch data from database\\r\\n\\r\\n    Parameters: same query provided by the user, not SQL query\\r\\n\\r\\n    Returns: response includes data retrieved from database\\r\\n\\r\\n    '''\\r\\n    try:    \\r\\n        logger.info(f\\"" NLP2SQL Method called with query : {query} \\"")\\r\\n        value = True\\r\\n        if value == True:\\r\\n            access_token = os.environ.get('ACCESS_TOKEN')    \\r\\n            headers = {   \\r\\n            'access-token': access_token,   \\r\\n            'accept': 'application/json',   \\r\\n            'Content-Type': 'application/json',    \\r\\n            }    \\r\\n            json_data = {\\""query\\"":query}\\r\\n            url = chatbot_params[\\""nlp2Sql_adapter_url\\""]\\r\\n            response = requests.post(url, headers=headers, json=json_data,verify=False)\\r\\n            logger.info(f\\""API request status {response.status_code}\\"")\\r\\n            user_prompt = '''\\r\\n            \\r\\n            Generate summary for each of the tickets in the given data with ticket number as the heading after that there should summary of each fields of that ticket like Short Description,Created By,Created Date,Assigned To,State.\\r\\n            {data}\\r\\n            \\r\\n            Generated summary for a ticket should look like: \\r\\n            Ticket Number: INC424244\\r\\n            Ticket Details: \\r\\n            This ticket has short description as Process_Invoice_Id:12345, the ticket was created by admin, on Thu, 01 Oct 2020 17:11:11 GMT with high priority and it's been assigned to DAS group and the ticket is currently in open state.\\r\\n            \\r\\n            '''\\r\\n            answer = getFormatedResponse(response.text,user_prompt=user_prompt)\\r\\n            response = {'chat_system_response':answer,\\r\\n                'type': 'Text',\\r\\n                'chat_suggestions':None\\r\\n                }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            return \\""Unable to refresh DB!!\\""\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in fetch_details_from_database method : {e} \\"")\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victcpast-24:2443/infer""}}","LEONLP2S39547"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""ITSM"",""parentCategory"":""LEOTLSBM21587"",""alias"":""ITSM"",""attributes"":{},""id"":""LEOITSMQ55029"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOITSMQ55029"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""Search"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Search"",""attributes"":{},""id"":""LEOSRCHY97728"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOSRCHY97728"
"Chatbot","Core","{""formats"":{""Tools"":""text""},""classname"":""Chat_Utility"",""name"":""Chat Utility"",""parentCategory"":"""",""alias"":""Chat Utility"",""id"":""LEOCHTLT38766"",""codeGeneration"":{""requirements"":[],""imports"":[""from flask_cors import CORS"",""from langchain_community.chat_models import AzureChatOpenAI"",""from langchain.memory import ConversationBufferMemory"",""import json,requests,ast"",""from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder"",""from langchain.agents import AgentExecutor, create_structured_chat_agent"",""import sqlite3""],""script"":""'''\\r\\rSetup sqlite db table to store chat \\r\\r'''\\r\\rsql_db_path = 'Chat1.db'\\r\\rsql_table_name = 'Chat'\\r\\rconnection_obj = sqlite3.connect(sql_db_path,check_same_thread=False)\\r\\rcursor_obj = connection_obj.cursor()\\r\\rtable = \\""\\""\\"" CREATE TABLE IF NOT EXISTS Chat (\\r\\r            chat_user_id VARCHAR(25) NOT NULL,\\r\\r            chat_id CHAR(25) NOT NULL,\\r\\r            chat_user_query VARCHAR(1000),\\r\\r            chat_system_response VARCHAR(1000),\\r\\r            rating REAL,\\r\\r            feedback VARCHAR(1000)\\r\\r        ); \\""\\""\\""\\r\\rcursor_obj.execute(table)\\r\\rconnection_obj.close()\\r    \\r\\r    \\rdef intialise_memory(previous_chat_history):\\r    memory = ConversationBufferMemory(memory_key=\\""chat_history\\"", return_messages=True,input_key=\\""input\\"", output_key=\\""output\\"")\\r    if len(previous_chat_history) > 0:\\r        for context_obj in previous_chat_history:\\r            ai_message = str(context_obj.chat_system_response)\\r            memory.chat_memory.add_user_message(context_obj.chat_user_query)\\r            memory.chat_memory.add_ai_message(ai_message)\\r    return memory\\r\\r\\rdef getFormatedResponse(data,user_prompt):\\r    llm2 = AzureChatOpenAI(openai_api_key=os.environ.get(\\""app_openai_api_key\\""),openai_api_base=os.environ.get(\\""app_openai_api_base\\""),deployment_name='gtp35turbo', temperature=0, openai_api_version='2023-07-01-preview')\\r    prompt = ChatPromptTemplate.from_template(user_prompt)\\r    chain = prompt | llm2 \\r    try:\\r        response = chain.invoke({\\""data\\"": data})\\r        formatted_response = response.content\\r    except:\\r        formatted_response = \\""API overloaded, please try again after sometime...\\""\\r    return formatted_response\\r\\r\\rsystem = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\r\\r{tools}\\rUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\r\\rValid \\""action\\"" values: \\""Final Answer\\"" or {tool_names}\\r\\rProvide only ONE action per $JSON_BLOB, as shown:\\r\\r```\\r{{\\r\\r\\""action\\"": $TOOL_NAME,\\r\\r\\""action_input\\"": $INPUT\\r\\r}}\\r\\r```\\rFollow this format:\\r\\rQuestion: input question to answer\\rThought: consider previous responses and subsequent steps\\rAction:\\r\\r```\\r\\r$JSON_BLOB\\r\\r```\\rObservation: action result\\r... (repeat Thought/Action/Observation N times)\\r\\rThought: I know what to respond\\r\\rAction:\\r\\r```\\r\\r{{\\r\\r\\""action\\"": \\""Final Answer\\"",\\r\\""action_input\\"": \\""Final response to human\\""\\r\\r}}\\r\\rBegin! Reminder to ALWAYS respond with a valid json blob of a single action. You should always try to use tools for the user's query. Do not ask for default parameters of tools.If the user asks to fetch or get something from database then don't convert the user query to SQL query just pass the same natural language user query as the action_input. If user provides important information such as role ID, system ID, case ID, save this information in memory and use this information for subsequent user queries. Respond to user provided information with Thank you for providing details, how may i help you. Respond directly only when none of the tools can help in answering. Format is Action:```$JSON_BLOB```then Observation'''\\r\\rhuman = '''\\r\\r{input}\\r\\r{agent_scratchpad}\\r\\r(reminder to respond in a JSON blob no matter what)'''\\r\\rprompt = ChatPromptTemplate.from_messages(\\r            [\\r                (\\""system\\"", system),\\r                MessagesPlaceholder(\\""chat_history\\"", optional=True),\\r                (\\""human\\"", human),\\r            ]\\r        )\\r\\r\\rdef refreshDB():\\r    logger.info(f\\""Refresh DB method called\\"")\\r    url = \\""https://victlpth5-04:8095/api/event/trigger/refreshTickets?org=leo1311&corelid=&param=%257B%257D&datasourceName=LEALCLCL12132\\""\\r    response = requests.get(url=url,verify=False)\\r    logger.info(f\\""Response Status:  {response.status_code}\\"")\\r    if response.status_code == 200:\\r        return True\\r    return False\\r    \\r\\rdef generateBearerToken():\\r    logger.info(f\\""Generate Bearer Token method called\\"")\\r    url = \\""https://login.microsoftonline.com/63ce7d59-2f3e-42cd-a8cc-be764cff5eb6/oauth2/token\\""\\r    payload = 'client_id=9090f1c5-d381-4ef6-b845-4bac98d02fbe&client_secret=CNl8Q~IA-EUpiyA5Kkh97-4uH3ajo2PqQOkpHbp~&grant_type=client_credentials&scope=b3490b10-6bd3-4f66-908d-fa1950e46598%2F.default'\\r    headers = {\\r    'Content-Type': 'application/x-www-form-urlencoded'\\r    }\\r    response = requests.request(\\""POST\\"", url, headers=headers, data=payload)\\r    logger.info(f\\""Response Status:  {response.status_code}\\"")\\r    return json.loads(response.text)['access_token']\\r    \\rdef get_chain_response(chat_user_query,memory):\\r\\r    try:\\r        chat_system_response={}\\r        Answer_dict = {}\\r        logger.info(f\\""Get Chain Response method called with query: {chat_user_query}\\"")\\r        openai_api_key = os.environ.get(\\""app_openai_api_key\\"")\\r        openai_api_base = os.environ.get(\\""app_openai_api_base\\"")\\r        tools_list = chatbot_params[\\""Chat_Utility_Tools\\""].split(\\"",\\"")\\r        tools = [eval(item) for item in tools_list]\\r        llm = AzureChatOpenAI(openai_api_key= openai_api_key, openai_api_base = openai_api_base, deployment_name='gpt-4', temperature=0, openai_api_version='2023-05-15')\\r        logger.info(\\""init llm\\"")\\r        agent = create_structured_chat_agent(llm, tools,prompt)\\r        agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True,return_intermediate_steps=True, memory=memory,handle_parsing_errors=True)\\r        logger.info(\\""init agent\\"")\\r        try:\\r            chat_system_response = agent_executor.invoke({\\""input\\"": chat_user_query})\\r        except Exception as e:\\r            logger.info(\\""Not able to invoke agent executor in get_chain_response.\\"")\\r            chat_system_response[\\""output\\""] = \\""API overloaded, please try again after sometime...\\""\\r            chat_system_response[\\""intermediate_steps\\""] = []\\r        try:\\r            Answer_dict = json.loads(chat_system_response[\\""output\\""])            \\r        except Exception as e:\\r            system_response = chat_system_response[\\""output\\""]\\r            Answer_dict[\\""chat_system_response\\""]= system_response\\r        if len(chat_system_response['intermediate_steps'])>0:\\r            temp_dict = dict(chat_system_response['intermediate_steps'][0][0])\\r            value = temp_dict.get('tool','NA')\\r            if \\""_Exception\\"" not in value:\\r                response_dict ={\\r                    'tool':temp_dict.get('tool','NA'),    \\r                    'tool_input':json.dumps(temp_dict.get('tool_input','NA')),\\r                    'Answer':chat_system_response[\\""output\\""]\\r                }   \\r            user_prompt = \\""\\""\\""\\r                        You are an IT Agent, User have asked you a query and you have to give a short description in first person naration that how as an Agent you are trying to understand the question \\r                        and then choosing the correct tool which can be used to get the answer for the query and at last give a short summary about the tool output \\r                        and the final response should be within 100 words and don't include anything which is not mentioned in the data provided in the final response and \\r                        don't use customer word instead of that use user word and if the tool input is dictionary or list of dictionary then the understand the data inside it and use it in your final response for the tool input  and if there is nothing mentioned about the tool used and input to the tool then only summarize the tool output in the data provided within 60 words.\\r                        {data} \\r                        \\""\\""\\""\\r            cot = getFormatedResponse(response_dict,user_prompt)\\r            Answer_dict[\\""chat_chain_of_thoughts\\""]= cot\\r        else:\\r            Answer_dict[\\""chat_chain_of_thoughts\\""] = []\\r            \\r        return Answer_dict           \\r        # try:\\r        #     chat_system_response = json.loads(chat_system_response[\\""output\\""])\\r\\r        # except Exception as e:\\r        #     system_response = chat_system_response[\\""output\\""]\\r        #     chat_system_response = dict()\\r        #     chat_system_response = {\\""chat_system_response\\"":  system_response}\\r            \\r        # chat_chain_of_thoughts = list()\\r        # chat_system_response['chat_chain_of_thoughts'] = chat_chain_of_thoughts\\r        # return chat_system_response\\r    except Exception as e:\\r        logger.info(f\\""Exception in get_chain_response method: {e}\\"")\\rdef get_mysql_connection():\\r    mydb = mysql.connector.connect(\\r    host=os.environ.get('app_host_name'),\\r    user=os.environ.get('app_mysql_user'),\\r    password=os.environ.get('app_mysql_password'),\\r    database=os.environ.get('database_name'),\\r    port= os.environ.get('port')\\r    )\\r    logger.info(f\\""Mysql db connected sussessfully \\"")\\r    \\r    mycursor = mydb.cursor(dictionary=True)\\r    return mydb,mycursor\\r\\rdef insert_record(chat_user_id: str, session_id: str,chat_time: str):\\r    table_name = \\""ITSM_V3_session_table1\\""\\r    connection, cursor= get_mysql_connection()\\r\\r    check_query = f\\""SELECT COUNT(*) FROM {table_name} WHERE chat_user_id='{chat_user_id}' AND session_id='{session_id}';\\""\\r    print(\\""executing select statement\\"")\\r    cursor.execute(check_query)\\r    # cursor.execute(check_query)\\r    print(\\""select statement executed\\"")\\r    existing_record_count = cursor.fetchone()['COUNT(*)']\\r    print(\\""existing_record_count\\"",existing_record_count)\\r\\r    if existing_record_count == 0:\\r        # No existing record with the same chat_user_id and session_id, insert the new record\\r        insert_query = f\\""INSERT INTO {table_name} (chat_user_id, session_id,chat_time) VALUES ('{chat_user_id}', '{session_id}','{chat_time}')\\""\\r        print(insert_query)\\r        print(\\""executing insert statement\\"")\\r        cursor.execute(insert_query)\\r        print(\\""insert statement executed\\"")\\r        connection.commit()\\r        print(f\\""Inserted record: ({chat_user_id}, {session_id},{chat_time})\\"")\\r    else:\\r        print(f\\""Skipped record: ({chat_user_id}, {session_id},{chat_time}) (chat_user_id and session_id already exist)\\"")\\r\\r    connection.close()\\r\\r\\rdef response_input(session_id, chat_user_query,query_time, chat_chain_of_thoughts,chat_response_time,response,response_time):\\r    # Connect to the SQLite database\\r    table_name = \\""ITSM_V3_session_table2\\""\\r    connection,cursor = get_mysql_connection()\\r    # Get the next auto-increment value for the id column\\r    cursor.execute(f\\""SELECT MAX(id) + 1 FROM {table_name}\\"")\\r    next_id = list(cursor.fetchone().values())[0] or 1\\r    response_json = json.dumps(response)\\r    default_response = '\\""API overloaded, please try again after sometime...\\""'\\r    if response_json != default_response:\\r        response_json = response_json.replace(\\""'\\"",\\""\\"")\\r        response_json = response_json.replace('\\""','')\\r        if chat_chain_of_thoughts != []:\\r            chat_chain_of_thoughts = chat_chain_of_thoughts.replace(\\""'\\"",\\""\\"")\\r        query = f\\""INSERT INTO {table_name} (id, session_id, chat_user_query,query_time,chat_chain_of_thoughts,chat_response_time, message,response_time) VALUES ({next_id}, '{session_id}', '{chat_user_query}','{query_time}','{chat_chain_of_thoughts}','{chat_response_time}', '{response_json}','{response_time}')\\""\\r        cursor.execute(query)\\r        # Commit the changes and close the connection\\r        connection.commit()\\r    connection.close()\\r    print(f\\""Data inserted successfully with ID: {next_id}\\"")\\r    \\rdef fetch_all_records(chat_user_id, session_id):\\r    table_name = \\""ITSM_V3_session_table2\\""\\r    connection, cursor= get_mysql_connection()\\r    # Fetch data from the table based on session_id\\r    cursor.execute(f\\""SELECT id, session_id, chat_user_query,query_time,chat_chain_of_thoughts,chat_response_time, message,response_time FROM {table_name} WHERE session_id='{session_id}'\\"")\\r    rows = cursor.fetchall()\\r\\r    # Convert rows to the desired format\\r    result = []\\r    update_chat = []\\r    for row in rows:\\r        row_list = list(row.values())\\r        id, session_id, chat_user_query,query_time,chat_chain_of_thoughts,chat_response_time, message,response_time = row_list\\r        record = {'session_id': session_id, 'chat_user_query': chat_user_query,'query_time': query_time, 'chat_chain_of_thoughts': chat_chain_of_thoughts,'chat_response_time': chat_response_time, 'message': message,'response_time': response_time}\\r        current_chat_context = create_chat_context_object(session_id, chat_user_id, chat_user_query,message,chat_response_time)\\r        chat_id = session_id\\r        if chat_id not in chat_history:\\r            chat_history[chat_id] = []\\r            \\r        update_chat = chat_history[chat_id]\\r        update_chat.append(current_chat_context)\\r        chat_history[chat_id] = update_chat\\r        result.append(record)\\r    connection.close()\\r    return result\\r\\r\\r\\rdef fetch_session_records(chat_user_id):\\r    table_name1 = \\""ITSM_V3_session_table1\\""\\r    table_name2 = \\""ITSM_V3_session_table2\\""\\r    user_prompt = \\""\\""\\""\\r        generate the summary of Topic discussed in the context in 20 words. Remember to not summerize with extra unnecessary words in summary.\\r        {data} \\r        \\""\\""\\""\\r\\r    connection, cursor= get_mysql_connection()\\r\\r    cursor.execute(f\\""SELECT session_id FROM {table_name1} WHERE chat_user_id='{chat_user_id}'\\"")\\r    rows = cursor.fetchall()\\r\\r    # Convert rows to a list of session_id values\\r    session_ids = []\\r    for row in rows:\\r        session_ids.append(list(row.values())[0])\\r    \\r    session_ids = session_ids[-5:]  # Limiting to the last 5 sessions\\r    print(\\""session_ids -->\\"", session_ids)\\r    records = []\\r    for session_id in session_ids:\\r    # Limiting to 2 records per session_id...\\r        cursor.execute(f\\""SELECT chat_user_query, message FROM {table_name2} WHERE session_id='{session_id}' LIMIT 2\\"")\\r        rows = cursor.fetchall()\\r        answer = getFormatedResponse(rows,user_prompt)\\r        summary_session = {\\""session_id\\"": session_id, \\""summary\\"": answer}\\r        records.append(summary_session)\\r    \\r    connection.close()\\r    all_records = {\\""chat_user_id\\"": chat_user_id, \\""records\\"": records}\\r\\r    return all_records\\r\\rdef fetch_last_record(chat_user_id,session_id):\\r    table_name = \\""ITSM_V3_session_table2\\""\\r    connection, cursor= get_mysql_connection()\\r\\r    # Fetch the last query and response for the given session_id\\r    cursor.execute(f\\""SELECT chat_user_query, message FROM {table_name} WHERE session_id='{session_id}' ORDER BY id DESC LIMIT 1\\"")\\r    row = list(cursor.fetchone().values())\\r    print(row)\\r    connection.close()\\r\\r    if row:\\r        query, response = row\\r        print({'chat_user_id':chat_user_id,'chat_user_query': query, 'message ': response})\\r        return {'chat_user_id':chat_user_id,'chat_user_query': query, 'message ': response}\\r    else:\\r        return None\\r\\rdef fetch_chat_time(session_id):\\r    table_name = \\""ITSM_V3_session_table1\\""\\r    connection, cursor= get_mysql_connection()\\r    cursor.execute(f\\""SELECT chat_time FROM {table_name} WHERE session_id='{session_id}'\\"")\\r    row = list(cursor.fetchone().values())\\r    connection.close()\\r    return row[0]\\r\\r\\rdef present_time():\\r    import datetime\\r    now = datetime.datetime.now()\\r    current_time = now.strftime(\\""%I:%M %p\\"")\\r    return current_time\\r\\r\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""Tools"":""nlp2Sql_adapter,ticket_summarization_adapter,create_ticket,semantic_search_adapter,get_case_status,ticket_Assignment,create_case,get_related_tickets,userAssignment""}}","LEOCHTLT38766"
"Chatbot","Core","{""formats"":{""Tools"":""list""},""classname"":""Agent"",""name"":""Agent"",""parentCategory"":"""",""alias"":""Agent"",""id"":""LEOAGNTM90087"",""codeGeneration"":{""requirements"":[],""imports"":[""from langchain_community.chat_models import AzureChatOpenAI"",""from langchain.agents import AgentExecutor, create_structured_chat_agent"",""import json,ast""],""script"":""\\ndef get_chain_response(chat_user_query,memory):\\n\\n    try:\\n        \\n        logger.info(f\\""Get chain response method called with query: {chat_user_query}\\"")\\n        openai_api_key = os.environ.get(\\""app_openai_api_key\\"")\\n        openai_api_base = os.environ.get(\\""app_openai_api_base\\"")\\n        tools = chatbot_params[\\""Agent_Tools\\""]\\n        logger.info(f\\""@@@@@@@@@@@ TOOLS RECEIVED: {tools} @@@@@@@@@@@@@@\\"")\\n        # tools = [nlp2Sql_adapter,ticket_summarization_adapter,create_ticket,semantic_search_adapter,get_case_status,ticket_Assignment,create_case,get_related_tickets,userAssignment]\\n        llm = AzureChatOpenAI(openai_api_key= openai_api_key, openai_api_base = openai_api_base, deployment_name='gpt-4', temperature=0, openai_api_version='2023-05-15')\\n        logger.info(\\""Got LLM\\"")\\n        agent = create_structured_chat_agent(llm, tools,prompt)\\n        agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True,return_intermediate_steps=True, memory=memory,handle_parsing_errors=True)\\n        logger.info(\\""Init Agent\\"")\\n        chat_system_response = agent_executor.invoke({\\""input\\"": chat_user_query})\\n        try:\\n            \\n            chat_system_response = json.loads(chat_system_response[\\""output\\""])\\n            \\n        except Exception as e:\\n            \\n            system_response = chat_system_response[\\""output\\""]\\n            chat_system_response = dict()\\n            chat_system_response = {\\""chat_system_response\\"":  system_response}\\n        chat_chain_of_thoughts = list()\\n        chat_system_response['chat_chain_of_thoughts'] = chat_chain_of_thoughts\\n        \\n        return chat_system_response\\n        \\n    except Exception as e:\\n        logger.info(f\\""Exception in get_chain_response method: {e}\\"")\\n\\n\\n\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""Tools"":""nlp2Sql_adapter,ticket_summarization_adapter,create_ticket,semantic_search_adapter,get_case_status,ticket_Assignment,create_case,get_related_tickets,userAssignment""}}","LEOAGNTM90087"
"Chatbot","Core","{""formats"":{""headers"":""textarea"",""auth_type"":""text"",""method"":""text"",""requestBody"":""textarea"",""URL"":""text""},""classname"":""api_tool"",""name"":""API Request"",""parentCategory"":""LEOTLSBM21587"",""alias"":""API Request"",""id"":""LEOAP_TL88013"",""codeGeneration"":{""requirements"":[""ast"",""base64"",""cryptography"",""urllib"",""langchain"",""requests""],""imports"":[""import ast"",""import json"",""import logging as logger"",""import os"",""import base64"",""from cryptography.hazmat.backends import default_backend"",""from cryptography.hazmat.primitives import hashes"",""from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC"",""from cryptography.hazmat.primitives.ciphers.aead import AESGCM"",""from urllib.parse import urlparse"",""from requests.auth import HTTPBasicAuth"",""from langchain.tools import tool""],""script"":""@tool(\\""api_tool\\"", return_direct=True)\\r\\ndef api_tool():\\r\\n    \\""\\""\\""\\r\\n    Use only this tool to call the api request\\r\\n   \\r\\n    Returns:\\r\\n        A response from the api tool.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        no_proxy = os.environ.get(\\""no_proxy\\"",None)\\r\\n        DECRYPTION_KEY = os.environ.get(\\""DECRYPTION_KEY\\"",None)\\r\\n        CONNECT_TIMEOUT=int(os.environ.get(\\""CONNECT_TIMEOUT\\"",10))\\r\\n        READ_TIMEOUT=int(os.environ.get(\\""READ_TIMEOUT\\"",200))\\r\\n        VAULT_URI =os.environ.get(\\""VAULT_URI\\"",None)\\r\\n        VAULT_VERSION=os.environ.get(\\""VAULT_VERSION\\"",None)\\r\\n        VAULT_APPLICATION_NAME=os.environ.get(\\""VAULT_APPLICATION_NAME\\"",None)\\r\\n        VAULT_PROFILES=os.environ.get(\\""VAULT_PROFILES\\"",None)\\r\\n        VAULT_APPROLE_ROLEID=os.environ.get(\\""VAULT_APPROLE_ROLEID\\"",None)\\r\\n        VAULT_APPROLE_SECRETID=os.environ.get(\\""VAULT_APPROLE_SECRETID\\"",None)\\r\\n\\r\\n        method = chatbot_params[\\""api_tool_method\\""]\\r\\n        URL = chatbot_params[\\""api_tool_URL\\""]        \\r\\n        requestBody = chatbot_params[\\""api_tool_requestBody\\""] \\r\\n        if requestBody != None:\\r\\n            requestBody = ast.literal_eval(requestBody)\\r\\n          \\r\\n        headers=chatbot_params[\\""api_tool_headers\\""] \\r\\n        if headers != None:\\r\\n            headers = ast.literal_eval(headers)\\r\\n \\r\\n        auth_type = chatbot_params[\\""api_tool_auth_type\\""] \\r\\n        auth_details = os.environ.get(\\""auth_details\\"",None)\\r\\n        if auth_details != None:\\r\\n            auth_details = ast.literal_eval(auth_details)\\r\\n        proxy_value=os.environ.get(\\""proxies\\"",None)\\r\\n        vaultkey = os.environ.get(\\""vaultkey\\"",None)\\r\\n        salt = os.environ.get(\\""salt\\"",None)\\r\\n        params = os.environ.get(\\""params\\"",None)\\r\\n        if params != None:\\r\\n            params = ast.literal_eval(params)\\r\\n        os.environ['no_proxy']=no_proxy\\r\\n        PROXIES = {}\\r\\n        def getPassword(vaultKey):\\r\\n            #get Token\\r\\n            authUrl= urljoin(VAULT_URI,'/auth/approle/login')\\r\\n            authParams = {}\\r\\n            authParams[\\""role_id\\""] =  VAULT_APPROLE_ROLEID\\r\\n            authParams[\\""secret_id\\""] = VAULT_APPROLE_SECRETID\\r\\n            VAULT_HOST = urlparse(VAULT_URI).hostname\\r\\n            PROXIES = {}\\r\\n            PROXIES['http'] = ''\\r\\n            PROXIES['https'] = ''\\r\\n            response = requests.request(method='POST',url=authUrl,data = json.dumps(authParams),proxies=PROXIES,verify=False)\\r\\n            token=\\""\\""\\r\\n            if response.status_code == 200 :\\r\\n                responseJson = response.json()\\r\\n                url  =  urljoin(VAULT_URI,(VAULT_APPLICATION_NAME+'/'+VAULT_PROFILES))\\r\\n                token = responseJson['auth']['client_token']\\r\\n\\r\\n                header = {}\\r\\n                header['X-Vault-Token'] = token\\r\\n                response = requests.request(method='GET', url=url, headers=header, verify=False,proxies=PROXIES)\\r\\n                if(response.status_code == 200):\\r\\n                    try:\\r\\n                        resJson = response.json()\\r\\n                        for key in resJson['data']['data']:\\r\\n                            if key == vaultKey:\\r\\n                                return resJson['data']['data'][key]\\r\\n                    except:\\r\\n                        logger.error(\\""Error while retieving key from Vault\\"")\\r\\n                else:\\r\\n                    logger.error(\\""Error while retieving key from Vault. Response Code : {0}\\"".format(response.status_code))\\r\\n            else :\\r\\n                logger.error('Token Failure')\\r\\n\\r\\n        def decrypt(encrypted, salt):\\r\\n            try:\\r\\n                if (encrypted[0:3] != 'enc'):\\r\\n                    logger.info(\\""Password not encrypted\\"")\\r\\n                    return encrypted\\r\\n                stringkey = DECRYPTION_KEY\\r\\n                logger.info(\\""Decrypting Text\\"")\\r\\n\\r\\n                key_bytes = stringkey.encode('utf-8')\\r\\n                salt = base64.urlsafe_b64decode(salt)\\r\\n                # DERIVE key (from password and salt)\\r\\n                kdf = PBKDF2HMAC(\\r\\n                    algorithm=hashes.SHA512(),\\r\\n                    length=32,\\r\\n                    salt=salt,\\r\\n                    iterations=1000000,\\r\\n                    backend=default_backend()\\r\\n                )\\r\\n                key = kdf.derive(key_bytes)\\r\\n\\r\\n                # GENERATE random nonce (number used once)\\r\\n                nonce = salt\\r\\n                # ENCRYPTION\\r\\n                aesgcm = AESGCM(key)\\r\\n\\r\\n                # DECRYPTION\\r\\n                decrypted_cipher_text_bytes = aesgcm.decrypt(\\r\\n                    nonce=nonce,\\r\\n                    data=base64.urlsafe_b64decode(encrypted[3:]),\\r\\n                    associated_data=None\\r\\n                )\\r\\n                decrypted = decrypted_cipher_text_bytes.decode('utf-8')\\r\\n                print(decrypted,\\""decrypted\\"")\\r\\n                return decrypted\\r\\n            except:\\r\\n                logger.error(\\""Text Decryption failed\\"")\\r\\n                return encrypted\\r\\n\\r\\n        if isinstance(requestBody, dict):\\r\\n            requestBody=json.dumps(requestBody)\\r\\n            \\r\\n        if proxy_value:\\r\\n            PROXIES={'http': proxy_value,\\r\\n                     'https':proxy_value}\\r\\n        if auth_type.lower() == \\""basicauth\\"":\\r\\n            username = auth_details.get(\\""username\\"")\\r\\n            enc_password = auth_details.get(\\""password\\"")\\r\\n            password=None\\r\\n            if str(enc_password).startswith('enc'):\\r\\n                password = decrypt(enc_password, salt)\\r\\n            elif vaultkey != None:\\r\\n                password = getPassword(vaultkey)  \\r\\n            else:\\r\\n                password=enc_password\\r\\n            \\r\\n            AUTH = HTTPBasicAuth(username, password)\\r\\n            response = requests.request(method=method, url=URL, headers=headers, params=params,\\r\\n                                        proxies=PROXIES, auth=AUTH, verify=False, data=requestBody,\\r\\n                                        timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))\\r\\n           \\r\\n        elif auth_type.lower() == \\""bearertoken\\"":\\r\\n            auth_token = auth_details.get(\\""authToken\\"")\\r\\n            if auth_token!= \\""\\"":\\r\\n                headers['Authorization'] = \\""Bearer\\"" + \\"" \\"" + auth_token\\r\\n                response = requests.request(method=method, url=URL, headers=headers, params=params,\\r\\n                                        proxies=PROXIES, verify=False, data=requestBody,\\r\\n                                        timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))\\r\\n                \\r\\n        elif auth_type.lower() == \\""oauth\\"":\\r\\n            auth_url = auth_details.get(\\""auth_url\\"")\\r\\n            auth_params = auth_details.get(\\""auth_params\\"")\\r\\n            auth_headers = auth_details.get(\\""auth_headers\\"")\\r\\n            header_prefix = auth_details.get(\\""header_prefix\\"")\\r\\n            auth_method = auth_details.get(\\""auth_method\\"" , \\""GET\\"")\\r\\n            token_element = auth_details.get(\\""token_element\\"")\\r\\n\\r\\n            authResponse = requests.request(method=auth_method, url=auth_url ,data=auth_params, headers = auth_headers,\\r\\n                                            timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))\\r\\n            \\r\\n            if token_element!=None:\\r\\n                auth_token = json.loads(str(authResponse.text)).get(token_element)\\r\\n            \\r\\n            else:\\r\\n                auth_token= authResponse.json()\\r\\n            if auth_token!= \\""\\"":\\r\\n                headers['Authorization'] = header_prefix + \\"" \\"" + auth_token\\r\\n                response = requests.request(method=method, url=URL, headers=headers, params=params,\\r\\n                                        proxies=PROXIES, verify=False, data=requestBody,\\r\\n                                        timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))\\r\\n\\r\\n        elif auth_type.lower() == \\""noauth\\"":\\r\\n            logger.info(\\""inside no auth\\"")\\r\\n            logger.info(\\""displaying proxies\\"",PROXIES)\\r\\n            response = requests.request(method=method, url=URL, headers=headers, params=params,\\r\\n                                        proxies=PROXIES, verify=False, data=requestBody,\\r\\n                                        timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))\\r\\n        \\r\\n        logger.info(\\""Response Code: {0}\\"".format(response.status_code))\\r\\n        return response.text\\r\\n    \\r\\n    except Exception as e:\\r\\n        return(f\\""Exception in api_tool Method: {e}\\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""headers"":"""",""auth_type"":"""",""method"":"""",""requestBody"":"""",""URL"":""""}}","LEOAP_TL88013"
"Chatbot","Core","{""formats"":{""deployment_name"":""text"",""openai_api_version"":""text""},""classname"":""get_formatted_response"",""name"":""Response Formatting"",""parentCategory"":"""",""alias"":""Response Formatting"",""attributes"":{""deployment_name"":""gtp35turbo"",""openai_api_version"":""2023-07-01-preview""},""id"":""LEORSPNS69557"",""codeGeneration"":{""requirements"":[],""imports"":[""from langchain_community.chat_models import AzureChatOpenAI"",""from langchain_core.prompts.chat import ChatPromptTemplate""],""script"":""def getFormatedResponse(data,user_prompt):\\r\\n    logger.info(f\\""Formatted Response method called\\"")\\r\\n    llm2 = AzureChatOpenAI(openai_api_key=os.environ.get(\\""app_openai_api_key\\""),openai_api_base=os.environ.get(\\""app_openai_api_base\\""),deployment_name=chatbot_params[\\""get_formatted_response_deployment_name\\""], temperature=0, openai_api_version=chatbot_params['get_formatted_response_openai_api_version'])\\r\\n    prompt = ChatPromptTemplate.from_template(user_prompt)\\r\\n    chain = prompt | llm2\\r\\n    response = chain.invoke({\\""data\\"": data})\\r\\n    formatted_response = response.content\\r\\n    return formatted_response\\r\\n\\r\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[]}","LEORSPNS69557"
"Chatbot","Core","{""formats"":{},""classname"":""related_tickets"",""name"":""Related Tickets"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Related Tickets"",""attributes"":{},""id"":""LEORLTDT78515"",""codeGeneration"":{""requirements"":[""langchain==0.1.16"",""pydantic==1.10.10"",""openai==0.28.0"",""mysql-connector-python""],""imports"":[""import os"",""import logging"",""import re"",""from langchain.tools import tool"",""from langchain.embeddings import AzureOpenAIEmbeddings"",""from qdrant_client import QdrantClient"",""import openai"",""import sqlite3"",""import mysql.connector"",""import json""],""script"":""\\n@tool(\\""related_tickets\\"", return_direct=True)\\ndef related_tickets(short_desc: str):\\n    \\""\\""\\""\\n    This tool is use to fetch tickets with similar short description.\\n    Parameters:\\n        short_description (str): The short description of a ticket.\\n    Returns:\\n        A list of tickets with short descriptions similar to the input short description.\\n    \\""\\""\\""\\n\\n    embedding_type = 'azureopenai'\\n    db_type = 'qdrant'\\n    query = re.sub('_(.*?):.*', r\\"" \\\\1\\"", short_desc)\\n    k = 5\\n\\n    try:\\n        global flag\\n        flag = True\\n        embeddings = AzureOpenAIEmbeddings(api_key=os.environ.get(\\""app_openai_api_key\\""),api_version=\\""2023-03-15-preview\\"",azure_deployment=\\""openaiada2\\"",model=\\""text-embedding-ada-002\\"",openai_api_type=\\""azure\\"",base_url=os.environ.get(\\""app_openai_api_base\\""))\\n        client = QdrantClient(url=os.environ.get(\\""QDRANT_URL\\""))\\n\\n        if str(type(embeddings))==\\""<class 'langchain_community.embeddings.azure_openai.AzureOpenAIEmbeddings'>\\"":\\n            model=embeddings.deployment\\n            openai.api_key = embeddings.openai_api_key\\n            openai.api_version = embeddings.openai_api_version\\n            openai.api_type = embeddings.openai_api_type\\n            openai.api_base = embeddings.openai_api_base\\n            query_vector = openai.Embedding.create(engine = model,input=query,model=embeddings.model)['data'][0]['embedding']\\n        else:\\n            logger.info(f\\""Embedding type not supported...\\"")\\n\\n        hits = client.search(collection_name=os.environ.get(\\""INDEX_NAME\\""),query_vector=query_vector,limit=k)\\n        similar_docs=[]\\n        for hit in hits:\\n            similar_docs.append(hit.payload)\\n\\n    except Exception as e:\\n        logger.info(f\\""Exception in AzureOpenAI {e}\\"")\\n        raise e\\n\\n    context_list=[]\\n    for i in range(len(similar_docs)):\\n            d={}\\n            d[\\""page_content\\""]=similar_docs[i][\\""_text\\""]\\n            d[\\""metadata\\""]=similar_docs[i]['_id']\\n            context_list.append(d)\\n\\n    db_config = {\\n        \\""host\\"": os.environ.get('app_host_name'),\\n        \\""user\\"": os.environ.get('app_mysql_user'),\\n        \\""password\\"": os.environ.get('app_mysql_password'),\\n        \\""database\\"": os.environ.get('database_name'),\\n        \\""port\\"":os.environ.get('port')\\n    }\\n    \\n    connection = mysql.connector.connect(**db_config)\\n\\n    try:\\n        table_name = os.environ.get(\\""table_name\\"")\\n        cursor=connection.cursor(buffered=True)\\n        num_list=[]\\n        \\n        sql_query=\\""SHOW COLUMNS FROM {0}\\"".format(table_name)\\n        cursor.execute(sql_query)\\n        num=cursor.fetchall()\\n        for i in num:\\n            num_list.append(i[0])\\n        c=1\\n        final_dic={}\\n        for i in context_list:\\n            sql_query=\\""SELECT * FROM {0} WHERE number=%s\\"".format(table_name)\\n           \\n            cursor.execute(sql_query,(i['metadata'],))\\n            data=cursor.fetchall()\\n            d={}\\n            for i in range(len(num_list)):\\n\\n                d[num_list[i]]=data[0][i]\\n            final_dic[c]=d\\n            c+=1\\n        \\n    except sqlite3.connector.Error as err:\\n        logger.info(f'Thre is some error whiling extractiong data as:',err)\\n        raise err\\n    finally:\\n        cursor.close()\\n        connection.close()\\n\\n    answer_data = final_dic\\n        \\n    answer_list = []\\n    for key, value in answer_data.items():\\n        response_dict = {\\n            'Ticket Number':value.get('number',None),\\n            'Description':value.get(\\""shortdescription\\"", None),\\n            'Assigned To':value.get(\\""assignedto\\"", None),\\n            'Assignment Group': value.get(\\""assignmentgroup\\"",None),\\n            'Category': value.get(\\""category\\"",None),\\n            'Created by': value.get(\\""createdby\\"",None),\\n            'Created Date': value.get(\\""createdDate\\"",None),\\n            'Priority': value.get(\\""priority\\"",None),\\n            'State': value.get(\\""state\\"",None)                 \\n        }\\n        answer_list.append(response_dict)\\n        \\n    user_prompt = \\""\\""\\""\\n        Generate summary for each of the tickets in the given data with ticket number as the heading then the summary of that ticket. \\n        {data} \\n        \\""\\""\\""\\n    Answer = getFormatedResponse(answer_list,user_prompt)\\n    global ticket_number\\n    global references\\n    references = context_list\\n    response = {\\n        'chat_system_response':Answer,\\n        'type': 'Text',\\n        'chat_suggestions':None\\n            }\\n    return json.dumps(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""]}","LEORLTDT78515"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""LLM Invoke"",""parentCategory"":"""",""alias"":""LLM Invoke"",""attributes"":{},""id"":""LEOLMNVK68229"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOLMNVK68229"
"Chatbot","Core","{""formats"":{""temperature"":""float"",""openai_version"":""text"",""deploymentName"":""text""},""classname"":""azureLLM"",""name"":""AzureLLM"",""parentCategory"":""LEOLMNVK68229"",""alias"":""AzureLLM"",""id"":""LEOAZRPN92471"",""codeGeneration"":{""requirements"":[""langchain==0.1.16"",""openai==1.51.2"",""langchain-openai==0.1.25""],""imports"":[""from langchain.chat_models.azure_openai import AzureChatOpenAI"",""import os"",""import openai""],""script"":""def AzureLLM():\\n    model = AzureChatOpenAI(\\n            azure_endpoint = os.environ.get('app_azure_openai_base',None),\\n            api_key = os.environ.get('app_azure_openai_api_key',None),\\n            openai_api_version = chatbot_params[\\""azureLLM_openai_version\\""],\\n            deployment_name = chatbot_params[\\""azureLLM_deploymentName\\""],\\n            temperature = float(chatbot_params[\\""azureLLM_temperature\\""])\\n            )\\n    return model\\n""},""category"":""LLM"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""temperature"":""0.9"",""openai_version"":"""",""deploymentName"":""""}}","LEOAZRPN92471"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""Database Connections"",""parentCategory"":"""",""alias"":""Database Connections"",""attributes"":{},""id"":""LEODTBSC40574"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""DB"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEODTBSC40574"
"Chatbot","Core","{""formats"":{""database_name"":""text"",""port"":""text"",""host"":""text""},""classname"":""get_mysql_connection"",""name"":""MySQL Connection"",""parentCategory"":""LEODTBSC40574"",""alias"":""MySQL Connection"",""id"":""LEOMYSQL50914"",""codeGeneration"":{""requirements"":[""mysql-connector-python""],""imports"":[""import mysql.connector""],""script"":""def get_mysql_connection():\\r\\n    db_obj = mysql.connector.Connect(\\r\\n                host = chatbot_params['get_mysql_connection_host'],\\r\\n                user = os.environ.get('app_mysql_user'),\\r\\n                password = os.environ.get('app_mysql_password'),\\r\\n                database = chatbot_params['get_mysql_connection_database_name'],\\r\\n                port = chatbot_params['get_mysql_connection_port']\\r\\n                )\\r\\n    cursor_obj = db_obj.cursor(dictionary=True)\\r\\n    return cursor_obj, db_obj\\n""},""category"":""DB"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""database_name"":""None"",""port"":""3306"",""host"":""""}}","LEOMYSQL50914"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""Helper Functions"",""parentCategory"":"""",""alias"":""Helper Functions"",""attributes"":{},""id"":""LEOHLPRF97774"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOHLPRF97774"
"Chatbot","Core","{""formats"":{""project_id"":""text"",""project_name"":""text""},""classname"":""get_adapter_response"",""name"":""Trigger Adapter "",""parentCategory"":""LEOHLPRF97774"",""alias"":""Trigger Adapter "",""id"":""LEOTRGRD73467"",""codeGeneration"":{""requirements"":[""requests==2.27.1""],""imports"":[""import json"",""import requests""],""script"":""def get_adapter_response(url, index=None, query=None, type=\\""post\\""):\\r\\n    '''\\r\\n    using this method to get the adapter response\\r\\n    '''\\r\\n    logger.info(f\\"" get_adapter_response called with index: {index} and query: {query} \\"")\\r\\n    header = {\\r\\n        'Accept': 'application/json, text/plain, */*',\\r\\n        'Content-Type': 'application/json',\\r\\n        'Project': chatbot_params[\\""get_adapter_response_project_id\\""],\\r\\n        'ProjectName': chatbot_params[\\""get_adapter_response_project_name\\""],\\r\\n        'Access-Token': os.environ.get(\\""app_access_token\\""),\\r\\n    }\\r\\n    payload = {\\""query\\"": query, \\""index_name\\"": index}\\r\\n    if type == \\""post\\"":\\r\\n        response = requests.post(url=url, headers=header, json=payload, verify=False)\\r\\n        logger.info(response)\\r\\n        if response.status_code == 200:\\r\\n            try:\\r\\n\\r\\n                answer = json.loads(response.text)[0].get(\\""Answer\\"")\\r\\n                return answer\\r\\n            except Exception as e:\\r\\n                response = f\\""Error in parsing the response: {e}\\""\\r\\n                return response\\r\\n        else:\\r\\n            response = f\\""The is an error in POST response, response status: {response.status_code}\\""\\r\\n            return response\\r\\n    else:\\r\\n        response = requests.get(url=url, headers=header, verify=False)\\r\\n\\r\\n        logger.debug(f\\"" get_adapter_response returns {response}\\"")\\r\\n        if response.status_code == 200:\\r\\n            return json.loads(response.text)\\r\\n        else:\\r\\n            response = f\\""The is an error in GET response, response status: {response.status_code}\\""\\r\\n            return response\\r\\n\\r\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""project_id"":"""",""project_name"":""""}}","LEOTRGRD73467"
"Chatbot","Core","{""formats"":{""project_id"":""text"",""project_name"":""text""},""classname"":""trigger_emf_workflow"",""name"":""Trigger Workflow"",""parentCategory"":""LEOHLPRF97774"",""alias"":""Trigger Workflow"",""id"":""LEOTRGRW77490"",""codeGeneration"":{""requirements"":[""requests==2.27.1""],""imports"":[""import requests"",""import json""],""script"":""def trigger_emf_workflow(payload):\\r\\n    url = os.environ['Leap_URL']+\\""/api/emf/triggerWorkflow\\""\\r\\n    payload = json.dumps(payload)\\r\\n    header = {\\r\\n        'Accept': 'application/json, text/plain, */*',\\r\\n        'Content-Type': 'application/json',\\r\\n        'Project': chatbot_params[\\""trigger_emf_workflow_project_id\\""],\\r\\n        'ProjectName': chatbot_params[\\""trigger_emf_workflow_project_name\\""],\\r\\n        'Access-Token': os.environ.get(\\""app_access_token\\""),\\r\\n    }\\r\\n    response = requests.request(\\""POST\\"", url, headers=header, data=payload, verify=False)\\r\\n    logger.info(response.text)\\r\\n    if response.status_code == 202:\\r\\n        workflowInstanceid = json.loads(response.text)[\\""workflowInstanceId\\""]\\r\\n        workflowId = json.loads(response.text)[\\""workflowId\\""]\\r\\n    return workflowInstanceid, workflowId\\r\\n\\r\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""project_id"":"""",""project_name"":""""}}","LEOTRGRW77490"
"Chatbot","Core","{""formats"":{""ICAP_URL"":""text""},""classname"":""icap_extract_tickets"",""name"":""Extract Tickets"",""parentCategory"":""LEOHLPRF97774"",""alias"":""Extract Tickets"",""attributes"":{""ICAP_URL"":""""},""id"":""LEOEXTRC52164"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests""],""script"":""def icap_extract_tickets(shortdescription):\\r\\n    url = chatbot_params[\\""icap_extract_tickets_ICAP_URL\\""]+\\""/api/Extractors/extract_tickets?applicationId=2\\""\\r\\n    logger.info(f\\""Extracting Tickets with shortdescription: {shortdescription}\\"")\\r\\n\\r\\n    payload = json.dumps({\\r\\n        \\""shortDescription\\"": shortdescription,\\r\\n        \\""BotId\\"": \\""27\\""\\r\\n    })\\r\\n    headers = {\\r\\n        'Content-Type': 'application/json'\\r\\n    }\\r\\n    try:\\r\\n        response = requests.request(\\""POST\\"", url, headers=headers, data=payload)\\r\\n        logger.info(response.text)\\r\\n        return response.text\\r\\n    \\r\\n    except Exception as e:\\r\\n        logger.info(e)\\r\\n        return \\""Error in Extracting Ticket\\""\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOEXTRC52164"
"Chatbot","Core","{""formats"":{""ICAP_URL"":""text""},""classname"":""icap_classify_ticket"",""name"":""Classify Ticket"",""parentCategory"":""LEOHLPRF97774"",""alias"":""Classify Ticket"",""id"":""LEOCLSFY99047"",""codeGeneration"":{""requirements"":[],""imports"":[""import json"",""import requests""],""script"":""def icap_classify_ticket(number):\\r\\n    logger.info(f\\""Classifying number: {number}\\"")\\r\\n    url = chatbot_params[\\""icap_classify_ticket_ICAP_URL\\""]+\\""/api/Classifier/classify_ticket?applicationId=2\\""\\r\\n\\r\\n    payload = json.dumps({\\r\\n        \\""ticketId\\"": number\\r\\n    })\\r\\n    headers = {\\r\\n        'Content-Type': 'application/json'\\r\\n    }\\r\\n    try:\\r\\n        response = requests.request(\\""POST\\"", url, headers=headers, data=payload)\\r\\n        logger.info(response.text)\\r\\n        return response.text\\r\\n    except Exception as e:\\r\\n        logger.info(e)\\r\\n        return \\""Error in Classifying Ticket\\""\\r\\n\\r\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""ICAP_URL"":""""}}","LEOCLSFY99047"
"Chatbot","Core","{""formats"":{""ICAP_URL"":""text""},""classname"":""icap_verify_ticket"",""name"":""Verify Ticket"",""parentCategory"":""LEOHLPRF97774"",""alias"":""Verify Ticket"",""id"":""LEOVRFYT75311"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import json""],""script"":""def icap_verify_ticket(number):\\r\\n    logger.info(f\\""Verifying ticket number: {number}\\"")\\r\\n    url = chatbot_params[\\""icap_verify_ticket_ICAP_URL\\""]+\\""/api/Classifier/verify_ticket?applicationId=2\\""\\r\\n    payload = json.dumps({\\r\\n        \\""ticketId\\"": number,\\r\\n        \\""ManualVerificationNeeded\\"": \\""no\\""\\r\\n    })\\r\\n    headers = {\\r\\n        'Content-Type': 'application/json'\\r\\n    }\\r\\n\\r\\n    try:\\r\\n        response = requests.request(\\""POST\\"", url, headers=headers, data=payload)\\r\\n        logger.info(response.text)\\r\\n        return response.text\\r\\n    except Exception as e:\\r\\n        logger.info(e)\\r\\n        return \\""Error in Verifying Ticket\\""\\r\\n\\r\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""ICAP_URL"":""""}}","LEOVRFYT75311"
"Chatbot","Core","{""formats"":{""Leap_URL"":""text"",""project_id"":""text"",""project_name"":""text""},""classname"":""create_case"",""name"":""Create Case"",""parentCategory"":""LEOHLPRF97774"",""alias"":""Create Case"",""attributes"":{""Leap_URL"":""https://leap2:7000/"",""project_id"":"""",""project_name"":""""},""id"":""LEOCRTCS25530"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def create_case(type, payload):\\r\\n    '''\\r\\n    use to create case\\r\\n    '''\\r\\n    logger.info(f\\""Create case method called\\"")\\r\\n    projectName = chatbot_params[\\""create_case_project_name\\""]\\r\\n    header = {\\r\\n        'Accept': 'application/json, text/plain, */*',\\r\\n        'Content-Type': 'application/json',\\r\\n        'Project': chatbot_params[\\""create_case_project_id\\""],\\r\\n        'ProjectName': projectName,\\r\\n        'Access-Token': os.environ[\\""app_access_token\\""]\\r\\n    }\\r\\n    url = f'''{chatbot_params[\\""create_case_Leap_URL\\""]}/api/inbox/startProcess/{projectName}/{type}/Manual?ocrIdList='''\\r\\n    json_data = payload\\r\\n    response = requests.post(url=url, headers=header, json=json_data, verify=False)\\r\\n    logger.info(\\""Create Case \\"" + response.text)\\r\\n    return response.text\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOCRTCS25530"
"Chatbot","Core","{""formats"":{""Leap_URL"":"""",""project_id"":""text"",""project_name"":""""},""classname"":""getCaseStatus"",""name"":""Case Status"",""parentCategory"":""LEOHLPRF97774"",""alias"":""Case Status"",""attributes"":{""Leap_URL"":"""",""project_id"":"""",""project_name"":""""},""id"":""LEOCSSTS24214"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests""],""script"":""def getCaseStatus(case_id: str):\\r\\n    '''\\r\\n    use to get status of the case based on case ID provided, case ID is of the format RA:00000165.\\r\\n    '''\\r\\n    logger.info(f\\""Case status method called with case id {case_id}\\"")\\r\\n    projectName = chatbot_params[\\""getCaseStatus_project_name\\""]\\r\\n    header = {\\r\\n        'Accept': 'application/json, text/plain, */*',\\r\\n        'Content-Type': 'application/json',\\r\\n        'Project': chatbot_params[\\""getCaseStatus_project_id\\""],\\r\\n        'ProjectName': projectName,\\r\\n        'Access-Token': os.environ[\\""app_access_token\\""]\\r\\n    }\\r\\n    url = f'''{chatbot_params[\\""getCaseStatus_Leap_URL\\""]}/api/aip/datasets/searchData?page=0&size=1&sortEvent=BID&sortOrder=-1&datasetName=JCIJC_DM87192&projectName={projectName}&searchParams=%7B'and':%5B%7B'or':%7B'property':'business_key_','equality':'=','value':\\""{case_id}\\""%7D%7D%5D%7D'''\\r\\n    response = requests.get(url=url, headers=header, verify=False)\\r\\n    print(response)\\r\\n    if response.status_code == 200:\\r\\n        status_message = response.text\\r\\n        response = llm_invoke(status_message, \\""summarize the data {data}\\"")\\r\\n        return response\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOCSSTS24214"
"Chatbot","Core","{""formats"":{""Imports"":""text"",""Requirements"":""text"",""script"":""textarea""},""classname"":""Generic Tool"",""name"":""Tool Script"",""parentCategory"":"""",""alias"":""Tool Script"",""id"":""LEOTLSCR55683"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""Imports"":"""",""Requirements"":"""",""script"":""#Define your tool method here""}}","LEOTLSCR55683"
"SemanticSearch","Core","{""id"":3,""name"":""Dataset Extractor"",""category"":""DatasetExtractor"",""parentCategory"":""2"",""classname"":""DatasetExtractorSemantic"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import boto3\\r\\nfrom typing import List, Optional\\r\\nimport logging as logger \\r\\nimport pathlib\\r\\nimport os\\r\\nimport json\\r\\nimport shutil\\r\\nimport requests\\r\\nimport boto3\\r\\nimport json\\r\\nfrom typing import List, Optional, Union, Dict, Sequence, Any\\r\\nfrom urllib.parse import quote_plus\\r\\n#from leaputils import Security\\r\\nimport sqlite3\\r\\n# !pip install unstructured\\r\\n# !pip install \\""unstructured[pdf]\\""\\r\\n\\r\\nclass DatasetExtractorConfig:\\r\\n    \\""\\""\\""\\r\\n    DatasetExtractorConfig class to get dataset config.\\r\\n    Args:\\r\\n        config (dict): Dataset config\\r\\n    \\""\\""\\""\\r\\n    def __init__(self, config: Optional[Any]={}) -> None:   \\r\\n        self.local_path: str = config.get('local_path','RAG_data')            \\r\\n        self.query: str = config.get('query',None)   \\r\\n        self.index_search: str = config.get('index_search', 'Default_Index')              \\r\\nclass DatasetExtractor:                            \\r\\n    \\""\\""\\""\\r\\n    Extract dataset. Currently We support S3 datastore.        \\r\\n    Args:\\r\\n        dataset_id (str): Dataset id to get dataset config  \\r\\n        local_path (str): Local file path to store file downloaded from data store,   \\r\\n        index_search (str): To store embedding at particular index          \\r\\n    Returns:\\r\\n        if dataset_type is S3: file_path (str): Local file path\\r\\n        if dataset_type is MYSQL, POSTGRESQL, MSSQL: metadata (dict): metadata\\r\\n    \\""\\""\\""\\r\\n    def __init__(self, dataset_id:str, organization:str) -> None:\\r\\n        self.organization = organization\\r\\n        self.dataset_id = dataset_id        \\r\\n    def get_data(self, config) -> str:         \\r\\n        try:\\r\\n            # get dataset config dict\\r\\n            datasetcofig = self.getdatasetconfig(dataset_id=self.dataset_id, organization=self.organization)              \\r\\n            view = datasetcofig['views']      \\r\\n            dataset_type = datasetcofig['datasource']['type']        \\r\\n            metadata = {\\r\\n                'dataset_id': self.dataset_id,\\r\\n                'organization': self.organization,\\r\\n                'dataset_type': dataset_type,                   \\r\\n            }\\r\\n            connection_details = dict()\\r\\n            if dataset_type == 'S3':  \\r\\n                if view == 'Folder View':\\r\\n                    connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r\\n                    s3_access_key = connection_dict['accessKey']\\r\\n                    s3_secret_key = connection_dict['secretKey']\\r\\n                    s3_end_point_url = connection_dict['url']\\r\\n                    attribute = json.loads(datasetcofig['attributes'])\\r\\n                    bucket = attribute['bucket']    \\r\\n                    path = attribute['path'] \\r\\n                    local_path = '/temp/'+config.local_path+'_'+self.dataset_id\\r\\n                    count = 0\\r\\n                    flag = False\\r\\n                    for i in os.listdir(\\""/temp/\\""):\\r\\n                        if i.find(local_path[1:])==0:\\r\\n                            flag = True\\r\\n                            count += 1\\r\\n                    if flag:\\r\\n                        local_path = local_path+(count*'_')\\r\\n                    file_path = self.s3_download_data(end_point_url = s3_end_point_url, access_key = s3_access_key, secret_key=s3_secret_key, bucket = bucket, obj_key = path, local_path = local_path)\\r\\n                    metadata['source'] = file_path \\r\\n                    return metadata\\r\\n                else:                \\r\\n                    connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r\\n                    s3_access_key = connection_dict['accessKey']\\r\\n                    s3_secret_key = connection_dict['secretKey']\\r\\n                    s3_end_point_url = connection_dict['url'] \\r\\n                    attribute = json.loads(datasetcofig['attributes'])\\r\\n                    bucket = attribute['bucket']               \\r\\n                    path = attribute['path']   \\r\\n                    obj_key = attribute['object']  \\r\\n                    key = f'{path}/{obj_key}'                \\r\\n                    local_path = '/temp/'+config.local_path+'_'+self.dataset_id                           \\r\\n                    count = 0\\r\\n                    flag = False\\r\\n                    for i in os.listdir(\\""/temp/\\""):\\r\\n                        if i.find(local_path[1:])==0:\\r\\n                            flag = True\\r\\n                            count += 1\\r\\n                    if flag:\\r\\n                        local_path = local_path+(count*'_')                                 \\r\\n                    file_path = self.s3_download_data(end_point_url = s3_end_point_url, access_key = s3_access_key, secret_key=s3_secret_key, bucket = bucket, obj_key = key, local_path = local_path)\\r\\n                    metadata['source'] = file_path                \\r\\n                    return metadata\\r\\n            elif dataset_type == 'MYSQL':   \\r\\n                connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r\\n                attributes = json.loads(datasetcofig['attributes'])            \\r\\n                db_user_name = connection_dict['userName']           \\r\\n                db_url = connection_dict['url']\\r\\n                db_password = connection_dict['password']   \\r\\n                salt = datasetcofig['datasource']['salt']\\r\\n                sql_query = attributes['Query'] \\r\\n                connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r\\n                self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r\\n                return metadata            \\r\\n            elif dataset_type == 'POSTGRESQL':\\r\\n                connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r\\n                attributes = json.loads(datasetcofig['attributes'])            \\r\\n                db_user_name = connection_dict['userName']           \\r\\n                db_url = connection_dict['url']\\r\\n                db_password = connection_dict['password']   \\r\\n                salt = datasetcofig['datasource']['salt']\\r\\n                sql_query = attributes['Query'] \\r\\n                connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r\\n                self.store_connection_details(connection_details, connection_string, config.index_search)\\r\\n                return metadata         \\r\\n            elif dataset_type == 'MSSQL':\\r\\n                connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r\\n                attributes = json.loads(datasetcofig['attributes'])            \\r\\n                db_user_name = connection_dict['userName']           \\r\\n                db_url = connection_dict['url']\\r\\n                db_password = connection_dict['password']   \\r\\n                salt = datasetcofig['datasource']['salt']\\r\\n                sql_query = attributes['Query'] \\r\\n                connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r\\n                self.store_connection_details(connection_details, connection_string, config.index_search)\\r\\n                return metadata \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in DatasetExtractor as: ',e)\\r\\n            return e\\r\\n    #helper functions\\r\\n    def getdatasetconfig(self, dataset_id:str, organization:str):\\r\\n        '''\\r\\n        call ai-plat api to get dataset config\\r\\n        return {dataset_config}\\r\\n        '''  \\r\\n        try:                  \\r\\n                      \\r\\n            api_referer = os.environ.get(\\""API_URL\\"")\\r\\n            url = f'{api_referer}/api/aip/services/fetchDatasetDetails/{dataset_id}/{organization}'\\r\\n            logger.info(url)\\r\\n            headers = {\\r\\n            'access-token': os.environ.get('app_access_token'),\\r\\n            'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B',\\r\\n            'Project':'2'\\r\\n            }\\r\\n            response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r\\n            dataset_config = json.loads(response.text)\\r\\n            return dataset_config\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in getdatasetconfig as: ', e)\\r\\n            raise e\\r\\n    def s3_download_data(self, end_point_url:str = '', access_key:str = '', secret_key:str = '', bucket:str = '', obj_key:str = '', local_path:str='/data'):\\r\\n        '''\\r\\n        download data from s3         \\r\\n        return local file path       \\r\\n        '''     \\r\\n        try:\\r\\n            s3_client = boto3.resource(service_name='s3',\\r\\n                        endpoint_url=end_point_url,\\r\\n                        aws_access_key_id=access_key,\\r\\n                        aws_secret_access_key=secret_key,\\r\\n                        verify=False)\\r\\n            bucket_object = s3_client.Bucket(bucket)\\r\\n            object_save_path = ''\\r\\n            logger.info(bucket_object.objects.filter(Prefix=obj_key))        \\r\\n            if os.path.exists(local_path):\\r\\n                shutil.rmtree(local_path)\\r\\n            os.makedirs(local_path)\\r\\n            model_path = os.path.join(local_path)\\r\\n            for my_bucket_object in bucket_object.objects.filter(Prefix=obj_key):   \\r\\n                if my_bucket_object.key.endswith('/'):\\r\\n                    pass\\r\\n                elif my_bucket_object.key.find('.aip')>=0:\\r\\n                    pass\\r\\n                else:                 \\r\\n                    object_save_path = (f\\""{model_path}/{pathlib.Path(my_bucket_object.key).name}\\"")\\r\\n                    bucket_object.download_file(my_bucket_object.key, object_save_path)\\r\\n            return local_path \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in s3_download_data as: ', e)\\r\\n            raise e\\r\\n    def get_connection_details(self, db_user_name:str, db_password:str, db_url:str, salt:str, sql_query:str, prefix:str):\\r\\n        '''\\r\\n        Get connection string for the database.\\r\\n        Args:\\r\\n            db_user_name (str): Database user name.\\r\\n            db_password (str): Database password.\\r\\n            db_url (str): Database URL.\\r\\n            salt (str): Salt used to encrypt the password.\\r\\n            prefix (str): Protocol prefix.\\r\\n        Returns:\\r\\n            connection_details (dict): Dictionary containing connection string, user name, password, host, port, and database.           \\r\\n        '''\\r\\n        try:\\r\\n            connection_details = {}\\r\\n            # decrypt password\\r\\n            db_password = Security.decrypt(db_password,salt)\\r\\n            # Remove protocol prefix and leading slash\\r\\n            stripped_url = db_url[len(prefix):]\\r\\n            # Split by components, assuming username and password are not present\\r\\n            host, port_database = stripped_url.split(\\""/\\"", 1)\\r\\n            # Extract host and port\\r\\n            host, port = host.split(\\"":\\"")\\r\\n            # Extract database\\r\\n            database = port_database.strip(\\""/\\"")\\r\\n            # Form connection string\\r\\n            connection_string = f'mysql://{db_user_name}:{quote_plus(db_password)}@{host}:{port}/{database}'\\r\\n            connection_details['db_user_name'] = db_user_name\\r\\n            connection_details['db_password'] = db_password\\r\\n            connection_details['db_host'] = host\\r\\n            connection_details['db_port'] = port\\r\\n            connection_details['database'] = database\\r\\n            connection_details['query'] = sql_query\\r\\n            return connection_string, connection_details\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_connection_details as: ', e)\\r\\n            raise e\\r\\n    def store_connection_details(self, connection_details:dict, connection_string:str, index_search:str, metadata:dict):         \\r\\n        try:\\r\\n            # Connect to the database or create it if it doesn't exist\\r\\n            root_dir = \\""/RAG_DB\\""\\r\\n            os.makedirs(root_dir, exist_ok=True)  # Create only if needed\\r\\n            root_path = os.path.abspath(root_dir)\\r\\n            path = os.path.join(root_path,'rag_database.db')\\r\\n            # path = os.path.join('../database','rag_database.db')\\r\\n            conn = sqlite3.connect(path) \\r\\n            # Create a cursor object to execute SQL commands\\r\\n            cursor = conn.cursor()\\r\\n            # Create a table (replace with your desired table name and columns)\\r\\n            cursor.execute('''\\r\\n            CREATE TABLE IF NOT EXISTS dataset_details (\\r\\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\\r\\n                dataset_id TEXT NOT NULL,\\r\\n                organization TEXT NOT NULL,\\r\\n                connection_string TEXT NOT NULL,\\r\\n                connection_details JSON NOT NULL,\\r\\n                metadata JSON,\\r\\n                index_search TEXT NOT NULL\\r\\n            )\\r\\n            ''')\\r\\n            connection_details_json = json.dumps(connection_details)\\r\\n            metadata_json = json.dumps(metadata)\\r\\n            # Prepare SQL query with placeholders for values\\r\\n            sql = \\""INSERT INTO dataset_details (dataset_id, ORGANIZATION, connection_string, connection_details, metadata, index_search) VALUES (?, ?, ?, ?, ?, ?)\\""\\r\\n            # Data to be inserted (replace with your actual values)\\r\\n            values = (self.dataset_id, self.organization, connection_string, connection_details_json, metadata_json, index_search)\\r\\n            # Execute the query with prepared values\\r\\n            cursor.execute(sql, values)\\r\\n            # Commit the changes to the database\\r\\n            conn.commit()\\r\\n            logger.info(\\""data inserted successfully!....\\"")                   \\r\\n        except sqlite3.connector.Error as e:\\r\\n            logger.info('Thre is some error whiling storing data as:',e)\\r\\n            raise e\\r\\n        finally:\\r\\n            # Close database connection properly\\r\\n            cursor.close()\\r\\n            conn.close()\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Dataset Extractor"",""attributes"":{}}","SemanticSearchCore-3"
"SemanticSearch","Core","{""id"":9,""name"":""Query VectorDB"",""category"":""QueryVectorDB"",""parentCategory"":""7"",""classname"":""QueryVectorDB"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nfrom typing import List,Optional,Dict,Union,Sequence,Any\\r\\nimport logging as logger\\r\\nclass VectordbConfig:\\r\\n    def __init__(self,config_json:Optional[Any]={}) :\\r\\n        self.DB_Type : Optional[str] = config_json.get('DB_Type',\\""Faiss\\"")\\r\\n        self.elastic_search_url : str = config_json.get('elastic_search_url',os.environ.get('app_elastic_search_url'))\\r\\n        self.index_name : Optional[str] = config_json.get('index_name',\\""Vector_Store\\"")\\r\\n        self.qdrant_url : Optional[str] = config_json.get('qdrant_url',os.environ.get('app_qdrant_url'))\\r\\n        self.force_recreate : Optional[bool] = config_json.get('force_recreate',False)\\r\\n        self.query : Optional[str] = config_json.get('query',\\""Hi, How are you?\\"")\\r\\n        self.k : Optional[int] = config_json.get('k',4)\\r\\n        \\r\\nclass QueryVectorDB:\\r\\n    def __init__(self,embedding) -> None:\\r\\n        self.embeddings = embedding\\r\\n    \\r\\n    def get_similar_docs(self,config):\\r\\n        \\""\\""\\""`Meta Faiss` vector store.\\r\\n        To use, you must have the ``faiss`` python package installed.\\r\\n        Example:\\r\\n            .. code-block:: python\\r\\n                from langchain_community.embeddings.openai import OpenAIEmbeddings\\r\\n                from langchain_community.vectorstores import FAISS\\r\\n                embeddings = OpenAIEmbeddings()\\r\\n                texts = [\\""FAISS is an important library\\"", \\""LangChain supports FAISS\\""]\\r\\n                faiss = FAISS.from_texts(texts, embeddings)\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            if config.DB_Type.lower() == 'faiss':\\r\\n                return self.get_similar_docs_faiss_db(embeddings=self.embeddings,persist_directory='/temp/faiss_db/'+config.index_name,query=config.query,k=config.k)                \\r\\n            elif config.DB_Type.lower() == 'chroma':  \\r\\n                return self.get_similar_docs_chroma_db(embeddings=self.embeddings,persist_directory='/temp/chroma_db/'+config.index_name,query=config.query,k=config.k)        \\r\\n            elif config.DB_Type.lower() == 'elasticsearch':                   \\r\\n                return self.get_similar_docs_elastic_db(embeddings=self.embeddings,docs=self.docs,es_url=config.elastic_search_url,index_name=config.index_name,query=config.query,k=config.k)\\t\\t    \\r\\n            elif config.DB_Type.lower() == 'qdrant':      \\r\\n                return self.get_similar_docs_qdrant_db(embeddings=self.embeddings,qdrant_url=config.qdrant_url,collection_name=config.index_name,query=config.query,k=config.k)\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs', e)\\r\\n            raise e\\r\\n        \\r\\n    def get_similar_docs_chroma_db(self,embeddings:Any=None,persist_directory:str=\\""\\"",query:str='',k:int=None):\\r\\n        \\""\\""\\""`ChromaDB` vector store.\\r\\n        To use, you should have the ``chromadb`` python package installed.\\r\\n        Example:\\r\\n        .. code-block:: python\\r\\n        from langchain_community.vectorstores import Chroma\\r\\n        from langchain_community.embeddings.openai import OpenAIEmbeddings\\r\\n        embeddings = OpenAIEmbeddings()\\r\\n        vectorstore = Chroma(\\""langchain_store\\"", embeddings)\\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.vectorstores import Chroma\\r\\n            db3 = Chroma(persist_directory=persist_directory, embeddings=embeddings)\\r\\n            docs = db3.similarity_search(query,k=k)\\r\\n            \\r\\n            return docs\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs_chroma_db', e)\\r\\n            raise e\\r\\n    def get_similar_docs_faiss_db(self,embeddings:Any=None,persist_directory:str=\\""\\"",query:str='',k:int=None):   \\r\\n        try:     \\r\\n            from langchain.vectorstores import FAISS\\r\\n            db = FAISS.load_local(persist_directory, embeddings,allow_dangerous_deserialization=True)\\r\\n            docs = db.similarity_search(query,k=k)\\r\\n            \\r\\n            return docs\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs_faiss_db', e)\\r\\n            raise e\\r\\n    def get_similar_docs_qdrant_db(self,qdrant_url:str='',embeddings:Any='',query:str='',collection_name:str='',k:int=None):\\r\\n        \\""\\""\\""\\r\\n        QdrantvectorDb\\r\\n        Args:[]\\r\\n            embeddings=self.embeddings, # Vector embeddings to use for similarity search\\r\\n            qdrant_url=config.qdrant_url, # URL of Qdrant search server \\r\\n            collection_name=config.index_name, # Name of indexed document collection in Qdrant \\r\\n            query=config.query # Search query text to find similar documents for\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from qdrant_client import QdrantClient\\r\\n            \\r\\n            client = QdrantClient(url=qdrant_url)\\r\\n            if 'HuggingFaceEmbeddings' in str(type(embeddings)):\\r\\n                from sentence_transformers import SentenceTransformer\\r\\n                model=embeddings.model_name\\r\\n                name=model.split('/')\\r\\n                encoder = SentenceTransformer(name[1])\\r\\n                query_vector=encoder.encode(query).tolist()\\r\\n            elif 'AzureOpenAIEmbeddings' in str(type(embeddings)):\\r\\n                \\r\\n                import openai\\r\\n                model=embeddings.deployment\\r\\n                openai.api_key = embeddings.openai_api_key\\r\\n                openai.api_version = embeddings.openai_api_version\\r\\n                openai.api_type = embeddings.openai_api_type\\r\\n                openai.api_base = embeddings.openai_api_base\\r\\n                query_vector = openai.Embedding.create(\\r\\n                engine = model,\\r\\n                input=query,\\r\\n                model=embeddings.model,\\r\\n        )['data'][0]['embedding']\\r\\n            elif 'BedrockEmbeddings' in str(type(embeddings)):\\r\\n                import json\\r\\n                model_id = embeddings.model_id\\r\\n                bedrock = embeddings.client\\r\\n                input_text = query\\r\\n                body = json.dumps({\\r\\n                    \\""inputText\\"": input_text\\r\\n                    })\\r\\n                accept = \\""application/json\\""\\r\\n                content_type = \\""application/json\\""\\r\\n                query_vector = bedrock.invoke_model(\\r\\n                    body=body, modelId=model_id, accept=accept, contentType=content_type\\r\\n                )\\r\\n                query_vector = json.loads(query_vector.get('body').read())['embedding']\\r\\n            hits = client.search(collection_name=collection_name,query_vector=query_vector,limit=k)\\r\\n            l=[]\\r\\n            for hit in hits:\\r\\n                l.append(hit.payload)        \\r\\n            \\r\\n            return l\\r\\n        except Exception as e:  \\r\\n            logger.info('Exception in get_similar_docs_qdrant_db', e)\\r\\n            raise e\\r\\n        \\r\\n    def get_similar_docs_elastic_db(self,embeddings:Any=None,docs:Any=None,query:str='',es_url=\\""\\"",index_name=\\""\\"",k:int=None):\\r\\n        \\""\\""\\""`Elasticsearch` vector store.\\r\\n        Args:\\r\\n            index_name: Name of the Elasticsearch index to create.\\r\\n            es_url: URL of the Elasticsearch instance to connect to.\\r\\n            cloud_id: Cloud ID of the Elasticsearch instance to connect to.\\r\\n            es_user: Username to use when connecting to Elasticsearch.\\r\\n            es_password: Password to use when connecting to Elasticsearch.\\r\\n            es_api_key: API key to use when connecting to Elasticsearch.\\r\\n            es_connection: Optional pre-existing Elasticsearch connection.\\r\\n            vector_query_field: Optional. Name of the field to store\\r\\n                                the embedding vectors in.\\r\\n            query_field: Optional. Name of the field to store the texts in.\\r\\n            strategy: Optional. Retrieval strategy to use when searching the index.\\r\\n                        Defaults to ApproxRetrievalStrategy. Can be one of\\r\\n                        ExactRetrievalStrategy, ApproxRetrievalStrategy,\\r\\n                        or SparseRetrievalStrategy.\\r\\n            distance_strategy: Optional. Distance strategy to use when\\r\\n                                searching the index.\\r\\n                                Defaults to COSINE. Can be one of COSINE,\\r\\n                                    EUCLIDEAN_DISTANCE, or DOT_PRODUCT. \\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.vectorstores import ElasticsearchStore\\r\\n            db = ElasticsearchStore.from_documents(\\r\\n            docs,\\r\\n            embeddings,\\r\\n            es_url=es_url,\\r\\n            index_name=index_name,\\r\\n            )\\r\\n            db.client.indices.refresh(index=index_name)\\r\\n            results = db.similarity_search(query,k=k)            \\r\\n            return results\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs_elastic_db', e)\\r\\n            raise e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Query VectorDB"",""attributes"":{}}","SemanticSearchCore-9"
"SemanticSearch","Core","{""id"":6,""name"":""Vector Store"",""category"":""VectorStore"",""parentCategory"":""2"",""classname"":""VectorStore"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nfrom typing import List,Optional,Any\\r\\nimport logging as logger\\r\\nclass VectorConfig:\\r\\n    def __init__(self,config_json:Optional[Any]={}) :\\r\\n        self.DB_Type : Optional[str] = config_json.get('DB_Type',\\""Faiss\\"")\\r\\n        self.elastic_search_url : str = config_json.get('elastic_search_url',os.environ.get('app_elastic_search_url'))\\r\\n        self.index_name : Optional[str] = config_json.get('index_name',\\""Vector_Store\\"")\\r\\n        self.qdrant_url : Optional[str] = config_json.get('qdrant_url',os.environ.get('app_qdrant_url'))\\r\\n        self.force_recreate : Optional[bool] = config_json.get('force_recreate',False)\\r\\nclass VectorStore:      \\r\\n    def __init__(self,embedding,docs) -> None:\\r\\n        self.embedding_function = embedding\\r\\n        self.docs = docs\\r\\n    def SaveEmbedding(self,config:dict):\\r\\n        \\""\\""\\""Store Embeddings In vector db. We support FaissDB,chromadb,ElasticsearchDB, it's not required if you prepared your own vector db.\\r\\n        Args:\\r\\n            DB_Type (str): The type of vector store to be used available options are Elasticsearch,Faiss,Chroma .\\r\\n            embedding_function (Callable): the embedding function to use.\\r\\n            docs(list[Document]): List of splitted text that has to be stored in vector DB.        \\r\\n            elastic_search_url(str): Elastic Search hosted URL \\r\\n            index_name(str): The Index Name Default is \\""Vector_Store\\"".\\r\\n            qdrant_url(str): qdrant URL \\r\\n            force_recreate(bool): Create new index, If it exists it will delete and create new index while it is True, If False, It will append the existing Index and if index is not present will create new Index.\\r\\n                                    Default is False.\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            # logger.info(config.persist_directory)\\r\\n            if config.DB_Type.strip().lower() == 'faiss':\\r\\n                return self.FaissDB(embedding_function=self.embedding_function,docs=self.docs,persist_directory=\\""/temp/faiss_db/\\""+config.index_name)\\r\\n            elif config.DB_Type.strip().lower() == 'chroma':\\r\\n                return self.ChromaDB(embedding_function=self.embedding_function,docs=self.docs,persist_directory=\\""/temp/chroma_db/\\""+config.index_name)\\r\\n            elif config.DB_Type.strip().lower() == 'elasticsearch':\\r\\n                return self.ElasticsearchDB(embedding=self.embedding_function, docs=self.docs, es_url=config.elastic_search_url, index_name=config.index_name)\\r\\n            elif config.DB_Type.strip().lower() == 'qdrant':\\r\\n                return self.Qdrant(embeddings=self.embedding_function,texts=self.docs,url=config.qdrant_url,collection_name=config.index_name,force_recreate=config.force_recreate)\\r\\n            else:\\r\\n                logger.info('Unsupported VectorStore type)')\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in SaveEmbedding',e)\\r\\n            raise e\\r\\n        \\r\\n    def FaissDB(self,embedding_function:Any=None,docs:Any=None,persist_directory:str=''):\\r\\n        try:\\r\\n            from langchain.vectorstores import FAISS\\r\\n            faiss_db = FAISS.from_documents(docs, embedding_function)                    \\r\\n            if os.path.exists(persist_directory):\\r\\n                local_db = FAISS.load_local(persist_directory, embedding_function,allow_dangerous_deserialization=True)\\r\\n                local_db.merge_from(faiss_db)\\r\\n                local_db.save_local(persist_directory)\\r\\n            else:\\r\\n                faiss_db.save_local(persist_directory)\\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in FaissDB', e)\\r\\n            raise e\\r\\n        \\r\\n    def ChromaDB(self,embedding_function:Any=None,docs:Any=None, persist_directory:str=None):\\r\\n        try:\\r\\n            from langchain.vectorstores import Chroma\\r\\n            db = Chroma.from_documents(documents=docs, embedding=embedding_function, persist_directory=persist_directory)\\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in ChromaDB', e)\\r\\n            raise e\\r\\n        \\r\\n    def ElasticsearchDB(self, embedding:Any=None, docs:Any=None, es_url:str='', index_name:str=\\""Vector_Store\\""):   \\r\\n        try:     \\r\\n            from langchain.vectorstores import ElasticVectorSearch       \\r\\n            db = ElasticVectorSearch(elasticsearch_url=es_url, index_name=index_name, embedding=embedding)        \\r\\n            db.add_documents(docs)        \\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in ElasticsearchDB', e)\\r\\n            raise e\\r\\n        \\r\\n    def Qdrant(self,embeddings:Any=None,texts:str='',url:str='',collection_name:str='',force_recreate:bool=False):\\r\\n        try:\\r\\n            from langchain.vectorstores import Qdrant\\r\\n            qdrant = Qdrant.from_documents(\\r\\n            texts,\\r\\n            embeddings,\\r\\n            url= url,\\r\\n            collection_name=collection_name,\\r\\n            force_recreate=force_recreate,\\r\\n            )\\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Qdrant', e)\\r\\n            raise e \\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Vector Store"",""attributes"":{}}","SemanticSearchCore-6"
"SemanticKernel","Core","{""id"":3,""name"":""AzureChatCompletionService"",""category"":""semantic kernel"",""parentCategory"":""1"",""classname"":""AzureChatCompletionService"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion""],""requirements"":[],""script"":""def AzureChatCompletionService(dataobj,llm_model_deployment_name_param,azure_openai_endpoint_param,azure_openai_api_key_param):\\n    kernel=dataobj['kernel']\\n    kernel.add_chat_service(\\n        'chat_completion',\\n        AzureChatCompletion(llm_model_deployment_name_param,azure_openai_endpoint_param,azure_openai_api_key_param),\\n    )\\n    dataobj['kernel']=kernel\\n    return dataobj\\n""},""formats"":{""LLM_MODEL_DEPLOYMENT_NAME"":""text"",""AZURE_OPENAI_ENDPOINT"":""text"",""AZURE_OPENAI_API_KEY"":""text""},""alias"":""AzureChatCompletionService"",""attributes"":{""LLM_MODEL_DEPLOYMENT_NAME"":""gtp35turbo"",""AZURE_OPENAI_ENDPOINT"":""https://azureft.openai.azure.com/"",""AZURE_OPENAI_API_KEY"":""85b968a4b5c84d849c99661788c2c1ed""}}","SemanticKernelCore-3"
"SemanticKernel","Core","{""id"":13,""name"":""Azure Cognitive Search"",""category"":""AzureAiSearch"",""parentCategory"":""12"",""classname"":""AzureCognitiveSearch"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore""],""requirements"":[],""script"":""def AzureCognitiveSearch(dataobj,cognitive_search_endpoint_param,cognitive_search_api_key_param):\\n    COGNITIVE_SEARCH_ENDPOINT=cognitive_search_endpoint_param  \\n    COGNITIVE_SEARCH_API_KEY=cognitive_search_api_key_param  \\n    kernel=dataobj['kernel']\\n    kernel.register_memory_store(memory_store=AzureCognitiveSearchMemoryStore(1536,search_endpoint=COGNITIVE_SEARCH_ENDPOINT, admin_key=COGNITIVE_SEARCH_API_KEY))\\n    dataobj['kernel']=kernel\\n    return dataobj\\n""},""formats"":{""cognitive_search_endpoint"":""text"",""cognitive_search_api_key"":""text""},""alias"":""Azure Cognitive Search"",""attributes"":{""cognitive_search_endpoint"":""https://memory-search.search.windows.net"",""cognitive_search_api_key"":""nqNqlUPa4Z5qcD6GiVOvEoEHPos6pqsHtYwP42dFdXAzSeDe0hq7""}}","SemanticKernelCore-13"
"SemanticKernel","Core","{""id"":23,""name"":""AddTimeSkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""AddTimeSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.core_skills import  TimeSkill""],""requirements"":[],""script"":""def AddTimeSkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    kernel.import_skill(TimeSkill(),'timeskill')\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\n""},""formats"":{},""alias"":""AddTimeSkill"",""attributes"":{}}","SemanticKernelCore-23"
"SemanticKernel","Core","{""id"":22,""name"":""AddTextSkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""AddTextSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.core_skills import TextSkill""],""requirements"":[],""script"":""def AddTextSkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    kernel.import_skill(TextSkill(),'textskill')\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\n""},""formats"":{},""alias"":""AddTextSkill"",""attributes"":{}}","SemanticKernelCore-22"
"SemanticKernel","Core","{""id"":20,""name"":""AddTextMemorySkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""AddTextMemorySkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.core_skills import TextMemorySkill""],""requirements"":[],""script"":""def AddTextMemorySkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    kernel.import_skill(TextMemorySkill(),'texttomemory')\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\n""},""formats"":{},""alias"":""AddTextMemorySkill"",""attributes"":{}}","SemanticKernelCore-20"
"SemanticKernel","Core","{""id"":19,""name"":""AddMathSkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""AddMathSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.core_skills import MathSkill""],""requirements"":[],""script"":""def AddMathSkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    kernel.import_skill(MathSkill(), 'math')\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\n""},""formats"":{},""alias"":""AddMathSkill"",""attributes"":{}}","SemanticKernelCore-19"
"SemanticKernel","Core","{""id"":8,""name"":""Base"",""category"":""Base"",""parentCategory"":"""",""classname"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Base""}","SemanticKernelCore-8"
"SemanticKernel","Core","{""id"":9,""name"":""Python Script"",""category"":""BaseConfig"",""parentCategory"":""8"",""classname"":""PythonScriptConfig"",""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""\\n\\n""},""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""alias"":""Python Script"",""attributes"":{""FunctionName"":""PythonScript"",""requirements"":"""",""params"":"""",""script"":""def PythonScript( dataset):\\\\n    #python-script Data\\\\r\\\\r    return dataset""}}","SemanticKernelCore-9"
"SemanticKernel","Core","{""id"":12,""name"":""Vector Store"",""category"":""Vector Store"",""parentCategory"":"""",""classname"":""Vector Store"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""\\n\\n""},""formats"":{},""alias"":""Vector Store"",""attributes"":{}}","SemanticKernelCore-12"
"SemanticKernel","Core","{""id"":6,""name"":""Skills"",""category"":""Skills"",""parentCategory"":"""",""classname"":""Skills"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""\\n\\n""},""formats"":{},""alias"":""Skills"",""attributes"":{}}","SemanticKernelCore-6"
"SemanticKernel","Core","{""id"":18,""name"":""Connector"",""category"":""Connector"",""parentCategory"":"""",""classname"":""connectiontodb"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""def Connector_<id>(connections_param={}):\\n    print('Connector call')\\n    return connections_param\\n\\n\\n\\n\\n\\n""},""formats"":{""connections"":""""},""alias"":""Connector"",""attributes"":{""connections"":""""}}","SemanticKernelCore-18"
"SemanticKernel","Core","{""id"":1,""name"":""Kernel Services"",""category"":""semantic kernel"",""parentCategory"":"""",""classname"":""Kernel Services"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""\\n\\n\\n\\n""},""formats"":{},""alias"":""Kernel Services"",""attributes"":{}}","SemanticKernelCore-1"
"SemanticKernel","Core","{""id"":24,""name"":"""",""category"":"""",""parentCategory"":""6"",""classname"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[""from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter"",""from currency_converter import CurrencyConverter""],""requirements"":[""CurrencyConverter""],""script"":""def CurrencyConvertSkill(kernel):\\r\\n    # from currency_converter_skill import CurrencyConverterskill\\r\\n    import typing as t\\r\\n    \\r\\n    \\r\\n    if t.TYPE_CHECKING:\\r\\n        from semantic_kernel.kernel import Kernel\\r\\n        from semantic_kernel.orchestration.sk_context import SKContext\\r\\n    import logging\\r\\n    class CurrencyConverterskill:\\r\\n        '''\\r\\n        Description: CurrencyConverter provides a functions to convert ammount of one currency into another.\\r\\n\\r\\n        Usage:\\r\\n            kernel.import_skill(CurrencyConverterskill(), skill_name='CurrencyConverter')\\r\\n\\r\\n        '''\\r\\n\\r\\n        @sk_function(\\r\\n            description='convert currency amount to another currency',\\r\\n            name='currency_converter_fn',\\r\\n            input_description='The ammount to convert',\\r\\n        )\\r\\n        async def currency_converter_fn(self,context:str)-> str:\\r\\n            # return input\\r\\n            result= await self.currency_obj_function.invoke_async(context)\\r\\n            logging.info('reult from currency converter',result)\\r\\n            # return  str(result)\\r\\n            try:\\r\\n                obj = json.loads(str(result)) \\r\\n            except:\\r\\n                return ''\\r\\n\\r\\n            ammount=float(obj['Amount'])\\r\\n            currency_from=obj['currencyfrom']\\r\\n            currency_to=obj['currencyTo']\\r\\n\\r\\n            if ammount is not None and currency_from is not None and currency_to is not None:\\r\\n                try:\\r\\n                    c = CurrencyConverter()\\r\\n                    p=c.convert(ammount,currency_from,currency_to)\\r\\n                    p=round(p,2)\\r\\n                    return str('converted currency amount is '+str(p))\\r\\n                except:\\r\\n                    raise Exception\\r\\n            else:\\r\\n                raise Exception('something went wrong')\\r\\n        \\r\\n\\r\\n        sk_prompt = '''\\r\\n        {{$input}}\\r\\n\\r\\n        check this if it is for currency converter related then \\r\\n        extract bellow three things from this\\r\\n        Amount : \\r\\n        currencyfrom : currency code of country which need to converted in other currency\\r\\n        currencyTo : currency code of country in which amount need to be converted\\r\\n        \\r\\n\\r\\n        only give responce in json format but do not generate answer on your own in text\\r\\n        key will be only (Amount,currencyfrom,currencyTo)\\r\\n\\r\\n        example :\\r\\n\\r\\n        {\\r\\n            'Amount':10,\\r\\n            'currencyfrom': 'INR', \\r\\n            'currencyTo': 'USD'\\r\\n        }\\r\\n        \\r\\n        \\r\\n        '''\\r\\n        \\r\\n        \\r\\n        def __init__(self, kernel: 'Kernel'):\\r\\n            self.currency_obj_function = kernel.create_semantic_function(\\r\\n                self.sk_prompt,\\r\\n                skill_name='currencyConverter',\\r\\n                max_tokens=1000,\\r\\n                temperature=0.1,\\r\\n                top_p=0.5\\r\\n            )\\r\\n\\r\\n    kernel.import_skill(CurrencyConverterskill(kernel),'currenceConverter')\\r\\n    return kernel\\n""},""attributes"":{},""formats"":{},""alias"":""""}","SemanticKernelCore-24"
"SemanticKernel","Core","{""id"":2,""name"":""Addkernel"",""category"":""semantic kernel"",""parentCategory"":""1"",""classname"":""Addkernel"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel import Kernel"",""from semantic_kernel.core_skills import FileIOSkill, MathSkill, TextSkill, TimeSkill, ConversationSummarySkill, TextMemorySkill,WebSearchEngineSkill"",""import openai""],""requirements"":[""semantic-kernel==0.3.14.dev0"",""azure-search-documents==11.4.0b8"",""openai==0.27.10"",""azure-common==1.1.28"",""azure-core==1.29.4""],""script"":""openai.proxy = {\\r\\n    'http' : 'http://blrproxy.ad.infosys.com:80',\\r\\n    'https' : 'http://blrproxy.ad.infosys.com:80'\\r\\n}\\r\\n\\r\\ndef Addkernel():\\r\\n    kernel=Kernel()\\r\\n    dataobj={}\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\n""},""formats"":{},""alias"":""Addkernel"",""attributes"":{}}","SemanticKernelCore-2"
"SemanticKernel","Core","{""id"":5,""name"":""AzureTextEmbeddingService"",""category"":""semantic kernel"",""parentCategory"":""1"",""classname"":""AzureTextEmbeddingService"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding""],""requirements"":[],""script"":""def AzureTextEmbeddingService(dataobj,embedding_model_deployment_name_param,azure_openai_endpoint_param):\\n    azure_openai_api_key_param=os.environ.get(\\""app_azure_openai_api_key\\"",\\""\\"")\\n    kernel=dataobj['kernel']\\n    kernel.add_text_embedding_generation_service('ada', \\n    AzureTextEmbedding(embedding_model_deployment_name_param,azure_openai_endpoint_param,azure_openai_api_key_param))\\n    dataobj['kernel']=kernel\\n    return dataobj\\n\\n\\n\\n\\n""},""formats"":{""EMBEDDING_MODEL_DEPLOYMENT_NAME"":""text"",""AZURE_OPENAI_ENDPOINT"":""text""},""alias"":""AzureTextEmbeddingService"",""attributes"":{""EMBEDDING_MODEL_DEPLOYMENT_NAME"":""openaiada2"",""AZURE_OPENAI_ENDPOINT"":""https://azureft.openai.azure.com/""}}","SemanticKernelCore-5"
"SemanticKernel","Core","{""id"":17,""name"":""WebSearchSkill"",""category"":""WebSearchSkill"",""parentCategory"":""6"",""classname"":""WebSearchSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.connectors.search_engine.google_connector import GoogleConnector"",""from semantic_kernel.core_skills import WebSearchEngineSkill""],""requirements"":[],""script"":""import asyncio\\nfrom aiohttp import ClientSession\\n\\ndef WebSearchSkill(dataobj,api_key_param,search_engine_id_param):\\n    kernel=dataobj['kernel']\\n    context = kernel.create_new_context()\\n    connector =GoogleConnector(api_key_param,search_engine_id_param)\\n    kernel.import_skill(WebSearchEngineSkill(connector), skill_name='WebSearch')\\n    dataobj['kernel']=kernel\\n    return dataobj\\n""},""formats"":{""api_key"":""text"",""search_engine_id"":""text""},""alias"":""WebSearchSkill"",""attributes"":{""api_key"":""AIzaSyCMgVj7Jg1SWiFamaZECm59FrZCXcqyohI"",""search_engine_id"":""318a5c0d34698480a""}}","SemanticKernelCore-17"
"SemanticKernel","Core","{""id"":15,""name"":""Save In Memory"",""category"":""vector store"",""parentCategory"":""12"",""classname"":""SaveInMemory"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""import uuid"",""import asyncio""],""requirements"":[],""script"":"" \\ndef SaveInMemory(dataobj,collection_name_param):\\n    kernel=dataobj['kernel']\\n    def save_in_memory(data,collection_name=collection_name_param)\\n        id = uuid.uuid1()\\n        asyncio.run(kernel.memory.save_information_async(collection_name, id=str(id), text=data))\\n    dataobj['kernel']=kernel\\n    dataobj['save_in_memory']=save_in_memory\\n    return dataobj\\n""},""formats"":{""collection_name"":""text""},""alias"":""Save In Memory"",""attributes"":{""collection_name"":""hr_doc""}}","SemanticKernelCore-15"
"SemanticKernel","Core","{""id"":21,""name"":""AddFileIOSkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""AddFileIOSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.core_skills import FileIOSkill""],""requirements"":[],""script"":""def AddFileIOSkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    kernel.import_skill(FileIOSkill(),'fileio')\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\n""},""formats"":{},""alias"":""AddFileIOSkill"",""attributes"":{}}","SemanticKernelCore-21"
"SemanticKernel","Core","{""id"":11,""name"":""Chatbot"",""category"":""BaseConfig"",""parentCategory"":""8"",""classname"":""Chatbot"",""inputEndpoints"":[""in1""],""outputEndpoints"":[],""codeGeneration"":{""imports"":[""import gradio as gr"",""import asyncio""],""requirements"":[],""script"":""def Chatbot(dataobj,prompt_param,port_param):\\n    kernel=dataobj['kernel']\\n    \\n    chat_fn = kernel.create_semantic_function(\\n        prompt_param,\\n        max_tokens=1000,\\n        temperature=0.1,\\n        top_p=0.5\\n    )\\n\\n    context = kernel.create_new_context()\\n    context['history']=''\\n\\n    def respond(message, chat_history):\\n            context['input']=message\\n            context['document'] = await dataobj['getDataFromVectorMemory'](name)\\n            bot_message= asyncio.run(chat_fn.invoke_async(context=context))\\n            bot_message=str(bot_message)\\n            chat_history.append((message, bot_message))\\n             \\n            context['history'] += f'\\\\nUser: {message}\\\\nChatBot: {bot_message}\\\\n'\\n            return '', chat_history\\n\\n    with gr.Blocks() as demo:\\n        chatbot = gr.Chatbot()\\n        msg = gr.Textbox()\\n        clear = gr.ClearButton([msg, chatbot])\\n        msg.submit(respond, [msg, chatbot], [msg, chatbot])\\n\\n\\n    demo.launch(server_name='0.0.0.0', server_port=port_param, inbrowser=False, inline=False)\\n \\n\\n\\n\\n\\n""},""formats"":{""prompt"":""textarea"",""port"":""text""},""alias"":""Chatbot"",""attributes"":{""prompt"":""ChatBot can have a conversation with you about any topic. please give answer   on the basis of below document   {{$document}}   It can give explicit instructions or say 'I don't know' if it does not have an answer.  {{$history}} User: {{$input}} ChatBot: \\""\\""\\"""",""port"":""8868""}}","SemanticKernelCore-11"
"SemanticKernel","Core","{""id"":16,""name"":""Get Data From Memory"",""category"":"""",""parentCategory"":""12"",""classname"":""GetDataFromMemory"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""\\ndef GetDataFromMemory(dataobj,collection_name_param,limit_param):\\n    kernel=dataobj['kernel']\\n    async def fun(question,collection_name=collection_name_param,limit=limit_param):\\n        result = await kernel.memory.search_async(collection_name,question,limit)\\n        return str(result[0].text)\\n    dataobj['kernel']=kernel\\n    dataobj['getDataFromVectorMemory']=fun\\n    \\n    return dataobj\\n     \\n\\n\\n""},""formats"":{""collection_name"":""text"",""limit"":""integer""},""alias"":""Get Data From Memory"",""attributes"":{""collection_name"":""hr-emp-policy"",""limit"":""3""}}","SemanticKernelCore-16"
"SemanticKernel","Core","{""id"":26,""name"":""AddConversationSummarySkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""AddConversationSummarySkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.core_skills import ConversationSummarySkill""],""requirements"":[],""script"":""def AddConversationSummarySkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    summarize=kernel.import_skill(ConversationSummarySkill(kernel),'summarise')\\r\\n    def summary_fn(content):\\r\\n        content=str(content)\\r\\n        return summarize['SummarizeConversation'](content)\\r\\n    dataobj['kernel']=kernel\\r\\n    dataobj['summary_fn']=summary_fn\\r\\n    return dataobj\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""AddConversationSummarySkill"",""attributes"":{}}","SemanticKernelCore-26"
"SemanticKernel","Core","{""id"":25,""name"":""CurrencyConvertSkill"",""category"":""skills"",""parentCategory"":""6"",""classname"":""CurrencyConvertSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter"",""from currency_converter import CurrencyConverter"",""import typing as t""],""requirements"":[""CurrencyConverter""],""script"":""def CurrencyConvertSkill(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    if t.TYPE_CHECKING:\\r\\n        from semantic_kernel.kernel import Kernel\\r\\n        from semantic_kernel.orchestration.sk_context import SKContext\\r\\n    import logging\\r\\n    class CurrencyConverterskill:\\r\\n            '''\\r\\n            Description: CurrencyConverter provides a functions to convert ammount of one currency into another.\\r\\n            Usage:\\r\\n                kernel.import_skill(CurrencyConverterskill(), skill_name='CurrencyConverter')\\r\\n            '''\\r\\n            @sk_function(\\r\\n                description='convert currency amount to another currency',\\r\\n                name='currency_converter_fn',\\r\\n                input_description='The ammount to convert',\\r\\n            )\\r\\n            @sk_function_context_parameter(name='history', description='history of conversation')\\r\\n            async def currency_converter_fn(self,context:'SKContext')-> str:\\r\\n                \\r\\n                result= await self.currency_obj_function.invoke_async(context=context)\\r\\n                \\r\\n                try:\\r\\n                    obj = json.loads(str(result)) \\r\\n                    \\r\\n                    ammount=float(obj['Amount'])\\r\\n                    currency_from=obj['currencyfrom']\\r\\n                    currency_to=obj['currencyTo']\\r\\n                    \\r\\n                    c = CurrencyConverter()\\r\\n                    p=c.convert(ammount,currency_from,currency_to)\\r\\n                    p=round(p,2)\\r\\n                    convertedCurrencyResult=str(f'ammount {ammount} in {currency_from} is equivalant to {str(p)} in {currency_to}')\\r\\n                    print(convertedCurrencyResult)\\r\\n                    return convertedCurrencyResult\\r\\n\\r\\n                except Exception as e:\\r\\n                    logger.info('error occured in currency converter..',e)\\r\\n                    return ''\\r\\n\\r\\n            sk_prompt = '''\\r\\n            {{$history}}\\r\\n            you are currency converter tool that provoide json object which contain ammount and currency codes\\r\\n            read above conversation and understand if it is for currency converter related then \\r\\n            try to understand which ammount is need to convert in which currency \\r\\n            and then only extract bellow three things from this conversation \\r\\n            \\r\\n            Amount : \\r\\n            currencyfrom : currency code of country which need to converted in other currency\\r\\n            currencyTo : currency code of country in which amount need to be converted\\r\\n            only give responce in json format and do not generate answer on your own in text\\r\\n\\r\\n            do not give any other info just give bellow in json formate\\r\\n            and give property name enclosed in double quotes\\r\\n            key will be only (Amount,currencyfrom,currencyTo)\\r\\n            example :\\r\\n\\r\\n            {\\r\\n                'Amount':10,\\r\\n                'currencyfrom': 'INR', \\r\\n                'currencyTo': 'USD'\\r\\n            }\\r\\n\\r\\n            '''\\r\\n\\r\\n            def __init__(self, kernel: 'Kernel'):\\r\\n                self.kernel=kernel\\r\\n                self.currency_obj_function = self.kernel.create_semantic_function(\\r\\n                    self.sk_prompt,\\r\\n                    skill_name='currencyConverter',\\r\\n                    max_tokens=1000,\\r\\n                    temperature=0.1,\\r\\n                    top_p=0.5\\r\\n                )\\r\\n\\r\\n    kernel.import_skill(CurrencyConverterskill(kernel),'CurrencyConverter')\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\r\\n\\n""},""formats"":{},""alias"":""CurrencyConvertSkill"",""attributes"":{}}","SemanticKernelCore-25"
"SemanticKernel","Core","{""id"":27,""name"":""Planner"",""category"":""Planner"",""parentCategory"":"""",""classname"":""Planner"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Planner""}","SemanticKernelCore-27"
"SemanticKernel","Core","{""id"":4,""name"":""AzureTextCompletionService"",""category"":""semantic kernel"",""parentCategory"":""1"",""classname"":""AzureTextCompletionService"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion""],""requirements"":[],""script"":""def AzureTextCompletionService(dataobj,llm_model_deployment_name_param,azure_openai_endpoint_param,azure_openai_api_key_param):\\n    kernel=dataobj['kernel']\\n    kernel.add_text_completion_service(\\n        'text_completion',\\n        AzureTextCompletion(llm_model_deployment_name_param,azure_openai_endpoint_param,azure_openai_api_key_param),\\n    )\\n    dataobj['kernel']=kernel\\n    \\n    return dataobj\\n\\n\\n""},""formats"":{""LLM_MODEL_DEPLOYMENT_NAME"":""text"",""AZURE_OPENAI_ENDPOINT"":""text"",""AZURE_OPENAI_API_KEY"":""text""},""alias"":""AzureTextCompletionService"",""attributes"":{""LLM_MODEL_DEPLOYMENT_NAME"":""gtp35turbo"",""AZURE_OPENAI_ENDPOINT"":""https://azureft.openai.azure.com/"",""AZURE_OPENAI_API_KEY"":""85b968a4b5c84d849c99661788c2c1ed""}}","SemanticKernelCore-4"
"SemanticKernel","Core","{""id"":28,""name"":""AddActionPlanner"",""category"":""Planner"",""parentCategory"":""27"",""classname"":""AddActionPlanner"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.planning import ActionPlanner""],""requirements"":[],""script"":""def AddActionPlanner(dataobj,goal_param):\\r\\n    kernel=dataobj['kernel']\\r\\n    async def planner_func(context,Goal=goal_param):\\r\\n        planner = ActionPlanner(kernel)\\r\\n        plan = await planner.create_plan_async(goal=Goal)\\r\\n        result = await plan.invoke_async(context=context)\\r\\n        return result\\r\\n    dataobj['planner']=planner_func\\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{""goal"":""textarea""},""alias"":""AddActionPlanner"",""attributes"":{""goal"":""""}}","SemanticKernelCore-28"
"SemanticKernel","Core","{""id"":7,""name"":""NLToSQL"",""category"":""skills"",""parentCategory"":""6"",""classname"":""NLToSQL"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter""],""requirements"":[],""script"":""def NLToSQL(dataobj):\\r\\n    kernel=dataobj['kernel']\\r\\n    import typing as t\\r\\n    if t.TYPE_CHECKING:\\r\\n        from semantic_kernel.kernel import Kernel\\r\\n        from semantic_kernel.orchestration.sk_context import SKContext\\r\\n    \\r\\n    class NaturalLanguageToSqlSkill:\\r\\n        '''\\r\\n        Description:  NaturalLanguageToSqlSkill take user input as natural language query and table information (table name and scheema) and generate sql query for that.\\r\\n\\r\\n        Usage:\\r\\n            kernel.import_skill(NaturalLanguageToSqlSkill(), skill_name='NaturalLangToSql')\\r\\n\\r\\n        '''\\r\\n\\r\\n        @sk_function(\\r\\n            description='generate sql query on given natural language',\\r\\n            name='nl_to_sql',\\r\\n            input_description='natural language, table information, table name and sheema',\\r\\n        )\\r\\n        @sk_function_context_parameter(name='NL_query', description='Natural language query to generate sql')\\r\\n        @sk_function_context_parameter(name='Table_Name', description='name of sql table')\\r\\n        @sk_function_context_parameter(name='Table_Schema', description='name of table scheemas')\\r\\n        async def nl_to_sql(self,context:'SKContext')-> str:\\r\\n            # return input\\r\\n            result= await self.semantic_function.invoke_async(context=context)\\r\\n            return str(result)\\r\\n    \\r\\n        sk_prompt = '''\\r\\n        \\r\\n            you are sql query geneation tool your task is to generate sql query \\r\\n            given that following table information \\r\\n            table with name : {{$Table_Name}}\\r\\n            and comma seperated schema's : {{$Table_Schema}}\\r\\n            \\r\\n            Convert the below natural language question into a sql query and write only sql query do not write any other info\\r\\n            question :  {{$NL_query}}\\r\\n            if question is not related to sql query don't give any answer\\r\\n            \\r\\n            give only sql query\\r\\n            '''\\r\\n        \\r\\n        \\r\\n        def __init__(self, kernel: 'Kernel'):\\r\\n            self.kernel=kernel\\r\\n            self.semantic_function = self.kernel.create_semantic_function(\\r\\n                self.sk_prompt,\\r\\n                skill_name='NaturalLanguageToSql',\\r\\n                max_tokens=1000,\\r\\n                temperature=0.1,\\r\\n                top_p=0.5\\r\\n            )\\r\\n\\r\\n    kernel.import_skill(NaturalLanguageToSqlSkill(kernel),'NaturalLangToSql')\\r\\n    \\r\\n    dataobj['kernel']=kernel\\r\\n    return dataobj\\r\\n    \\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""NLToSQL"",""attributes"":{}}","SemanticKernelCore-7"
"SemanticKernel","Core","{""id"":10,""name"":""CustomSkill"",""category"":""BaseConfig"",""parentCategory"":""8"",""classname"":""CustomSkill"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[""from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter""],""requirements"":[],""script"":""def CustomSkill(dataobj,prompt_param,function_name_param,skill_name_param,description_param,temperature_param,top_p_param,max_tokens_param):\\r\\n    kernel=dataobj['kernel']\\r\\n    \\r\\n    custom_skill=kernel.create_semantic_function(\\r\\n        prompt_template=prompt_param,\\r\\n        function_name=function_name_param,\\r\\n        skill_name= skill_name_param,\\r\\n        description= description_param,\\r\\n        temperature= temperature_param,\\r\\n        top_p= top_p_param,\\r\\n        max_tokens=max_tokens_param\\r\\n    )\\r\\n \\r\\n    dataobj['kernel']=kernel\\r\\n    dataobj['CustomSkill']=custom_skill\\r\\n    return dataobj\\n""},""formats"":{""prompt"":""textarea"",""function_name"":""text"",""skill_name"":""text"",""description"":""textarea"",""temperature"":""int"",""top_p"":""int"",""max_tokens"":""int""},""alias"":""CustomSkill"",""attributes"":{""prompt"":"""",""function_name"":"""",""skill_name"":"""",""description"":"""",""temperature"":""0.1"",""top_p"":""0.5"",""max_tokens"":""1000""}}","SemanticKernelCore-10"
"SemanticKernel","Core","{""formats"":{""url"":""text""},""classname"":""Qdrant"",""name"":""Qdrant"",""parentCategory"":""12"",""alias"":""Qdrant"",""id"":""HRQDRNT17798"",""codeGeneration"":{""requirements"":[],""imports"":[""from semantic_kernel.connectors.memory.qdrant import QdrantMemoryStore""],""script"":""def Qdrant(dataobj,url_param):\\n    os.environ['NO_PROXY']='10.*,localhost,0.0.0.0,victlpast02,0.0.0.0,127.0.0.1,10.81.78.146,victcbfc160,10.156.89.8,10.224.216.10,10.66.15.100'\\n    os.environ['no_proxy']='10.*,localhost,0.0.0.0,victlpast02,127.0.0.1,10.81.78.146,victcbfc160,10.156.89.8,10.224.216.10,10.66.15.100'\\n    kernel=dataobj['kernel']\\n    qdrant_store=QdrantMemoryStore(vector_size=1536,url=url_param)\\n    kernel.register_memory_store(qdrant_store)\\n    dataobj['kernel']=kernel\\n    return dataobj\\n\\n\\n\\n\\n\\n""},""category"":"""",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""url"":""http://10.66.15.100:30312""}}","HRQDRNT17798"
"SemanticKernel","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""DatasetExtractor"",""name"":""Dataset Extractor"",""parentCategory"":"""",""alias"":""Dataset Extractor"",""id"":""CORDTSTX45505"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\n""},""category"":""Extractor"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset"":""""}}","CORDTSTX45505"
"SemanticKernel","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""MYSQLExtractorConfig"",""name"":""MYSQL Extractor"",""parentCategory"":""CORDTSTX45505"",""alias"":""MYSQL Extractor"",""id"":""CORMYSQL65307"",""codeGeneration"":{""requirements"":[""mysql-connector-python""],""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""import json"",""from leaputils import Security""],""script"":""def MYSQLExtractor(dataset_param=''):\\r    attributes = json.loads(dataset_param['attributes'])\\r    datasource = json.loads(dataset_param['datasource']['connectionDetails'])\\r    def getConnection():\\r        username = datasource['userName']\\r        password = Security.decrypt(datasource['password'], dataset_param['datasource']['salt'])\\r        url = datasource['url']\\r        host = urlparse(url[5:]).hostname\\r        port = urlparse(url[5:]).port\\r        database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port=port)\\r        return connection\\r    connection = getConnection()\\r\\r    query = attributes['Query']  # self.mapQueryParams()\\r    cursor = connection.cursor(dictionary=True)\\r    cursor.execute(query)\\r    results = cursor.fetchall()\\r    return results\\r\\r\\n""},""category"":""ExtractorConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset"":""""}}","CORMYSQL65307"
"SemanticKernel","Core","{""formats"":{""dataset"":""dropdwon""},""classname"":""DatasetSourceConfig"",""name"":""Dataset Source"",""parentCategory"":""CORDTSTX45505"",""alias"":""Dataset Source"",""attributes"":{""dataset"":""""},""id"":""CORDTSTS26549"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DataSourcedictREST(dataset_datasource_Url='', dataset_datasource_AuthDetails_username='', dataset_datasource_salt='', dataset_datasource_AuthDetails_password=''):\\r\\n    DSdict = {\\r\\n        'Url': dataset_datasource_Url,\\r\\n        'salt': dataset_datasource_salt,\\r\\n        'AuthDetails': {\\r\\n            'username': dataset_datasource_AuthDetails_username,\\r\\n            'password': dataset_datasource_AuthDetails_password\\r\\n        }\\r\\n    }\\r\\n    return DSdict\\r\\n\\r\\n\\r\\n\\n""},""category"":""DatasetSourceConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""]}","CORDTSTS26549"
"SemanticKernel","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""DatasetExtractorConfig"",""name"":""MINIO Extractor"",""parentCategory"":""CORDTSTX45505"",""alias"":""MINIO Extractor"",""attributes"":{""dataset"":""""},""id"":""CORMNXTR24926"",""codeGeneration"":{""requirements"":[""minio""],""imports"":[""from urllib.parse import urlparse"",""from minio import Minio""],""script"":""\\n\\nd\\ne\\nf\\n \\nD\\na\\nt\\na\\ns\\ne\\nt\\nE\\nx\\nt\\nr\\na\\nc\\nt\\no\\nr\\nM\\nI\\nN\\nI\\nO\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\nu\\nr\\nl\\n=\\n'\\n'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\na\\nc\\nc\\ne\\ns\\ns\\nK\\ne\\ny\\n=\\n'\\n'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\ns\\ne\\nc\\nr\\ne\\nt\\nK\\ne\\ny\\n=\\n'\\n'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n=\\n'\\n'\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n=\\n'\\n'\\n)\\n:\\n\\n\\n \\n \\n \\n \\nU\\nR\\nL\\n \\n=\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\nu\\nr\\nl\\n\\n\\n \\n \\n \\n \\ns\\ne\\nc\\nu\\nr\\ne\\n \\n=\\n \\nT\\nr\\nu\\ne\\n \\ni\\nf\\n \\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\ns\\nc\\nh\\ne\\nm\\ne\\n \\n=\\n=\\n \\n'\\nh\\nt\\nt\\np\\ns\\n'\\n \\ne\\nl\\ns\\ne\\n \\nF\\na\\nl\\ns\\ne\\n\\n\\n \\n \\n \\n \\nc\\nl\\ni\\ne\\nn\\nt\\n \\n=\\nM\\ni\\nn\\ni\\no\\n(\\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\nh\\no\\ns\\nt\\nn\\na\\nm\\ne\\n+\\n'\\n:\\n'\\n+\\ns\\nt\\nr\\n(\\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\np\\no\\nr\\nt\\n)\\n,\\na\\nc\\nc\\ne\\ns\\ns\\n_\\nk\\ne\\ny\\n=\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\na\\nc\\nc\\ne\\ns\\ns\\nK\\ne\\ny\\n,\\ns\\ne\\nc\\nr\\ne\\nt\\n_\\nk\\ne\\ny\\n=\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\ns\\ne\\nc\\nr\\ne\\nt\\nK\\ne\\ny\\n,\\ns\\ne\\nc\\nu\\nr\\ne\\n=\\ns\\ne\\nc\\nu\\nr\\ne\\n)\\n\\n\\n \\n \\n \\n \\ni\\nf\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n.\\ns\\np\\nl\\ni\\nt\\n(\\n'\\n.\\n'\\n)\\n[\\n-\\n1\\n]\\n \\n=\\n=\\n \\n'\\nc\\ns\\nv\\n'\\n:\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\no\\nb\\nj\\n \\n=\\n \\nc\\nl\\ni\\ne\\nn\\nt\\n.\\ng\\ne\\nt\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n \\n=\\n \\np\\nd\\n.\\nr\\ne\\na\\nd\\n_\\nc\\ns\\nv\\n(\\no\\nb\\nj\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\nt\\nu\\nr\\nn\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n\\n\\n \\n \\n \\n \\ne\\nl\\ns\\ne\\n:\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n \\n=\\n \\n'\\n.\\n/\\n'\\n \\n+\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\ns\\nu\\nl\\nt\\n \\n=\\n \\nc\\nl\\ni\\ne\\nn\\nt\\n.\\nf\\ng\\ne\\nt\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n,\\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\nt\\nu\\nr\\nn\\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n\\n\\n""},""category"":""ExtractorConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""]}","CORMNXTR24926"
"SemanticKernel","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""NutanixExtractor"",""name"":""Nutanix Extractor"",""parentCategory"":""CORDTSTX45505"",""alias"":""Nutanix Extractor"",""id"":""CORNTNXT11687"",""codeGeneration"":{""requirements"":[],""imports"":[""import re,shutil,pathlib,json,boto3,botocore"",""from pathlib import Path""],""script"":""def NutanixExtractor_<id>(dataset_param={}):\\r    if isinstance(dataset_param['attributes'],str):\\r\\r\\t    dataset_attributes = json.loads(dataset_param['attributes'])\\r\\r    else:\\r\\r\\t    dataset_attributes = dataset_param['attributes']\\r\\r    bucket = dataset_attributes['bucket']\\r\\r    key = dataset_attributes['path']\\r\\r    # bucket = json.loads(dataset_param['attributes'])['bucket']\\r\\r    # key = json.loads(dataset_param['attributes'])['path']\\r\\r    WORKING_DIRECTORY ='dataset_file'\\r\\r    credentials = {'secret_key':'g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0','endpoint':'https://10.82.53.110','access_key':'GISeSU7xd6WBnXrU-QbffBee7WsCxaE2'}\\r\\r    if os.path.exists(WORKING_DIRECTORY):\\r\\r        shutil.rmtree(WORKING_DIRECTORY)\\r\\r    if not os.path.exists(WORKING_DIRECTORY):\\r\\r        os.makedirs(WORKING_DIRECTORY)\\r\\r    model_path = os.path.join(WORKING_DIRECTORY)\\r\\r\\r\\r    endpoint = credentials['endpoint']\\r\\r    access_key = credentials['access_key']\\r\\r    secret_key = credentials['secret_key']\\r\\r    s3 = boto3.resource(service_name='s3', endpoint_url=endpoint, aws_access_key_id=access_key,\\r\\r                        aws_secret_access_key=secret_key, verify=False)\\r\\r    bucket_object = s3.Bucket(bucket)\\r\\r    head, file_name = os.path.split(key)\\r\\r    for my_bucket_object in bucket_object.objects.filter(Prefix=key):\\r\\r        logger.info(my_bucket_object)\\r\\r        object_save_path = (\\r\\r            f'{model_path}/{pathlib.Path(my_bucket_object.key).name}'\\r\\r        )\\r\\r        bucket_object.download_file(my_bucket_object.key, object_save_path)\\r\\r    \\r    downloaded_dir_path =  os.path.dirname(__file__)\\r    dataset_file_path = os.path.join(downloaded_dir_path,object_save_path)\\r\\r    logger.info(dataset_file_path,'download completed')\\r\\r    return dataset_file_path\\r\\r\\r\\r\\r\\r\\n""},""category"":""Extractor"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset"":""""}}","CORNTNXT11687"
"SemanticKernel","Core","{""formats"":{""port"":""integer"",""script"":""textarea""},""classname"":""FlaskAPP"",""name"":""FlaskAPP"",""parentCategory"":""8"",""alias"":""FlaskAPP"",""id"":""CORFLSKP94823"",""codeGeneration"":{""requirements"":[""flask""],""imports"":[""from flask import Flask,jsonify,request""],""script"":""def FlaskAPP(port_param=5000,script_param=''):\\n    app.run(debug=False, host='0.0.0.0', port = port_param)\\n\\n\\n\\n""},""category"":""FlaskAPP"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""port"":"""",""script"":""""}}","CORFLSKP94823"
"SemanticKernel","Core","{""formats"":{""script"":""textArea""},""classname"":""PythonClass"",""name"":""Python Class"",""parentCategory"":""8"",""alias"":""Python Class"",""attributes"":{""script"":""""},""id"":""CORPYTHN79243"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""PythonClass"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""]}","CORPYTHN79243"
