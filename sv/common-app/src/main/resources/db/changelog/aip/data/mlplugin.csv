"name","plugin","type","config","org"
"genericPython","[{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"vocabSize\":\"text\"},\"classname\":\"CountVectorizerFeatureExtractorConfig\",\"name\":\"Count Vectorizer\",\"alias\":\"Count Vectorizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"vocabSize\":\"\"},\"id\":0,\"category\":\"FeatureExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #CountVectorizer\\n    transformer_<id> = CountVectorizer(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\", vocabSize=int(\\\"<vocabSize>\\\"))\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import CountVectorizer\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"TfFeatureExtractorConfig\",\"name\":\"TF\",\"alias\":\"TF\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"FeatureExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #MLHashingTF\\n    transformer_<id> = MLHashingTF(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\")\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import HashingTF as MLHashingTF\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"IdfFeatureExtractorConfig\",\"name\":\"IDF\",\"alias\":\"IDF\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"FeatureExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #MLIDF\\n    transformer_<id> = MLIDF(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\")\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import IDF as MLIDF\"]}},{\"requiredJars\":[],\"formats\":{\"script\":\"textarea\"},\"classname\":\"PythonScriptTransformerConfig\",\"name\":\"Python Script Transformer\",\"alias\":\"Python Script Transformer\",\"attributes\":{\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"            \\r\",\"        return dataset\"]},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"textarea\",\"outputCol\":\"textarea\",\"tags\":\"textarea\"},\"classname\":\"WordLemmetizerTransformerConfig\",\"name\":\"Word Lemmetizer\",\"alias\":\"Word Lemmetizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"tags\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"textarea\",\"outputCol\":\"textarea\"},\"classname\":\"WordStemmerTransformerConfig\",\"name\":\"Word Stemmer\",\"alias\":\"Word Stemmer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"smoothing\":\"text\",\"modelType\":\"text\"},\"classname\":\"NaiveBayesTransformerConfig\",\"name\":\"Naive Bayes\",\"alias\":\"Naive Bayes\",\"attributes\":{\"smoothing\":\"\",\"modelType\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #NaiveBayes\\n    transformer_<id> = NaiveBayes(smoothing=float(\\\"<smoothing>\\\"), modelType=\\\"<modelType>\\\")\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.classification import NaiveBayes\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"TokenizerTransformerConfig\",\"name\":\"Tokenizer\",\"alias\":\"Tokenizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #Tokenizer\\n    transformer_<id> = Tokenizer(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\")\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import Tokenizer\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"pattern\":\"text\"},\"classname\":\"RegexTokenizerTransformerConfig\",\"name\":\"Regex Tokenizer\",\"alias\":\"Regex Tokenizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"pattern\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #RegexTokenizer\\n    transformer_<id> = RegexTokenizer(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\", pattern=\\\"<pattern>\\\")\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import  RegexTokenizer\"]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"outputCol\":\"text\"},\"classname\":\"VectorAssemblerTransformerConfig\",\"name\":\"Vector Assembler\",\"alias\":\"Vector Assembler\",\"attributes\":{\"inputCols\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #VectorAssembler\\n    vainputColumns = \\\"<inputCols>\\\".split(\\\",\\\")\\n    vaoutputColumn = \\\"<outputCol>\\\"\\n    transformer_<id> = VectorAssembler(inputCols=vainputColumns, outputCol=vaoutputColumn)\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"withStd\":\"text\",\"withMean\":\"text\"},\"classname\":\"StandardScalerTransformerConfig\",\"name\":\"Standard Scaler\",\"alias\":\"Standard Scaler\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"withStd\":\"\",\"withMean\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #StandardScaler\\n    transformer_<id> = StandardScaler(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\", withStd=\\\"<withStd>\\\", withMean=\\\"<withMean>\\\")\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import StandardScaler\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"K\":\"text\"},\"classname\":\"PCATransformerConfig\",\"name\":\"PCA\",\"alias\":\"PCA\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"K\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #PCA\\n    transformer_<id> = PCA(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\", k=int(\\\"<K>\\\"))\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.feature import PCA\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"K\":\"text\"},\"classname\":\"KMeansTransformerConfig\",\"name\":\"K Means\",\"alias\":\"K Means\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"K\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #KMeans\\n    transformer_<id> = KMeans(inputCol= \\\"<inputCol>\\\", outputCol=\\\"<outputCol>\\\", k=int(\\\"<K>\\\"))\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[\"from pyspark.ml.clustering import KMeans\"]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"checkpointInterval\":\"text\",\"K\":\"text\"},\"classname\":\"LDAConfig\",\"name\":\"LDA\",\"alias\":\"LDA\",\"attributes\":{\"inputCol\":\"\",\"checkpointInterval\":\"\",\"K\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"modelName\":\"text\"},\"classname\":\"ModelSinkConfig\",\"name\":\"Model Sink\",\"alias\":\"Model Sink\",\"attributes\":{\"modelName\":\"\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #ModelSink\\n    modelPath = os.path.join(os.environ[\\\"JOB_DIRECTORY\\\"],\'models\',\'spark\')\\n    logging.info(\\\"Saving Model at path:\\\"+modelPath )\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    modelPath = os.path.join(modelPath, \'<modelName>\')\\n    model.write().overwrite().save(modelPath)\",\"imports\":[\"import os\"]}},{\"requiredJars\":[],\"formats\":{\"modelName\":\"text\"},\"classname\":\"ModelSourceConfig\",\"name\":\"Model Source\",\"alias\":\"Model Source\",\"attributes\":{\"modelName\":\"\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"PostProcessingScriptConfig\",\"name\":\"Post Processing Script\",\"alias\":\"Post Processing Script\",\"attributes\":{\"params\":\"\",\"script\":\"\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"PreProcessingScriptConfig\",\"name\":\"Pre Processing Script\",\"alias\":\"Pre Processing Script\",\"attributes\":{\"params\":\"\",\"script\":\"\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\",\"numTrees\":\"text\"},\"classname\":\"RandomForestClassifierConfig\",\"name\":\"Random Forest Classifier\",\"alias\":\"Random Forest Classifier\",\"attributes\":{\"labelCol\":\"\",\"featuresCol\":\"\",\"numTrees\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"stopWords\":\"text\"},\"classname\":\"StopWordsRemoverTransformerConfig\",\"name\":\"Stop Words Remover\",\"alias\":\"Stop Words Remover\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"stopWords\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"n\":\"text\",\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"NgramTransformerConfig\",\"name\":\"N Gram\",\"alias\":\"N Gram\",\"attributes\":{\"n\":\"\",\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"inverse\":\"text\"},\"classname\":\"DiscreteCosineTransformTransformerConfig\",\"name\":\"Discrete Cosine Transform\",\"alias\":\"Discrete Cosine Transform\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"inverse\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"outputCols\":\"text\"},\"classname\":\"StringIndexerTransformerConfig\",\"name\":\"String Indexer\",\"alias\":\"String Indexer\",\"attributes\":{\"inputCols\":\"\",\"outputCols\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #StringIndexer\\n    listSInputColumns = \\\"<inputCols>\\\".split(\',\')\\n    listSIOutputColumns = \\\"<outputCols>\\\".split(\',\')\\n    for i in range(0, len(listSInputColumns)):\\n        transformer_<id> = StringIndexer(inputCol=listSInputColumns[i], outputCol=listSIOutputColumns[i],handleInvalid=\'keep\')\\n        pipelineStages.append(transformer_<id>)\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"p\":\"text\"},\"classname\":\"NormalizerTransformerConfig\",\"name\":\"Normalizer\",\"alias\":\"Normalizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"p\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"MinMaxScalerTransformerConfig\",\"name\":\"Min Max Scaler\",\"alias\":\"Min Max Scaler\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"statement\":\"text\"},\"classname\":\"SQLTransformerConfig\",\"name\":\"SQL Transformer\",\"alias\":\"SQL Transformer\",\"attributes\":{\"statement\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\"},\"classname\":\"AnomolyDetectionConfig\",\"name\":\"Anomoly Detection\",\"alias\":\"Anomoly Detection\",\"attributes\":{\"inputCols\":\"\"},\"id\":0,\"category\":\"DomainSolutionConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"predictionCount\":\"text\",\"seasonality\":\"text\"},\"classname\":\"ARIMAModelConfig\",\"name\":\"ARIMA\",\"alias\":\"ARIMA\",\"attributes\":{\"inputCol\":\"\",\"predictionCount\":\"\",\"seasonality\":\"\"},\"id\":0,\"category\":\"DomainSolutionConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"predictionCount\":\"text\",\"groupCol\":\"text\"},\"classname\":\"ARIMAGroupedConfig\",\"name\":\"ARIMA Grouped\",\"alias\":\"ARIMA Grouped\",\"attributes\":{\"inputCol\":\"\",\"predictionCount\":\"\",\"groupCol\":\"\"},\"id\":0,\"category\":\"DomainSolutionConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"outputCol\":\"text\",\"ambiguousWords\":\"text\"},\"classname\":\"CleanTicketsConfig\",\"name\":\"Clean Tickets\",\"alias\":\"Clean Tickets\",\"attributes\":{\"inputCols\":\"\",\"outputCol\":\"clean_text\",\"ambiguousWords\":\"\"},\"id\":0,\"category\":\"DomainSolutionConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"UniqueIdColumn\":\"text\",\"GroupByColumn\":\"text\",\"TextColumn\":\"text\",\"customStopWords\":\"text\",\"vocabSize\":\"text\",\"ClusterCount\":\"text\",\"checkPointInterval\":\"text\",\"saveModels\":\"checkbox\"},\"classname\":\"IncidentClusteringLDAConfig\",\"name\":\"Incident Clustering\",\"alias\":\"Incident Clustering\",\"attributes\":{\"UniqueIdColumn\":\"\",\"GroupByColumn\":\"\",\"TextColumn\":\"\",\"customStopWords\":\"\",\"vocabSize\":\"\",\"ClusterCount\":\"\",\"checkPointInterval\":\"\",\"saveModels\":\"\"},\"id\":0,\"category\":\"DomainSolutionConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"topics\",\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\"},\"classname\":\"PhishingDetectionConfig\",\"name\":\"Phishing Detection\",\"alias\":\"Phishing Detection\",\"attributes\":{\"inputCols\":\"\"},\"id\":0,\"category\":\"DomainSolutionConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\"},\"classname\":\"LogisticRegressionConfig\",\"name\":\"Logistic Regression\",\"alias\":\"Logistic Regression\",\"attributes\":{\"labelCol\":\"\",\"featuresCol\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\"},\"classname\":\"DecisionTreeClassifierConfig\",\"name\":\"Decision Tree Classifier\",\"alias\":\"Decision Tree Classifier\",\"attributes\":{\"labelCol\":\"\",\"featuresCol\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\",\"maxIter\":\"text\"},\"classname\":\"GBTClassifierConfig\",\"name\":\"GBT Classifier\",\"alias\":\"GBT Classifier\",\"attributes\":{\"labelCol\":\"\",\"featuresCol\":\"\",\"maxIter\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\",\"maxIter\":\"text\",\"regParam\":\"text\"},\"classname\":\"LinearSVCConfig\",\"name\":\"Linear SVC\",\"alias\":\"Linear SVC\",\"attributes\":{\"labelCol\":\"\",\"featuresCol\":\"\",\"maxIter\":\"\",\"regParam\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\",\"maxIter\":\"text\",\"regParam\":\"text\",\"elasticNetParam\":\"text\"},\"classname\":\"LinearRegressionConfig\",\"name\":\"Linear Regression\",\"alias\":\"Linear Regression\",\"attributes\":{\"labelCol\":\"\",\"featuresCol\":\"\",\"maxIter\":\"\",\"regParam\":\"\",\"elasticNetParam\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #LinearRegression\\n    analyzer_<id> = LinearRegression(featuresCol=\\\"<featuresCol>\\\",labelCol=\\\"<labelCol>\\\",maxIter=int(\\\"<maxIter>\\\"), regParam=float(\\\"<regParam>\\\"), elasticNetParam=float(\\\"<elasticNetParam>\\\"))\",\"imports\":[\"from pyspark.ml.regression  import LinearRegression\"]}},{\"requiredJars\":[],\"formats\":{\"featuresCol\":\"text\",\"labelCol\":\"text\"},\"classname\":\"RandomForestRegressorConfig\",\"name\":\"Random Forest Regressor\",\"alias\":\"Random Forest Regressor\",\"attributes\":{\"featuresCol\":\"\",\"labelCol\":\"\"},\"id\":0,\"category\":\"AnalyzerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"classname\":\"DatasetExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Dataset\",\"attributes\":{\"dataset\":\"\",\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"MYSQL\":{\"script\":\"    dataset_<id> = spark.read.format(\\\"jdbc\\\").options(url=\\\"<dataset.datasource.connectionDetails.url>\\\",dbtable=\\\"(<dataset.attributes.Query>)t1\\\",user=\\\"<dataset.datasource.connectionDetails.userName>\\\",password=Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")).load()\",\"imports\":[\"from leaputils import Security\"]},\"FILE\":{\"CSV\":{\"script\":\"    dataset_<id> = spark.read.format(\'com.databricks.spark.csv\').options(header=\'true\', inferschema=\'true\',\\n                                                                               delimiter=\\\"<dataset.attributes.delimiter>\\\").load(\\\"<dataset.attributes.path>\\\")\\n    \",\"imports\":[]}}}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"seed\":\"text\"},\"classname\":\"Word2VecFeatureExtractorConfig\",\"name\":\"Word2Vec\",\"alias\":\"Word2Vec\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"seed\":\"\"},\"id\":0,\"category\":\"FeatureExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"outputCol\":\"text\",\"categoricalCols\":\"text\"},\"classname\":\"FeatureHasherFeatureExtractorConfig\",\"name\":\"FeatureHasher\",\"alias\":\"FeatureHasher\",\"attributes\":{\"inputCols\":\"\",\"outputCol\":\"\",\"categoricalCols\":\"\"},\"id\":0,\"category\":\"FeatureExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"BinarizerTransformerConfig\",\"name\":\"Binarizer\",\"alias\":\"Binarizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"PolynomialExpansionConfig\",\"name\":\"PolynomialExpansion\",\"alias\":\"PolynomialExpansion\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"labels\":\"text\"},\"classname\":\"IndexToStringTransformerConfig\",\"name\":\"IndexToString\",\"alias\":\"IndexToString\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"labels\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"outputCols\":\"text\"},\"classname\":\"OneHotEncoderEstimatorTransformerConfig\",\"name\":\"OneHotEncoderEstimator\",\"alias\":\"OneHotEncoderEstimator\",\"attributes\":{\"inputCols\":\"\",\"outputCols\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\n    #OneHotEncoder\\n    oheinputColumns = \\\"<inputCols>\\\".split(\',\')\\n    oheoutputColumn = \\\"<outputCols>\\\".split(\',\')\\n    transformer_<id> = OneHotEncoder(inputCols=oheinputColumns, outputCols=oheoutputColumn)\\n    pipelineStages.append(transformer_<id>)\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"VectorIndexerTransformerConfig\",\"name\":\"VectorIndexer\",\"alias\":\"VectorIndexer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"classname\":\"MaxAbsScalerTransformerConfig\",\"name\":\"MaxAbsScaler\",\"alias\":\"MaxAbsScaler\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"splits\":\"text\"},\"classname\":\"BucketizerTransformerConfig\",\"name\":\"Bucketizer\",\"alias\":\"Bucketizer\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"splits\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"scalingVec\":\"text\"},\"classname\":\"ElementWiseProductTransformerConfig\",\"name\":\"ElementWiseProduct\",\"alias\":\"ElementWiseProduct\",\"attributes\":{\"inputCol\":\"\",\"outputCol\":\"\",\"scalingVec\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"size\":\"text\"},\"classname\":\"VectorSizeHintTransformerConfig\",\"name\":\"VectorSizeHint\",\"alias\":\"VectorSizeHint\",\"attributes\":{\"inputCol\":\"\",\"size\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"classname\":\"DatasetLoaderConfig\",\"name\":\"Dataset Loader\",\"alias\":\"Dataset Loader\",\"attributes\":{\"dataset\":\"\",\"applySchema\":false},\"id\":0,\"category\":\"LoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"MYSQL\":{\"script\":\"\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    if mode.lower() in (\'overwrite\', \'append\', \'error\', \'errorifexists\', \'ignore\'):\\n        dataset_<id>.write.format(\'jdbc\').options(\\n            url=\\\"<dataset.datasource.connectionDetails.url>\\\",\\n            dbtable=\\\"<dataset.attributes.tableName>\\\",\\n            user=\\\"<dataset.datasource.connectionDetails.userName>\\\",\\n            password=Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")).mode(mode).save()\\n    \\n    elif mode.lower() in (\'update\'):\\n        columnList = dataset_<id>.columns\\n        url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n        tablename = \\\"<dataset.attributes.tableName>\\\"\\n        username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n        password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n        host = urlparse(url[5:]).hostname\\n        port = urlparse(url[5:]).port\\n        database = urlparse(url[5:]).path.rsplit(\'/\', 1)[1]\\n    \\n        def process_partition(iterator):\\n            cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n            mycursor = cnx.cursor()\\n            data_list = []\\n            for row in iterator:\\n                paramsDict = {}\\n                values = []\\n                for i in range(0, len(columnList)):\\n                    paramsDict[columnList[i]] = row[i]\\n                    values.append(row[i])\\n    \\n                columns = \', \'.join(\'`{0}`\'.format(k) for k in paramsDict)\\n                duplicates = \', \'.join(\'{0}=VALUES({0})\'.format(k) for k in paramsDict)\\n                place_holders = \', \'.join(\'%s\'.format(k) for k in paramsDict)\\n    \\n                query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n                query = \\\"{0} ON DUPLICATE KEY UPDATE {1}\\\".format(query, duplicates)\\n                data_list.append(values)\\n            if len(data_list) > 0:\\n                mycursor.executemany(query, data_list)\\n                cnx.commit()\\n    \\n            mycursor.close()\\n            cnx.close()\\n    \\n        dataset_<id>.foreachPartition(process_partition)\",\"imports\":[\"import mysql\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"]},\"FILE\":{\"CSV\":{\"script\":\"    dataset.coalesce(1).write.format(\\\"com.databricks.spark.csv\\\").mode(\'overwrite\').option(\\\"header\\\", \\\"true\\\").save(<dataset.attributes.path>)\",\"imports\":[]}}}},{\"requiredJars\":[],\"formats\":{\"query\":\"text\"},\"classname\":\"HiveExtractorConfig\",\"name\":\"Hive Extractor\",\"alias\":\"Hive Extractor\",\"attributes\":{\"query\":\"\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"tableName\":\"text\",\"writeMode\":\"text\"},\"classname\":\"HiveLoaderConfig\",\"name\":\"Hive Loader\",\"alias\":\"Hive Loader\",\"attributes\":{\"tableName\":\"\",\"writeMode\":\"\"},\"id\":0,\"category\":\"LoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"arguments\":\"text\",\"id\":\"text\"},\"classname\":\"XYZ_test\",\"name\":\"XYZ_test\",\"alias\":\"XYZ_test\",\"attributes\":{\"arguments\":\"\",\"id\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}}]","DragAndDrop","{\"commands\":[\"@!PYTHONPATH!@ @!icip.pipelineScript.directory!@@!pipelinename!@/@!pipelinename!@_generatedCode.py\"],\"environment\":{\"PYTHONPATH\":\"python3\"}}","Demo"
"DragNDropLitePlugin","[{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"DatasetExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Dataset\",\"attributes\":{\"dataset\":\"\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"REST\":{\"script\":\"def DatasetExtractor_<id>():\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.NoProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n    dataset = response\\n    return dataset\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"],\"requirements\":[]},\"AWS\":{\"script\":\"\\ndef DatasetExtractor_<id>():\\n    s3_client = boto3.client(\'s3\')\\n    url = \'<dataset.attributes.Url>\'\\n    o = urlparse(url, allow_fragments=False)\\n    bucket = o.netloc.split(\'.\')[0]\\n    key =o.path.lstrip(\'/\')\\n    result = s3_client.list_objects(Bucket = bucket, Prefix=key)\\n    extension = key.split(\'.\')[-1]\\n    for o in result.get(\'Contents\'):\\n        data = s3_client.get_object(Bucket=bucket, Key=o.get(\'Key\'))\\n        contents = data[\'Body\'].read()\\n        if extension == \'csv\':\\n            contents=str(contents,\'utf-8\')\\n            contents = StringIO(contents)\\n            dataset = pd.read_csv(contents)\\n        elif extension == \'json\':\\n            dataset = json.loads(contents.decode(\'utf-8\'))\\n        else:\\n            dataset = contents.decode(\'utf-8\')\\n    return dataset\\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"import pandas as pd\",\"import json\",\"import boto3\"],\"requirements\":[\"boto3\",\"pandas\"]},\"MYSQL\":{\"script\":\"\\ndef DatasetExtractor_<id>():\\n    def getConnection():\\n        username = \'<dataset.datasource.connectionDetails.userName>\'\\n        password = Security.decrypt(\'<dataset.datasource.connectionDetails.password>\',\'<dataset.datasource.salt>\')\\n        url = \'<dataset.datasource.connectionDetails.url>\'\\n        host = urlparse(url[5:]).hostname\\n        port =urlparse(url[5:]).port\\n        database = urlparse(url[5:]).path.rsplit(\'/\', 1)[1]\\n        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port = port)\\n        return connection\\n\\n    connection = getConnection()\\n    query = \'<dataset.attributes.Query>\' # self.mapQueryParams()\\n    cursor = connection.cursor(dictionary=True)\\n    cursor.execute(query)\\n    results = cursor.fetchall()\\n    return results\\n\",\"imports\":[\"import mysql.connector\",\"from leaputils import Security\",\"from urllib.parse import urlparse\"],\"requirements\":[\"mysql-connector-python\"]},\"H2\":{\"script\":\"def DatasetExtractor_<id>():\\n    def getConnection(self):\\n        username = \'<dataset.datasource.connectionDetails.userName>\'\\n        password = Security.decrypt(\'<dataset.datasource.connectionDetails.password>\',\'<dataset.datasource.salt>\')\\n        url = \'<dataset.datasource.connectionDetails.url>\'\\n        drivername = \'org.h2.Driver\'\\n        driverpath = os.path.join(os.environ[\'SPARK_HOME\'], \'jars/h2-1.4.200.jar\')\\n        connection = jaydebeapi.connect(drivername, url, [username, password], driverpath)\\n        return connection\\n    \\n    connection = self.getConnection()\\n    query = self.mapQueryParams()\\n    cursor = connection.cursor()\\n    cursor.execute(query)\\n    results = cursor.fetchall()\\n    return results\",\"imports\":[\"from leap.core.iExtractor import Extractor\",\"from leap.utils.Utilities import Utilities\",\"import ast\",\"import logging as logger\",\"from leap.utils import vault\",\"import os\",\"import jaydebeapi\"],\"requirements\":[\"jaydebeapi\"]},\"MSSQL\":{\"script\":\"def DatasetExtractor_<id>():\\n\\n    def getConnection(self):\\n\\n        username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n\\n        password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n\\n        url = \\\"<dataset.datasource.connectionDetails.url>\\\"\\n\\n        temp1 = self.url.split(\\\"//\\\")\\n\\n        temp2 = temp1[1].split(\\\";\\\")\\n\\n        server = temp2[0]\\n\\n        database = (temp2[1].split(\\\"=\\\"))[1]\\n\\n        isTrusted = \\\"no\\\"\\n\\n        if username == \\\"\\\":\\n\\n        isTrusted = \\\"yes\\\"\\n\\n        regex = \\\\\\\"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\\"\\n\\n\\n        if(re.search(regex, server.split(\\\":\\\")[0])):\\n\\n            server=server.replace(\\\":\\\",\\\",\\\")\\n\\n\\n        connectionString = \\\\\\\"DRIVER={0};SERVER={1}; \\\\\\\"\\n\\n                           \\\\\\\"DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\\".format(\\n\\n            \\\"ODBC Driver 17 for SQL SERVER\\\", server, database, username, password, isTrusted)\\n\\n        connection = pyodbc.connect(connectionString)\\n\\n        return connection\\n\\n    \\n\\n    connection = self.getConnection()\\n\\n    query = self.mapQueryParams()\\n\\n    cursor = connection.cursor()\\n\\n    cursor = cursor.execute(query)\\n\\n    columns = [column[0] for column in cursor.description]\\n\\n    results =[]\\n\\n    for row in cursor.fetchall():\\n\\n        results.append(dict(zip(columns, row)))\\n\\n    return results\",\"imports\":[\"from leap.core.iExtractor import Extractor\",\"from leap.utils.Utilities import Utilities\",\"import ast\",\"import logging as logger\",\"from leap.utils.configVariables import *\",\"import mysql.connector\",\"from urllib.parse import urlparse\",\"from leap.utils import vault\",\"from leap.core.iExtractor import Extractor\",\"from leap.utils.Utilities import Utilities\",\"import ast\",\"import logging as logger\",\"from urllib.parse import urlparse\",\"from leap.utils import vault\",\"import mysql.connector\"],\"requirements\":[\"mssql-connector-python\",\"pyodbc\"]},\"servicenow\":{\"script\":\"def DatasetExtractor_<id>():\\n    url = \\\"<dataset.attributes.url>\\\"\\n    authParams = \\\"<dataset.attributes.authParams>\\\"\\n    auth = \\\"<dataset.datasource.connectionDetails.auth>\\\"\\n    authUrl = \\\"<dataset.datasource.connectionDetails.authUrl>\\\"\\n    authToken = \\\"<dataset.datasource.connectionDetails.authToken>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    query = \\\"<dataset.attributes.Query>\\\"\\n    user = \\\"<dataset.attributes.UserName>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    vaultkey = \\\"<dataset.attributes.VaultKey>\\\"\\n    if vaultkey != \\\"\\\":\\n            password = \\\"<dataset.attributes.Vault.Password>\\\"\\n    else:\\n            password = Utilities.decrypt(\\\"<dataset.datasource.connectionDetails.Password>\\\",\\n                                              \\\"<dataset.datasource.connectionDetails.salt>\\\")\\n    documentElement = \\\"<dataset.attributes.DocumentElement>\\\"\\n    type= \\\"<dataset.datasource.connectionDetails.Type>\\\"\\n    script= \\\"<dataset.datasource.connectionDetails.Script>\\\"\\n    \\n    \\n    logging.info(\\\"Connecting to URL {0}\\\".format(url))\\n    PROXIES = {}\\n    hostname = urlparse(url).hostname\\n    if authUrl != \'\':\\n        authParams = json.loads(self.authParams)\\n        params = []\\n        for key in authParams:\\n            params.append(\\\"{0}={1}\\\".format(key, authParams[key]))\\n        params.append(\\\"{0}={1}\\\".format(\'q\', query))\\n        authResponse = requests.get(url=authUrl, params=params, timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n        authToken = authResponse.json()\\n    if auth.lower() == \'bearertoken\':\\n        HEADERS[\'Authorization\'] = \\\"Bearer \\\" + authToken\\n    elif auth.lower() == \'basicauth\':\\n        auth = HTTPBasicAuth(user,password)\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    if type.lower() == \\\"get\\\":\\n\\n            response = requests.get( url=url, headers=HEADERS, proxies=PROXIES, auth=auth,\\n                                        verify=False, params = PARAMS,\\n                                        timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    elif type.lower() == \\\"post\\\":\\n            #values = {\'script\' : self.script}\\n            response = requests.post(url=url, headers=HEADERS, params=params, auth = auth,\\n                                        proxies=PROXIES, verify=False, data=script,\\n                                        timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    elif type.lower() == \\\"put\\\":\\n            response = requests.put(url=url, headers=HEADERS, params=PARAMS, auth = auth, proxies=PROXIES, verify=False,\\n                                    timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    elif type.lower() == \\\"delete\\\":\\n            response = requests.delete(url=surl, headers=HEADERS, proxies=PROXIES, auth=auth, verify=False, params=PARAMS,\\n                                        timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n    else:\\n            response = requests.get(url=url, headers=HEADERS, proxies=PROXIES, auth=auth,\\n                                    verify=False, params=PARAMS,\\n                                    timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    logger.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n    \\n    response= dataset\\n\\n    return dataset\\n\\n    \\n    \\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\",\"from leap.core.iExtractor import Extractor\",\"from leap.utils import configVariables\",\"from leap.utils import vault\",\"from leap.utils.Utilities import Utilities\",\"import logging as logger\"],\"requirments\":[\"requests\"]}}},{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"DatasetLoaderConfig\",\"name\":\"Dataset Loader\",\"alias\":\"Dataset Loader\",\"attributes\":{\"dataset\":\"\"},\"id\":0,\"category\":\"LoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"codeGeneration\":{\"MYSQL\":{\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"Drop table IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"`{0}` TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"`{0}`\\\".format(k) for k in paramsDict)\\n            duplicates = \\\", \\\".join(\\\"{0}=VALUES({0})\\\".format(k) for k in paramsDict)\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON DUPLICATE KEY UPDATE {1}\\\".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            logging.error(\\\"{0}:{1}\\\".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()\",\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"]},\"REST\":{\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"]},\"AWS\":{\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    filename = url.split(\'/\')[-1]\\n    extension = filename.split(\'.\')[-1]\\n\\n    data_directory = \\\"/opt/ml/processing/output\\\"\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\\"Saving data\\\")\\n    if extension == \'.csv\':\\n        dataset.to_csv(file_path)\\n    elif extension == \'pkl\':\\n        pickle.dumps(dataset, open(file_path, \'wb\'))\\n    else:\\n        with open(file_path, \'w\') as f:\\n            f.writelines(dataset)\\n\\n\",\"imports\":[\"import pandas as pd\",\"import pickle\",\"import os\"]},\"MSSQL\":{\"script\":\"def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\\"<dataset.attributes.writeMode>\\\\\\\"\\n\\n    url=\\\\\\\"<dataset.datasource.connectionDetails.url>\\\\\\\"\\n\\n    tablename = \\\\\\\"<dataset.attributes.tableName>\\\\\\\"\\n\\n    username = \\\\\\\"<dataset.datasource.connectionDetails.userName>\\\\\\\"\\n\\n    password = Security.decrypt(\\\\\\\"<dataset.datasource.connectionDetails.password>\\\\\\\",\\\\\\\"<dataset.datasource.salt>\\\\\\\")\\n\\n    temp1 = self.url.split(\\\"//\\\")\\n\\n    temp2 = temp1[1].split(\\\";\\\")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\\"=\\\"))[1]\\n\\n    isTrusted = \\\"no\\\"\\n\\n    if username == \\\"\\\":\\n\\n    isTrusted = \\\"yes\\\"\\n\\n    regex = \\\\\\\"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\\"\\n\\n\\n    if(re.search(regex, server.split(\\\":\\\")[0])):\\n\\n        server=server.replace(\\\":\\\",\\\",\\\")\\n\\n\\n    connectionString = \\\\\\\"DRIVER={0};SERVER={1}; \\\\\\\"\\n\\n                       \\\\\\\"DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\\".format(\\n\\n        \\\"ODBC Driver 17 for SQL SERVER\\\", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\\"overwrite\\\\\\\":\\n\\n        cursor.execute(\\\\\\\"Drop table IF EXISTS {0}\\\\\\\".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\\", \\\\\\\".join([\\\\\\\"`{0}` TEXT\\\\\\\".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\\".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\\", \\\\\\\".join(\\\\\\\"`{0}`\\\\\\\".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\\", \\\\\\\".join(\\\\\\\"{0}=VALUES({0})\\\\\\\".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\\", \\\\\\\".join(\\\\\\\"%s\\\\\\\".format(k) for k in paramsDict)\\n\\n            query = \\\\\\\"INSERT INTO {0} ({1}) VALUES ({2})\\\\\\\".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\\"update\\\\\\\"):\\n\\n                query = \\\\\\\"{0} ON DUPLICATE KEY UPDATE {1}\\\\\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\\"{0}:{1}\\\\\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()\",\"imports\":[\"from leap.core.iLoader import Loader\",\"from leap.utils.Utilities import Utilities\",\"import logging as logger\",\"from leap.utils import vault\",\"import pyodbc\",\"import re\",\"from datetime import datetime\",\"import os\"]}}},{\"requiredJars\":[],\"formats\":{\"requirements\":\"text\",\"script\":\"textarea\"},\"classname\":\"ScriptTransformerConfig\",\"name\":\"ScriptTransformer\",\"alias\":\"Script Transformer\",\"attributes\":{\"requirements\":\"\",\"script\":\"\\rdef ScriptTransformer_<id>( dataset):\\n    #pre-process Data\\r\\r    return dataset\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in1\",\"in2\"],\"outputEndpoints\":[\"out1\",\"out2\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"requirements\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"PreProcessingScriptConfig\",\"name\":\"Pre Processing Script\",\"alias\":\"Pre Processing Script\",\"attributes\":{\"requirements\":\"\",\"script\":\"\\rdef PreProcessingScript_<id>( dataset):\\n    #pre-process Data\\r\\r    return dataset\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"requirements\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"PostProcessingScriptConfig\",\"name\":\"Post Processing Script\",\"alias\":\"Post Processing Script\",\"attributes\":{\"requirements\":\"\",\"script\":\"\\rdef PostProcessingScript_<id>( dataset):\\n    #Post-process Data\\r\\r    return dataset\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\"],\"outputEndpoints\":[\"out1\",\"out2\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"modelName\":\"text\",\"bucket\":\"text\",\"chunkCount\":\"text\"},\"classname\":\"ModelSinkConfig\",\"name\":\"Model Sink\",\"alias\":\"Model Sink\",\"attributes\":{\"modelName\":\"\",\"bucket\":\"Demo\",\"chunkCount\":\"1\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef ModelSink_<id>(model):\\n    #ModelSink\\n    modelPath = os.path.join(os.environ[\\\"JOB_DIRECTORY\\\"],\'models\',\'classicmlpoc\')\\n    print(\\\"Saving Model at path:\\\"+modelPath )\\n    logging.info(\\\"Saving Model at path:\\\"+modelPath )\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    modelPath = os.path.join(modelPath, \'<modelName>\')\\n    print(\\\"Saving Model at path:\\\"+modelPath )\\n    if modelPath.split(\'.\')[-1] == \'pkl\':\\n        pickle.dump(model, open(modelPath, \'wb\'))\\n        r = pickle.load(open(modelPath, \'rb\'))\\n        fileid = FileServer.uploadFile(modelPath, \'<bucket>\', <chunkCount>)\\n        print(\'fileid\', fileid)\\n        os.remove(modelPath)\\n    else:\\n        model.write().overwrite().save(modelPath)\",\"imports\":[\"import pickle\",\"from leaputils import FileServer\",\"import requests\",\"import hashlib\",\"import shutil\",\"import logging as logger\",\"import os\"]}},{\"requiredJars\":[],\"formats\":{\"FitIntercept\":\"text\",\"copyX\":\"text\",\"nJobs\":\"text\",\"positive\":\"text\"},\"attributes\":{\"FitIntercept\":\"True\",\"copyX\":\"True\",\"nJobs\":\"None\",\"positive\":\"True\"},\"classname\":\"LinearR\",\"category\":\"Regression\",\"name\":\"LinearR\",\"alias\":\"LinearR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef LinearR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputLR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',LinearRegression(fit_intercept=<FitIntercept>, copy_X = <copyX>, n_jobs = <nJobs>, positive = <positive>))]\\n    pipeLR=Pipeline(InputLR)\\n\\n    pipeLR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LR = pipeLR.predict(X_test)\\n    print(pipeLR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    \\n    return pipeLR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import LinearRegression\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"alpha\":\"text\",\"FitIntercept\":\"text\",\"precompute\":\"text\",\"copyX\":\"text\",\"maxIter\":\"text\",\"tol\":\"text\",\"warm_start\":\"text\",\"positive\":\"text\",\"random_state\":\"text\",\"selection\":\"text\"},\"attributes\":{\"alpha\":\"1.0\",\"FitIntercept\":\"True\",\"precompute\":\"False\",\"copyX\":\"True\",\"maxIter\":\"1000\",\"tol\":\"0.0001\",\"warm_start\":\"False\",\"positive\":\"True\",\"random_state\":\"None\",\"selection\":\"cyclic\"},\"classname\":\"LassoR\",\"category\":\"Regression\",\"name\":\"LassoR\",\"alias\":\"LassoR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef LassoR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputLassoR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',Lasso(alpha=<alpha>, fit_intercept=<FitIntercept>, precompute=<precompute>, copy_X=<copyX>, max_iter=<maxIter>, tol=<tol>, warm_start=<warm_start>, positive=<positive>, random_state=<random_state>, selection=\'<selection>\'))]\\n    pipeLassoR=Pipeline(InputLassoR)\\n\\n    pipeLassoR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeLassoR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    \\n    return pipeLassoR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import Lasso\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"alpha\":\"text\",\"FitIntercept\":\"text\",\"copyX\":\"text\",\"maxIter\":\"text\",\"tol\":\"text\",\"solver\":\"text\",\"random_state\":\"text\"},\"attributes\":{\"alpha\":\"1.0\",\"FitIntercept\":\"True\",\"copyX\":\"True\",\"maxIter\":\"1000\",\"tol\":\"0.0001\",\"solver\":\"auto\",\"random_state\":\"None\"},\"classname\":\"RidgeR\",\"category\":\"Regression\",\"name\":\"RidgeR\",\"alias\":\"RidgeR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef RidgeR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputRidgeR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',Ridge(alpha=<alpha>, fit_intercept=<FitIntercept>, copy_X=<copyX>, max_iter=<maxIter>, tol=<tol>, solver=\'<solver>\', random_state=<random_state>))]\\n    pipeRidgeR=Pipeline(InputRidgeR)\\n\\n    pipeRidgeR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeRidgeR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    \\n    return pipeRidgeR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import Ridge\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"alpha\":\"text\",\"l1_ratio\":\"text\",\"FitIntercept\":\"text\",\"precompute\":\"text\",\"copyX\":\"text\",\"maxIter\":\"text\",\"tol\":\"text\",\"warm_start\":\"text\",\"positive\":\"text\",\"random_state\":\"text\",\"selection\":\"text\"},\"attributes\":{\"alpha\":\"1.0\",\"l1_ratio\":\"0.5\",\"FitIntercept\":\"True\",\"precompute\":\"False\",\"copyX\":\"True\",\"maxIter\":\"1000\",\"tol\":\"0.0001\",\"warm_start\":\"False\",\"positive\":\"True\",\"random_state\":\"None\",\"selection\":\"cyclic\"},\"classname\":\"ElasticnetR\",\"category\":\"Regression\",\"name\":\"ElasticnetR\",\"alias\":\"ElasticnetR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef ElasticnetR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputEN=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',ElasticNet(alpha=<alpha>, l1_ratio=<l1_ratio>, fit_intercept=<FitIntercept>, precompute=<precompute>, copy_X=<copyX>, max_iter=<maxIter>, tol=<tol>, warm_start=<warm_start>, positive=<positive>, random_state=<random_state>, selection=\'<selection>\'))]\\n    pipeEN=Pipeline(InputEN)\\n\\n    pipeEN.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeEN.score(dataset[\'X_train\'],dataset[\'Y_train\']))  \\n    \\n    return pipeEN\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import ElasticNet\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"n_estimators\":\"text\",\"criterion\":\"text\",\"max_depth\":\"text\",\"min_samples_split\":\"text\",\"min_samples_leaf\":\"text\",\"min_weight_fraction_leaf\":\"text\",\"max_features\":\"text\",\"max_leaf_nodes\":\"text\",\"min_impurity_decrease\":\"text\",\"bootstrap\":\"text\",\"oob_score\":\"text\",\"n_jobs\":\"text\",\"random_state\":\"text\",\"verbose\":\"text\",\"warm_start\":\"text\",\"ccp_alpha\":\"text\",\"max_samples\":\"text\"},\"attributes\":{\"n_estimators\":\"100\",\"criterion\":\"squared_error\",\"max_depth\":\"None\",\"min_samples_split\":\"2\",\"min_samples_leaf\":\"1\",\"min_weight_fraction_leaf\":\"0.0\",\"max_features\":\"1.0\",\"max_leaf_nodes\":\"None\",\"min_impurity_decrease\":\"0.0\",\"bootstrap\":\"True\",\"oob_score\":\"False\",\"n_jobs\":\"None\",\"random_state\":\"None\",\"verbose\":\"0\",\"warm_start\":\"False\",\"ccp_alpha\":\"0.0\",\"max_samples\":\"None\"},\"classname\":\"RandomForestR\",\"category\":\"Regression\",\"name\":\"RandomForestR\",\"alias\":\"RandomForestR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef RandomForestR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    Input=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',RandomForestRegressor(n_estimators=<n_estimators>, criterion=\'<criterion>\', max_depth=<max_depth>, min_samples_split=<min_samples_split>, min_samples_leaf=<min_samples_leaf>, min_weight_fraction_leaf=<min_weight_fraction_leaf>, max_features=<max_features>, max_leaf_nodes=<max_leaf_nodes>, min_impurity_decrease=<min_impurity_decrease>, bootstrap=<bootstrap>, oob_score=<oob_score>, n_jobs=<n_jobs>, random_state=<random_state>, verbose=<verbose>, warm_start=<warm_start>, ccp_alpha=<ccp_alpha>, max_samples=<max_samples>))]\\n    pipe=Pipeline(Input)\\n\\n    pipe.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_RFF = pipe.predict(X_test)\\n    print(pipe.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n\\n    return pipe\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.ensemble import RandomForestRegressor\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"kernel\":\"text\",\"degree\":\"text\",\"gamma\":\"text\",\"coef0\":\"text\",\"tol\":\"text\",\"C\":\"text\",\"epsilon\":\"text\",\"shrinking\":\"text\",\"cache_size\":\"text\",\"verbose\":\"text\",\"max_iter\":\"text\"},\"attributes\":{\"kernel\":\"rbf\",\"degree\":\"3\",\"gamma\":\"scale\",\"coef0\":\"0.0\",\"tol\":\"0.001\",\"C\":\"1.0\",\"epsilon\":\"0.1\",\"shrinking\":\"True\",\"cache_size\":\"200\",\"verbose\":\"False\",\"max_iter\":\"-1\"},\"classname\":\"SVR\",\"category\":\"Regression\",\"name\":\"SVR\",\"alias\":\"SVR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef SVR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputSVR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',SVR(kernel=\'<kernel>\', degree=3, gamma=\'scale\', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1))]\\n    pipeSVR=Pipeline(InputSVR)\\n\\n    pipeSVR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeSVR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    print(((pipeSVR.predict(dataset[\'X_train\']) - dataset[\'Y_train\']) ** 2).mean())\\n    \\n    return pipeSVR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.svm import SVR\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"fileId\":\"text\",\"fileName\":\"text\",\"bucket\":\"text\"},\"attributes\":{\"fileId\":\"\",\"fileName\":\"\",\"bucket\":\"Demo\"},\"classname\":\"InferenceR\",\"category\":\"Regression\",\"name\":\"InferenceR\",\"alias\":\"InferenceR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef InferenceR_<id>(dataset):\\n    bucket = \'<bucket>\'\\n    download_path = downloadFile(\'<fileId>\', \'<fileName>\', \'<bucket>\')\\n    print(\'download_path: \', download_path)\\n    # load the model\\n    unpickler = pickle.Unpickler(open(download_path, \'rb\'))\\n    load_model = unpickler.load()\\n    print(load_model)\\n    pipe_pred = load_model.predict(dataset[\'X_test\'])\\n    print(pipe_pred)\\n    \\n    # OUT\\n    if isinstance(pipe_pred,tuple):\\n        pipe_pred, prediction_results = pipe_pred\\n        output=pd.DataFrame({\'Id\':dataset[\'dataset_id\']})\\n        output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\n        print(\'out\', output.to_dict(\'records\'))\\n    else:\\n        output=pd.DataFrame({\'Id\':dataset[\'dataset_id\'],\'Result\':pipe_pred})\\n    os.remove(download_path)\\n    return output.to_dict(\'records\')\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from leaputils import FileServer\",\"import pickle\",\"import os\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"question\":\"text\",\"prompt\":\"text\"},\"attributes\":{\"question\":\"what is the identification number?\",\"prompt\":\"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"},\"classname\":\"ProcessQuery\",\"name\":\"ProcessQuery\",\"alias\":\"ProcessQuery\",\"id\":0,\"category\":\"DocumentComprehension\",\"inputEndpoints\":[\"dataset1\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"def ProcessQuery_<id>( dataset):\\n    def demo_process_vqa(input_img, question,pretrained_model, task_prompt):\\n        input_img = Image.open(input_img).convert(\'RGB\')\\n        user_prompt = task_prompt.replace(\'{user_input}\', question)\\n        output = pretrained_model.inference(input_img, prompt=user_prompt)[\'predictions\'][0]\\n        return output\\n    \\n    task_prompt = \'<prompt>\'\\n    pretrained_model = DonutModel.from_pretrained(\'naver-clova-ix/donut-base-finetuned-docvqa\')\\n    if torch.cuda.is_available():\\n        pretrained_model.half()\\n        device = torch.device(\'cuda\')\\n        pretrained_model.to(device)\\n    else:\\n        pretrained_model.encoder.to(torch.bfloat16)\\n\\n    pretrained_model.eval()\\n    input_img = \'/app/jobs/models/images/invoice.png\'\\n    print(type(input_img))\\n    answer = demo_process_vqa(input_img,\'<question>\',pretrained_model, task_prompt)\\n    print(answer)\",\"imports\":[\"import torch\",\"import os\",\"from donut import DonutModel\",\"from PIL import Image\"],\"requirements\":[]}},{\"requiredJars\":[],\"formats\":{\"K\":\"text\"},\"attributes\":{\"K\":\"5\"},\"classname\":\"KNearestNeighbor\",\"category\":\"Classification\",\"name\":\"KNearestNeighbor\",\"alias\":\"KNearestNeighbor\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef KNearestNeighbor_<id>(dataset):\\n    knn = KNeighborsClassifier(n_neighbors=<K>)\\n    knn.fit(dataset[0],dataset[1])\\n    return knn\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.neighbors import KNeighborsClassifier\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"GaussianNB\",\"category\":\"Classification\",\"name\":\"GaussianNB\",\"alias\":\"GaussianNB\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef GaussianNB_<id>(dataset):\\n    nb = GaussianNB()\\n    nb.fit(dataset[0],dataset[1])\\n    return nb\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.naive_bayes import GaussianNB\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"RandomForest\",\"category\":\"Classification\",\"name\":\"RandomForest\",\"alias\":\"RandomForest\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef RandomForest_<id>(dataset):\\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42)\\n    rfc.fit(dataset[0],dataset[1])\\n    return rfc\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.ensemble import RandomForestClassifier\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"SVC\",\"category\":\"Classification\",\"name\":\"SVC\",\"alias\":\"SVC\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef SVC_<id>(dataset):\\n    model = SVC()\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.svm import SVC\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"LogisticRegression\",\"category\":\"Classification\",\"name\":\"LogisticRegression\",\"alias\":\"LogisticRegression\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef LogisticRegression_<id>(dataset):\\n    model = LogisticRegression()\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import LogisticRegression\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"DecisionTree\",\"category\":\"Classification\",\"name\":\"DecisionTree\",\"alias\":\"DecisionTree\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef DecisionTree_<id>(dataset):\\n    model = DecisionTreeClassifier(random_state = 1)\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.tree import DecisionTreeClassifier\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"file_path\":\"text\",\"vect_type\":\"text\",\"clf_type\":\"text\",\"test_size\":\"text\",\"alpha\":\"text\",\"text_heading\":\"text\",\"class_heading\":\"text\",\"model_name\":\"text\"},\"attributes\":{\"file_path\":\"None\",\"vect_type\":\"TFIDF\",\"clf_type\":\"model\",\"test_size\":\"0.2\",\"alpha\":\"0.01\",\"text_heading\":\"text\",\"class_heading\":\"class\",\"model_name\":\"test\"},\"classname\":\"Naive Bayes\",\"category\":\"Classification\",\"name\":\"naivebayes\",\"alias\":\"naivebayes\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef DecisionTree_<id>(dataset):\\n    model = DecisionTreeClassifier(random_state = 1)\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.tree import DecisionTreeClassifier\",\"from sklearn.preprocessing import LabelEncoder\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"model_type\":\"text\",\"OutputFeatures\":\"list\",\"preprocessing\":\"text\",\"preprocessing_sample_ratio\":\"text\",\"preprocessing_oversample_minority\":\"text\",\"preprocessing_undersample_majority\":\"text\",\"preprocessing_split_type\":\"text\",\"preprocessing_split_column\":\"text\",\"preprocessing_split_probabilities\":\"list\",\"defaults\":\"text\",\"backend\":\"text\"},\"classname\":\"LudwibConfig\",\"name\":\"LudwibConfig\",\"alias\":\"LudwibConfig\",\"attributes\":{\"model_type\":\"ecd\",\"OutputFeatures\":\"\",\"preprocessing\":\"False\",\"preprocessing_sample_ratio\":\"1.0\",\"preprocessing_oversample_minority\":\"None\",\"preprocessing_undersample_majority\":\"None\",\"preprocessing_split_type\":\"random\",\"preprocessing_split_column\":\"None\",\"preprocessing_split_probabilities\":\"None\",\"defaults\":\"False\",\"backend\":\"False\"},\"id\":0,\"category\":\"Ludwig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef LudwibConfig_<id>(dataset):\\n    train_df = dataset[\'train_df\']\\n    output = <OutputFeatures>\\n    out_features = []\\n    for out in output:\\n        out_features.append(out[\'name\'])\\n    cols = list(set(train_df.columns) - set(out_features)) # add\\n    features = train_df[cols]\\n\\n    #extract categorical features\\n    categorical_features = []\\n    for p in features:\\n        if train_df[p].dtype == \'object\':\\n            categorical_features.append(p)\\n    print(\'categorical features:\', categorical_features, \'\\\\n\')\\n\\n    # get numerical features\\n    numerical_features = list(set(features) - set(categorical_features))\\n    print(\'numerical features:\', numerical_features, \'\\\\n\')\\n\\n    # template for config\\n    config = {\'model_type\': \'<model_type>\', \'input_features\':[], \'output_features\': [], \'trainer\':{}}\\n\\n    # setup input features for categorical features\\n    for p in categorical_features:\\n        a_feature = {\\n            \'name\': p.replace(\' \',\'_\'), \\n            \'type\': \'category\'\\n        }\\n        config[\'input_features\'].append(a_feature)\\n\\n    # setup input features for numerical features\\n    for p in numerical_features:\\n        a_feature = {\\n            \'name\': p.replace(\' \', \'_\'), \\n            \'type\': \'number\'\\n        }\\n        config[\'input_features\'].append(a_feature)\\n\\n    # set up output variable\\n    output = <OutputFeatures>\\n    for out in output:\\n        config[\'output_features\'].append({\'name\': out[\'name\'], \'type\':out[\'value\']})\\n        \\n    if config[\'model_type\'] == \'ecd\':\\n        # config[\'combiner\'] = None # add\\n        pass\\n        \\n    if <preprocessing>:\\n        config[\'preprocessing\'] = {\\n            \'sample_ratio\': <preprocessing_sample_ratio>,\\n            \'oversample_minority\': <preprocessing_oversample_minority>,\\n            \'undersample_majority\': <preprocessing_undersample_majority>,\\n            \'split\':{\\n                \'type\': \'<preprocessing_split_type>\'\\n            }\\n        }\\n    \\n    if \'<preprocessing_split_column>\' != \'None\': # add\\n        config[\'preprocessing\'][\'split\'][\'column\'] = \'<preprocessing_split_column>\'\\n\\n    if <preprocessing> and \'<preprocessing_split_type>\' != \'fixed\': # add\\n        prob = <preprocessing_split_probabilities>\\n        config[\'preprocessing\'][\'split\'][\'probabilities\'] = [pr[\'name\'] for pr in prob]\\n    \\n\\n    # set default preprocessing and encoder for numerical features\\n    if <defaults>:\\n        config[\'defaults\'] = { # add\\n            \'number\': {\\n                \'preprocessing\': {\\n                    \'missing_value_strategy\': \'fill_with_mean\', \\n                    \'normalization\': \'zscore\'\\n                },\\n                \'encoder\': {\\n                    \'type\': \'dense\',\\n                    \'num_layers\': 2\\n                },\\n            },\\n            \'category\': {\\n                \'encoder\': {\\n                    \'type\': \'sparse\'\\n                },\\n                \'decoder\': {\\n                    \'top_k\': 2\\n                },\\n                \'loss\': {\\n                    \'confidence_penalty\': 0.1  \\n                }\\n            }\\n        }\\n\\n    # set up trainer\\n    if config[\'model_type\'] == \'gbm\':\\n      config[\'trainer\'] = {\\n             \'bagging_fraction\': 0.8,\\n             \'bagging_freq\': 1,\\n             \'bagging_seed\': 3,\\n             \'boosting_rounds_per_checkpoint\': 50,\\n             \'boosting_type\': \'gbdt\',\\n             \'cat_l2\': 10.0,\\n             \'cat_smooth\': 10.0,\\n             \'cegb_penalty_split\': 0.0,\\n             \'cegb_tradeoff\': 1.0,\\n             \'drop_rate\': 0.1,\\n             \'drop_seed\': 4,\\n             \'early_stop\': 5,\\n             \'eval_batch_size\': 1048576,\\n             \'evaluate_training_set\': False,\\n             \'extra_seed\': 6,\\n             \'extra_trees\': False,\\n             \'feature_fraction\': 0.75,\\n             \'feature_fraction_bynode\': 1.0,\\n             \'feature_fraction_seed\': 2,\\n             \'feature_pre_filter\': True,\\n             \'lambda_l1\': 0.25,\\n             \'lambda_l2\': 0.2,\\n             \'learning_rate\': 0.03,\\n             \'linear_lambda\': 0.0,\\n             \'max_bin\': 255,\\n             \'max_cat_threshold\': 32,\\n             \'max_cat_to_onehot\': 4,\\n             \'max_delta_step\': 0.0,\\n             \'max_depth\': 18,\\n             \'max_drop\': 50,\\n             \'min_data_in_leaf\': 20,\\n             \'min_data_per_group\': 100,\\n             \'min_gain_to_split\': 0.03,\\n             \'min_sum_hessian_in_leaf\': 0.001,\\n             \'neg_bagging_fraction\': 1.0,\\n             \'num_boost_round\': 1000,\\n             \'num_leaves\': 82,\\n             \'other_rate\': 0.1,\\n             \'path_smooth\': 0.0,\\n             \'pos_bagging_fraction\': 1.0,\\n             \'skip_drop\': 0.5,\\n             \'top_rate\': 0.2,\\n             \'tree_learner\': \'serial\',\\n             \'uniform_drop\': False,\\n             \'validation_field\': None,\\n             \'validation_metric\': None,\\n             \'verbose\': -1,\\n             \'xgboost_dart_mode\': False}\\n    else:\\n      config[\'trainer\'] = {\'batch_size\': \'auto\', # add\\n             \'bucketing_field\': None,\\n             \'checkpoints_per_epoch\': 0,\\n             \'early_stop\': 5,\\n             \'epochs\': 100,\\n             \'eval_batch_size\': None,\\n             \'evaluate_training_set\': False,\\n             \'gradient_clipping\': {\'clipglobalnorm\': 0.5,\\n                                   \'clipnorm\': None,\\n                                   \'clipvalue\': None},\\n             \'increase_batch_size_eval_metric\': \'loss\',\\n             \'increase_batch_size_eval_split\': \'training\',\\n             \'increase_batch_size_on_plateau\': 0,\\n             \'increase_batch_size_on_plateau_patience\': 5,\\n             \'increase_batch_size_on_plateau_rate\': 2.0,\\n             \'learning_rate\': 0.001,\\n             \'learning_rate_scaling\': \'linear\',\\n             \'learning_rate_scheduler\': {\'decay\': None,\\n                                         \'decay_rate\': 0.96,\\n                                         \'decay_steps\': 10000,\\n                                         \'reduce_eval_metric\': \'loss\',\\n                                         \'reduce_eval_split\': \'training\',\\n                                         \'reduce_on_plateau\': 0,\\n                                         \'reduce_on_plateau_patience\': 10,\\n                                         \'reduce_on_plateau_rate\': 0.1,\\n                                         \'staircase\': False,\\n                                         \'warmup_evaluations\': 0,\\n                                         \'warmup_fraction\': 0.0},\\n             \'max_batch_size\': 1099511627776,\\n             \'optimizer\': {\'amsgrad\': False,\\n                           \'betas\': [0.9, 0.999],\\n                           \'eps\': 1e-08,\\n                           \'type\': \'adam\',\\n                           \'weight_decay\': 0.0},\\n             \'regularization_lambda\': 0.0,\\n             \'regularization_type\': \'l2\',\\n             \'should_shuffle\': True,\\n             \'steps_per_checkpoint\': 0,\\n             \'train_steps\': None,\\n             \'use_mixed_precision\': False,\\n             \'validation_field\': None,\\n             \'validation_metric\': None}\\n\\n\\n    if <backend>:\\n        config[\'backend\'] = {\\n            \'type\': \'<backend_type>\',\\n            \'cache_dir\': \'<backend_cache_dir>\',\\n            \'cache_credentials\': \'<backend_cache_credentials>\',\\n            \'processor\': {\\n                \'type\': \'<backend_processor_type>\'\\n            },\\n            \'trainer\': {\\n                \'strategy\': \'<backend_trainer_strategy>\'\\n            },\\n            \'loader\': {\\n                \'fully_executed\': \'<backend_loader_fully_executed>\',\\n                \'window_size_bytes\': \'<backend_loader_window_size_bytes>\'\\n            }\\n        }\\n\\n    print(\'config:\', config)\\n    return {\'config\': config, \'train_df\': dataset[\'train_df\'], \'test_df\': dataset[\'test_df\']}\\n\",\"imports\":[\"import pandas as pd\"],\"requirements\":[]}},{\"requiredJars\":[],\"formats\":{\"backend\":\"text\",\"gpus\":\"text\",\"gpu_memory_limit\":\"text\",\"allow_parallel_threads\":\"text\",\"callbacks\":\"text\"},\"attributes\":{\"backend\":\"None\",\"gpus\":\"None\",\"gpu_memory_limit\":\"None\",\"allow_parallel_threads\":\"True\",\"callbacks\":\"None\"},\"classname\":\"Ludwig\",\"category\":\"Ludwig\",\"name\":\"Ludwig\",\"alias\":\"Ludwig\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Ludwig_<id>(dataset):\\n    print(os.getcwd())\\n    modelPath = os.path.join(os.environ[\\\"JOB_DIRECTORY\\\"],\'models\',\'classicmlpoc\')\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    os.chdir(modelPath)\\n    print(os.getcwd())\\n    model = LudwigModel(config=dataset[\'config\'], logging_level=logging.INFO, backend=<backend>, gpus=<gpus>, gpu_memory_limit=<gpu_memory_limit>, allow_parallel_threads=<allow_parallel_threads>, callbacks=<callbacks>)\\n    # Trains the model. This cell might take a few minutes.\\n    train_stats, preprocessed_data, output_directory = model.train(training_set=dataset[\'train_df\'],\\n                                                               test_set=dataset[\'test_df\'])\\n    \\n    print(\'train_stats\', train_stats)\\n    print(\'preprocessed_data\', preprocessed_data)\\n    print(\'output_directory\', output_directory)\\n    return model\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from ludwig.api import LudwigModel\"],\"requirements\":[\"scikit-learn\"]}}]","DragNDropLite","{\"commands\":[\"@!PYTHONPATH!@ @!icip.pipelineScript.directory!@@!pipelinename!@/@!pipelinename!@_generatedCode.py\",\"\",\"\"],\"environment\":{\"PYTHONPATH\":\"python3\",\"AWS_ACCESS_KEY\":\"AKIAWSEIAMU6H5SKF2E2\",\"AWS_SECRET_KEY\":\"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\",\"AWS_REGION\":\"us-east-1\",\"HTTP_PROXY\":\"http://10.68.248.39:80\",\"HTTPS_PROXY\":\"http://10.68.248.39:80\"}}","Demo"
"AzurePlugin","[{\"requiredJars\":[],\"formats\":{\"azuredataset\":\"restdataset\",\"restdatasetname\":\"Fetchdatasets\",\"payload\":{\"userId\":\"admin@infosys.com\",\"platform\":\"azure\"},\"response\":\"displayName\"},\"classname\":\"ExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Extractor\",\"attributes\":{\"azuredataset\":[]},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    dataset_name = \'{args_namespace.dataset_name}\'\\n    training_data = Dataset.get_by_name(workspace=workspace, name=dataset_name)\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"script\":\"textarea\",\"compute_target\":\"textarea\",\"pip_dependencies\":\"textarea\",\"arguments\":\"list\",\"inputs\":\"textarea\"},\"classname\":\"TransformerConfig\",\"name\":\"Python Script\",\"alias\":\"Python Script\",\"attributes\":{\"script\":[],\"compute_target\":\"\",\"pip_dependencies\":\"\",\"arguments\":[],\"inputs\":\"\"},\"id\":0,\"category\":\"TransformerConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    #Run Configrations\\n    pipeline_run_config = RunConfiguration()\\n    pipeline_run_config.target = step_compute_target\\n    pipeline_run_config.environment.python.conda_dependencies = CondaDependencies.create(\\n        conda_packages=conda_dependencies,\\n        pip_packages=pip_dependencies\\n    )\\n\\n    # Managing arguments ----\\n    arg_params = []\\n    arguments_lst = []\\n    for arg in {args_namespace.arguments}:\\n        arg_type = arg[\\\"step_method\\\"]\\n        keys = list(arg.keys())\\n        arg_unclean = keys[1]\\n        arg_clean = arg_unclean.split(\'--\')[-1]\\n        arg_var = arg[arg_unclean]\\n\\n        print(\\\"ARG_VAR\\\", arg_var)\\n        arguments_lst.append(\\\"--\\\" + arg_clean)\\n\\n        if arg_var in globals():\\n            training_data = globals()[f\'{{arg_var}}\']\\n            # print(\\\"TRAINING--------------------------------\\\", training_data)\\n            arg_params.append(globals()[f\'{{arg_var}}\'])\\n            arguments_lst.append(globals()[f\'{{arg_var}}\'])\\n        else:\\n            if arg_type == \\\"OutputFileDatasetConfig\\\":\\n                globals(\\n                )[f\'{{arg_var}}\'] = OutputFileDatasetConfig(\\n                    name=f\'{{arg_clean}}\'\\n                )\\n\\n                training_data = arg_var\\n                print(\\\"TRAINING+++++++++++++++++\\\", training_data)\\n                arg_params.append(globals()[f\'{{arg_var}}\'])\\n                arguments_lst.append(globals()[f\'{{arg_var}}\'])\\n\\n            elif arg_type == \\\"PipelineParameter\\\":\\n                globals()[f\'{{arg_var}}\'] = PipelineParameter(\\n                    f\'{{arg_clean}}\',\\n                    default_value=f\'{{arg_var}}\'\\n                )\\n                \\n                arg_params.append(globals()[f\'{{arg_var}}\'])\\n                arguments_lst.append(globals()[f\'{{arg_var}}\'])\\n\\n            elif arg_type == \\\"PipelineData\\\":\\n                globals()[f\'{{arg_var}}\'] = PipelineData(\\n                name=f\\\"{{arg_var}}\\\",\\n                datastore=workspace.get_default_datastore(),\\n                pipeline_output_name=f\\\"{{arg_var}} + \'_output\'\\\",\\n                training_output=TrainingOutput(type=arg_clean)\\n                )\\n\\n                arg_params.append(globals()[f\'{{arg_var}}\'])\\n                arguments_lst.append(globals()[f\'{{arg_var}}\'])\\n                \\n            else:\\n                raise Exception(\\\"Incorrect argument type!\\\")\\n\\n    inputs_lst = []\\n    for input_vals in {args_namespace.inputs}:\\n        if input_vals in globals():\\n            inputs_lst.append(globals()[input_vals])\\n        else:\\n            if input_vals == dataset_name:\\n                input_dataset = Dataset.get_by_name(\\n                    workspace=workspace,\\n                    name=input_vals)\\n                inputs_lst.append(\\n                    input_dataset.as_named_input(input_vals))\\n            else:\\n                inputs_lst.append(input_vals)\\n\\n    script_name = \'{args_namespace.script_name}\'\\n    script = {args_namespace.script}\\n    with open(script_name, \'w+\') as f:\\n        # write elements of list\\n        for line in script:\\n            f.write(\'%s\\\\\\\\n\' %line)\\n            \\n    f.close()\\n\\n    py_script_step = PythonScriptStep(\\n        name=\'{args_namespace.name}\',\\n        script_name=script_name,\\n        compute_target=step_compute_target,\\n        runconfig=pipeline_run_config,\\n        arguments=arguments_lst,\\n        inputs=inputs_lst,\\n        allow_reuse=\'{args_namespace.allow_reuse}\')\\n\\n    pipeline_steps.append(py_script_step)\\n\\n\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"DatasetName\":\"text\"},\"classname\":\"FetchDataset\",\"name\":\"FetchDataset\",\"alias\":\"FetchDataset\",\"attributes\":{\"DatasetName\":\"housingPricesData\"},\"id\":0,\"category\":\"ComponentsConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef FetchDataset():\\n    dataset_name =\'<DatasetName>\'\\n    svc_pr = ServicePrincipalAuthentication(\\n    tenant_id=credentials[\'tenant_id\'],\\n    service_principal_id=credentials[\'service_principal_id\'],\\n    service_principal_password=credentials[\'service_principal_password\'],\\n    )\\n\\n    ws = Workspace(\\n        subscription_id=credentials[\'subscription_id\'],\\n        resource_group=credentials[\'resource_group\'],\\n        workspace_name=credentials[\'workspace_name\'],\\n        auth=svc_pr,\\n    )\\n\\n    training_data = Dataset.get_by_name(workspace=ws, name=dataset_name).to_pandas_dataframe()\\n    print(training_data.head(5))\\n    \\n    return training_data\\n\"}},{\"requiredJars\":[],\"formats\":{\"ColumnsToDrop\":\"text\",\"OutputColumn\":\"text\"},\"classname\":\"TrainLinearRegression\",\"name\":\"TrainLinearRegression\",\"alias\":\"TrainLinearRegression\",\"attributes\":{\"ColumnsToDrop\":\"Col1,Col2,Col3\",\"OutputColumn\":\"Coln\"},\"id\":0,\"category\":\"ComponentsConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    from azureml.core.model import Model, Dataset\\n    from azureml.core.run import Run, _OfflineRun\\n    from azureml.core import Workspace\\n    import argparse\\n    import pandas as pd \\n    import numpy as np \\n    from sklearn.model_selection import train_test_split\\n    from sklearn.linear_model import LinearRegression\\n\\n    df0=training_data\\n    print(\'Dropping columns:\')\\n\\n    columnstodrop=\'{args_namespace.ColumnsToDrop}\'\\n    output_column=\'{args_namespace.OutputColumn}\'\\n    \\n    columnstodrop=columnstodrop.split(\',\')\\n\\n    newdata=df0.drop(columnstodrop,axis=1)\\n\\n    print(\'Training Model:=============\')\\n    xtrain = newdata.drop([columnstodrop],axis=1)\\n    ytrain = newdata[output_column]\\n\\n    model = LinearRegression()\\n    model.fit(xtrain, ytrain)\\n\\n    print(\'Done Training\')\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"ModelName\":\"text\",\"ModelFileName\":\"text\"},\"classname\":\"SaveModel\",\"name\":\"SaveModel\",\"alias\":\"SaveModel\",\"attributes\":{\"ModelName\":\"\",\"ModelFileName\":\"\"},\"id\":0,\"category\":\"ComponentsConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    from azureml.core.model import Dataset\\n    from azureml.core.run import Run, _OfflineRun\\n    from azureml.core import Workspace\\n    import argparse\\n    import pandas as pd \\n    import numpy as np \\n    from sklearn.model_selection import train_test_split\\n    from sklearn.linear_model import LinearRegression\\n    import joblib\\n    from azureml.core import Model\\n    from azureml.core.resource_configuration import ResourceConfiguration\\n    import sklearn\\n\\n    print(\'Trying to save model locally=========\')\\n\\n    model_name=\'{args_namespace.ModelName}\'\\n    model_file=\'{args_namespace.ModelFileName}\'\\n\\n    model_file_path=model_file+\'.pkl\'\\n    joblib.dump(model, model_file_path)\\n\\n    save_model = Model.register(workspace=ws,\\n                           model_name=model_name,                # Name of the registered model in your workspace.\\n                           model_path=\'./\'+model_file_path,  # Local file to upload and register as a model.\\n                           model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\\n                           model_framework_version=sklearn.__version__  # Version of scikit-learn used to create the model.\\n                          )\\n\\n    print(\'Name:\', save_model.name)\\n    print(\'Version:\', save_model.version)\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"task\":\"taskdropdown\",\"compute_target\":\"textarea\",\"label_column_name\":\"textarea\",\"outputs\":\"list\"},\"classname\":\"AutoMLConfig\",\"name\":\"Auto ML\",\"alias\":\"Auto ML\",\"attributes\":{\"task\":\"\",\"compute_target\":\"\",\"label_column_name\":\"\",\"outputs\":[]},\"id\":0,\"category\":\"AutoMLConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \\\"{args_namespace.vm_size}\\\"\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    #Run Configrations\\n    pipeline_run_config = RunConfiguration()\\n    pipeline_run_config.target = step_compute_target\\n    pipeline_run_config.environment.python.conda_dependencies = CondaDependencies.create(\\n        conda_packages=conda_dependencies,\\n        pip_packages=pip_dependencies\\n    )\\n    try:\\n        step_training_data = globals()[f\'{{training_data}}\'].read_delimited_files()\\n    except:\\n        step_training_data = training_data\\n\\n    #Handling outputs from step\\n    op_params = []\\n    outputs={args_namespace.outputs}\\n    for op in outputs:\\n        op_type = op[\\\"step_method\\\"]\\n        keys = list(op.keys())\\n        op_unclean = keys[1]\\n        op_clean = op_unclean.split(\'--\')[-1]\\n        op_var = op[op_unclean]\\n\\n        if op_type == \\\"PipelineData\\\":\\n            globals()[f\'{{op_var}}\'] = PipelineData(\\n                name=f\\\"{{op_var}}\\\",\\n                datastore=workspace.get_default_datastore(),\\n                pipeline_output_name=f\\\"{{op_var}} + \'_output\'\\\",\\n                training_output=TrainingOutput(type=op_clean)\\n            )\\n\\n            op_params.append(globals()[f\'{{op_var}}\'])\\n        else:\\n            raise Exception(\\\"Incorrect argument type!\\\")\\n\\n    #AutoML Configrations\\n    automl_config = AutoMLConfig(\\n        task=\'{args_namespace.task}\',\\n        compute_target=step_compute_target,\\n        run_configuration=pipeline_run_config,\\n        training_data=step_training_data,\\n        label_column_name = \'{args_namespace.label_column_name}\',\\n        iterations=3,\\n        iteration_timeout_minutes=60,\\n        experiment_timeout_hours=3.0,\\n        primary_metric=\'{args_namespace.primary_metric}\',\\n        path=\'outputs/\',\\n        debug_log=\'debug.log\',\\n        featurization=\'auto\',\\n    )\\n\\n    automl_step = AutoMLStep(\\n                        name=\'{args_namespace.name}\',\\n                        automl_config=automl_config,\\n                        passthru_automl_config=False,\\n                        outputs=op_params,\\n                        enable_default_model_output=True,\\n                        enable_default_metrics_output=False,\\n                        allow_reuse=\'{args_namespace.allow_reuse}\')\\n\\n    pipeline_steps.append(automl_step)\\n\\\"\\\"\\\"\"}},{\"id\":\"mVgmt\",\"alias\":\"Tax US  Processing\",\"name\":\"Tax US  Processing\",\"classname\":\"ModelConfig\",\"category\":\"ModelConfig\",\"attributes\":{\"pre-built_model\":\"prebuilt-taxUS\",\"endpoint\":\"https://demofr22apr.cognitiveservices.azure.com/\",\"key\":\"633714795e354a5f90607e0a430e8686\",\"mini_batch_size\":3,\"compute_target\":\"DefLabelNC6\"},\"position_x\":572,\"position_y\":-58,\"connectors\":[],\"connattributes\":[],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    az_dict = training_data.__dict__\\n    datasource = json.loads(az_dict[\\\"_definition\\\"])[\'blocks\'][0][\'arguments\'][\'path\'][\'resourceDetails\'][0][\'path\']\\n\\n    ds = workspace.get_default_datastore()\\n\\n    prebuilt_model = \'{args_namespace.prebuilt_model}\'\\n    script_path = f\\\"{args_namespace.prebuilt_model}.py\\\"\\n    env_yaml_file = \\\"conda_env_v_1_0_0.yml\\\"\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        script = {args_namespace.script}\\n        f.write(\'%s\' %script)\\n    with open(env_yaml_file, \'w+\') as f:\\n        # write elements of list\\n        env_file = {args_namespace.env_file}\\n        f.write(\'%s\' %env_file)\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\\\"experiment_env\\\", env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n\\n    output_dir = PipelineData(name = \\\"model_output\\\", datastore = ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        entry_script = prebuilt_model + \\\".py\\\",\\n        mini_batch_size = str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold = 10,\\n        output_action = \\\"append_row\\\",\\n        append_row_file_name = prebuilt_model + \\\".json\\\",\\n        environment = batch_env,\\n        compute_target = step_compute_target,\\n        node_count = 2,\\n        run_invocation_timeout =  1800,\\n        run_max_try = 9\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name = prebuilt_model,\\n        parallel_run_config = parallel_run_config,\\n        inputs = [training_data.as_named_input(dataset_name + (\\\"_dataset\\\"))],\\n        output = output_dir,\\n        arguments = [\'--endpoint\', \'{args_namespace.endpoint}\', \'--key\', \'{args_namespace.key}\', \'--datasource\', datasource],\\n        allow_reuse = True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"formats\":{\"pre-built_model\":\"dropValues\",\"dropValues\":[\"prebuilt-invoice\",\"prebuilt-receipt\",\"prebuilt-id\",\"prebuilt-taxUS\",\"prebuilt-document\",\"prebuilt-businesscard\"],\"endpoint_parameter\":\"textarea\",\"key\":\"textarea\"}},{\"id\":\"mVgmt\",\"alias\":\"ID  Processing\",\"name\":\"ID  Processing\",\"classname\":\"ModelConfig\",\"category\":\"ModelConfig\",\"attributes\":{\"pre-built_model\":\"prebuilt-id\",\"endpoint\":\"https://demofr22apr.cognitiveservices.azure.com/\",\"key\":\"633714795e354a5f90607e0a430e8686\",\"mini_batch_size\":3,\"compute_target\":\"DefLabelNC6\"},\"position_x\":572,\"position_y\":-58,\"connectors\":[],\"connattributes\":[],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    az_dict = training_data.__dict__\\n    datasource = json.loads(az_dict[\\\"_definition\\\"])[\'blocks\'][0][\'arguments\'][\'path\'][\'resourceDetails\'][0][\'path\']\\n\\n    ds = workspace.get_default_datastore()\\n\\n    prebuilt_model = \'{args_namespace.prebuilt_model}\'\\n    script_path = f\\\"{args_namespace.prebuilt_model}.py\\\"\\n    env_yaml_file = \\\"conda_env_v_1_0_0.yml\\\"\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        script = {args_namespace.script}\\n        f.write(\'%s\' %script)\\n    with open(env_yaml_file, \'w+\') as f:\\n        # write elements of list\\n        env_file = {args_namespace.env_file}\\n        f.write(\'%s\' %env_file)\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\\\"experiment_env\\\", env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n\\n    output_dir = PipelineData(name = \\\"model_output\\\", datastore = ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        entry_script = prebuilt_model + \\\".py\\\",\\n        mini_batch_size = str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold = 10,\\n        output_action = \\\"append_row\\\",\\n        append_row_file_name = prebuilt_model + \\\".json\\\",\\n        environment = batch_env,\\n        compute_target = step_compute_target,\\n        node_count = 2,\\n        run_invocation_timeout =  1800,\\n        run_max_try = 9\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name = prebuilt_model,\\n        parallel_run_config = parallel_run_config,\\n        inputs = [training_data.as_named_input(dataset_name + (\\\"_dataset\\\"))],\\n        output = output_dir,\\n        arguments = [\'--endpoint\', \'{args_namespace.endpoint}\', \'--key\', \'{args_namespace.key}\', \'--datasource\', datasource],\\n        allow_reuse = True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"formats\":{\"pre-built_model\":\"dropValues\",\"dropValues\":[\"prebuilt-invoice\",\"prebuilt-receipt\",\"prebuilt-id\",\"prebuilt-taxUS\",\"prebuilt-document\",\"prebuilt-businesscard\"],\"endpoint_parameter\":\"textarea\",\"key\":\"textarea\"}},{\"id\":\"mVgmt\",\"alias\":\"BusinessCard  Processing\",\"name\":\"BusinessCard  Processing\",\"classname\":\"ModelConfig\",\"category\":\"ModelConfig\",\"attributes\":{\"pre-built_model\":\"prebuilt-businesscard\",\"endpoint\":\"https://demofr22apr.cognitiveservices.azure.com/\",\"key\":\"633714795e354a5f90607e0a430e8686\",\"mini_batch_size\":3,\"compute_target\":\"DefLabelNC6\"},\"position_x\":572,\"position_y\":-58,\"connectors\":[],\"connattributes\":[],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    az_dict = training_data.__dict__\\n    datasource = json.loads(az_dict[\\\"_definition\\\"])[\'blocks\'][0][\'arguments\'][\'path\'][\'resourceDetails\'][0][\'path\']\\n\\n    ds = workspace.get_default_datastore()\\n\\n    prebuilt_model = \'{args_namespace.prebuilt_model}\'\\n    script_path = f\\\"{args_namespace.prebuilt_model}.py\\\"\\n    env_yaml_file = \\\"conda_env_v_1_0_0.yml\\\"\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        script = {args_namespace.script}\\n        f.write(\'%s\' %script)\\n    with open(env_yaml_file, \'w+\') as f:\\n        # write elements of list\\n        env_file = {args_namespace.env_file}\\n        f.write(\'%s\' %env_file)\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\\\"experiment_env\\\", env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n\\n    output_dir = PipelineData(name = \\\"model_output\\\", datastore = ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        entry_script = prebuilt_model + \\\".py\\\",\\n        mini_batch_size = str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold = 10,\\n        output_action = \\\"append_row\\\",\\n        append_row_file_name = prebuilt_model + \\\".json\\\",\\n        environment = batch_env,\\n        compute_target = step_compute_target,\\n        node_count = 2,\\n        run_invocation_timeout =  1800,\\n        run_max_try = 9\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name = prebuilt_model,\\n        parallel_run_config = parallel_run_config,\\n        inputs = [training_data.as_named_input(dataset_name + (\\\"_dataset\\\"))],\\n        output = output_dir,\\n        arguments = [\'--endpoint\', \'{args_namespace.endpoint}\', \'--key\', \'{args_namespace.key}\', \'--datasource\', datasource],\\n        allow_reuse = True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"formats\":{\"pre-built_model\":\"dropValues\",\"dropValues\":[\"prebuilt-invoice\",\"prebuilt-receipt\",\"prebuilt-id\",\"prebuilt-taxUS\",\"prebuilt-document\",\"prebuilt-businesscard\"],\"endpoint_parameter\":\"textarea\",\"key\":\"textarea\"}},{\"id\":\"mVgmt\",\"alias\":\"Receipt  Processing\",\"name\":\"Receipt  Processing\",\"classname\":\"ModelConfig\",\"category\":\"ModelConfig\",\"attributes\":{\"pre-built_model\":\"prebuilt-receipt\",\"endpoint\":\"https://demofr22apr.cognitiveservices.azure.com/\",\"key\":\"633714795e354a5f90607e0a430e8686\",\"mini_batch_size\":3,\"compute_target\":\"DefLabelNC6\"},\"position_x\":572,\"position_y\":-58,\"connectors\":[],\"connattributes\":[],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    az_dict = training_data.__dict__\\n    datasource = json.loads(az_dict[\\\"_definition\\\"])[\'blocks\'][0][\'arguments\'][\'path\'][\'resourceDetails\'][0][\'path\']\\n\\n    ds = workspace.get_default_datastore()\\n\\n    prebuilt_model = \'{args_namespace.prebuilt_model}\'\\n    script_path = f\\\"{args_namespace.prebuilt_model}.py\\\"\\n    env_yaml_file = \\\"conda_env_v_1_0_0.yml\\\"\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        script = {args_namespace.script}\\n        f.write(\'%s\' %script)\\n    with open(env_yaml_file, \'w+\') as f:\\n        # write elements of list\\n        env_file = {args_namespace.env_file}\\n        f.write(\'%s\' %env_file)\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\\\"experiment_env\\\", env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n\\n    output_dir = PipelineData(name = \\\"model_output\\\", datastore = ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        entry_script = prebuilt_model + \\\".py\\\",\\n        mini_batch_size = str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold = 10,\\n        output_action = \\\"append_row\\\",\\n        append_row_file_name = prebuilt_model + \\\".json\\\",\\n        environment = batch_env,\\n        compute_target = step_compute_target,\\n        node_count = 2,\\n        run_invocation_timeout =  1800,\\n        run_max_try = 9\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name = prebuilt_model,\\n        parallel_run_config = parallel_run_config,\\n        inputs = [training_data.as_named_input(dataset_name + (\\\"_dataset\\\"))],\\n        output = output_dir,\\n        arguments = [\'--endpoint\', \'{args_namespace.endpoint}\', \'--key\', \'{args_namespace.key}\', \'--datasource\', datasource],\\n        allow_reuse = True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"formats\":{\"pre-built_model\":\"dropValues\",\"dropValues\":[\"prebuilt-invoice\",\"prebuilt-receipt\",\"prebuilt-id\",\"prebuilt-taxUS\",\"prebuilt-document\",\"prebuilt-businesscard\"],\"endpoint_parameter\":\"textarea\",\"key\":\"textarea\"}},{\"id\":\"mVgmt\",\"alias\":\"Invoice  Processing\",\"name\":\"Invoice  Processing\",\"classname\":\"ModelConfig\",\"category\":\"ModelConfig\",\"attributes\":{\"pre-built_model\":\"prebuilt-invoice\",\"endpoint\":\"https://demofr22apr.cognitiveservices.azure.com/\",\"key\":\"633714795e354a5f90607e0a430e8686\",\"mini_batch_size\":3,\"compute_target\":\"DefLabelNC6\"},\"position_x\":572,\"position_y\":-58,\"connectors\":[],\"connattributes\":[],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    #Compute Target\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    az_dict = training_data.__dict__\\n    datasource = json.loads(az_dict[\\\"_definition\\\"])[\'blocks\'][0][\'arguments\'][\'path\'][\'resourceDetails\'][0][\'path\']\\n\\n    ds = workspace.get_default_datastore()\\n\\n    prebuilt_model = \'{args_namespace.prebuilt_model}\'\\n    script_path = f\\\"{args_namespace.prebuilt_model}.py\\\"\\n    env_yaml_file = \\\"conda_env_v_1_0_0.yml\\\"\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        script = {args_namespace.script}\\n        f.write(\'%s\' %script)\\n    with open(env_yaml_file, \'w+\') as f:\\n        # write elements of list\\n        env_file = {args_namespace.env_file}\\n        f.write(\'%s\' %env_file)\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\\\"experiment_env\\\", env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n\\n    output_dir = PipelineData(name = \\\"model_output\\\", datastore = ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        entry_script = prebuilt_model + \\\".py\\\",\\n        mini_batch_size = str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold = 10,\\n        output_action = \\\"append_row\\\",\\n        append_row_file_name = prebuilt_model + \\\".json\\\",\\n        environment = batch_env,\\n        compute_target = step_compute_target,\\n        node_count = 2,\\n        run_invocation_timeout =  1800,\\n        run_max_try = 9\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name = prebuilt_model,\\n        parallel_run_config = parallel_run_config,\\n        inputs = [training_data.as_named_input(dataset_name + (\\\"_dataset\\\"))],\\n        output = output_dir,\\n        arguments = [\'--endpoint\', \'{args_namespace.endpoint}\', \'--key\', \'{args_namespace.key}\', \'--datasource\', datasource],\\n        allow_reuse = True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"formats\":{\"pre-built_model\":\"dropValues\",\"dropValues\":[\"prebuilt-invoice\",\"prebuilt-receipt\",\"prebuilt-id\",\"prebuilt-taxUS\",\"prebuilt-document\",\"prebuilt-businesscard\"],\"endpoint_parameter\":\"textarea\",\"key\":\"textarea\"}},{\"formats\":{\"compute_target\":\"textarea\",\"model\":\"restdataset\",\"restdatasetname\":\"FetchModels\",\"payload\":{\"userId\":\"lalith.basna@ad.infosys.com\",\"platform\":\"Azure\"},\"response\":\"displayName\",\"type\":\"BatchPrediction\",\"model_extension\":\"dropValues\",\"dropValues\":[\".pkl\",\".pt\"],\"score_script\":\"textarea\"},\"alias\":\"Batch  Prediction\",\"name\":\"Batch  Prediction\",\"classname\":\"BatchConfig\",\"category\":\"BatchConfig\",\"codeGeneration\":{\"script\":\"\"},\"attributes\":{\"compute_target\":\"\",\"model\":[],\"model_extension\":\".pkl\",\"script\":[]},\"position_x\":\"440\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"extrd\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"context\":[{\"azuredataset\":\"\"}],\"connattributes\":{\"azuredataset\":\"\"}},{\"alias\":\"Tabular Scoring Script\",\"name\":\"Tabular Scoring Script\",\"classname\":\"TabularScoringconfigScript\",\"category\":\"BatchConfig\",\"attributes\":{},\"position_x\":\"440\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"extrd\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    ds = workspace.get_default_datastore()\\n    model_name = \'{args_namespace.model_name}\'\\n\\n    tmp_folder = model_name\\n\\n    if os.path.exists(tmp_folder):\\n        shutil.rmtree(tmp_folder)\\n    if not os.path.exists(tmp_folder):    \\n        os.makedirs(tmp_folder)\\n    \\n    model = Model(workspace=workspace, name=model_name)\\n    downloaded_path = model.download(target_dir=tmp_folder)\\n\\n    script_path = os.path.join(tmp_folder, \'score.py\')\\n    script = {args_namespace.script}\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        f.write(\'%s\' %script)\\n    f.close()\\n\\n    env_yaml_file = os.path.join(downloaded_path, \'conda_env_v_1_0_0.yml\')\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\'experiment_env\', env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n    print(\'Configuration ready.\')\\n\\n    output_dir = PipelineData(name=\'scores\', datastore=ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        source_directory=tmp_folder,\\n        entry_script=\'score.py\',\\n        mini_batch_size=str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold=10,\\n        output_action=\'append_row\',\\n        append_row_file_name=\'parallel_run_step.csv\',\\n        environment=batch_env,\\n        compute_target=step_compute_target,\\n        node_count=2\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name=\'{args_namespace.batch_name}\',\\n        parallel_run_config=parallel_run_config,\\n        inputs=[training_data.as_named_input(\'batch_data_set\')],\\n        output=output_dir,\\n        arguments=[],\\n        allow_reuse=True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"context\":[{\"azuredataset\":\"\"}],\"connattributes\":{\"azuredataset\":\"\"}},{\"alias\":\"Custom Scoring Script\",\"name\":\"Custom Scoring Script\",\"classname\":\"CustomScoringconfigScript\",\"category\":\"BatchConfig\",\"formats\":{\"score_script\":\"textarea\",\"env_script\":\"textarea\"},\"attributes\":{\"script\":[],\"env_yaml\":[]},\"position_x\":\"440\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"extrd\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n    try:\\n        step_compute_target = ComputeTarget(workspace=workspace, name=\'{args_namespace.compute_target}\')\\n    except:\\n        vm_size = \'{args_namespace.vm_size}\'\\n        compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size)\\n        step_compute_target = ComputeTarget.create(\\n            workspace=workspace,\\n            name=\'{args_namespace.compute_target}\',\\n            provisioning_configuration=compute_config)\\n        step_compute_target.wait_for_completion(show_output=True)\\n\\n    ds = workspace.get_default_datastore()\\n    model_name = \'{args_namespace.model_name}\'\\n\\n    tmp_folder = model_name\\n\\n    if os.path.exists(tmp_folder):\\n        shutil.rmtree(tmp_folder)\\n    if not os.path.exists(tmp_folder):    \\n        os.makedirs(tmp_folder)\\n    \\n    model = Model(workspace=workspace, name=model_name)\\n    downloaded_path = model.download(target_dir=tmp_folder)\\n\\n    script_path = os.path.join(tmp_folder, \'score.py\')\\n    script = {args_namespace.script}\\n    with open(script_path, \'w+\') as f:\\n        # write elements of list\\n        for line in script:\\n            f.write(\'%s\\\\\\\\n\' %line)\\n    f.close()\\n\\n    env_yaml_file = os.path.join(downloaded_path, \'conda_env_v_1_0_0.yml\')\\n    env = {args_namespace.env}\\n    with open(env_yaml_file, \'w+\') as f:\\n        # write elements of list\\n        for line in env:\\n            f.write(\'%s\\\\\\\\n\' %line)\\n    f.close()\\n\\n    # Create an Environment for the experiment\\n    batch_env = Environment.from_conda_specification(\'experiment_env\', env_yaml_file)\\n    batch_env.docker.base_image = DEFAULT_CPU_IMAGE\\n    print(\'Configuration ready.\')\\n\\n    output_dir = PipelineData(name=\'scores\', datastore=ds)\\n\\n    parallel_run_config = ParallelRunConfig(\\n        source_directory=tmp_folder,\\n        entry_script=\'score.py\',\\n        mini_batch_size=str(\'{args_namespace.mini_batch_size}\'),\\n        error_threshold=10,\\n        output_action=\'append_row\',\\n        append_row_file_name=\'parallel_run_step.csv\',\\n        environment=batch_env,\\n        compute_target=step_compute_target,\\n        node_count=2\\n    )\\n\\n    parallelrun_step = ParallelRunStep(\\n        name=\'{args_namespace.batch_name}\',\\n        parallel_run_config=parallel_run_config,\\n        inputs=[training_data.as_named_input(\'batch_data_set\')],\\n        output=output_dir,\\n        arguments=[],\\n        allow_reuse=True\\n    )\\n\\n    pipeline_steps.append(parallelrun_step)\\n\\\"\\\"\\\"\"},\"requiredJars\":[],\"context\":[{\"azuredataset\":\"\"}],\"connattributes\":{\"azuredataset\":\"\"}}]","Azure","{\"commands\":[\"@!PYTHONPATH!@ @!icip.pipelineScript.directory!@@!pipelinename!@/@!pipelinename!@_generatedCode.py\",\"\"],\"environment\":{\"PYTHONPATH\":\"python3\"}}","Demo"
"VertexPlugin","[{\"requiredJars\":[],\"formats\":{\"dataset\":\"textarea\",\"dataset_type\":\"dropValues\",\"dropValues\":[\"image\",\"tabular\",\"text\"],\"gcs_source\":\"textarea\"},\"classname\":\"ExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Extractor\",\"attributes\":{\"dataset_name\":\"\",\"dataset_type\":\"\",\"gcs_source\":\"\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n        # Extractor\\n        if \'{args_namespace.dataset_type}\'.lower() == \'tabular\':\\n            dataset_create_op = gcc_aip.TabularDatasetCreateOp(\\n                project=project,\\n                display_name=\'{args_namespace.dataset_name}\',\\n                gcs_source=\'{args_namespace.gcs_source}\',\\n            )\\n\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"task\":\"dropValues1\",\"dropValues1\":[\"image-classification\",\"image-object-detection\",\"tabular-classification\",\"tabular-regression\",\"text-classification\",\"text-extraction\",\"text-sentiment\",\"video-classification\",\"video-action-recognition\",\"video-object-tracking\"],\"optimization_objective\":\"dropValues2\",\"dropValues2\":[\"maximize-au-roc\",\"minimize-log-loss\",\"maximize-au-prc\",\"maximize-precision-at-recall\",\"maximize-recall-at-precision\",\"minimize-log-loss\",\"minimize-rmse\",\"minimize-mae\",\"minimize-rmsle\"],\"target_column\":\"textarea\",\"column_transformations\":\"textarea\"},\"classname\":\"AutoMLConfig\",\"name\":\"Auto ML\",\"alias\":\"Auto ML\",\"attributes\":{\"task\":\"\",\"optimization_objective\":\"\",\"target_column\":\"\",\"column_transformations\":\"\"},\"id\":0,\"category\":\"AutoMLConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n        # AutoML\\n        if \'{args_namespace.task}\' in [\'tabular-classification\', \'tabular-regression\']:\\n            training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\\n                project=project,\\n                display_name=\'train-automl\' + \'{args_namespace.display_name}\',\\n                optimization_prediction_type=\'{args_namespace.prediction_type}\',\\n                optimization_objective=\'{args_namespace.optimization_objective}\',\\n                column_transformations=\\\"{args_namespace.column_transformations}\\\",\\n                dataset=dataset_create_op.outputs[\'dataset\'],\\n                target_column=\'{args_namespace.target_column}\',\\n            )\\n\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"endpoint_name\":\"textarea\"},\"classname\":\"CreateEndpoint\",\"name\":\"Endpoint\",\"alias\":\"Endpoint\",\"attributes\":{\"endpoint_name\":\"\"},\"id\":0,\"category\":\"CreateEndpoint\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n        # Endpoint\\n        endpoint_name = \'{args_namespace.endpoint_name}\'.lower().replace(\'_\', \'-\')\\n        endpoint_op = gcc_aip.EndpointCreateOp(\\n            project=project,\\n            location=region,\\n            display_name=endpoint_name,\\n        )\\n\\n\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"dedicated_resources_machine_type\":\"dropValues1\",\"dropValues1\":[\"n1-standard-2\",\"n1-standard-4\",\"n1-standard-8\",\"n1-standard-16\",\"n1-standard-32\",\"n1-highmem-2\",\"n1-highmem-4\",\"n1-highmem-8\",\"n1-highmem-16\",\"n1-highmem-32\",\"n1-highcpu-4\",\"n1-highcpu-8\",\"n1-highcpu-16\",\"n1-highcpu-32\"],\"dedicated_resources_min_replica_count\":\"textarea\",\"dedicated_resources_max_replica_count\":\"textarea\"},\"classname\":\"ModelDeployment\",\"name\":\"Model Deployment\",\"alias\":\"Model Deployment\",\"attributes\":{\"dedicated_resources_machine_type\":\"\",\"dedicated_resources_min_replica_count\":\"\",\"dedicated_resources_max_replica_count\":\"\"},\"id\":0,\"category\":\"CreateEndpoint\",\"inputEndpoints\":[\"in1\",\"in2\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\n        # ModelDeployment\\n        gcc_aip.ModelDeployOp(\\n            model=training_op.outputs[\'model\'],\\n            endpoint=endpoint_op.outputs[\'endpoint\'],\\n            dedicated_resources_machine_type=\'{args_namespace.dedicated_resources_machine_type}\',\\n            dedicated_resources_min_replica_count=\'{args_namespace.dedicated_resources_min_replica_count}\',\\n            dedicated_resources_max_replica_count=\'{args_namespace.dedicated_resources_max_replica_count}\',\\n        )\\n\\n\\\"\\\"\\\"\"}},{\"requiredJars\":[],\"formats\":{\"model\":\"dropdown\",\"type\":\"vertex-ai\",\"predictions_format\":\"dropValues\",\"dropValues\":[\"jsonl\",\"csv\"],\"gcs_destination_prefix\":\"textarea\"},\"classname\":\"BatchConfig\",\"name\":\"Batch Prediction\",\"alias\":\"Batch Prediction\",\"attributes\":{\"model\":\"\",\"predictions_format\":\"\",\"gcs_destination_prefix\":\"\"},\"id\":0,\"category\":\"BatchConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]}]","Vertex","{\"commands\":[\"@!PYTHONPATH!@ @!icip.pipelineScript.directory!@@!pipelinename!@/@!pipelinename!@_generatedCode.py\",\"\",\"\",\"\"],\"environment\":{\"PYTHONPATH\":\"python3\"}}","Demo"
"IcmmPlugin","[{\"requiredJars\":[],\"formats\":{\"Mlflowdataset\":\"restdataset\",\"restdatasetname\":\"Fetchdatasets\",\"payload\":{\"userId\":\"Swarnamala_M@ad.infosys.com\",\"platform\":\"mlflow\"},\"response\":\"displayName\"},\"classname\":\"Dataset ExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Extractor\",\"attributes\":{\"Mlflowdataset\":[]},\"id\":0,\"category\":\"Dataset ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"payload\":{\"userId\":\"admin@infosys.com\",\"platform\":\"mlflow\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Attribute Config\",\"name\":\"Attribute Name\",\"alias\":\"Attribute Name\",\"attributes\":{},\"id\":0,\"category\":\"AttributeConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"classifierconfig\":\"api\",\"api\":\"http://10.177.28.36:9090/model/list_models/\",\"Remove URL\":\"checkbox\",\"Remove Email\":\"checkbox\",\"Remove HTML Tags\":\"checkbox\",\"Remove Numbers\":\"checkbox\",\"Remove Stopwords\":\"checkbox\",\"Remove Special Characters\":\"checkbox\",\"Remove Org Names\":\"checkbox\",\"Lemmatization\":\"checkbox\",\"Filter Small Words\":\"checkbox\",\"Repetition\":\"checkbox\",\"Encode Decode ASCII\":\"checkbox\",\"Choose Token Limit Classes\":\"dropValues\",\"dropValues\":[\"All\",\"Others\"],\"Enter Token length\":\"textarea\",\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Pre-Processing Config\",\"name\":\"Text PreProcess\",\"alias\":\"Text PreProcess\",\"attributes\":{\"classifierconfig\":[],\"Remove URL\":[],\"Remove Email\":[],\"Remove HTML Tags\":[],\"Remove Numbers\":[],\"Remove Stopwords\":[],\"Remove Special Characters\":[],\"Remove Org Names\":[],\"Lemmatization\":[],\"Filter Small Words\":[],\"Repetition\":[],\"Encode Decode ASCII\":[],\"Choose Token Limit Classes\":[],\"Enter Token length\":[]},\"id\":0,\"category\":\"Pre-Processing Config\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Data Balancing\":\"textarea\",\"Data Augmentation\":\"dropValues\",\"dropValues\":[\"Data Augmentation with Synonym\",\"Data Augmentation with Translation\"],\"payload\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response\":\"trainSetName\",\"payload1\":{\"validateUser\":{\"appID\":\"5f7f4f4e2b06ff4bd1f31cb7\",\"classifierID\":\"_id\",\"emailId\":\"admin@user\",\"orgID\":\"5f7f4f2e2b06ff4bd1f31cb6\",\"password\":\"123456@a\",\"role\":\"Admin\"}}},\"classname\":\"Pre-Processing Config\",\"name\":\"Data PreProcess\",\"alias\":\"Data PreProcess\",\"attributes\":{\"classifierconfig\":[],\"Data Balancing\":[],\"Data Augmentation\":[],\"class\":\"\"},\"id\":0,\"category\":\"Pre-Processing Config\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Choose a technique for validation\":\"dropValues\",\"dropValues\":[\"fields_dropout_25_patience_10_dsme\",\"finance_attribute_iteration10_shuffled2(svm)_dsme\",\"qa_model_albert_dsme\",\"fields_attribute_iteration_11_50_epochs_plus_dsme\",\"Finance Fields itr 10 (SVM)_dsme\",\"Clauses Refund\",\"Clauses Attribute Threshold\",\"Finance Fields MNB Test\",\"legal_termination\",\"Sample_trainset_new_UI\",\"config_ui_test\",\"config_ui_test_xlnet\",\"FileUpload\",\"uichange\"],\"Keywords\":\"Taglist\",\"Multiple Match\":\"checkbox\",\"Case Sensitive\":\"checkbox\",\"Root WordMatch\":\"checkbox\",\"Regex Match\":\"checkbox\",\"payload\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response\":\"trainSetName\"},\"classname\":\"Conventional TrainsetConfig\",\"name\":\"Lookup Dictionary\",\"alias\":\"Lookup Dictionary\",\"attributes\":{\"Choose a technique for validation\":[],\"Keywords\":[],\"Multiple Match\":[],\"Case Sensitive\":[],\"Root WordMatch\":[],\"Regex Match\":[]},\"id\":0,\"category\":\"Conventional TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Label\":\"textarea\",\"Keywords\":\"Taglist\",\"payload\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response\":\"trainSetName\"},\"classname\":\"ML/DL TrainsetConfig\",\"name\":\"Text Classifier\",\"alias\":\"Text Classifier\",\"attributes\":{\"Label\":[],\"Keywords\":[]},\"id\":0,\"category\":\"ML/DL TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Choose an Algorithm\":\"dropValues\",\"dropValues\":[\"SVM\",\"MNB\"],\"Choose a Vectorizer\":\"dropValues1\",\"dropValues1\":[\"tfidVectorizer\",\"CountVectorizer\"],\"Enter threshold value\":\"textarea\",\"Choose a Tokenizer\":\"dropValues2\",\"dropValues2\":[\"Paragraph\",\"Sentence\",\"Line\",\"Table\",\"Document\",\"Page\"]},\"classname\":\"ML/DL TrainsetConfig\",\"name\":\"Classical Paragraph Classifier\",\"alias\":\"Classical Paragraph Classifier\",\"attributes\":{\"Choose an Algorithm\":[],\"Choose a Vectorizer\":[],\"Enter threshold value\":[]},\"id\":0,\"category\":\"ML/DL TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"classifierconfig\":\"api\",\"api\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"Add Tokenizer\":\"dropValues\",\"dropValues\":[\"Sentence\",\"Line\",\"Table\",\"Document\",\"Page\",\"Paragraph\"],\"Choose a Dataset\":\"dropValues1\",\"dropValues1\":[\"sample\",\"Legal_14_Clauses_Dataset_04Jan2022_502Docs_Multilabel\",\"MSA_Sentence_Dataset_Version2_Retagged\",\"clauses\"],\"Choose an Architecture\":\"dropValues2\",\"dropValues2\":[\"BERT\",\"XLNET\",\"GOOGLET5\",\"ROBERTA\"],\"payload\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response\":\"trainSetName\"},\"classname\":\"ML/DL TrainsetConfig\",\"name\":\"DL Paragraph Classifier\",\"alias\":\"DL Paragraph Classifier\",\"attributes\":{\"Add Tokenizer\":[],\"Choose a Dataset\":[],\"Choose an Architecture\":[]},\"id\":0,\"category\":\"ML/DL TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter Model Name\":\"textarea\",\"Choose a Category\":\"dropValues\",\"dropValues\":[\"Multinomial-NaiveBayes\",\"Gaussial-NaiveBayes\",\"Random Forest\",\"Ada Boost\"],\"Label\":\"textarea\",\"Keywords\":\"list\"},\"classname\":\"ML/DL TrainsetConfig\",\"name\":\"Label Classifier\",\"alias\":\"Label Classifier\",\"attributes\":{\"Enter Model Name\":[],\"Choose a Category\":[],\"Label\":[],\"Keywords\":[]},\"id\":0,\"category\":\"ML/DL TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Label\":\"textarea\",\"Keywords\":\"list\"},\"classname\":\"Conventional TrainsetConfig\",\"name\":\"Replace Dictionary\",\"alias\":\"Replace Dictionary\",\"attributes\":{\"Label\":[],\"Keywords\":[]},\"id\":0,\"category\":\"Conventional TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Label\":\"textarea\",\"Keywords\":\"list\"},\"classname\":\"Conventional TrainsetConfig\",\"name\":\"Custom NER\",\"alias\":\"Custom NER\",\"attributes\":{\"Label\":[],\"Keywords\":[]},\"id\":0,\"category\":\"Conventional TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Label\":\"textarea\",\"Keywords\":\"list\",\"Type\":\"dropValues\",\"dropValues\":[\"date\",\"text\",\"amount\",\"integer\",\"other\"]},\"classname\":\"Conventional TrainsetConfig\",\"name\":\"Table Search\",\"alias\":\"Table Search\",\"attributes\":{\"Label\":[],\"Keywords\":[],\"Type\":[]},\"id\":0,\"category\":\"Conventional TrainsetConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"Enter String\":\"textarea\",\"Case Sensitive\":\"dropValues1\",\"dropValues1\":[\"True\",\"False\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"String Match\",\"alias\":\"String Match\",\"attributes\":{\"Enter String\":[],\"Case Sensitive\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter POS tag\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"POS Match\",\"alias\":\"POS Match\",\"attributes\":{\"Enter POS tag\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter Start String\":\"textarea\",\"Enter End String\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Block Text\",\"alias\":\"Block Text\",\"attributes\":{\"Enter Start String\":[],\"Enter End String\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter RegEx\":\"textarea\",\"Case sensitive\":\"dropValues\",\"dropValues\":[\"True\",\"False\"],\"Match\":\"dropValues1\",\"dropValues1\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Regex Match\",\"alias\":\"Regex Match\",\"attributes\":{\"Enter RegEx\":[],\"Case sensitive\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter String\":\"textarea\",\"Case sensitive\":\"dropValues\",\"dropValues\":[\"True\",\"False\"],\"Match\":\"dropValues1\",\"dropValues1\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Root word match\",\"alias\":\"Root word match\",\"attributes\":{\"Enter String\":[],\"Case sensitive\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Lookup Dictionary\":\"textarea\",\"Input Type\":\"dropValues\",\"dropValues\":[\"PARADATA\",\"TABLEDATA\",\"PAGEDATA\",\"PAGEWISEDATA\"],\"Output Type\":\"dropValues2\",\"dropValues2\":[\"HYBRID\",\"PAGESTRUCTURE\",\"DEFAULT\"],\"Match\":\"dropValues1\",\"dropValues1\":[\"First\",\"Last\",\"All\"]},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"LookUP\",\"alias\":\"LookUP\",\"attributes\":{\"Select Lookup Dictionary\":[],\"Input Type\":[],\"Output Type\":[],\"Match\":\"\"},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Attributes Used\":\"Taglist\",\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Derive\",\"alias\":\"Derive\",\"attributes\":{\"Attributes Used\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select training set\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Replace\",\"alias\":\"Replace\",\"attributes\":{\"Select training set\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select NLP rule\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Custom NLP\",\"alias\":\"Custom NLP\",\"attributes\":{\"Select NLP rule\":[],\"Match\":[]},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter tag\":\"textarea\",\"Enter threshold value\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"]},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Document Analysis\",\"alias\":\"Document Analysis\",\"attributes\":{\"Enter tag\":[],\"Enter threshold value\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Tag\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Sequence Model Technique\",\"alias\":\"Sequence Model Technique\",\"attributes\":{\"Select tag\":[],\"Match\":[]},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter String\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Subset Technique\",\"alias\":\"Subset Technique\",\"attributes\":{\"Enter String\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Enter String\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Custom Built Technique\",\"alias\":\"Custom Built Technique\",\"attributes\":{\"Enter String\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Merging Technique\",\"alias\":\"MergingTechnique\",\"attributes\":{},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Form Extraction Technique\",\"alias\":\"Form Extraction Technique\",\"attributes\":{},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Apply On\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Table TechniquesConfig\",\"name\":\"Table Column Extraction\",\"alias\":\"Table Column Extraction\",\"attributes\":{\"Apply On\":[],\"Match\":[]},\"id\":0,\"category\":\"Table TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Table Data Set\":\"textarea\",\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Table TechniquesConfig\",\"name\":\"Table Search\",\"alias\":\"Table Search\",\"attributes\":{\"Select Table Data Set\":[]},\"id\":0,\"category\":\"Table TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Table TechniquesConfig\",\"name\":\"Table Filter\",\"alias\":\"Table Filter\",\"attributes\":{\"Match\":[]},\"id\":0,\"category\":\"Table TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Train Set\":\"textarea\",\"Output Type\":\"dropValues\",\"dropValues\":[\"pageData\"],\"pageDataFlag\":\"dropValues1\",\"dropValues1\":[\"true\",\"false\"],\"Match\":\"dropValues2\",\"dropValues2\":[\"First\",\"Last\",\"All\"],\"Threshold\":\"textarea\",\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Paragraph classifier\",\"alias\":\"Paragraph classifier\",\"attributes\":{\"Select Train Set\":[],\"Output Type\":[],\"pageDataFlag\":[],\"Match\":[],\"Threshold\":\"\"},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Add Page Count\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Image classifier\",\"alias\":\"Image classifier\",\"attributes\":{\"Add Page Count\":[],\"Match\":[]},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Model\":\"textarea\",\"Select Class\":\"dropValues\",\"dropValues\":[\"Title Page\",\"Material Page\",\"Summary Page\",\"Other\"],\"Match\":\"dropValues1\",\"dropValues1\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Page classifier\",\"alias\":\"Page classifier\",\"attributes\":{\"Select Model\":[],\"Select Class\":[],\"Match\":[]},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select trained Model\":\"textarea\",\"Select Class\":\"dropValues\",\"dropValues\":[\"Title Page\",\"Material Page\",\"Summary Page\",\"Other\"],\"Match\":\"dropValues1\",\"dropValues1\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Document classifier\",\"alias\":\"Document classifier\",\"attributes\":{\"Select trained Model\":[],\"Select Class\":[],\"Match\":[]},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Train Set\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"Label Value classifier\",\"alias\":\"Label Value classifier\",\"attributes\":{\"Select Train Set\":[],\"Match\":[]},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Match With\":\"textarea\",\"Apply On\":\"dropValues\",\"dropValues\":[\"All\",\"Organization\",\"Person\",\"Location\"],\"Match\":\"dropValues1\",\"dropValues1\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"NER Match\",\"alias\":\"NER Match\",\"attributes\":{\"Match With\":[],\"Apply On\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Select Train Set\":\"textarea\",\"Match\":\"dropValues\",\"dropValues\":[\"First\",\"Last\",\"All\"],\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Conventional TechniquesConfig\",\"name\":\"Cutom NER\",\"alias\":\"Custom NER\",\"attributes\":{\"Select Train Set\":[],\"Match\":[]},\"id\":0,\"category\":\"Conventional TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"ML/DL TechniquesConfig\",\"name\":\"DeepLearning ParaClassifier\",\"alias\":\"DeepLearning ParaClassifier\",\"attributes\":{},\"id\":0,\"category\":\"ML/DL TechniquesConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"payload\":{\"userId\":\"demo_mlflow@infosys.com\"},\"label\":\"api1\",\"api1\":\"http://10.217.10.232:8102/configuration/getalltrainingsets/\",\"payload1\":{\"validateUser\":{\"emailId\":\"admin@user\",\"password\":\"123456@a\",\"role\":\"Admin\",\"orgID\":\"5f8d6567fb9d17629a972af1\",\"appID\":\"5f8d657cfb9d17629a972af2\"}},\"response1\":\"trainSetName\"},\"classname\":\"Data LoaderConfig\",\"name\":\"Data Saving\",\"alias\":\"Data Saving\",\"attributes\":{},\"id\":0,\"category\":\"Data LoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]}]","ICMM","{\"commands\":[\"\"],\"environment\":{}}","Demo"
"MlflowPlugin","[{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"dataset_type\":\"dropValues\",\"type\":\"Mlflow\",\"dropValues\":[\"tabular\",\"text\",\"image\"],\"task\":\"dropValues1\",\"dropValues1\":\"\"},\"classname\":\"ExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Extractor\",\"attributes\":{\"dataset\":\"\",\"dataset_type\":\"tabular\",\"task\":\"single_label_classification\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"mlflowdataset\":\"api\",\"OutputColumn\":\"datasetdropdown\",\"type\":\"MlflowAutoml\",\"api\":\"http://victlptst-19:8000/training/automl_training/\",\"payload\":{\"userId\":\"admin@infosys.com\",\"platform\":\"mlflow\",\"domain\":\"\",\"experiment_name\":\"\",\"label\":\"\",\"model_name\":\"\",\"time_limit\":\"\",\"train_dataset\":\"\",\"val_dataset\":\"\"},\"domain\":\"domainDropValues\",\"domainDropValues\":[\"image\",\"tabular\",\"text\"],\"experiment_name\":\"textarea\",\"model_name\":\"textarea\",\"time_limit\":\"textarea\",\"dataset\":\"\"},\"classname\":\"AutoMLConfig\",\"name\":\"Auto ML\",\"alias\":\"Auto ML\",\"attributes\":{\"domain\":\"\",\"OutputColumn\":\"\",\"experiment_name\":\"\",\"model_name\":\"\",\"time_limit\":\"60\"},\"id\":0,\"category\":\"AutoMLConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"mlflowdataset\":\"api\",\"api\":\"http://victlptst-19:8000/endpoints/create_endpoint/\",\"type\":\"MlflowEndpoint\",\"payload\":{\"userId\":\"admin@infosys.com\",\"platform\":\"mlflow\",\"name\":\"\",\"model_name\":\"\",\"version\":\"\"},\"model_name\":\"textarea\",\"version\":\"textarea\",\"endpoint_name\":\"textarea\"},\"classname\":\"CreateEndpoint\",\"name\":\"Endpoint\",\"alias\":\"Endpoint\",\"attributes\":{\"endpoint_name\":\"\",\"model_name\":\"\",\"version\":\"\"},\"id\":0,\"category\":\"CreateEndpoint\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"mlflowdataset\":\"api\",\"api\":\"http://victlptst-19:8000/endpoints/batchpredict/\",\"type\":\"BatchPrediction\",\"payload\":{\"userId\":\"admin@infosys.com\",\"platform\":\"mlflow\",\"endpoint_name\":\"\"},\"endpoint_name\":\"textarea\",\"dataset\":\"dropdown\"},\"classname\":\"BatchConfig\",\"name\":\"Batch Prediction\",\"alias\":\"Batch Prediction\",\"attributes\":{\"endpoint_name\":\"\",\"dataset\":\"\"},\"id\":0,\"category\":\"BatchConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]}]","Mlflow","{\"commands\":[\"\"],\"environment\":{}}","Demo"
"AWSPlugin","[{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"DatasetExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Dataset\",\"attributes\":{\"dataset\":\"\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"REST\":{\"script\":\"def DatasetExtractor_<id>():\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.NoProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n    dataset = response\\n    return dataset\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"],\"requirements\":[]},\"AWS\":{\"script\":\"\\ndef DatasetExtractor_<id>():\\n    s3_client = boto3.client(\'s3\')\\n    url = \'<dataset.attributes.Url>\'\\n    o = urlparse(url, allow_fragments=False)\\n    bucket = o.netloc.split(\'.\')[0]\\n    key =o.path.lstrip(\'/\')\\n    result = s3_client.list_objects(Bucket = bucket, Prefix=key)\\n    extension = key.split(\'.\')[-1]\\n    for o in result.get(\'Contents\'):\\n        data = s3_client.get_object(Bucket=bucket, Key=o.get(\'Key\'))\\n        contents = data[\'Body\'].read()\\n        if extension == \'csv\':\\n            contents=str(contents,\'utf-8\')\\n            contents = StringIO(contents)\\n            dataset = pd.read_csv(contents)\\n        elif extension == \'json\':\\n            dataset = json.loads(contents.decode(\'utf-8\'))\\n        else:\\n            dataset = contents.decode(\'utf-8\')\\n    return dataset\\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"import pandas as pd\",\"import json\",\"import boto3\"],\"requirements\":[\"boto3\",\"pandas\"]},\"MYSQL\":{\"script\":\"\\ndef DatasetExtractor_<id>():\\n    def getConnection(self):\\n        username = \'<dataset.datasource.connectionDetails.userName>\'\\n        password = Security.decrypt(\'<dataset.datasource.connectionDetails.password>\',\'<dataset.datasource.salt>\')\\n        url = \'<dataset.datasource.connectionDetails.url>\'\\n        host = urlparse(url[5:]).hostname\\n        port =urlparse(url[5:]).port\\n        database = urlparse(url[5:]).path.rsplit(\'/\', 1)[1]\\n        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port = port)\\n        return connection\\n\\n    connection = self.getConnection()\\n    query = self.mapQueryParams()\\n    cursor = connection.cursor(dictionary=True)\\n    cursor.execute(query)\\n    results = cursor.fetchall()\\n    return results\\n\",\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\"],\"requirements\":[\"mysql-connector-python\"]},\"H2\":{\"script\":\"def DatasetExtractor_<id>():\\n    def getConnection(self):\\n        username = \'<dataset.datasource.connectionDetails.userName>\'\\n        password = Security.decrypt(\'<dataset.datasource.connectionDetails.password>\',\'<dataset.datasource.salt>\')\\n        url = \'<dataset.datasource.connectionDetails.url>\'\\n        drivername = \'org.h2.Driver\'\\n        driverpath = os.path.join(os.environ[\'SPARK_HOME\'], \'jars/h2-1.4.200.jar\')\\n        connection = jaydebeapi.connect(drivername, url, [username, password], driverpath)\\n        return connection\\n    \\n    connection = self.getConnection()\\n    query = self.mapQueryParams()\\n    cursor = connection.cursor()\\n    cursor.execute(query)\\n    results = cursor.fetchall()\\n    return results\",\"imports\":[\"from leap.core.iExtractor import Extractor\",\"from leap.utils.Utilities import Utilities\",\"import ast\",\"import logging as logger\",\"from leap.utils import vault\",\"import os\",\"import jaydebeapi\"],\"requirements\":[\"jaydebeapi\"]},\"MSSQL\":{\"script\":\"def DatasetExtractor_<id>():\\n\\n    def getConnection(self):\\n\\n        username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n\\n        password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n\\n        url = \\\"<dataset.datasource.connectionDetails.url>\\\"\\n\\n        temp1 = self.url.split(\\\"//\\\")\\n\\n        temp2 = temp1[1].split(\\\";\\\")\\n\\n        server = temp2[0]\\n\\n        database = (temp2[1].split(\\\"=\\\"))[1]\\n\\n        isTrusted = \\\"no\\\"\\n\\n        if username == \\\"\\\":\\n\\n        isTrusted = \\\"yes\\\"\\n\\n        regex = \\\\\\\"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\\"\\n\\n\\n        if(re.search(regex, server.split(\\\":\\\")[0])):\\n\\n            server=server.replace(\\\":\\\",\\\",\\\")\\n\\n\\n        connectionString = \\\\\\\"DRIVER={0};SERVER={1}; \\\\\\\"\\n\\n                           \\\\\\\"DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\\".format(\\n\\n            \\\"ODBC Driver 17 for SQL SERVER\\\", server, database, username, password, isTrusted)\\n\\n        connection = pyodbc.connect(connectionString)\\n\\n        return connection\\n\\n    \\n\\n    connection = self.getConnection()\\n\\n    query = self.mapQueryParams()\\n\\n    cursor = connection.cursor()\\n\\n    cursor = cursor.execute(query)\\n\\n    columns = [column[0] for column in cursor.description]\\n\\n    results =[]\\n\\n    for row in cursor.fetchall():\\n\\n        results.append(dict(zip(columns, row)))\\n\\n    return results\",\"imports\":[\"from leap.core.iExtractor import Extractor\",\"from leap.utils.Utilities import Utilities\",\"import ast\",\"import logging as logger\",\"from leap.utils.configVariables import *\",\"import mysql.connector\",\"from urllib.parse import urlparse\",\"from leap.utils import vault\",\"from leap.core.iExtractor import Extractor\",\"from leap.utils.Utilities import Utilities\",\"import ast\",\"import logging as logger\",\"from urllib.parse import urlparse\",\"from leap.utils import vault\",\"import mysql.connector\"],\"requirements\":[\"mssql-connector-python\",\"pyodbc\"]},\"servicenow\":{\"script\":\"def DatasetExtractor_<id>():\\n    url = \\\"<dataset.attributes.url>\\\"\\n    authParams = \\\"<dataset.attributes.authParams>\\\"\\n    auth = \\\"<dataset.datasource.connectionDetails.auth>\\\"\\n    authUrl = \\\"<dataset.datasource.connectionDetails.authUrl>\\\"\\n    authToken = \\\"<dataset.datasource.connectionDetails.authToken>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    query = \\\"<dataset.attributes.Query>\\\"\\n    user = \\\"<dataset.attributes.UserName>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    vaultkey = \\\"<dataset.attributes.VaultKey>\\\"\\n    if vaultkey != \\\"\\\":\\n            password = \\\"<dataset.attributes.Vault.Password>\\\"\\n    else:\\n            password = Utilities.decrypt(\\\"<dataset.datasource.connectionDetails.Password>\\\",\\n                                              \\\"<dataset.datasource.connectionDetails.salt>\\\")\\n    documentElement = \\\"<dataset.attributes.DocumentElement>\\\"\\n    type= \\\"<dataset.datasource.connectionDetails.Type>\\\"\\n    script= \\\"<dataset.datasource.connectionDetails.Script>\\\"\\n    \\n    \\n    logging.info(\\\"Connecting to URL {0}\\\".format(url))\\n    PROXIES = {}\\n    hostname = urlparse(url).hostname\\n    if authUrl != \'\':\\n        authParams = json.loads(self.authParams)\\n        params = []\\n        for key in authParams:\\n            params.append(\\\"{0}={1}\\\".format(key, authParams[key]))\\n        params.append(\\\"{0}={1}\\\".format(\'q\', query))\\n        authResponse = requests.get(url=authUrl, params=params, timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n        authToken = authResponse.json()\\n    if auth.lower() == \'bearertoken\':\\n        HEADERS[\'Authorization\'] = \\\"Bearer \\\" + authToken\\n    elif auth.lower() == \'basicauth\':\\n        auth = HTTPBasicAuth(user,password)\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    if type.lower() == \\\"get\\\":\\n\\n            response = requests.get( url=url, headers=HEADERS, proxies=PROXIES, auth=auth,\\n                                        verify=False, params = PARAMS,\\n                                        timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    elif type.lower() == \\\"post\\\":\\n            #values = {\'script\' : self.script}\\n            response = requests.post(url=url, headers=HEADERS, params=params, auth = auth,\\n                                        proxies=PROXIES, verify=False, data=script,\\n                                        timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    elif type.lower() == \\\"put\\\":\\n            response = requests.put(url=url, headers=HEADERS, params=PARAMS, auth = auth, proxies=PROXIES, verify=False,\\n                                    timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    elif type.lower() == \\\"delete\\\":\\n            response = requests.delete(url=surl, headers=HEADERS, proxies=PROXIES, auth=auth, verify=False, params=PARAMS,\\n                                        timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n    else:\\n            response = requests.get(url=url, headers=HEADERS, proxies=PROXIES, auth=auth,\\n                                    verify=False, params=PARAMS,\\n                                    timeout=(configVariables.CONNECT_TIMEOUT, configVariables.READ_TIMEOUT))\\n\\n    logger.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n    \\n    response= dataset\\n\\n    return dataset\\n\\n    \\n    \\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\",\"from leap.core.iExtractor import Extractor\",\"from leap.utils import configVariables\",\"from leap.utils import vault\",\"from leap.utils.Utilities import Utilities\",\"import logging as logger\"],\"requirments\":[\"requests\"]}}},{\"requiredJars\":[],\"formats\":{\"Dataset\":\"restdataset\",\"restdatasetname\":\"Fetchdatasets\",\"payload\":{\"userId\":\"admin@infosys.com\",\"platform\":\"amazon\"},\"response\":\"displayName\",\"TargetAttributeName\":\"textarea\"},\"attributes\":{\"Dataset\":[],\"TargetAttributeName\":\"\"},\"classname\":\"Select Dataset\",\"category\":\"Dataset\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Extractor\",\"id\":0,\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\'\'\'\\ndata_source{args_namespace.index} = {{\\n        \'DataSource\': {{\\n            \'S3DataSource\': {{\\n                \'S3DataType\': \'S3Prefix\',\\n                \'S3Uri\': \'{args_namespace.s3_uri}\'\\n            }}\\n        }},\\n        \'TargetAttributeName\': \'{args_namespace.target_attribute_name}\',    \\n}}\\n\'\'\'\"}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"InputDataConfig\",\"category\":\"Dataset\",\"name\":\"Dataset Joiner\",\"alias\":\"Dataset Joiner\",\"id\":0,\"inputEndpoints\":[\"in\",\"in\",\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\'\'\'\\ninput_data_config = [{\', \'.join(args_namespace.data_sources) if args_namespace.data_sources else \'\'}]\\n\\n\'\'\'\"}},{\"requiredJars\":[],\"formats\":{\"MaxCandidates\":\"textarea\"},\"attributes\":{\"MaxCandidates\":\"\"},\"classname\":\"AutoMLJobConfig\",\"category\":\"AutoML Model\",\"name\":\"Model Config\",\"alias\":\"Model Config\",\"id\":0,\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\'\'\'\\nauto_ml_job_config = {{\\n    \'CompletionCriteria\': {{\'MaxCandidates\': {args_namespace.max_candidates if args_namespace.max_candidates else 0}}}\\n}}        \\n\\n\'\'\'\"}},{\"requiredJars\":[],\"formats\":{\"S3OutputPath\":\"textarea\"},\"attributes\":{\"S3OutputPath\":\"\"},\"classname\":\"OutputDataConfig\",\"category\":\"AutoML Model\",\"name\":\"Model Sink\",\"alias\":\"Model Sink\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\'\'\'\\noutput_data_config = {{\\n    \'S3OutputPath\': \'{args_namespace.S3_output_path}\'\\n}}\\n        \\n\'\'\'\"}},{\"requiredJars\":[],\"formats\":{\"AutoMLJobName\":\"textarea\",\"RoleArn\":\"textarea\"},\"attributes\":{\"AutoMLJobName\":\"\",\"RoleArn\":\"\"},\"classname\":\"AutoMLTraining\",\"category\":\"AutoML Model\",\"name\":\"Model Trainer\",\"alias\":\"Model Trainer\",\"id\":0,\"inputEndpoints\":[\"in\",\"in\",\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"f\'\'\'\\nsagemaker_client = boto3.Session().client(service_name=\'sagemaker\', region_name=credentials[\'region\'], aws_access_key_id=credentials[\'aws_access_key_id\'], aws_secret_access_key=credentials[\'aws_secret_access_key\'])\\n\\nsagemaker_job = sagemaker_client.create_auto_ml_job(\\n    AutoMLJobName=\'{args_namespace.auto_ml_job_name}\',\\n    RoleArn=\'{args_namespace.role_arn}\',\\n    {   \\n        \', \'.join([f\'{key}={value}\' for key, value in args_namespace.auto_ml_training_parameters.items()]) if args_namespace.auto_ml_training_parameters else \'\'\\n    }\\n)\\n\\nprint(sagemaker_job)\\n\\n\'\'\'\"}},{\"requiredJars\":[],\"formats\":{\"EndpointName\":\"textarea\"},\"attributes\":{\"EndpointName\":\"\"},\"classname\":\"ModelDeployConfig\",\"category\":\"AutoML Model\",\"name\":\"Model Deploy\",\"alias\":\"Model Deploy\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"codeGeneration\":{\"script\":\"f\'\'\'\\nmodel_deploy_config = {{\\n    \'EndpointName\': \'{args_namespace.endpoint_name}\'\\n}}        \\n\\n\'\'\'\"}},{\"requiredJars\":[],\"formats\":{\"model\":\"dropdown\"},\"attributes\":{\"model\":\"\"},\"classname\":\"SelectModel\",\"category\":\"AutoML Model\",\"name\":\"Model Select\",\"alias\":\"Model Select\",\"id\":0,\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\"}},{\"requiredJars\":[],\"formats\":{\"_\":\"textarea\"},\"attributes\":{\"_\":\"\"},\"classname\":\"BatchPrediction\",\"category\":\"Prediction\",\"name\":\"Batch Prediction\",\"alias\":\"Batch Prediction\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"codeGeneration\":{\"script\":\"\"}},{\"requiredJars\":[],\"formats\":{\"query\":\"textarea\",\"min_length\":\"textarea\",\"threshold\":\"textarea\",\"model\":\"dropValues\",\"dropValues\":[\"multi-qa-MiniLM-L6-dot-v1\",\"multi-qa-MiniLM-L6-cos-v1\",\"all-MiniLM-L6-v2\",\"nq-distilbert-base-v1\"]},\"classname\":\"Query-based Summarizer\",\"name\":\"Query-based Summarizer\",\"alias\":\"Query-based Summarizer\",\"attributes\":{\"query\":\"soft landing\",\"model\":\"multi-qa-MiniLM-L6-dot-v1\",\"min_length\":\"30\",\"threshold\":\"0.70\"},\"codeGeneration\":{\"script\":\"fr\'\'\'\\nclass QueryBasedSummarizer:\\n    def __init__(\\n        self, model: str, min_len: int = 30, threshold_score: float = 0.70\\n    ) -> None:\\n        self.model = SentenceTransformer(model)\\n        self.min_passage_len = min_len\\n        self.threshold_score = threshold_score\\n\\n    def __score_passages(self, query: str, passages: list) -> dict:\\n        query_embedding = self.model.encode(query)\\n        passage_embedding = self.__get_passage_embeddings(passages)\\n        scores = util.pytorch_cos_sim(query_embedding, passage_embedding).tolist()[0]  # type: ignore\\n        return {{index: score for index, score in enumerate(scores)}}\\n\\n    def __get_passage_embeddings(self, passages: list) -> numpy.ndarray:\\n        passage_embedding = self.model.encode(passages)\\n        return passage_embedding  # type: ignore\\n\\n    def __extract_sentences_from_text(self, text: str) -> list:\\n        sections = text.replace(\'\\\\n\', \' \').replace(\'\\\\r\', \' \').split(\'.\')\\n        passages = [\\n            [\'\', f\'{{self.__remove_unicode(section.strip())}}.\']\\n            for section in sections\\n            if len(self.__remove_unicode(section.strip())) > self.min_passage_len\\n        ]\\n        return passages\\n\\n    def __remove_unicode(self, text: str) -> str:\\n        string_encode = text.encode(\'ascii\', \'ignore\')\\n        return string_encode.decode()\\n\\n    def __call__(self, query: str, text: str) -> str:\\n        passages = self.__extract_sentences_from_text(text)\\n        scores = self.__score_passages(query, passages)\\n        summary = \' \'.join(\\n            [\\n                passages[w][1]\\n                for w in sorted(scores, key=scores.get, reverse=True)  # type: ignore\\n                if scores[w] > self.threshold_score\\n            ]\\n        )\\n        return summary\\n\'\'\'\",\"imports\":\"f\'\'\'\\nimport numpy\\nfrom sentence_transformers import SentenceTransformer, util\\n\'\'\'\",\"function_call\":\"f\'\'\'\\n    text = QueryBasedSummarizer(\\n                model=\'{args_namespace.model}\', \\n                min_len={args_namespace.min_length}, \\n                threshold_score={args_namespace.threshold})(query=\'{args_namespace.query}\', text=text)\\n\'\'\'\\n\",\"pipeline_function\":\"f\'\'\'\\ndef text_pipeline_{args_namespace.index}(text: str, *args: Any, **kwargs: Any) -> str:\\n\\n    text = QueryBasedSummarizer(\\n                model=\'{args_namespace.model}\', \\n                min_len={args_namespace.min_length}, \\n                threshold_score={args_namespace.threshold})(query=\'{args_namespace.query}\', text=text)\\n    \\n    {args_namespace.function_calls}\\n    \\n    return text\\n\'\'\'\\n\"},\"id\":0,\"category\":\"Summarizers\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out\",\"out\"]},{\"requiredJars\":[],\"formats\":{\"min_length\":\"textarea\",\"max_length\":\"textarea\"},\"classname\":\"Longformer Summarizers\",\"name\":\"Longformer\",\"alias\":\"Longformer\",\"attributes\":{\"min_length\":\"10\",\"max_length\":\"50\"},\"codeGeneration\":{\"script\":\"f\'\'\'\\ndef get_longformer_summary(text: str, min_length: int = 10, max_length: int = 50):\\n    summarizer = pipeline(task=\'summarization\', model=\'./distilbart-cnn-12-6/\')\\n    ARTICLE = text\\n    max_chunk = 500\\n    ARTICLE = ARTICLE.replace(\'.\', \'.<eos>\')\\n    ARTICLE = ARTICLE.replace(\'?\', \'?<eos>\')\\n    ARTICLE = ARTICLE.replace(\'!\', \'!<eos>\')\\n    sentences = ARTICLE.split(\'<eos>\')\\n    current_chunk = 0\\n    chunks = []\\n    for sentence in sentences:\\n        if len(chunks) == current_chunk + 1:\\n            if len(chunks[current_chunk]) + len(sentence.split(\' \')) <= max_chunk:\\n                chunks[current_chunk].extend(sentence.split(\' \'))\\n            else:\\n                current_chunk += 1\\n                chunks.append(sentence.split(\' \'))\\n        else:\\n            chunks.append(sentence.split(\' \'))\\n\\n    for chunk_id in range(len(chunks)):\\n        chunks[chunk_id] = \' \'.join(chunks[chunk_id])\\n\\n    res = summarizer(\\n        chunks, max_length=max_length, min_length=min_length, do_sample=False\\n    )\\n    summarized = \' \'.join([summ[\'summary_text\'] for summ in res])  # type: ignore\\n\\n    if summarized[-1] != \'.\':\\n        summary = summarized.rsplit(\'.\', 1)[0]\\n        summary1 = summary + \'.\'\\n        return summary1\\n    else:\\n        return summarized\\n\'\'\'\",\"imports\":\"f\'\'\'\\nfrom transformers import pipeline\\n\'\'\'\",\"function_call\":\"f\'\'\'\\n    text = get_longformer_summary(text=text, min_length={args_namespace.min_length}, max_length={args_namespace.max_length})\\n\'\'\'\",\"pipeline_function\":\"f\'\'\'\\ndef text_pipeline_{args_namespace.index}(text: str, *args: Any, **kwargs: Any) -> str:\\n    text = get_longformer_summary(text=text, min_length={args_namespace.min_length}, max_length={args_namespace.max_length})\\n    \\n    {args_namespace.function_calls}\\n    \\n    return text\\n\'\'\'\"},\"id\":0,\"category\":\"Summarizers\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out\",\"out\"]},{\"requiredJars\":[],\"formats\":{\"max_length\":\"textarea\"},\"classname\":\"Bert Summarizers\",\"name\":\"Bert\",\"alias\":\"Bert\",\"attributes\":{\"max_length\":\"50\"},\"codeGeneration\":{\"script\":\"f\'\'\'\\ndef get_bert_summary(text: str, max_len: int = 50) -> str:\\n    bert_model = Summarizer()\\n    summary = bert_model(text, num_sentences=max_len)\\n    return summary\\n\'\'\'\",\"imports\":\"f\'\'\'\\nfrom summarizer import Summarizer\\n\'\'\'\",\"function_call\":\"f\'\'\'\\n    text = get_bert_summary(text=text, max_len={args_namespace.max_length})\\n\'\'\'\",\"pipeline_function\":\"f\'\'\'\\ndef text_pipeline_{args_namespace.index}(text: str, *args: Any, **kwargs: Any) -> str:\\n    text = get_bert_summary(text=text, max_len={args_namespace.max_length})\\n    \\n    {args_namespace.function_calls}\\n    \\n    return text\\n\'\'\'\"},\"id\":0,\"category\":\"Summarizers\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out\",\"out\"]},{\"requiredJars\":[],\"formats\":{\"text\":\"textarea\"},\"classname\":\"Text Extractors\",\"name\":\"Text Extractor\",\"alias\":\"Text Extractor\",\"attributes\":{\"text\":\"Soft landing moment for hearing more about a soft landing than I am about.The team are basically saying we can achieve this soft landing.We all hope for a soft landing.And the data appear to be increasingly supportive of a soft landing.Andrew from your vantage point do you think that I hate saying this because I\'m going to get pilloried, but a soft landing looks more likely because of how much prices have rolled over.So Mona, are you all in on soft landing? Are you loading the vote on? You know, I think soft landing right now, I think will still depend on this is a still unprecedented Fed cycle here.So the narrative at the moment is soft landing increasingly likely globally, partly because of what\'s going on in Europe.Not about me moving on the point is really though about a soft landing, it\'s not just him.Fed communication, waiting for that soft landing some of the data recently spoke to that this data point did not know and that\'s the reason why I find it fascinating.If you if you look through the underlying inflationary pressure and it kind of goes back to what you\'re talking about, can we get a soft landing? We have a really tight labor market, we have labor costs that are rising, that\'s gonna show through the higher prices Andrew.So, so it\'s not a comfortable environment so that the dollar can weaken and we can sit here with people talking about soft landings negative for the dollar and rethink and take positions off says we\'re willing to look through something which I don\'t think you can analyze even as an amateur as anything other than a ball increasing event.The problem is every time you get a sniff, just a sniff of some kind of soft landing financial conditions start to loosen again, They don\'t tighten.And how much is that really the story behind the dollar? How much is that? The story behind some of the risk appetite people seeing maybe not directly deceleration people talking more and more about some the soft landing.We saw in the UK already we have over 5% wage growth but we\'re going to have double-digit inflation, so we\'re not going to be spending spending like crazy in bars and restaurants for the next nine months but we protect the economies, we increase the chance of a soft landing and that at a global level means we still have a difficult problem because we\'re struggling to get out, Put back up fast enough to cope with what we\'ve already done lisa mentioned the fund manager survey from Bank of America earlier this morning.\"},\"codeGeneration\":{\"script\":\"f\\\"\\\"\\\"\\ndef get_text() -> str:\\n    text = \'\'\'{args_namespace.text}\'\'\'\\n    return text\\n\\\"\\\"\\\"\",\"imports\":\"f\'\'\'\\nimport types\\nfrom typing import Any, Callable\\n\'\'\'\",\"decorator\":\"f\'\'\'\\ndef text_summarizer(\\n    summarizer_pipeline: Callable[[str, Any, Any], str]\\n) -> Callable[[Any, Any], str]:\\n    text = get_text()\\n\\n    def summarize(*args: Any, **kwargs: Any) -> str:\\n        nonlocal text\\n        summary = summarizer_pipeline(text, *args, **kwargs)\\n        return summary\\n\\n    return summarize\\n\'\'\'\",\"decorator_call\":\"f\'\'\'\\n@text_summarizer\'\'\'\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\",\"out\",\"out\",\"out\"]},{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"DatasetExtractorConfig\",\"name\":\"Dataset Extractor\",\"alias\":\"Dataset Dataset\",\"attributes\":{\"dataset\":\"\"},\"id\":0,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"REST\":{\"script\":\"def DatasetExtractor_<id>():\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.NoProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=requestBody,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n    dataset = response\\n    return dataset\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"],\"requirements\":[]},\"AWS\":{\"script\":\"\\ndef DatasetExtractor_<id>():\\n    s3_client = boto3.client(\'s3\')\\n    url = \'<dataset.attributes.Url>\'\\n    o = urlparse(url, allow_fragments=False)\\n    bucket = o.netloc.split(\'.\')[0]\\n    key =o.path.lstrip(\'/\')\\n    result = s3_client.list_objects(Bucket = bucket, Prefix=key)\\n    extension = key.split(\'.\')[-1]\\n    for o in result.get(\'Contents\'):\\n        data = s3_client.get_object(Bucket=bucket, Key=o.get(\'Key\'))\\n        contents = data[\'Body\'].read()\\n        if extension == \'csv\':\\n            contents=str(contents,\'utf-8\')\\n            contents = StringIO(contents)\\n            dataset = pd.read_csv(contents)\\n        elif extension == \'json\':\\n            dataset = json.loads(contents.decode(\'utf-8\'))\\n        else:\\n            dataset = contents.decode(\'utf-8\')\\n    return dataset\\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"import pandas as pd\",\"import json\",\"import boto3\"],\"requirements\":[\"boto3\"]}}},{\"requiredJars\":[],\"formats\":{\"TextCol\":\"text\",\"OutputCol\":\"text\",\"StopWords\":\"text\"},\"classname\":\"TextCleaner\",\"name\":\"Text Cleaner\",\"alias\":\"Text Cleaner\",\"attributes\":{\"TextCol\":\"\",\"OutputCol\":\"\",\"StopWords\":\"\"},\"id\":0,\"category\":\"Keyword Extraction\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef TextCleaner_<id>(dataset):\\n    nltk.download(\'stopwords\')\\n    nltk.download(\'punkt\')\\n    stop_words = set(stopwords.words(\\\"english\\\"))\\n    customStopwords = set(\\\"<StopWords>\\\".split(\\\",\\\"))\\n    stop_words= stop_words.union(customStopwords)\\n    results = []\\n    def removeStopWords(text):\\n        return \\\" \\\".join([w for w in text.lower().split() if w not in stop_words ])\\n    def alphaNum(text):\\n        alphanumeric = \\\"\\\"\\n        for character in text:\\n            if character.isalnum():\\n                alphanumeric += character\\n            else:\\n                alphanumeric += \\\" \\\"\\n        alphanumeric = re.sub(\\\" +\\\", \\\" \\\", alphanumeric)\\n        finalTokens = [t for t in alphanumeric.split(\\\" \\\") if not t.isnumeric()]\\n        return \\\" \\\".join(finalTokens)\\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            rmAlphaNum = alphaNum(row[\\\"<TextCol>\\\"])\\n            row[\\\"<OutputCol>\\\"] = removeStopWords(rmAlphaNum)\\n            results.append(row)\\n    elif isinstance(dataset,dict):\\n        rmAlphaNum = alphaNum(dataset[\\\"<TextCol>\\\"])\\n        dataset[\\\"<OutputCol>\\\"] = removeStopWords(rmAlphaNum)\\n    else:\\n        rmAlphaNum = alphaNum(dataset)\\n        dataset = removeStopWords(rmAlphaNum)\\n    \\n    return dataset\",\"imports\":[\"from nltk.corpus import stopwords\",\"import nltk\"]}},{\"requiredJars\":[],\"formats\":{\"CleanTextCol\":\"text\",\"OriginalTextCol\":\"text\",\"OutputColumn\":\"text\",\"N\":\"text\",\"M\":\"text\",\"Count\":\"text\",\"BlacklistWords\":\"text\"},\"classname\":\"NGram\",\"name\":\"NGram\",\"alias\":\"NGram\",\"attributes\":{\"OriginalTextCol\":\"\",\"CleanTextCol\":\"\",\"OutputColumn\":\"Keywords\",\"N\":\"\",\"M\":\"\",\"Count\":\"\",\"BlacklistWords\":\"\"},\"id\":0,\"category\":\"Keyword Extraction\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef NGram_<id>(dataset):\\n    txt1 =\\\"\\\"\\n    txt2= \\\"\\\"\\n    if isinstance(dataset,dict):\\n        txt1 = dataset[\\\"<CleanTextCol>\\\"]\\n        txt2 = dataset[\\\"<OriginalTextCol>\\\"]\\n    elif isinstance(dataset,list):\\n        for row in dataset:\\n            txt1 =txt1 + \\\" \\\"+ row[\\\"<CleanTextCol>\\\"]\\n            txt2 =txt2 + \\\" \\\" + row[\\\"<OriginalTextCol>\\\"]\\n    logging.info(\\\"Generating NGrams\\\")\\n    # Getting biigrams \\n    customblackwords = list(set(\\\"<BlacklistWords>\\\".split(\\\",\\\")))\\n    customblackwords = [w.lower() for w in customblackwords]\\n    vectorizer = CountVectorizer(ngram_range = (<N>,<M>))\\n    X1 = vectorizer.fit_transform([txt1]) \\n    features = (vectorizer.get_feature_names())\\n    # Applying TFIDF\\n    vectorizer = TfidfVectorizer(ngram_range = (<N>,<M>))\\n    X2 = vectorizer.fit_transform([txt1])\\n    scores = (X2.toarray())\\n    # Getting top ranking features\\n    logging.info(\\\"Getting Top 50 Grams\\\")\\n    sums = X2.sum(axis = 0)\\n    data1 = []\\n    for col, term in enumerate(features):\\n        data1.append( (term, sums[0,col] ))\\n    ranking = pd.DataFrame(data1, columns = [\\\"term\\\",\\\"rank\\\"])\\n    words = (ranking.sort_values(\\\"rank\\\", ascending = False))\\n   \\n    top50df = words.nlargest(words.shape[0],\\\"rank\\\")\\n    results = []\\n    for index, row in top50df[[\\\"term\\\", \\\"rank\\\"]].iterrows():\\n        test_sub = \\\"(.{0,1}?)\\\".join(row[\\\"term\\\"].split(\\\" \\\")) + \\\"[^A-Za-z0-9]{1}\\\"\\n        res = [(i.start(),i.end()) for i in re.finditer(test_sub, txt2.lower())]\\n        if len(res)>0 and row[\\\"term\\\"].lower() not in customblackwords:\\n            results.append({\\n            \\\"keyword\\\": row[\\\"term\\\"],\\n            \\\"score\\\": row[\\\"rank\\\"],\\n            \\\"indices\\\":str(res),\\n            \\\"frequency\\\": len(res)\\n            })\\n    dataset[\\\"<OutputColumn>\\\"] = results[:50]\\n    return dataset\",\"imports\":[\"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\",\"import pandas as pd\",\"import numpy as np\",\"import re\",\"from datetime import datetime\"],\"requirements\":[\"scikit-learn\",\"pandas\",\"regex\",\"numpy\"]}},{\"requiredJars\":[],\"formats\":{\"URL\":\"textarea\"},\"classname\":\"Dataset Loader\",\"name\":\"URL Loader\",\"alias\":\"URL Loader\",\"attributes\":{\"URL\":\"\"},\"codeGeneration\":{\"script\":\"f\'\'\'\\ndef url_loader(summary: str) -> None:\\n    print(summary)\\n\\n\\nif __name__ == \'__main__\':\\n\\n    execution_functions = [\\n        f\\n        for f in globals().values()\\n        if type(f) == types.FunctionType and f.__name__ == \'summarize\'\\n    ]\\n\\n    summary_dict = {{}}\\n    for i, summary in enumerate([func() for func in execution_functions]):\\n        summary_dict[f\'pipeline_{{i}}\'] = summary\\n\'\'\'\",\"imports\":\"f\'\'\'\\nimport types\\n\'\'\'\",\"function_call\":\"f\'\'\'\\n    url_loader(summary=text)\\n\'\'\'\"},\"id\":0,\"category\":\"LoaderConfig\",\"inputEndpoints\":[\"in\",\"in\",\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"bucket\":\"text\",\"key\":\"text\"},\"classname\":\"DatasetLoaderConfig\",\"name\":\"S3 Loader\",\"alias\":\"S3 Loader\",\"attributes\":{\"bucket\":\"\",\"key\":\"\"},\"id\":0,\"category\":\"LoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef S3Loader_<id>(dataset):\\n    s3 = boto3.resource(service_name=\\\"s3\\\",)\\n    bucket = \\\"<bucket>\\\"\\n    key = \\\"<key>\\\"\\n    filename = key.split(\'/\')[-1]\\n    extension = filename.split(\'.\')[-1]\\n\\n    print(\\\"Saving data\\\")\\n    if extension == \'csv\':\\n        df = pd.DataFrame(dataset)\\n        csv_buffer = StringIO()\\n        df.to_csv(csv_buffer)\\n        s3.Object(bucket, key).put(Body=csv_buffer.getvalue())\\n    elif extension == \'pkl\':\\n        pickle_byte_obj = pickle.dumps(dataset)\\n        s3.Object(bucket,key).put(Body=pickle_byte_obj)\\n    elif extension == \'json\':\\n        s3.Object(bucket, key).put(Body=json.dumps(dataset))\\n    else:\\n        s3.Object(bucket, key).put(Body=dataset)\\n\\n\",\"imports\":[\"import pandas as pd\",\"import json\",\"import boto3\",\"from io import StringIO\",\"import pickle\"],\"requirements\":[\"boto3\"]}},{\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"classname\":\"DatasetLoaderConfig\",\"name\":\"Dataset Loader\",\"alias\":\"Dataset Loader\",\"attributes\":{\"dataset\":\"\",\"applySchema\":false},\"id\":0,\"category\":\"LoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"MYSQL\":{\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n    \\n\\n    cnx = mysql.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"Drop table IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"`{0}` TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"`{0}`\\\".format(k) for k in paramsDict)\\n            duplicates = \\\", \\\".join(\\\"{0}=VALUES({0})\\\".format(k) for k in paramsDict)\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON DUPLICATE KEY UPDATE {1}\\\".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            logging.error(\\\"{0}:{1}\\\".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()\",\"imports\":[\"import mysql.connector as mysql\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"]},\"REST\":{\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n\",\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"]},\"AWS\":{\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    filename = url.split(\'/\')[-1]\\n    extension = filename.split(\'.\')[-1]\\n\\n    data_directory = \\\"/opt/ml/processing/output\\\"\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\\"Saving data\\\")\\n    if extension == \'.csv\':\\n        dataset.to_csv(file_path)\\n    else:\\n        with open(file_path, \'w\') as f:\\n            f.writelines(dataset)\\n\\n\",\"imports\":[\"import pandas as pd\",\"import os\"]}}},{\"requiredJars\":[],\"formats\":{\"requirements\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"PreProcessingScriptConfig\",\"name\":\"Pre Processing Script\",\"alias\":\"Pre Processing Script\",\"attributes\":{\"requirements\":\"\",\"script\":\"\\rdef PreProcessingScript_<id>( dataset):\\n    #pre-process Data\\r\\r    return dataset\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"requirements\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"PostProcessingScriptConfig\",\"name\":\"Post Processing Script\",\"alias\":\"Post Processing Script\",\"attributes\":{\"requirements\":\"\",\"script\":\"\\rdef PostProcessingScript_<id>( dataset):\\n    #Post-process Data\\r\\r    return dataset\"},\"id\":0,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\"],\"outputEndpoints\":[\"out1\",\"out2\"],\"codeGeneration\":{\"script\":\"\",\"imports\":[]}},{\"requiredJars\":[],\"formats\":{\"CleanTextCol\":\"text\",\"OriginalTextCol\":\"text\",\"N\":\"text\",\"Count\":\"text\"},\"classname\":\"Phrase\",\"name\":\"Phrase\",\"alias\":\"Phrase\",\"attributes\":{\"OriginalTextCol\":\"\",\"CleanTextCol\":\"\",\"Count\":\"\"},\"id\":0,\"category\":\"Keyword Extraction\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Phrase_<id>(dataset):\\n    nlp = spacy.load(\\\"en_core_web_sm\\\")\\n    nlp.add_pipe(\\\"textrank\\\")\\n    timenow = datetime.now()\\n    totalRecords = len(dataset)\\n    logging.info(\\\"Fetched {0} records\\\".format(totalRecords))\\n    count =0\\n    textPhraseMappings = []\\n    text =\\\"\\\"\\n    text1= \\\"\\\"\\n    for row in dataset:\\n        text =text + \\\" \\\"+ row[\\\"<CleanTextCol>\\\"]\\n        text1 =text1 + \\\" \\\" + row[\\\"<OriginalTextCol>\\\"]\\n    for row in dataset:\\n        doc = nlp(text)\\n        count=0\\n        if len(doc._.phrases)>0:\\n            for item in doc._.phrases:\\n                count = count+1\\n                if count > <Count>:\\n                    break\\n                withoutSpace = item.text.replace(\\\" \\\" ,\\\"\\\")\\n                if not withoutSpace.isdigit() and len(item.text.split(\\\" \\\")) >1:\\n                    phrase = item.text\\n                    test_sub = \\\"(.{0,5}?)\\\".join(phrase.split(\\\" \\\"))\\n                    res = [(i.start(),i.end()) for i in re.finditer(test_sub, text1.lower())]\\n                    if len(res)>0:\\n                        textPhraseMappings.append({\\\"phrase\\\":phrase,\\\"count\\\":len(res), \\\"rank\\\":item.rank, \\\"indices\\\":str(res)})\\n    return textPhraseMappings\\n\",\"imports\":[\"import spacy\",\"import pytextrank\",\"import re\",\"from datetime import datetime\"],\"requirements\":[\"pytextrank\",\"spacy\",\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1.tar.gz\"]}},{\"requiredJars\":[],\"formats\":{\"First word\":\"textarea\",\"Second word\":\"textarea\"},\"attributes\":{\"First word\":\"\",\"Second word\":\"\"},\"classname\":\"Text Correlation Input\",\"category\":\"Text Correlation\",\"name\":\"Input Data\",\"alias\":\"Input Data\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{\"Vocab size\":\"textarea\"},\"attributes\":{\"Vocab size\":\"\"},\"classname\":\"Text Correlation\",\"category\":\"Text Correlation\",\"name\":\"Infersent\",\"alias\":\"Infersent\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"]},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"LinearR\",\"category\":\"Regression\",\"name\":\"LinearR\",\"alias\":\"LinearR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef LinearR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputLR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',LinearRegression())]\\n    pipeLR=Pipeline(InputLR)\\n\\n    pipeLR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LR = pipeLR.predict(X_test)\\n    print(pipeLR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    \\n    return pipeLR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import LinearRegression\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"LassoR\",\"category\":\"Regression\",\"name\":\"LassoR\",\"alias\":\"LassoR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef LassoR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputLassoR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',Lasso(alpha=1.0))]\\n    pipeLassoR=Pipeline(InputLassoR)\\n\\n    pipeLassoR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeLassoR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    \\n    return pipeLassoR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import Lasso\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"RidgeR\",\"category\":\"Regression\",\"name\":\"RidgeR\",\"alias\":\"RidgeR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef RidgeR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputRidgeR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',Ridge(alpha=1.0))]\\n    pipeRidgeR=Pipeline(InputRidgeR)\\n\\n    pipeRidgeR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeRidgeR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    \\n    return pipeRidgeR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import Ridge\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"ElasticnetR\",\"category\":\"Regression\",\"name\":\"ElasticnetR\",\"alias\":\"ElasticnetR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef ElasticnetR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputEN=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',ElasticNet(alpha = 1))]\\n    pipeEN=Pipeline(InputEN)\\n\\n    pipeEN.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeEN.score(dataset[\'X_train\'],dataset[\'Y_train\']))  \\n    \\n    return pipeEN\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import ElasticNet\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"RandomForestR\",\"category\":\"Regression\",\"name\":\"RandomForestR\",\"alias\":\"RandomForestR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef RandomForestR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    Input=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',RandomForestRegressor(n_estimators=50))]\\n    pipe=Pipeline(Input)\\n\\n    pipe.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_RFF = pipe.predict(X_test)\\n    print(pipe.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n\\n    return pipe\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.ensemble import RandomForestRegressor\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"SVR\",\"category\":\"Regression\",\"name\":\"SVR\",\"alias\":\"SVR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef SVR_<id>(dataset):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputSVR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',SVR())]\\n    pipeSVR=Pipeline(InputSVR)\\n\\n    pipeSVR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeSVR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    print(((pipeSVR.predict(dataset[\'X_train\']) - dataset[\'Y_train\']) ** 2).mean())\\n    \\n    return pipeSVR\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.svm import SVR\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"bucket\":\"text\",\"key\":\"text\"},\"attributes\":{\"bucket\":\"\",\"key\":\"\"},\"classname\":\"InferenceR\",\"category\":\"Regression\",\"name\":\"InferenceR\",\"alias\":\"InferenceR\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef InferenceR_<id>(dataset):\\n    s3 = boto3.resource(\'s3\')\\n    bucket = \'<bucket>\'\\n    key = \'<key>\'\\n    # load the model\\n    load_model = pickle.loads(s3.Bucket(bucket).Object(key).get()[\'Body\'].read())\\n    \\n    # load_model = pickle.load(open(filename, \'rb\'))\\n    pipe_pred = load_model.predict(dataset[\'X_test\'])\\n    \\n    # OUT\\n    output=pd.DataFrame({\'Id\':dataset[\'dataset_id\'],\'Result\':pipe_pred})\\n    return output\\n\",\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"import pickle\"],\"requirements\":[\"scikit-learn\"]}},{\"requiredJars\":[],\"formats\":{\"KeywordsColumn\":\"textarea\",\"FinbertKeywordsColumn\":\"textarea\"},\"attributes\":{\"KeywordsColumn\":\"\",\"FinbertKeywordsColumn\":\"finbertKeywords\"},\"classname\":\"finbert\",\"category\":\"Text Correlation\",\"name\":\"Finbert\",\"alias\":\"Finbert\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Finbert_<id>(dataset):\\n    def finbert_emb(finbert, s1, s2 = \'finance\'):\\n        # encode sentences to get their embeddings\\n        embedding1 = model.encode(s1, convert_to_tensor=True)\\n        embedding2 = model.encode(s2, convert_to_tensor=True)\\n\\n        # compute similarity scores of two embeddings\\n        cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\\n        return cosine_scores.item()\\n    keywordList =[]\\n    if isinstance(dataset,list):\\n        keywordList=dataset\\n    elif isinstance(dataset,dict):\\n        keywordList= dataset[\'<KeywordsColumn>\']\\n    \\n\\n    model = SentenceTransformer(\'ProsusAI/finbert\')\\n    finbert_keywords_list = []\\n\\n    for s1 in keywordList:\\n        words = s1[\'keyword\'].split(\' \')\\n        N = len(words)\\n        sum_of_scores = 0\\n        for word in words:\\n            sum_of_scores += finbert_emb(model, word)\\n        score = sum_of_scores/N\\n        if score > 0.875:\\n            s1[\'Finbert Score\'] = score\\n            # finbert_keywords_list.append(s1)\\n            finbert_keywords_list.append([s1, score])\\n    final_list = [s1[0] for s1 in sorted(finbert_keywords_list, key=lambda x: x[1], reverse=True)]\\n    dataset[\'<FinbertKeywordsColumn>\'] = finbert_keywords_list\\n    return dataset\\n\",\"imports\":[\"import pandas as pd\",\"from sentence_transformers import SentenceTransformer, util\"],\"requirements\":[\"sentence-transformers\"]}},{\"requiredJars\":[],\"formats\":{\"Acronyms\":\"list\",\"TextColumn\":\"text\",\"OutputColumn\":\"text\"},\"classname\":\"Acronym\",\"name\":\"Acronym\",\"alias\":\"Acronym\",\"attributes\":{\"TextColumn\":\"\",\"Acronyms\":\"\",\"OutputColumn\":\"AcronymText\"},\"id\":0,\"category\":\"Text Correlation\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Acronym_<id>(dataset):\\n    def filter_acroynm(word):\\n        start_ind = (word.find(\\\".\\\") - 1) if word.find(\\\".\\\") > 0 else 0\\n        end_ind = (word.rfind(\\\".\\\") + 1) if word.find(\\\".\\\") < len(word) else len(word) - 1\\n        out = word[start_ind:end_ind]\\n        out = out.replace(\\\" \\\", \\\"\\\")\\n        out = out.replace(\\\".\\\", \\\"\\\")\\n\\n        new_out = word[:start_ind] + out + word[end_ind:]\\n        return new_out\\n    def acroynm_pipeline(nlp, sent):\\n        doc = nlp(sent) # input sample text\\n        for ent in list(doc.ents):\\n            if ent.text.count(\\\".\\\") > 1:\\n                fil = filter_acroynm(ent.text)\\n                sent = sent.replace(ent.text, fil)\\n\\n        return sent\\n    whitelist = <Acronyms>\\n    d = dict()\\n    for ind, acronymns in enumerate(whitelist):\\n        d[acronymns[\\\"name\\\"]] = acronymns[\\\"value\\\"].split(\\\",\\\")\\n    \\n    data = dataset[\\\"<TextColumn>\\\"]\\n    nlp = spacy.load(\\\"en_core_web_sm\\\")\\n    data = acroynm_pipeline(nlp, data)\\n    data_new = data.lower()\\n    for key, val in d.items():\\n        for v in val:\\n          if v.lower() in data_new:\\n            print(v)\\n          data = data.replace(v, key)\\n    dataset[\\\"<OutputColumn>\\\"] = data\\n    return dataset\\n\",\"imports\":[\"import pandas as pd\",\"import spacy\"],\"requirements\":[]}},{\"requiredJars\":[],\"formats\":{\"TextCol\":\"text\",\"OutputColumn\":\"text\",\"BufferSize\":\"text\",\"ChunckSize\":\"text\"},\"attributes\":{\"TextColumn\":\"\",\"OutputColumn\":\"\",\"BufferSize\":\"2\",\"ChunckSize\":\"10\"},\"classname\":\"Pronoun To Noun\",\"category\":\"Text Correlation\",\"name\":\"Pronoun To Noun\",\"alias\":\"Pronoun To Noun\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef PronounToNoun_<id>(dataset):\\n    text = \'\'\\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            text = text + \' \' + row[\'<TextColumn>\']\\n    elif isinstance(dataset,dict):\\n        text = dataset[\'<TextColumn>\']\\n    else:\\n        text = dataset\\n\\n    def pro_to_noun(text):\\n        nlp = spacy.load(\'en_coreference_web_trf\')\\n        doc = nlp(text)\\n        def post_process(first_mention):\\n            return first_mention.text.replace(\'\\\\\'s\', \'\')\\n        token_mention_mapper = dict()\\n        output_string = \'\'\\n        clusters = [\\n            val for key, val in doc.spans.items() if key.startswith(\'coref_cluster\')\\n        ]\\n\\n        for cluster in clusters:\\n            first_mention = cluster[0]\\n            first_mention = post_process(first_mention)\\n            for mention_span in list(cluster)[1:]:\\n                token_mention_mapper[mention_span[0].idx] = first_mention + mention_span[0].whitespace_\\n                for token in mention_span[1:]:\\n                    token_mention_mapper[token.idx] = \'\'\\n        for token in doc:\\n            if token.idx in token_mention_mapper:\\n                output_string += token_mention_mapper[token.idx]\\n            else:\\n                output_string += token.text + token.whitespace_\\n        return output_string\\n\\n    out = str()\\n    nlp_sent = spacy.load(\'en_core_web_sm\')\\n    doc_sent = nlp_sent(text)\\n    sent_list = [sent.text for sent in doc_sent.sents]\\n    chunks_list = []\\n    buffer_sz = <BufferSize>\\n    chunk_sz = <ChunckSize>\\n    inc_steps = chunk_sz - buffer_sz\\n    ind = 0\\n    while(ind < (len(sent_list) - chunk_sz) + 1):\\n        chunk = \' \'.join(sent_list[ind:ind+chunk_sz])\\n        improved_chunk = pro_to_noun(chunk)\\n        doc_sent = nlp_sent(improved_chunk)\\n        buffer_sent = [sent.text for sent in doc_sent.sents]\\n        buffer_sent_len = len(buffer_sent)\\n        sent_list = sent_list[:ind] + buffer_sent + sent_list[ind+chunk_sz:]\\n        ind += inc_steps\\n    output_string = \' \'.join(sent_list)\\n  \\n    dataset[\'<OutputColumn>\']=output_string\\n    return dataset\\n\",\"imports\":[\"import spacy\",\"from spacy.tokens import Doc\"],\"requirements\":[]}},{\"requiredJars\":[],\"formats\":{\"TextCol\":\"text\",\"OutputColumn\":\"text\",\"BufferSize\":\"text\",\"ChunckSize\":\"text\"},\"attributes\":{\"TextColumn\":\"\",\"OutputColumn\":\"nounAttributedText\",\"BufferSize\":\"2\",\"ChunckSize\":\"10\"},\"classname\":\"Pronoun To Noun Ensemble\",\"category\":\"Text Correlation\",\"name\":\"Pronoun To Noun Ensemble\",\"alias\":\"Pronoun To Noun Ensemble\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef PronounToNounEnsemble_<id>(dataset):\\n    from copy import deepcopy\\n    from abc import ABC, abstractmethod\\n\\n    class IntersectionStrategy(ABC):\\n        def __init__(self, allen_model, hugging_model, nlp_sent):\\n            self.allen_clusters = []\\n            self.hugging_clusters = []\\n            self.allen_model = allen_model\\n            self.hugging_model = hugging_model\\n            self.nlp_sent = nlp_sent\\n            self.document = []\\n            self.doc = None\\n\\n        @abstractmethod\\n        def get_intersected_clusters(self):\\n            raise NotImplementedError\\n\\n        @staticmethod\\n        def get_span_noun_indices(nlp_sent, doc, cluster):\\n            text = \\\" \\\".join(list(tok.text for tok in doc))\\n            doc_sent = nlp_sent(text)\\n            spans = [doc_sent[span[0]:span[1]+1] for span in cluster]\\n            spans_pos = [[token.pos_ for token in span] for span in spans]\\n            span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\\n                if any(pos in span_pos for pos in [\\\"NOUN\\\", \\\"PROPN\\\"])]\\n            return span_noun_indices\\n\\n        @staticmethod\\n        def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\\n            head_idx = noun_indices[0]\\n            head_start, head_end = cluster[head_idx]\\n            head_span = doc[head_start:head_end+1]\\n            return head_span, [head_start, head_end]\\n\\n        @staticmethod\\n        def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\\n            return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\\n\\n        def coref_resolved_improved(self, doc: Doc, clusters: List[List[List[int]]]):\\n            resolved = [tok.text_with_ws for tok in doc]\\n            all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\\n\\n            for cluster in clusters:\\n                noun_indices = self.get_span_noun_indices(self.nlp_sent, doc, cluster)\\n                if noun_indices:\\n                    mention_span, mention = self.get_cluster_head(doc, cluster, noun_indices)\\n\\n                    for coref in cluster:\\n                        if coref != mention and not self.is_containing_other_spans(coref, all_spans):\\n                            final_token = doc[coref[1]]\\n                            if final_token.tag_ in [\\\"PRP$\\\", \\\"POS\\\"]:\\n                                resolved[coref[0]] = mention_span.text + \\\"\'s\\\" + final_token.whitespace_\\n                            else:\\n                                resolved[coref[0]] = mention_span.text + final_token.whitespace_\\n\\n                            for i in range(coref[0] + 1, coref[1] + 1):\\n                                resolved[i] = \\\"\\\"\\n\\n            return \\\"\\\".join(resolved)\\n\\n        def clusters(self, text):\\n            self.acquire_models_clusters(text)\\n            return self.get_intersected_clusters()\\n\\n        def resolve_coreferences(self, text: str):\\n            clusters = self.clusters(text)\\n            resolved_text = self.coref_resolved_improved(self.doc, clusters)\\n            return resolved_text\\n\\n        def acquire_models_clusters(self, text: str):\\n            allen_prediction = self.allen_model.predict(text)\\n            self.allen_clusters = allen_prediction[\\\"clusters\\\"]\\n            self.document = allen_prediction[\\\"document\\\"]\\n            self.doc = self.hugging_model(text)\\n            hugging_clusters = self._transform_huggingface_answer_to_allen_list_of_clusters()\\n            self.hugging_clusters = hugging_clusters\\n\\n        def _transform_huggingface_answer_to_allen_list_of_clusters(self):\\n            list_of_clusters = []\\n            clusters = [\\n              val for key, val in self.doc.spans.items() if key.startswith(\\\"coref_cluster\\\")\\n            ]\\n            for cluster in clusters:\\n                list_of_clusters.append([])\\n                for span in cluster:\\n                    list_of_clusters[-1].append([span[0].i, span[-1].i])\\n            return list_of_clusters\\n\\n    class PartialIntersectionStrategy(IntersectionStrategy):\\n        def get_intersected_clusters(self):\\n            intersected_clusters = []\\n            for allen_cluster in self.allen_clusters:\\n                intersected_cluster = []\\n                for hugging_cluster in self.hugging_clusters:\\n                    allen_set = set(tuple([tuple(span) for span in allen_cluster]))\\n                    hugging_set = set(tuple([tuple(span) for span in hugging_cluster]))\\n                    intersect = sorted([list(el) for el in allen_set.intersection(hugging_set)])\\n                    if len(intersect) > 1:\\n                        intersected_cluster += intersect\\n                if intersected_cluster:\\n                    intersected_clusters.append(intersected_cluster)\\n            return intersected_clusters\\n    class FuzzyIntersectionStrategy(PartialIntersectionStrategy):\\n        \\\"\\\"\\\" Is treated as a PartialIntersectionStrategy, yet first must map AllenNLP spans and Huggingface spans. \\\"\\\"\\\"\\n\\n        @staticmethod\\n        def flatten_cluster(list_of_clusters):\\n            return [span for cluster in list_of_clusters for span in cluster]\\n\\n        def _check_whether_spans_are_within_range(self, allen_span, hugging_span):\\n            allen_range = range(allen_span[0], allen_span[1]+1)\\n            hugging_range = range(hugging_span[0], hugging_span[1]+1)\\n            allen_within = allen_span[0] in hugging_range and allen_span[1] in hugging_range\\n            hugging_within = hugging_span[0] in allen_range and hugging_span[1] in allen_range\\n            return allen_within or hugging_within\\n\\n        def _add_span_to_list_dict(self, allen_span, hugging_span):\\n            if (allen_span[1]-allen_span[0] > hugging_span[1]-hugging_span[0]):\\n                self._add_element(allen_span, hugging_span)\\n            else:\\n                self._add_element(hugging_span, allen_span)\\n\\n        def _add_element(self, key_span, val_span):\\n            if tuple(key_span) in self.swap_dict_list.keys():\\n                self.swap_dict_list[tuple(key_span)].append(tuple(val_span))\\n            else:\\n                self.swap_dict_list[tuple(key_span)] = [tuple(val_span)]\\n\\n        def _filter_out_swap_dict(self):\\n            swap_dict = {}\\n            for key, vals in self.swap_dict_list.items():\\n                if self.swap_dict_list[key] != vals[0]:\\n                    swap_dict[key] = sorted(vals, key=lambda x: x[1]-x[0], reverse=True)[0]\\n            return swap_dict\\n\\n        def _swap_mapped_spans(self, list_of_clusters, model_dict):\\n            for cluster_idx, cluster in enumerate(list_of_clusters):\\n                for span_idx, span in enumerate(cluster):\\n                    if tuple(span) in model_dict.keys():\\n                        list_of_clusters[cluster_idx][span_idx] = list(model_dict[tuple(span)])\\n            return list_of_clusters\\n\\n        def get_mapped_spans_in_lists_of_clusters(self):\\n            self.swap_dict_list = {}\\n            for allen_span in self.flatten_cluster(self.allen_clusters):\\n                for hugging_span in self.flatten_cluster(self.hugging_clusters):\\n                    if self._check_whether_spans_are_within_range(allen_span, hugging_span):\\n                        self._add_span_to_list_dict(allen_span, hugging_span)\\n            swap_dict = self._filter_out_swap_dict()\\n\\n            allen_clusters_mapped = self._swap_mapped_spans(deepcopy(self.allen_clusters), swap_dict)\\n            hugging_clusters_mapped = self._swap_mapped_spans(deepcopy(self.hugging_clusters), swap_dict)\\n            return allen_clusters_mapped, hugging_clusters_mapped\\n\\n        def get_intersected_clusters(self):\\n            allen_clusters_mapped, hugging_clusters_mapped = self.get_mapped_spans_in_lists_of_clusters()\\n            self.allen_clusters = allen_clusters_mapped\\n            self.hugging_clusters = hugging_clusters_mapped\\n            return super().get_intersected_clusters()\\n\\n    text = \\\"\\\"\\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            text = text + \\\" \\\" + row[\\\"<TextColumn>\\\"]\\n    elif isinstance(dataset,dict):\\n        text = dataset[\\\"<TextColumn>\\\"]\\n    else:\\n        text = dataset\\n\\n\\n    def load_models():\\n        allen_url = \\\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\\\"\\n        # allen_url = \\\"/content/coref-spanbert-large-2021.03.10.tar.gz\\\"  \\n        device = 0 if torch.cuda.is_available() else -1\\n        predictor = Predictor.from_path(allen_url, cuda_device=device)\\n        nlp_coref = spacy.load(\\\"en_coreference_web_trf\\\")\\n        nlp_sent = spacy.load(\\\"en_core_web_sm\\\")\\n        return predictor, nlp_coref, nlp_sent\\n    \\n    def get_cluster_head_idx(doc, cluster):\\n        nlp_text = spacy.load(\\\"en_core_web_sm\\\")\\n        noun_indices = IntersectionStrategy.get_span_noun_indices(nlp_text, doc, cluster)\\n        return noun_indices[0] if noun_indices else 0\\n      \\n    def print_clusters(doc, clusters):\\n        def get_span_words(span, allen_document):\\n            return \\\" \\\".join(allen_document[span[0]:span[1]+1])\\n        allen_document, clusters = [t.text for t in doc], clusters\\n        for cluster in clusters:\\n            cluster_head_idx = get_cluster_head_idx(doc, cluster)\\n            if cluster_head_idx >= 0:\\n                cluster_head = cluster[cluster_head_idx]\\n                print(get_span_words(cluster_head, allen_document) + \\\" - \\\", end=\\\"\\\")\\n                print(\\\"[\\\", end=\\\"\\\")\\n                for i, span in enumerate(cluster):\\n                    print(get_span_words(span, allen_document) + (\\\"; \\\" if i+1 < len(cluster) else \\\"\\\"), end=\\\"\\\")\\n                print(\\\"]\\\")\\n\\n    def  pro_to_noun(text, predictor, nlp_coref, nlp_sent, fuzzy):\\n\\n        \\n\\n        clusters = fuzzy.clusters(text)\\n        # # print\\n        # print(\\\"intersection\\\", clusters)\\n\\n        clusters = predictor.predict(text)[\\\"clusters\\\"]\\n        # # print\\n        # print(\\\"allennlp\\\", clusters)\\n\\n        docp_coref = nlp_coref(chunk)\\n        hugging_clusters = fuzzy._transform_huggingface_answer_to_allen_list_of_clusters()\\n        # # print\\n        # print(\\\"spacy\\\", hugging_clusters)\\n\\n        improved_chunk = fuzzy.coref_resolved_improved(docp_coref, clusters)\\n      \\n        # print(improved_chunk)\\n\\n        return improved_chunk\\n\\n    out = str()\\n    start = time.time()\\n    predictor, nlp_coref, nlp_sent = load_models()\\n    fuzzy = FuzzyIntersectionStrategy(predictor, nlp_coref, nlp_sent)\\n    doc_sent = nlp_sent(text)\\n    sent_list = [sent.text for sent in doc_sent.sents]\\n    chunks_list = []\\n    buffer_sz = <BufferSize>\\n    chunk_sz = <ChunckSize>\\n    inc_steps = chunk_sz - buffer_sz\\n    ind = 0\\n    print(\\\"initai time \\\", time.time() - start)\\n    while(ind < (len(sent_list) - chunk_sz) + 1):\\n        print(\\\"pos: \\\", ind, (len(sent_list) - chunk_sz) + 1)\\n        chunk = \\\" \\\".join(sent_list[ind:ind+chunk_sz])\\n\\n        # print(\\\"Original text: \\\", chunk)\\n        start = time.time()\\n        improved_chunk = pro_to_noun(chunk, predictor, nlp_coref, nlp_sent, fuzzy)\\n        # print(\\\"improved_chunk: \\\", improved_chunk)\\n        print(\\\"time taken \\\", time.time() - start)\\n\\n        doc_sent = nlp_sent(improved_chunk)\\n        buffer_sent = [sent.text for sent in doc_sent.sents]\\n        buffer_sent_len = len(buffer_sent)\\n        sent_list = sent_list[:ind] + buffer_sent + sent_list[ind+chunk_sz:]\\n        \\n        ind += inc_steps\\n    output_string = \\\" \\\".join(sent_list)\\n  \\n    dataset[\\\"<OutputColumn>\\\"]=output_string\\n    return dataset\\n\",\"imports\":[\"import spacy\",\"from spacy.tokens import Doc, Span\",\"from typing import List\",\"from allennlp.predictors.predictor import Predictor\",\"from copy import deepcopy\",\"from abc import ABC, abstractmethod\"],\"requirements\":[\"allennlp\",\"allennlp-models\"]}},{\"requiredJars\":[],\"formats\":{\"TextCol\":\"text\",\"OutputColumn\":\"text\"},\"attributes\":{\"TextColumn\":\"\",\"OutputColumn\":\"\"},\"classname\":\"Punctuation Restoration\",\"category\":\"Text Correlation\",\"name\":\"Punctuation Restoration\",\"alias\":\"Punctuation Restoration\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef PunctuationRestoration_<id>(dataset):\\n    nlp_sent = spacy.load(\'en_core_web_sm\')\\n    tokenizer = T5Tokenizer.from_pretrained(\'SJ-Ray/Re-Punctuate\')\\n    model = TFT5ForConditionalGeneration.from_pretrained(\'SJ-Ray/Re-Punctuate\')\\n    \\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            text = text + \' \' + row[\'<TextColumn>\']\\n    elif isinstance(dataset,dict):\\n        text = dataset[\'<TextColumn>\']\\n    else:\\n        text = dataset\\n    doc_sent = nlp_sent(text)\\n    text_list = [sent.text[0].upper() + sent.text[1:] for sent in doc_sent.sents]\\n    \\n    chunk = 1\\n    text = []\\n    for ind in range(0, len(text_list), chunk):\\n        text.append(\' \'.join(text_list[ind:ind+chunk]))\\n\\n    out = \'\'\\n    text_list = [\'punctuate: \' + data for data in text]\\n\\n\\n    encoding = tokenizer(text_list, return_tensors=\'tf\', padding=True)\\n\\n    generated_ids = model.generate(**encoding)\\n\\n    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\n    generated_texts = [decoded_output[0].upper() + decoded_output[1:] for decoded_output in generated_texts]\\n    for ind, decoded in enumerate(generated_texts):\\n        if decoded.count(\'..\') > 2:\\n            generated_texts[ind] = text[ind]\\n    out = \' \'.join(generated_texts)\\n    dataset[\'<OutputColumn>\']=out\\n    return dataset\\n\",\"imports\":[\"import spacy\",\"from transformers import T5Tokenizer, TFT5ForConditionalGeneration\"],\"requirements\":[\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1.tar.gz\"]}},{\"requiredJars\":[],\"formats\":{\"TextCol\":\"text\",\"OutputColumn\":\"text\"},\"attributes\":{\"TextColumn\":\"\",\"OutputColumn\":\"\"},\"classname\":\"Re Punctuation Restoration\",\"category\":\"Text Correlation\",\"name\":\"Re Punctuation Restoration\",\"alias\":\"Re Punctuation\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef RePunctuationRestoration_<id>(dataset):\\n  \\n    nlp_sent = spacy.load(\\\"en_core_web_sm\\\")\\n    tokenizer = T5Tokenizer.from_pretrained(\\\"SJ-Ray/Re-Punctuate\\\")\\n    model = TFT5ForConditionalGeneration.from_pretrained(\\\"SJ-Ray/Re-Punctuate\\\")\\n    \\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            text = text + \\\" \\\" + row[\\\"<TextColumn>\\\"]\\n    elif isinstance(dataset,dict):\\n        text = dataset[\\\"<TextColumn>\\\"]\\n    else:\\n        text = dataset\\n    doc_sent = nlp_sent(text)\\n    text_list = [sent.text[0].upper() + sent.text[1:] for sent in doc_sent.sents]\\n    \\n    chunk = 1\\n    text = []\\n    for ind in range(0, len(text_list), chunk):\\n        text.append(\\\" \\\".join(text_list[ind:ind+chunk]))\\n\\n    out = \\\"\\\"\\n    text_list = [\\\"punctuate: \\\" + data for data in text]\\n\\n\\n    encoding = tokenizer(text_list, return_tensors=\\\"tf\\\", padding=True)\\n\\n    generated_ids = model.generate(**encoding)\\n\\n    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\n    \\n    generated_texts = [decoded_output[0].upper() + re.sub(r\\\"[^0-9]\\\\.[^0-9]\\\", \\\",\\\", decoded_output[1:-1]) + decoded_output[-1] for decoded_output in generated_texts]\\n    for ind, decoded in enumerate(generated_texts):\\n        print(generated_texts[ind])\\n        if decoded.count(\\\"..\\\") > 2:\\n            generated_texts[ind] = text[ind]\\n    out = \\\" \\\".join(generated_texts)\\n    dataset[\\\"<OutputColumn>\\\"]=out\\n    return dataset\\n\",\"imports\":[\"import spacy\",\"import re\",\"from transformers import T5Tokenizer, TFT5ForConditionalGeneration\"],\"requirements\":[\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1.tar.gz\"]}},{\"requiredJars\":[],\"formats\":{\"TextColumn\":\"text\",\"KeywordColumn\":\"text\",\"OutputColumn\":\"text\"},\"attributes\":{\"TextColumn\":\"\",\"KeywordColumn\":\"\",\"OutputColumn\":\"clusterOutput\"},\"classname\":\"Clustering\",\"category\":\"Text Correlation\",\"name\":\"Clustering\",\"alias\":\"Clustering\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Clustering_<id>(dataset):\\n\\n    def clustering(text_doc, consolidated_list, k = 3):\\n        res = []\\n        for j in range(0, len(consolidated_list)):\\n            word = consolidated_list[j]\\n            # word = word.replace(\\\" \\\", \\\".*\\\")\\n            test_sub = \\\"(.{0,5}?)\\\".join(word.split(\\\" \\\"))\\n            res.extend([i.start() for i in re.finditer(test_sub, text_doc)])\\n        print(res)\\n        res.sort()\\n        nums = np.array(res)\\n        km = ckwrap.ckmeans(nums, k)\\n        buckets = [[] for i in range(k)]\\n        for i in range(len(nums)):\\n            buckets[km.labels[i]].append(nums[i])\\n        start, end = 0, len(text_doc)\\n        cluster_intervals = []\\n        for i in range(len(buckets) - 1):\\n            inter = [start, buckets[i+1][0] - 1]\\n            start = buckets[i+1][0]\\n            cluster_intervals.append(inter)\\n        cluster_intervals.append([buckets[-1][0], end])\\n\\n        result = []\\n        col_names = [\\\"Keyword\\\", \\\"Start Index\\\", \\\"End Index\\\"]\\n        rows = []\\n        for i in range(len(cluster_intervals)):\\n            col_names.append(\\\"Cluster {} Keyword Frequency\\\".format(str(i)))\\n            col_names.append(\\\"Keywords Percentage in cluster {}\\\".format(str(i)))\\n\\n        for j in range(0, len(consolidated_list)):\\n            row = [consolidated_list[j]]\\n            word = consolidated_list[j]\\n            # word = word.replace(\\\" \\\", \\\".*\\\")\\n            test_sub = \\\"(.{0,5}?)\\\".join(word.split(\\\" \\\"))\\n            p = [i.start() for i in re.finditer(test_sub, text_doc)]\\n            pp = [i.end() for i in re.finditer(test_sub, text_doc)]\\n            d = {}\\n            for i in range(len(cluster_intervals)):\\n                d[i] = 0\\n\\n            try:\\n                row.append(p[0])\\n            except:\\n                row.append(-1)\\n            try:\\n                row.append(pp[0])\\n            except:\\n                row.append(-1)\\n\\n            for ind in p:\\n                for i in range(len(cluster_intervals)):\\n                    if ind >= cluster_intervals[i][0] and ind <= cluster_intervals[i][-1]:\\n                        d[i] += 1\\n                        break\\n            per = {}\\n            for key, val in d.items():\\n                try:\\n                    per[key] = \\\"{}%\\\". format(round((val/len(p))*100,2))\\n                    row.append(val)\\n                    row.append(per[key])\\n                except:\\n                    per[key] = \\\"{}%\\\". format(0)\\n                    row.append(0)\\n                    row.append(per[key])\\n                \\n            rows.append(row)\\n            result.append({consolidated_list[j]: d, \\\"percentage\\\": per})\\n        \\n        df = pd.DataFrame(rows, columns = col_names)\\n        df = df.sort_values(by=\\\"Start Index\\\")\\n        return result, cluster_intervals, df\\n\\n    def optimal(text_doc, consolidated_list, k):\\n        res = []\\n        for j in range(0, len(consolidated_list)):\\n            word = consolidated_list[j]\\n            # word = word.replace(\\\" \\\", \\\".*\\\")\\n            test_sub = \\\"(.{0,5}?)\\\".join(word.split(\\\" \\\"))\\n            res.extend([i.start() for i in re.finditer(test_sub, text_doc)])\\n        res.sort()\\n        nums = np.array(res)\\n        km = ckwrap.ckmeans(nums, k)\\n        buckets = [[] for i in range(k)]\\n        for i in range(len(nums)):\\n            buckets[km.labels[i]].append(nums[i])\\n        res1 = np.array(res)\\n        res1 = res1.reshape(-1, 1)\\n        lab = np.array(km.labels)\\n        lab = lab.reshape(-1, 1)\\n        score = metrics.silhouette_score(res1, lab)\\n        return score\\n\\n    max_cluster, max_score = 2, 0\\n    text_doc = dataset[\\\"<TextColumn>\\\"].lower()\\n\\n    merged_list = [key[\\\"keyword\\\"].lower() for key in dataset[\\\"<KeywordColumn>\\\"]]\\n    print(len(merged_list))\\n    for clusters in range(2, min(6, len(merged_list))):\\n        try:\\n            score = optimal(text_doc, merged_list,clusters)\\n            print(\\\"cluster score \\\", clusters, score)\\n        except:\\n            score = -1\\n        if score > max_score:\\n            max_score = score\\n            max_cluster = clusters\\n            print(\\\"max score \\\", max_score, max_cluster)\\n    print(\\\"Optimal Clusters is: \\\", max_cluster)\\n    ress, cluster_intervals, df = clustering(text_doc, merged_list, max_cluster)\\n    print(\\\"Cluster intervals \\\", cluster_intervals)\\n    keys = []\\n    vals = []\\n    for key, val in df.items():\\n        keys.append(key)\\n        vals.append(val.values.tolist())\\n\\n    res = []\\n\\n    for i in range(len(vals[0])):\\n        d = dict()\\n        for k in range(len(keys)):\\n            d[keys[k]] = vals[k][i]\\n        res.append(d)\\n\\n    output = dict()\\n    clust = dict()\\n    for index, cluster in enumerate(cluster_intervals):\\n        start, end = cluster\\n        cluster_name = \\\"Cluster\\\" + str(index)\\n        output[cluster_name] = text_doc[start:end]\\n    clust[\\\"clusterSegments\\\"] = output\\n    clust[\\\"clusterdistribution\\\"] = res\\n    dataset[\\\"<OutputColumn>\\\"] = clust\\n    return dataset\\n\",\"imports\":[\"import re\",\"import ckwrap\",\"import numpy as np\",\"from sklearn import metrics\"],\"requirements\":[\"ckwrap\"]}},{\"requiredJars\":[],\"formats\":{\"OriginalTextCol\":\"text\",\"TextCol\":\"text\",\"OutputColumn\":\"text\",\"Count\":\"text\",\"NGramRange\":\"text\",\"POSPattern\":\"text\",\"StopWords\":\"textarea\",\"Candidates\":\"textarea\",\"SeedKeywords\":\"textarea\",\"Diversity\":\"text\",\"ScoreThreshold\":\"text\"},\"classname\":\"AdaptKeybert\",\"name\":\"AdaptKeybert\",\"alias\":\"AdaptKeybert\",\"attributes\":{\"OriginalTextCol\":\"\",\"TextCol\":\"\",\"OutputColumn\":\"Keywords\",\"Count\":\"20\",\"NGramRange\":\"(1,3)\",\"POSPattern\":\"<J.*>*<N.*>+\",\"StopWords\":\"\",\"Candidates\":\"\",\"SeedKeywords\":\"\",\"Diversity\":\"0.5\",\"ScoreThreshold\":\"0.25\"},\"id\":0,\"category\":\"Keyword Extraction\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef AdaptKeybert_<id>(dataset):\\n    nltk.download(\\\"stopwords\\\")\\n    nltk.download(\\\"punkt\\\")\\n\\n    # tmp_model = SentenceTransformer(\\\"valurank/MiniLM-L6-Keyword-Extraction\\\")\\n    # kw_extractor = KeyBERT(tmp_model)\\n    kw_model = KeyBERT(domain_adapt=True, zero_adapt=True)\\n    text = \\\"\\\"\\n    originalText = \\\"\\\"\\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            text = text + \\\" \\\" + row[\\\"<TextCol>\\\"]\\n            originalText = originalText + \\\" \\\" + row[\\\"<OriginalTextCol>\\\"]\\n    elif isinstance(dataset,dict):\\n        text = dataset[\\\"<TextCol>\\\"]\\n        originalText = dataset[\\\"<OriginalTextCol>\\\"]\\n    else:\\n        text = dataset\\n        originalText = dataset\\n    stopwords=\\\"\\\"\\\"<StopWords>\\\"\\\"\\\"\\n    candidatewords = \\\"<Candidates>\\\"\\n    seedKeywords = \\\"<SeedKeywords>\\\"\\n    stopwords = None if  stopwords==\\\"\\\" else stopwords.split(\\\",\\\")\\n    seedKeywords = None if seedKeywords == \\\"\\\" else seedKeywords.split(\\\",\\\")\\n    candidatewords = None if  candidatewords==\\\"\\\" else candidatewords.split(\\\",\\\")\\n\\n    vec = KeyphraseCountVectorizer(pos_pattern=\\\"<POSPattern>\\\", stop_words=\\\"english\\\")\\n    kw_model.pre_train([text], [[\\\"supervised\\\", \\\"unsupervised\\\"]], lr=1e-3)\\n    kw_model.zeroshot_pre_train([\\\"supervised\\\", \\\"unsupervised\\\"], adaptive_thr=0.15)\\n    keywords = kw_model.extract_keywords(text, vectorizer=vec, stop_words=stopwords,diversity=<Diversity>,\\n                                             seed_keywords=seedKeywords,top_n=<Count>,\\n                                             keyphrase_ngram_range=<NGramRange>,candidates=candidatewords, use_mmr=True )\\n    # keywords = kw_extractor.extract_keywords(text, vectorizer=vec, stop_words=stopwords,diversity=0.5,\\n    #                                          seed_keywords=seedKeywords,top_n=50,\\n    #                                          keyphrase_ngram_range=(1,3),candidates=candidatewords )\\n\\n    keywordsList = [{\\\"keyword\\\": i[0], \\\"score\\\": i[1]} for i in keywords if i[1] >= <ScoreThreshold>]\\n    print(\\\"Length before removing blacklist: \\\", len(keywordsList))\\n    keywordsList = [{\\\"keyword\\\": i[0], \\\"score\\\": i[1]} for i in keywords if i[1] >= <ScoreThreshold> and i[0] not in stopwords]\\n    print(\\\"Length after removing blacklist: \\\", len(keywordsList))\\n    results = []\\n    for row in keywordsList:\\n        test_sub = \\\"(.{0,5}?)\\\".join(row[\\\"keyword\\\"].split(\\\" \\\"))\\n        res = [(i.start(),i.end()) for i in re.finditer(test_sub, originalText.lower())]\\n        if len(res)>0:\\n           results.append({\\n                \\\"keyword\\\": row[\\\"keyword\\\"],\\n                \\\"score\\\": row[\\\"score\\\"],\\n                \\\"indices\\\":str(res),\\n                \\\"frequency\\\": len(res)\\n                })\\n        else:\\n            print(\\\"missed keyword \\\", row[\\\"keyword\\\"])\\n    if isinstance(dataset,dict):\\n        dataset[\\\"<OutputColumn>\\\"]= results\\n    else:\\n        dataset =results\\n    return dataset\",\"imports\":[\"from adaptkeybert import KeyBERT\",\"import nltk\",\"import re\",\"from sentence_transformers import SentenceTransformer\",\"from keyphrase_vectorizers import KeyphraseCountVectorizer\"],\"requirements\":[\"adaptkeybert\"]}},{\"requiredJars\":[],\"formats\":{\"OriginalTextCol\":\"text\",\"TextCol\":\"text\",\"OutputColumn\":\"text\",\"Count\":\"text\",\"NGramRange\":\"text\",\"POSPattern\":\"text\",\"StopWords\":\"textarea\",\"Candidates\":\"textarea\",\"SeedKeywords\":\"textarea\",\"Diversity\":\"text\",\"ScoreThreshold\":\"text\"},\"classname\":\"Keybert\",\"name\":\"Keybert\",\"alias\":\"Keybert\",\"attributes\":{\"OriginalTextCol\":\"\",\"TextCol\":\"\",\"OutputColumn\":\"Keywords\",\"Count\":\"20\",\"NGramRange\":\"(1,3)\",\"POSPattern\":\"<J.*>*<N.*>+\",\"StopWords\":\"\",\"Candidates\":\"\",\"SeedKeywords\":\"\",\"Diversity\":\"0.5\",\"ScoreThreshold\":\"0.25\"},\"id\":0,\"category\":\"Keyword Extraction\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Keybert_<id>(dataset):\\n    nltk.download(\'stopwords\')\\n    nltk.download(\'punkt\')\\n\\n    tmp_model = SentenceTransformer(\'valurank/MiniLM-L6-Keyword-Extraction\')\\n    kw_extractor = KeyBERT(tmp_model)\\n    text = \'\'\\n    originalText = \'\'\\n    if isinstance(dataset,list):\\n        for row in dataset:\\n            text = text + \' \' + row[\'<TextCol>\']\\n            originalText = originalText + \' \' + row[\'<OriginalTextCol>\']\\n    elif isinstance(dataset,dict):\\n        text = dataset[\'<TextCol>\']\\n        originalText = dataset[\'<OriginalTextCol>\']\\n    else:\\n        text = dataset\\n        originalText = dataset\\n    stopwords=\'\'\'<StopWords>\'\'\'\\n    candidatewords = \'<Candidates>\'\\n    seedKeywords = \'<SeedKeywords>\'\\n    stopwords = None if  stopwords==\'\' else stopwords.split(\',\')\\n    seedKeywords = None if seedKeywords == \'\' else seedKeywords.split(\',\')\\n    candidatewords = None if  candidatewords==\'\' else candidatewords.split(\',\')\\n\\n    vec = KeyphraseCountVectorizer(pos_pattern=\'<POSPattern>\', stop_words=\'english\')\\n    keywords = kw_extractor.extract_keywords(text, vectorizer=vec, stop_words=stopwords,diversity=<Diversity>,\\n                                             seed_keywords=seedKeywords,top_n=<Count>,\\n                                             keyphrase_ngram_range=<NGramRange>,candidates=candidatewords )\\n\\n    keywordsList = [{\'keyword\': i[0], \'score\': i[1]} for i in keywords if i[1] >= <ScoreThreshold>  and i[0] not in stopwords]\\n    results = []\\n    for row in keywordsList:\\n        test_sub = \\\"(.{0,5}?)\\\".join(row[\\\"keyword\\\"].split(\\\" \\\"))\\n        res = [(i.start(),i.end()) for i in re.finditer(test_sub, originalText.lower())]\\n        if len(res)>0 :\\n           results.append({\\n                \\\"keyword\\\": row[\\\"keyword\\\"],\\n                \\\"score\\\": row[\\\"score\\\"],\\n                \\\"indices\\\":str(res),\\n                \\\"frequency\\\": len(res)\\n                })\\n    if isinstance(dataset,dict):\\n        dataset[\'<OutputColumn>\']= results\\n    else:\\n        dataset =results\\n    return dataset\",\"imports\":[\"from keybert import KeyBERT\",\"import nltk\",\"import re\",\"from sentence_transformers import SentenceTransformer\",\"from keyphrase_vectorizers import KeyphraseCountVectorizer\"],\"requirements\":[]}},{\"requiredJars\":[],\"formats\":{},\"attributes\":{},\"classname\":\"Docqna\",\"category\":\"Document\",\"name\":\"Docqna\",\"alias\":\"Docqna\",\"id\":0,\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"script\":\"\\ndef Docqna_<id>(dataset):\\n    return dataset\\n\",\"imports\":[],\"requirements\":[]}}]","AWS","{\"commands\":[\"@!PYTHONPATH!@ @!icip.pipelineScript.directory!@@!pipelinename!@/@!pipelinename!@_sagemaker.py\",\"\",\"\"],\"environment\":{\"PYTHONPATH\":\"python3\",\"HTTP_PROXY\":\"http://10.68.248.39:80\",\"HTTPS_PROXY\":\"http://10.68.248.39:80\",\"AWS_ACCESS_KEY\":\"\",\"AWS_SECRET_KEY\":\"\",\"AWS_REGION\":\"us-east-1\"}}","Demo"