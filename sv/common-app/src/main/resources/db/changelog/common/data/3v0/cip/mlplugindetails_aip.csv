"type","org","plugindetails","pluginname"
"DragNDropLite","Core","{""formats"":{""dataset"":[""dropdown""]},""classname"":""DatasetExtractor"",""name"":""Dataset Extractor"",""alias"":""Dataset Extractor"",""parentCategory"":"""",""id"":1,""codeGeneration"":{""REST"":{},""requirements"":[],""servicenow"":{},""imports"":[],""MYSQL"":{},""w"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r    import importlib.util\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'extractors', f'{extractortype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Extractor', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Extractor'] = module\\r    spec.loader.exec_module(module)\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""category"":""Extractor"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset"":[""""]}}","DragNDropLite-1"
"DragNDropLite","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""DatasetLoader"",""name"":""Dataset Loader"",""alias"":""Dataset Loader"",""parentCategory"":"""",""id"":2,""codeGeneration"":{""REST"":{},""requirements"":[],""imports"":[""import importlib""],""MYSQL"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{},""script"":""def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r    import importlib.util\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = os.path.join(EXTRA_PLUGINS_PATH, 'loaders', f'{loadertype}.py')\\r    print('FilePath', file_path)\\r    \\r    spec = importlib.util.spec_from_file_location('Loader', file_path)\\r    module = importlib.util.module_from_spec(spec)\\r    sys.modules['Loader'] = module\\r    spec.loader.exec_module(module)\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\n""},""category"":""Loader"",""inputEndpoints"":[""in""],""outputEndpoints"":[],""attributes"":{""dataset"":""""}}","DragNDropLite-2"
"DragNDropLite","Core","{\"formats\":{\"modelName\":\"text\",\"experimentName\":\"text\"},\"classname\":\"SaveModel\",\"name\":\"SaveModel\",\"alias\":\"SaveModel\",\"parentCategory\":\"147\",\"id\":7,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import pickle\",\"from leaputils import FileServer\",\"import requests\",\"import hashlib\",\"import shutil\",\"import logging as logger\",\"import os\"],\"script\":\"\\ndef SaveModel_<id>(model, modelname_param=\'your_model_name.pkl\', experimentname_param=\'\'):\\n    #SaveModel\\n    modelPath = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',experimentname_param)\\n    print(\'Saving Model at path:\'+modelPath )\\n    logging.info(\'Saving Model at path:\'+modelPath )\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    modelPath = os.path.join(modelPath, modelname_param)\\n    print(\'Saving Model at path:\'+modelPath )\\n    if modelPath.split(\'.\')[-1] == \'pkl\':\\n        pickle.dump(model, open(modelPath, \'wb\'))\\n        return modelPath\\n    else:\\n        model.write().overwrite().save(modelPath)\\n        return modelPath\\n\\n\"},\"category\":\"ModelBaseConfig\",\"inputEndpoints\":[\"model\"],\"outputEndpoints\":[\"modelPath\"],\"attributes\":{\"modelName\":\"your_model_name.pkl\",\"experimentName\":\"\"}}","DragNDropLite-7"
"DragNDropLite","Core","{\"formats\":{\"Execution script\":\"textarea\",\"ModelName\":\"text\",\"Description\":\"text\",\"Bootstrap script\":\"textarea\",\"Bucket\":\"text\",\"ExperimentName\":\"text\",\"TypeOfModel\":\"text\"},\"classname\":\"PublishModelConfig\",\"name\":\"PublishModel\",\"alias\":\"PublishModel\",\"parentCategory\":\"147\",\"id\":8,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import os.path\",\"import shutil\",\"import joblib\",\"from leaputils import FileServer\",\"import datetime\",\"import pickle\",\"import json\"],\"script\":\"\\ndef PublishModel_<id>(model, execution_script_param=\'\', description_param=\'\', modelname_param=\'\', bootstrap_script_param=\'\', experimentname_param=\'\', bucket_param=\'\', typeofmodel_param=\'\'):\\n    import torch\\n    #SaveModel\\n    modelPath = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',experimentname_param)\\n    print(\'Saving Model at path:\'+modelPath )\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    modelPath = os.path.join(modelPath)\\n    print(\'Saving Model at path:\'+modelPath )\\n    if modelname_param.split(\'.\')[-1] == \'pkl\':\\n        pickle.dump(model, open(os.path.join(modelPath, modelname_param), \'wb\'))\\n    elif modelname_param.split(\'.\')[-1] == \'pt\':\\n        torch.save(model,os.path.join(modelPath,modelname_param))\\n    elif modelname_param.split(\'.\')[-1] == \'joblib\':\\n        joblib.dump(model, open(os.path.join(modelPath,modelname_param), \'wb\'))\\n    else:\\n        model.write().overwrite().save(os.path.join(modelPath,modelname_param))\\n    modelPath=os.path.join(modelPath,modelname_param)\\n    experimentName = experimentname_param\\n    bucket = bucket_param\\n    bootstrapScript = bootstrap_script_param\\n    executeScript = execution_script_param\\n    modelId = FileServer.generateFilId(bucket)\\n\\n    # zipFilePath = FileServer.zipFile(modelPath,os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\'))\\n    # print(zipFilePath)\\n    fileId, fileName = FileServer.uploadFile(modelId,modelPath,bucket,False)\\n    apispec = {\\n          \'openapi\': \'3.0.2\',\\n          \'info\': {\\n            \'version\': \'1\',\\n            \'title\': experimentName\\n          },\\n          \'servers\': [\\n            {\\n              \'url\': os.environ[\'MODEL_SERVICE_URL\']\\n            }\\n          ],\\n          \'paths\': {\\n            \'/\'+modelId: {\\n              \'post\': {\\n                \'requestBody\': {\\n                  \'description\': \'request\',\\n                  \'content\': {\\n                    \'application/json\': {\\n                      \'schema\': {\\n                        \'$ref\': \'#/components/schemas/input\'\\n                      }\\n                    }\\n                  }\\n                },\\n                \'responses\': {\\n                  \'200\': {\\n                    \'description\': \'response\',\\n                    \'content\': {\\n                      \'application/json\': {\\n                        \'schema\': {\\n                          \'type\': \'string\'\\n                        }\\n                      }\\n                    }\\n                  },\\n                  \'default\': {\\n                    \'description\': \'error\',\\n                    \'content\': {\\n                      \'application/json\': {\\n                        \'schema\': {\\n                          \'type\': \'string\'\\n                        }\\n                      }\\n                    }\\n                  }\\n                }\\n              }\\n            }\\n          },\\n          \'components\': {\\n            \'schemas\': {\\n              \'input\': {\\n                \'properties\': {}\\n              }\\n            },\\n            \'securitySchemes\': {\\n              \'bearerAuth\': {\\n                \'type\': \'http\',\\n                \'scheme\': \'bearer\',\\n                \'bearerFormat\': \'JWT\'\\n              }\\n            },\\n            \'security\': [\\n              {\\n                \'bearerAuth\': []\\n              }\\n            ]\\n          }\\n    }\\n    model = {}\\n    model[\'apispec\']= json.dumps(apispec)\\n    model[\'error\'] = 0\\n    model[\'fileId\'] = modelId\\n    model[\'localupload\'] =100\\n    model[\'status\'] =0\\n    model[\'loadscript\'] =bootstrapScript\\n    model[\'executionscript\'] =executeScript\\n    model[\'metadata\']=json.dumps({\'type\':\'local\',\'modeltype\':\'uploadmodel\',\'createdtime\':\'\'+datetime.datetime.now().strftime(\'%m/%d/%Y, %H:%M:%S\') +\'\',\'version\':\'1\',\'framework\':\'\',\'pushtocodestore\':True,\'public\':True,\'overwrite\':True,\'summary\':\'\',\'taginfo\':\'\',\'frameworkVersion\':\'\',\'modelClassName\':\'\',\'inferenceClassName\':\'\',\'filePath\':\'\',\'inputType\':\'\',\'submittedBy\':\'admin\',\'filename\':\'\'+fileName+\'\'})\\n    model[\'modelname\']= experimentName\\n    model[\'description\']= description_param\\n    model[\'organization\'] = bucket\\n    model[\'serverupload\'] =100\\n    return [model]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"category\":\"ModelBaseConfig\",\"inputEndpoints\":[\"model\"],\"outputEndpoints\":[\"model\",\"endpoint\"],\"attributes\":{\"Execution script\":\"\",\"ModelName\":\"\",\"Description\":\"\",\"Bootstrap script\":\"\",\"Bucket\":\"\",\"ExperimentName\":\"\",\"TypeOfModel\":\"\"}}","DragNDropLite-8"
"DragNDropLite","Core","{\"formats\":{\"bucket\":\"text\",\"fileName\":\"text\",\"Destination_directory\":\"text\",\"fileId\":\"text\"},\"classname\":\"ModelSourceConfig\",\"name\":\"ModelSource\",\"alias\":\"ModelSource\",\"parentCategory\":\"147\",\"id\":9,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import requests\",\"import logging as logger\",\"import shutil\",\"import os\"],\"script\":\"\\ndef ModelSource_<id>(bucket_param=\'Demo\', filename_param=\'your_model_name.pkl\', destination_directory_param=\'models/classicmlpoc\', fileid_param=\'\'):\\n    fileId, fileName, bucket = fileid_param, filename_param, bucket_param\\n    FILE_SERVER_URL = os.environ[\'FILE_SERVER_URL\']\\n    print(\'FILE_SERVER_URL: \', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ[\'FILE_SERVER_TOKEN\']\\n    print(\'FILE_SERVER_TOKEN: \', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ[\'JOB_DIRECTORY\'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + \'/api/lastcount/\' + fileId + \'?bucket=\' + bucket,\\n                                           headers={\'access-token\': FILE_SERVER_TOKEN},\\n                                           proxies={\'http\': \'\', \'https\': \'\'})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search(\'<Integer>(.*?)</Integer>\', totalChunksResponse.text).group(1)\\n            totalChunks = int(\'0\')\\n        logger.info(\'Total Chunks: \' + str(totalChunks + 1))\\n\\n        # create a folder \'chunkfiles\'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, \'chunkfiles\')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info(\'dir already exists...\')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model\'s chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            filenam = \'file\' if fileId[0]==\'f\' else filename_param\\n            url = (FILE_SERVER_URL + \'/api/download/\' + fileId + \'/\') + filenam + \'?bucket=\' + bucket\\n            print(url)\\n            filedata = requests.get(url, headers={\'access-token\': FILE_SERVER_TOKEN}, proxies={\'http\': \'\', \'https\': \'\'})\\n            open(chunkfilePath + \'/\' + str(i), \'wb\').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, \'wb\') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, \'rb\')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n\"},\"category\":\"ModelBaseConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"attributes\":{\"bucket\":\"Demo\",\"fileName\":\"your_model_name.pkl\",\"Destination_directory\":\"models/classicmlpoc\",\"fileId\":\"\"}}","DragNDropLite-9"
"DragNDropLite","Core","{\"formats\":{\"requirements\":\"textarea\",\"script\":\"textarea\"},\"classname\":\"ModelPredictScriptConfig\",\"name\":\"Model Predict Script\",\"alias\":\"Model Predict Script\",\"parentCategory\":\"147\",\"id\":10,\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\"},\"category\":\"ModelBaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"requirements\":\"\",\"script\":\"\\rdef ModelPredictScript_<id>( dataset):\\n    #Model Predict Script\\r\\r    return dataset\"}}","DragNDropLite-10"
"DragNDropLite","Core","{\"formats\":{\"bucket\":\"text\",\"File\":\"file\"},\"classname\":\"UploadFile\",\"name\":\"UploadFile\",\"alias\":\"UploadFile\",\"parentCategory\":\"146\",\"id\":11,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import requests\",\"import logging as logger\",\"import shutil\",\"import os\",\"import hashlib\"],\"script\":\"\\ndef UploadFile_<id>(path, bucket_param=\'Demo\', file_param=\'\'):\\n    bucket, totalcount = bucket_param, 1\\n    FILE_SERVER_URL = os.environ[\'FILE_SERVER_URL\']\\n    FILE_SERVER_TOKEN = os.environ[\'FILE_SERVER_TOKEN\']\\n    def checksum(path):\\n        filepath = path\\n        sha256_hash = hashlib.sha256()\\n        with open(filepath, \'rb\') as f:\\n            # Read and update hash string value in blocks of 4K\\n            for byte_block in iter(lambda: f.read(4096), b\'\'):\\n                sha256_hash.update(byte_block)\\n            checksum = sha256_hash.hexdigest()\\n        print(\'Generated checksum is : \', checksum)\\n        return checksum\\n\\n    def generateFilId(bucket):\\n        fileIdRes = requests.get(\\n            FILE_SERVER_URL + \'/api/generate/fileid\' + \'?bucket=\' + bucket + \'&prefix=filedataset\',\\n            headers={\'access-token\': FILE_SERVER_TOKEN}, proxies={\'http\': \'\', \'https\': \'\'})\\n        if (fileIdRes.status_code == 200):\\n            fileId = fileIdRes.text\\n            print(\'Generated file id is : \', fileId)\\n            return fileId\\n        else:\\n            print(\'File id could not be generated\')\\n\\n    fileid = generateFilId(bucket)\\n    print(fileid)\\n    checksum = checksum(path)\\n    totalCnt = totalcount\\n    files = [(\'file\', (\'file\', open(path, \'rb\'), \'application/octet-stream\'))]\\n    try:\\n        uploadRes = requests.post(FILE_SERVER_URL + \'/api/upload/\' + fileid + \'/true?bucket=\' + bucket,\\n                                  headers={\'access-token\': FILE_SERVER_TOKEN}, data={\'checksum\': checksum,\\n                                                                                     \'totalcount\': totalCnt},\\n                                  files=files, proxies={\'http\': \'\', \'https\': \'\'})\\n    except Exception as ex:\\n        print(ex)\\n    if (uploadRes.status_code == 200):\\n        print(\'File uploaded successfully\')\\n        return uploadRes\\n    else:\\n        print(\'Error occurred while uploading a file\')\\n        print(uploadRes.text)\\n\\n\\n\"},\"category\":\"BaseConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"bucket\":\"Demo\",\"File\":\"\"}}","DragNDropLite-11"
"DragNDropLite","Core","{\"formats\":{\"tol\":\"integer\",\"l1_ratio\":\"integer\",\"copyX\":\"integer\",\"selection\":\"text\",\"alpha\":\"integer\",\"maxIter\":\"integer\",\"positive\":\"integer\",\"random_state\":\"integer\",\"precompute\":\"integer\",\"warm_start\":\"integer\",\"FitIntercept\":\"integer\"},\"classname\":\"ElasticnetR\",\"name\":\"ElasticnetR\",\"alias\":\"ElasticnetR\",\"parentCategory\":\"148\",\"id\":16,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import ElasticNet\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"script\":\"\\ndef ElasticnetR_<id>(dataset, tol_param=0.0001, l1_ratio_param=0.5, copyx_param=True, selection_param=\'cyclic\', alpha_param=1.0, maxiter_param=1000, positive_param=True, random_state_param=None, precompute_param=False, warm_start_param=False, fitintercept_param=True):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputEN=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',ElasticNet(alpha=alpha_param, l1_ratio=l1_ratio_param, fit_intercept=fitintercept_param, precompute=precompute_param, copy_X=copyx_param, max_iter=maxiter_param, tol=tol_param, warm_start=warm_start_param, positive=positive_param, random_state=random_state_param, selection=selection_param))]\\n    pipeEN=Pipeline(InputEN)\\n\\n    pipeEN.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeEN.score(dataset[\'X_train\'],dataset[\'Y_train\']))  \\n    \\n    return pipeEN\\n\\n\\n\"},\"category\":\"Regression\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"tol\":\"0.0001\",\"l1_ratio\":\"0.5\",\"copyX\":\"True\",\"selection\":\"cyclic\",\"alpha\":\"1.0\",\"maxIter\":\"1000\",\"positive\":\"True\",\"random_state\":\"None\",\"precompute\":\"False\",\"warm_start\":\"False\",\"FitIntercept\":\"True\"}}","DragNDropLite-16"
"DragNDropLite","Core","{\"formats\":{\"criterion\":\"text\",\"ccp_alpha\":\"integer\",\"oob_score\":\"integer\",\"n_jobs\":\"integer\",\"max_depth\":\"integer\",\"min_samples_split\":\"integer\",\"bootstrap\":\"integer\",\"n_estimators\":\"integer\",\"random_state\":\"integer\",\"min_impurity_decrease\":\"integer\",\"min_weight_fraction_leaf\":\"integer\",\"warm_start\":\"integer\",\"min_samples_leaf\":\"integer\",\"verbose\":\"integer\",\"max_samples\":\"integer\",\"max_features\":\"integer\",\"max_leaf_nodes\":\"integer\"},\"classname\":\"RandomForestR\",\"name\":\"RandomForestR\",\"alias\":\"RandomForestR\",\"parentCategory\":\"148\",\"id\":17,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.ensemble import RandomForestRegressor\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"script\":\"\\ndef RandomForestR_<id>(dataset, criterion_param=\'mse\', ccp_alpha_param=0.0, oob_score_param=False, max_depth_param=None, min_samples_split_param=2, n_jobs_param=None, n_estimators_param=100, min_weight_fraction_leaf_param=0.0, min_impurity_decrease_param=0.0, bootstrap_param=True, random_state_param=None, min_samples_leaf_param=1, warm_start_param=False, verbose_param=0, max_samples_param=None, max_features_param=1.0, max_leaf_nodes_param=None):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    Input=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',RandomForestRegressor(n_estimators=n_estimators_param, criterion=criterion_param, max_depth=max_depth_param, min_samples_split=min_samples_split_param, min_samples_leaf=min_samples_leaf_param, min_weight_fraction_leaf=min_weight_fraction_leaf_param, max_features=max_features_param, max_leaf_nodes=max_leaf_nodes_param, min_impurity_decrease=min_impurity_decrease_param, bootstrap=bootstrap_param, oob_score=oob_score_param, n_jobs=n_jobs_param, random_state=random_state_param, verbose=verbose_param, warm_start=warm_start_param, ccp_alpha=ccp_alpha_param, max_samples=max_samples_param))]\\n    pipe=Pipeline(Input)\\n\\n    pipe.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_RFF = pipe.predict(X_test)\\n    print(pipe.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n\\n    return pipe\\n\\n\\n\"},\"category\":\"Regression\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"criterion\":\"mse\",\"ccp_alpha\":\"0.0\",\"oob_score\":\"False\",\"n_jobs\":\"None\",\"max_depth\":\"None\",\"min_samples_split\":\"2\",\"bootstrap\":\"True\",\"n_estimators\":\"100\",\"random_state\":\"None\",\"min_impurity_decrease\":\"0.0\",\"min_weight_fraction_leaf\":\"0.0\",\"warm_start\":\"False\",\"min_samples_leaf\":\"1\",\"verbose\":\"0\",\"max_samples\":\"None\",\"max_features\":\"1.0\",\"max_leaf_nodes\":\"None\"}}","DragNDropLite-17"
"DragNDropLite","Core","{\"formats\":{\"tol\":\"integer\",\"epsilon\":\"integer\",\"shrinking\":\"integer\",\"C\":\"integer\",\"coef0\":\"integer\",\"cache_size\":\"integer\",\"kernel\":\"text\",\"degree\":\"integer\",\"max_iter\":\"integer\",\"gamma\":\"text\",\"verbose\":\"integer\"},\"classname\":\"SVR\",\"name\":\"SVR\",\"alias\":\"SVR\",\"parentCategory\":\"148\",\"id\":18,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.svm import SVR\",\"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\",\"from sklearn.pipeline import Pipeline\"],\"script\":\"\\ndef SVR_<id>(dataset, tol_param=0.001, epsilon_param=0.1, shrinking_param=True, c_param=1.0, coef0_param=0.0, cache_size_param=200, kernel_param=\'rbf\', degree_param=3, max_iter_param=-1, gamma_param=\'scale\', verbose_param=False):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputSVR=[(\'scale\',StandardScaler()),(\'polynomial\', PolynomialFeatures(include_bias=False)),(\'model\',SVR(kernel=kernel_param, degree=3, gamma=\'scale\', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1))]\\n    pipeSVR=Pipeline(InputSVR)\\n\\n    pipeSVR.fit(dataset[\'X_train\'],dataset[\'Y_train\'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeSVR.score(dataset[\'X_train\'],dataset[\'Y_train\']))\\n    print(((pipeSVR.predict(dataset[\'X_train\']) - dataset[\'Y_train\']) ** 2).mean())\\n    \\n    return pipeSVR\\n\\n\\n\"},\"category\":\"Regression\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"tol\":\"0.001\",\"epsilon\":\"0.1\",\"shrinking\":\"True\",\"C\":\"1.0\",\"coef0\":\"0.0\",\"cache_size\":\"200\",\"kernel\":\"rbf\",\"degree\":\"3\",\"max_iter\":\"-1\",\"gamma\":\"scale\",\"verbose\":\"False\"}}","DragNDropLite-18"
"DragNDropLite","Core","{\"formats\":{},\"classname\":\"InferenceR\",\"name\":\"InferenceR\",\"alias\":\"InferenceR\",\"parentCategory\":\"148\",\"id\":19,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from leaputils import FileServer\",\"import pickle\",\"import os\"],\"script\":\"\\ndef InferenceR_<id>(dataset, download_path):\\n    if isinstance(dataset,str):\\n        dataset, download_path = download_path, dataset\\n    print(\'download_path: \', download_path)\\n    # load the model\\n    unpickler = pickle.Unpickler(open(download_path, \'rb\'))\\n    load_model = unpickler.load()\\n    print(load_model)\\n    pipe_pred = load_model.predict(dataset[\'X_test\'])\\n    print(pipe_pred)\\n    \\n    # OUT\\n    if isinstance(pipe_pred,tuple):\\n        pipe_pred, prediction_results = pipe_pred\\n        output=pd.DataFrame({\'Id\':dataset[\'dataset_id\']})\\n        output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\n        print(\'out\', output.to_dict(\'records\'))\\n    else:\\n        output=pd.DataFrame({\'Id\':dataset[\'dataset_id\'],\'Result\':pipe_pred})\\n    os.remove(download_path)\\n    return output.to_dict(\'records\')\\n\\n\\n\"},\"category\":\"Regression\",\"inputEndpoints\":[\"in\",\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-19"
"DragNDropLite","Core","{\"formats\":{\"question\":\"text\",\"prompt\":\"text\"},\"classname\":\"ProcessQuery\",\"name\":\"ProcessQuery\",\"alias\":\"ProcessQuery\",\"parentCategory\":\"149\",\"id\":21,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import re\",\"import numpy as np\",\"import torch\",\"from transformers import DonutProcessor, VisionEncoderDecoderModel\",\"from PIL import Image\"],\"script\":\"def ProcessQuery_<id>(dataset, question_param=\'what is the identification number present?\', prompt_param=\'<s_docvqa><s_question>{user_input}</s_question><s_answer>\'):\\n    def read_img_to_3d_array(img_path):\\n        img = Image.open(img_path)\\n        img_array = np.asarray(img)\\n        if len(img_array.shape) == 3:\\n            #img_array = img_array[:, :, :3]\\n            pass\\n        else:\\n            assert len(img_array.shape) == 2\\n            h, w = img_array.shape\\n            img_array = img_array.reshape([h, w, 1])\\n            img_array = np.concatenate([img_array] * 3, axis = -1)\\n        assert len(img_array.shape) == 3\\n        img_array = img_array[:, :, :3]\\n        return img_array\\n\\n    def process_document(image, question,task_prompt):\\n        processor = DonutProcessor.from_pretrained(\'naver-clova-ix/donut-base-finetuned-docvqa\')\\n        model = VisionEncoderDecoderModel.from_pretrained(\'naver-clova-ix/donut-base-finetuned-docvqa\')\\n        device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\\n        model.to(device)\\n        # prepare encoder inputs\\n        pixel_values = processor(image, return_tensors=\'pt\').pixel_values\\n        # prepare decoder inputs\\n        \\n        prompt = task_prompt.replace(\'{user_input}\', question)\\n        decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\'pt\').input_ids\\n        # generate answer\\n        outputs = model.generate(\\n            pixel_values.to(device),\\n            decoder_input_ids=decoder_input_ids.to(device),\\n            max_length=model.decoder.config.max_position_embeddings,\\n            early_stopping=True,\\n            pad_token_id=processor.tokenizer.pad_token_id,\\n            eos_token_id=processor.tokenizer.eos_token_id,\\n            use_cache=True,\\n            num_beams=1,\\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\\n            return_dict_in_generate=True,\\n        )\\n        # postprocess\\n        sequence = processor.batch_decode(outputs.sequences)[0]\\n        sequence = sequence.replace(processor.tokenizer.eos_token, \'\').replace(processor.tokenizer.pad_token, \'\')\\n        sequence = re.sub(r\'<.*?>\', \'\', sequence, count=1).strip()  # remove first task start token\\n        return processor.token2json(sequence)\\n    task_prompt = prompt_param\\n    input_img = dataset\\n    question = question_param\\n    image  = read_img_to_3d_array(input_img)\\n    print(dataset,type(dataset))\\n    res = process_document(image, question,task_prompt)\\n    print(res)\\n    return [res]\\n\\n\"},\"category\":\"DocumentComprehension\",\"inputEndpoints\":[\"dataset1\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"question\":\"what is the identification number present?\",\"prompt\":\"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"}}","DragNDropLite-21"
"DragNDropLite","Core","{\"formats\":{\"K\":\"text\"},\"classname\":\"KNearestNeighbor\",\"name\":\"KNearestNeighbor\",\"alias\":\"KNearestNeighbor\",\"parentCategory\":\"150\",\"id\":22,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.neighbors import KNeighborsClassifier\",\"from sklearn.preprocessing import LabelEncoder\"],\"script\":\"\\ndef KNearestNeighbor_<id>(dataset, k_param=5):\\n    knn = KNeighborsClassifier(n_neighbors=k_param)\\n    knn.fit(dataset[0],dataset[1])\\n    return knn\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"K\":\"5\"}}","DragNDropLite-22"
"DragNDropLite","Core","{\"formats\":{\"hyper_parameter\":\"text\",\"model_name\":\"text\",\"label_heading\":\"text\",\"kernel\":\"text\",\"penalty\":\"text\",\"gamma\":\"text\",\"test_ratio\":\"text\",\"text_heading\":\"text\"},\"classname\":\"SVM\",\"name\":\"SVM\",\"alias\":\"SVM Train\",\"parentCategory\":\"150\",\"id\":25,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import os\",\"import pickle\",\"import json\",\"import secrets\",\"import logging\",\"import numpy as np\",\"import pandas as pd\",\"from collections import OrderedDict\",\"from sklearn.pipeline import Pipeline\",\"from sklearn.preprocessing import LabelEncoder\",\"from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)\",\"from sklearn.naive_bayes import MultinomialNB\",\"from sklearn.svm import LinearSVC, SVC\",\"from sklearn.model_selection import GridSearchCV, train_test_split\",\"from sklearn.utils import shuffle\",\"from sklearn.metrics import accuracy_score\"],\"script\":\"\\ndef SVM_<id>(dataset, hyper_parameter_param=\'default\', model_name_param=\'svmtest\', label_heading_param=\'label\', kernel_param=\'rbf\', penalty_param=1, gamma_param=\'scale\', test_ratio_param=0.2, text_heading_param=\'text\'):\\n    df = pd.DataFrame(dataset)\\n    test_ratio = test_ratio_param\\n    text_heading = text_heading_param\\n    label_heading = label_heading_param\\n    model_name = model_name_param\\n    hyper_parameter = hyper_parameter_param\\n    kernel = kernel_param\\n    penalty = penalty_param\\n    gamma = gamma_param\\n    if model_name is None:\\n        raise Exception(\'Model name is empty\')\\n    save_path = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n    if not save_path :\\n        raise Exception(\'Save path is a required parameter\')\\n    save_path = os.path.join(save_path,model_name)\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    df = df.replace(\'\', np.nan)\\n    df = df.dropna()\\n    train_df, test_df = train_test_split(df, test_size=test_ratio, random_state=42)\\n    Train_X = train_df[text_heading]\\n    Train_Y = train_df[label_heading]\\n    label_list = list(df[label_heading].unique())\\n    encoder = LabelEncoder()\\n    encoder.fit(label_list)\\n    np.save(os.path.join(save_path, \'classes.npy\'), encoder.classes_)\\n\\n    \\n    if hyper_parameter.lower() == \'default\':\\n        clf_model = Pipeline([(\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'SVM\', SVC(probability=True))])\\n        pkl_filename = os.path.join(save_path, \'default_\'+model_name+\'_.pkl\')\\n        clf_model = clf_model.fit(Train_X, Train_Y)\\n        with open(pkl_filename, \'wb\') as file:\\n            pickle.dump(clf_model, file)\\n    elif hyper_parameter.lower() == \'gridsearch\':\\n        hyper_param_dict = OrderedDict()\\n        hyper_param_dict[\'kernel\'] = [\'linear\']\\n\\n        hyper_param_dict[\'gamma\'] = [0.1]\\n\\n        hyper_param_dict[\'penalty\'] = [1]\\n\\n        hyper_param_dict[\'K_folds\'] = 2\\n\\n        kernel = hyper_param_dict[\'kernel\']\\n        gamma = hyper_param_dict[\'gamma\']\\n        penalty = hyper_param_dict[\'penalty\']\\n        K_folds = hyper_param_dict[\'K_folds\']\\n\\n        SVM_kernel_ = [ele for ele in kernel if ele.lower()!=\'linear\']\\n        param_grid = []\\n        if len(SVM_kernel_) == len(kernel):\\n            param_grid = [{ \'SVM__kernel\': kernel, \\n                            \'SVM__gamma\': gamma,\\n                            \'SVM__C\': penalty}]\\n\\n        else:\\n            param_grid = [{ \'SVM__kernel\': kernel, \\n                            \'SVM__gamma\': gamma,\\n                            \'SVM__C\': penalty},\\n                            {   \'SVM__kernel\': [\'linear\'], \\n                                \'SVM__C\': penalty\\n                            }]\\n\\n        svc_clf = Pipeline([(\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'SVM\', SVC(probability=True))])\\n        grid = GridSearchCV(svc_clf, param_grid, n_jobs=-1, cv=K_folds, scoring=\'accuracy\') \\n        grid.fit(Train_X, Train_Y)\\n        pkl_filename = os.path.join(save_path, model_name+\'grid_model.pkl\' )\\n        with open(pkl_filename, \'wb\') as file:\\n            pickle.dump(grid, file)\\n    elif hyper_parameter.lower() == \'manual\':\\n        if isinstance(kernel, list):\\n            kernel = kernel[0]\\n        if isinstance(penalty, list):\\n            penalty = penalty[0]\\n        if not isinstance(gamma, str):\\n            raise Exception(\'Gamma should be str i.e. scale\')\\n        clf_model = Pipeline([(\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'SVM\', SVC(C= penalty , kernel=kernel, gamma=gamma , probability=True))])\\n        pkl_filename = os.path.join(save_path,\'manual\'+model_name+\'.pkl\')\\n        model_details.update({\'kernel\':kernel, \'gamma\':gamma, \'penalty\':penalty})\\n        clf_model = clf_model.fit(Train_X, Train_Y)\\n        with open(pkl_filename, \'wb\') as file:\\n            pickle.dump(clf_model, file)\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"hyper_parameter\":\"default\",\"model_name\":\"svmtest\",\"label_heading\":\"label\",\"kernel\":\"rbf\",\"penalty\":1,\"gamma\":\"scale\",\"test_ratio\":0.2,\"text_heading\":\"text\"}}","DragNDropLite-25"
"DragNDropLite","Core","{\"formats\":{},\"classname\":\"LogisticRegression\",\"name\":\"LogisticRegression\",\"alias\":\"LogisticRegression\",\"parentCategory\":\"150\",\"id\":26,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.linear_model import LogisticRegression\",\"from sklearn.preprocessing import LabelEncoder\"],\"script\":\"\\ndef LogisticRegression_<id>(dataset):\\n    model = LogisticRegression()\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-26"
"DragNDropLite","Core","{\"formats\":{},\"classname\":\"DecisionTree\",\"name\":\"DecisionTree\",\"alias\":\"DecisionTree\",\"parentCategory\":\"150\",\"id\":27,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from sklearn.tree import DecisionTreeClassifier\",\"from sklearn.preprocessing import LabelEncoder\"],\"script\":\"\\ndef DecisionTree_<id>(dataset):\\n    model = DecisionTreeClassifier(random_state = 1)\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-27"
"DragNDropLite","Core","{\"formats\":{\"model_name\":\"text\",\"test_size\":\"text\",\"alpha\":\"text\",\"vect_type\":\"text\",\"clf_type\":\"text\",\"class_heading\":\"text\",\"text_heading\":\"text\"},\"classname\":\"naive_bayes\",\"name\":\"naive_bayes\",\"alias\":\"Naive Bayes\",\"parentCategory\":\"150\",\"id\":28,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import os, json\",\"import pandas as pd\",\"import codecs\",\"import nltk\",\"from pandas import DataFrame\",\"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\",\"from sklearn.naive_bayes import MultinomialNB\",\"from sklearn import metrics\",\"from sklearn.model_selection import train_test_split\",\"import joblib\",\"from sklearn.linear_model import LogisticRegression\",\"import numpy as np\",\"from threading import Thread\",\"from sklearn.pipeline import make_pipeline\"],\"script\":\"\\ndef naive_bayes_<id>(dataset, model_name_param=\'test\', test_size_param=0.2, vect_type_param=\'TFIDF\', alpha_param=0.01, clf_type_param=\'model\', class_heading_param=\'class\', text_heading_param=\'text\'):\\n\\tdataframe = pd.DataFrame(dataset)\\n\\tmodel_name = model_name_param\\n\\tif model_name is None:\\n\\t\\traise Exception(\'Model name is empty\')\\n\\tmodel_path  = \'/app/jobs/models/classicmlpoc\'\\n\\tmodel_path = os.path.join(model_path,model_name_param)\\n\\n\\tif not os.path.isdir(model_path):\\n\\t\\tos.makedirs(model_path)\\n\\tvect_type = vect_type_param\\n\\tif(vect_type==\'TFIDF\'):            \\n\\t\\tvectorizer = TfidfVectorizer()\\n\\telse: \\n\\t\\tvectorizer = CountVectorizer()             \\n\\tvectors = vectorizer.fit_transform(dataframe[text_heading_param])\\n\\tX_train, X_test, y_train, y_test = train_test_split(vectors, dataframe[class_heading_param], test_size=test_size_param, random_state=42)\\n\\tclf_type = clf_type_param\\n\\tif clf_type==\'naive bayes\':\\n\\t\\tclf = MultinomialNB(alpha=alpha_param)\\n\\telse: \\n\\t\\tclf = LogisticRegression(random_state=0, solver=\'lbfgs\',multi_class=\'multinomial\')  \\n\\tclf.fit(X_train, y_train)    \\n\\tpred = clf.predict(X_test)\\n\\tf1_sc=metrics.accuracy_score(y_test, pred)\\n\\tjoblib.dump(clf,model_path+\'/\'+\'classifier.pkl\')\\n\\tjoblib.dump(vectorizer,model_path+\'/\'+\'vectorizer.pkl\')\\n\\t\\n\\treturn clf\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model_name\":\"test\",\"test_size\":\"0.2\",\"alpha\":\"0.01\",\"vect_type\":\"TFIDF\",\"clf_type\":\"model\",\"class_heading\":\"class\",\"text_heading\":\"text\"}}","DragNDropLite-28"
"DragNDropLite","Core","{\"formats\":{\"model_name\":\"text\",\"text\":\"text\"},\"classname\":\"naive_bayes_predict\",\"name\":\"naive_bayes_predict\",\"alias\":\"Naive Bayes Predict\",\"parentCategory\":\"150\",\"id\":29,\"codeGeneration\":{\"requirements\":[\"nltk\"],\"imports\":[\"import os, json, joblib\",\"import pandas as pd\",\"import codecs\",\"import numpy as np\",\"import nltk\",\"from nltk.corpus import stopwords\",\"from nltk.stem.wordnet import WordNetLemmatizer\"],\"script\":\"\\ndef naive_bayes_predict_<id>(dataset, model_name_param=\'test\', text_param=\'text\') :\\n    model_name = model_name_param\\n    text = text_param\\n    model_path  = \'/app/jobs/models/classicmlpoc\'\\n    model_path = os.path.join(model_path,model_name)\\n    if model_path is None:\\n        raise Exception(\'Parameter Model path is empty\')\\n    \\n    if text is None:\\n        raise Exception(\'Text Input is required\')\\n\\n    vect=joblib.load(model_path+\'/\'+\'vectorizer.pkl\')\\n    classify=joblib.load(model_path+\'/\'+\'classifier.pkl\')\\n    def pre_process_text(text):\\n            #If using stemming...\\n            #stemmer = PorterStemmer()\\n            textArray=text.split()\\n            wnl = WordNetLemmatizer()\\n            processed_text = []\\n            for text in textArray:\\n                words_list = (str(text).lower()).split()\\n                final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words(\'english\')]\\n                #If using stemming...\\n                #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words(\'english\')]\\n                final_words_str = str((\' \'.join(final_words)))\\n                processed_text.append(final_words_str)   \\n            return \' \'.join(processed_text)\\n    if isinstance(text,str):\\n        out_prob=classify.predict_proba(vect.transform([text]))\\n        out_prob=np.round(out_prob[0],decimals=4)\\n        clas_li=classify.classes_\\n        out_dict={}\\n        for i in range(0,len(clas_li)):\\n            out_dict[clas_li[i]]=out_prob[i]\\n        pred_class=classify.predict(vect.transform([text]))\\n        # return {\'bestClass\':pred_class[0],\'allClassProab\':out_dict}\\n        print(\'text\',text,\'label\',pred_class[0])\\n        return {\'text\':text,\'label\':pred_class[0]}\\n    elif isinstance(text,list):\\n        result_list=[]\\n        for sentence in text:\\n            out_prob=classify.predict_proba(vect.transform([sentence]))\\n            out_prob=np.round(out_prob[0],decimals=4)\\n            clas_li=classify.classes_\\n            out_dict={}\\n            for i in range(0,len(clas_li)):\\n                out_dict[clas_li[i]]=out_prob[i]\\n            pred_class=classify.predict(vect.transform([sentence]))\\n            result_list.append({\'text \':sentence,\'label\':pred_class[0]}) \\n        print(result_list)\\n        return result_list\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model_name\":\"test\",\"text\":\"text\"}}","DragNDropLite-29"
"DragNDropLite","Core","{\"formats\":{\"model_name\":\"text\",\"class_heading\":\"text\",\"test_ratio\":\"text\",\"text_heading\":\"text\"},\"classname\":\"MNB\",\"name\":\"MNB\",\"alias\":\"MNB\",\"parentCategory\":\"150\",\"id\":30,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import os\",\"import pickle\",\"import json\",\"import secrets\",\"import logging\",\"import numpy as np\",\"import pandas as pd\",\"from sklearn.pipeline import Pipeline\",\"from sklearn.preprocessing import LabelEncoder\",\"from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)\",\"from sklearn.naive_bayes import MultinomialNB\",\"from sklearn.model_selection import train_test_split\"],\"script\":\"\\ndef MNB_<id>(dataset, model_name_param=\'test\', class_heading_param=\'class\', text_heading_param=\'text\', test_ratio_param=0.1):\\n    df = pd.DataFrame(dataset)\\n    model_name=model_name_param\\n    test_ratio = test_ratio_param\\n    if model_name is None:\\n        raise Exception(\'Model name is empty\')\\n    save_path  = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n    save_path = os.path.join(save_path,model_name)\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    df = df.replace(\'\', np.nan)\\n    df = df.dropna()\\n    train_df, test_df = train_test_split(df, test_size=test_ratio, random_state=42)\\n    Train_X = train_df[text_heading_param]\\n    Train_Y = train_df[class_heading_param]\\n    label_list = list(df[class_heading_param].unique())\\n    encoder = LabelEncoder()\\n    encoder.fit(label_list)\\n    np.save(os.path.join(save_path, \'classes.npy\'), encoder.classes_)\\n\\n    clf_model = Pipeline([(\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'clf\', MultinomialNB())])\\n    clf_model = clf_model.fit(Train_X, Train_Y)\\n    pkl_filename = os.path.join(save_path, model_name+\'.pkl\')\\n    with open(pkl_filename, \'wb\') as file:\\n        pickle.dump(clf_model, file)\\n    return save_path\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model_name\":\"test\",\"class_heading\":\"class\",\"test_ratio\":0.1,\"text_heading\":\"text\"}}","DragNDropLite-30"
"DragNDropLite","Core","{\"formats\":{\"model_name\":\"text\",\"text\":\"text\"},\"classname\":\"MNB_predict\",\"name\":\"MNB_predict\",\"alias\":\"MNB Predict\",\"parentCategory\":\"150\",\"id\":31,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import os\",\"import pickle\"],\"script\":\"\\ndef MNB_predict_<id>(dataset, model_name_param=\'test\', text_param=\'text\'):\\n    model_name = model_name_param\\n    pklfilename = model_name+\'.pkl\'\\n    texts = text_param\\n    texts = [texts]\\n    model_path = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n    model_path = os.path.join(model_path,model_name,pklfilename)\\n    with open(model_path, \'rb\') as file:\\n        pickle_model = pickle.load(file)\\n    result = pickle_model.predict(texts).tolist()\\n    return result\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model_name\":\"test\",\"text\":\"text\"}}","DragNDropLite-31"
"DragNDropLite","Core","{\"formats\":{\"dropValues1\":[\"naive_bayes\",\"logistic_regression\",\"mnb\",\"svm\"],\"dropValues\":[\"classification\",\"regression\"],\"task\":\"dropValues\",\"model_name\":\"text\",\"model_type\":\"dropValues1\",\"text\":\"text\"},\"classname\":\"Model_Inference\",\"name\":\"Model_Inference\",\"alias\":\"Model_Inference\",\"parentCategory\":\"150\",\"id\":32,\"codeGeneration\":{\"requirements\":[\"scikit-learn\",\"nltk\"],\"imports\":[\"import pandas as pd\",\"import numpy as np\",\"from leaputils import FileServer\",\"import pickle, joblib\",\"import os,json,codecs,nltk\",\"from nltk.corpus import stopwords\",\"from nltk.stem.wordnet import WordNetLemmatizer\"],\"script\":\"\\ndef Model_Inference_<id>(dataset, download_path, task_param=[], model_name_param=\'text\', model_type_param=[], text_param=\'text\'):\\n    task = task_param\\n    model_type = model_type_param\\n    model_name = model_name_param\\n    text = text_param\\n    if task == \'regression\':\\n        if isinstance(dataset,str):\\n            dataset, download_path = download_path, dataset\\n        print(\'download_path: \', download_path)\\n    # load the model\\\\n    \\n        unpickler = pickle.Unpickler(open(download_path, \'rb\'))\\n        load_model = unpickler.load()\\n        print(load_model)\\n        pipe_pred = load_model.predict(dataset[\'X_test\'])\\n        print(pipe_pred)\\n        if isinstance(pipe_pred,tuple):\\n            pipe_pred, prediction_results = pipe_pred\\n            output=pd.DataFrame({\'Id\':dataset[\'dataset_id\']})\\n            output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\n            print(\'out\', output.to_dict(\'records\'))\\n        else:\\n            output=pd.DataFrame({\'Id\':dataset[\'dataset_id\'],\'Result\':pipe_pred})\\n            os.remove(download_path)\\n        return output.to_dict(\'records\')\\n    elif task == \'classification\':\\n        if model_type in [\'naive_bayes\',\'logistic_regression\']:\\n            model_path = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n            model_path = os.path.join(model_path,model_name)\\n            vect=joblib.load(model_path+\'/\'+\'vectorizer.pkl\')\\n            classify=joblib.load(model_path+\'/\'+\'classifier.pkl\')\\n\\n            def pre_process_text(text):\\n                    #If using stemming...\\n                    #stemmer = PorterStemmer()\\n                    textArray=text.split()\\n                    wnl = WordNetLemmatizer()\\n                    processed_text = []\\n                    for text in textArray:\\n                        words_list = (str(text).lower()).split()\\n                        final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words(\'english\')]\\n                        #If using stemming...\\n                        #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words(\'english\')]\\n                        final_words_str = str((\' \'.join(final_words)))\\n                        processed_text.append(final_words_str)   \\n                    return \' \'.join(processed_text)\\n            if isinstance(text,str):\\n                out_prob=classify.predict_proba(vect.transform([text]))\\n                out_prob=np.round(out_prob[0],decimals=4)\\n                clas_li=classify.classes_\\n                out_dict={}\\n                for i in range(0,len(clas_li)):\\n                    out_dict[clas_li[i]]=out_prob[i]\\n                pred_class=classify.predict(vect.transform([text]))\\n                # return {\'bestClass\':pred_class[0],\'allClassProab\':out_dict}\\n                print(\'text\',text,\'label\',pred_class[0])\\n                #return {\'text\':text,\'label\':pred_class[0]}\\n            elif isinstance(text,list):\\n                result_list=[]\\n                for sentence in text:\\n                    out_prob=classify.predict_proba(vect.transform([sentence]))\\n                    out_prob=np.round(out_prob[0],decimals=4)\\n                    clas_li=classify.classes_\\n                    out_dict={}\\n                    for i in range(0,len(clas_li)):\\n                        out_dict[clas_li[i]]=out_prob[i]\\n                    pred_class=classify.predict(vect.transform([sentence]))\\n                    result_list.append({\'text \':sentence,\'label\':pred_class[0]}) \\n                print(result_list)\\n                #return result_list\\n        elif algo.lower() in [\'mnb\',\'svm\']:\\n            model_path = os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n            model_path = os.path.join(model_path,model_name,model_name+\'.pkl\')\\n            texts = text_param\\n            texts = [texts]\\n            with open(model_path, \'rb\') as file:\\n                pickle_model = pickle.load(file)\\n            result = pickle_model.predict(texts).tolist()\\n            print(result)\\n\\n\\n\"},\"category\":\"Classification\",\"inputEndpoints\":[\"in\",\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"dropValues1\":\"\",\"dropValues\":\"\",\"task\":[],\"model_name\":\"text\",\"model_type\":[],\"text\":\"text\"}}","DragNDropLite-32"
"DragNDropLite","Core","{\"formats\":{\"val_path\":\"text\",\"classes\":\"text\",\"train_path\":\"text\",\"yml_name\":\"text\"},\"classname\":\"ObjDetectCustomDataYml\",\"name\":\"ObjDetectCustomDataYml\",\"alias\":\"ObjDetectCustomDataYml\",\"parentCategory\":\"154\",\"id\":37,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from ultralytics import YOLO\",\"import os\"],\"script\":\"def ObjDetectCustomDataYml_<id>(dataset, val_path_param=\'/app/jobs/models/cheque_detection_image/val\', classes_param=\'IssueBank,ReceiverName,AcNo,Amt,ChqNo,DateIss,Sign\', train_path_param=\'/app/jobs/models/cheque_detection_image/train\', yml_name_param=\'custom_data.yml\'):\\n    \\n    yml_name = yml_name_param     # custom_data.yml\\n    train_path = train_path_param # \'/app/jobs/models/legacy_AI/Object_detection/custom_data/train\'\\n    val_path = val_path_param     # \'/app/jobs/models/legacy_AI/Object_detection/custom_data/val\'\\n    classes = classes_param       # \'class1,class2,class3\'\\n\\n    yml_path= \'/app/jobs/models/cheque_detection_image/\'+yml_name\\n\\n    classes = classes.split(\',\')\\n    n_classes = len(classes)\\n\\n    L = [\\n        \'train : {0}\\\\n\'.format(train_path),\\n        \'val : {0}\\\\n\\\\n\'.format(val_path),\\n        \'nc : {0}\\\\n\'.format(n_classes),\\n        \'names : {0}\'.format(classes)\\n    ]\\n\\n    with open(yml_path,\'w\') as file1:\\n        file1.writelines(L)\\n        \\n    return yml_path\\n\\n\"},\"category\":\"Cheque_detection\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"val_path\":\"/app/jobs/models/cheque_detection_image/val\",\"classes\":\"IssueBank,ReceiverName,AcNo,Amt,ChqNo,DateIss,Sign\",\"train_path\":\"/app/jobs/models/cheque_detection_image/train\",\"yml_name\":\"custom_data.yml\"}}","DragNDropLite-37"
"DragNDropLite","Core","{\"formats\":{\"sentence\":\"text\",\"model_name\":\"text\",\"display_flag\":\"text\"},\"classname\":\"Semantic_Similarity_predict\",\"name\":\"Semantic_Similarity_predict\",\"alias\":\"Semantic Similarity predict\",\"parentCategory\":\"155\",\"id\":46,\"codeGeneration\":{\"requirements\":[\"tensorflow_hub\",\"protoc==3.1\",\"tensorflow\"],\"imports\":[\"import tensorflow_hub as hub\",\"import numpy as np\",\"import tensorflow as tf\",\"from tensorflow.python.keras import backend as K\"],\"script\":\"def Semantic_Similarity_predict_<id>(dataset, sentence_param=\'text\', model_name_param=\'text\', display_flag_param=\'text\'):\\n\\n\\n\\n    def calculate_score(embed,sentences_pair, display_flag):\\n        embeddings = embed(sentences_pair)\\n        correlation_matrix = np.inner(embeddings, embeddings)\\n        matrix = np.round_(correlation_matrix, 2)\\n        if len(sentences_pair) == 2 and display_flag == \'score\':\\n            return {\'score\':matrix[0,1]}\\n        return {\'score\':matrix}\\n    sess = K.get_session()\\n    sentence = sentence_param\\n    sentences=[i for i in sentence]\\n    #     sentences=[{\'\'}]\\n    if sentences is None:\\n        raise Exception(\'Sentence parameter is empty\')\\n    if not isinstance(sentences,list):\\n        raise Exception(f\'Expected sentences to be of type list, but received as {type(sentences)}\')\\n    display_flag = display_flag_param\\n    model_path = r\'/app/jobs/models/pretrained_models/universal-sentence-encoder-large_5\'\\n    #     if model_path is not None:\\n    #         embed = embed\\n    #         print(\'----------------\',embed)\\n    #     else:\\n    embed= hub.load(model_path)\\n    model_path, embed = model_path, embed\\n    if isinstance(sentences[0],list):\\n        __result = []\\n        for sentence_pair in sentences:\\n            __result.append(calculate_score(embed, sentence_pair, display_flag))\\n    else:\\n        __result = calculate_score(embed, sentences, display_flag)\\n    print(__result)\\n\\n    # except Exception as error:\\n    #     raise Exception(error)\\n\\n\"},\"category\":\"Semantic Similarity\",\"inputEndpoints\":[\"in\",\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"sentence\":\"text\",\"model_name\":\"text\",\"display_flag\":\"text\"}}","DragNDropLite-46"
"DragNDropLite","Core","{\"formats\":{\"max_features\":\"text\",\"model_name\":\"text\",\"max_depth\":\"text\",\"test_size\":\"text\",\"min_samples_split\":\"text\",\"vect_type\":\"text\",\"bootstrap\":\"text\",\"n_estimators\":\"text\",\"class_heading\":\"text\",\"min_samples_leaf\":\"text\",\"text_heading\":\"text\"},\"classname\":\"sentiment_randomforest\",\"name\":\"sentiment_randomforest\",\"alias\":\"sentiment_randomforest\",\"parentCategory\":\"156\",\"id\":47,\"codeGeneration\":{\"requirements\":[\"scikit-learn\"],\"imports\":[\"import os\",\"import pickle\",\"import json\",\"import secrets\",\"import logging\",\"import joblib\",\"import numpy as np\",\"import pandas as pd\",\"from collections import OrderedDict\",\"from nltk.corpus import stopwords\",\"from threading import Thread\",\"from sklearn import metrics\",\"from nltk.stem.wordnet import WordNetLemmatizer\",\"from sklearn.preprocessing import LabelEncoder\",\"from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)\",\"from sklearn.ensemble import RandomForestClassifier\",\"from sklearn.model_selection import GridSearchCV, train_test_split\",\"from sklearn.utils import shuffle\"],\"script\":\"def sentiment_randomforest_<id>(dataset, max_features_param=[\'sqrt\',\'auto\'], model_name_param=\'text1\', test_size_param=0.2, max_depth_param=[10,None], min_samples_split_param=[5], vect_type_param=None, n_estimators_param=[500,1000], bootstrap_param=True, class_heading_param=\'class\', min_samples_leaf_param=[1], text_heading_param=\'text\'):\\n    model_name= model_name_param\\n    vect_type =  vect_type_param\\n    test_size =  test_size_param\\n    max_features = max_features_param\\n    n_estimators  = n_estimators_param\\n    max_depth=    max_depth_param\\n    min_samples_split = min_samples_split_param\\n    min_samples_leaf = min_samples_leaf_param\\n    bootstrap= bootstrap_param\\n    text_heading= text_heading_param\\n    class_heading=class_heading_param\\n    \\n    \\n    model_path =  os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n    print(model_path)\\n    if model_path is None:\\n        raise Exception(\'Model path is a required parameter\')\\n    model_path = os.path.join(model_path,model_name)\\n\\n    if not os.path.isdir(model_path):\\n        os.makedirs(model_path)\\n\\n    if isinstance(max_features, str):\\n        max_features = (max_features)\\n    GridParameters = {\\n                \'max_features\': max_features,\\n                \'n_estimators\': n_estimators,\\n                \'max_depth\': max_depth,\\n                \'min_samples_split\': min_samples_split,\\n                \'min_samples_leaf\': min_samples_leaf,\\n                \'bootstrap\': [bootstrap]} \\n\\n    \\n    dataframe = pd.DataFrame(dataset)\\n\\n    if not all([True if i in dataframe.columns else False for i in [text_heading, class_heading]]):\\n        raise Exception(\'Column name mismatch.\')\\n\\n    if(vect_type==\'TFIDF\'):            \\n        vectorizer = TfidfVectorizer()\\n    else: \\n        vectorizer = CountVectorizer()             \\n    vectors = vectorizer.fit_transform(dataframe[text_heading])\\n    X_train, X_test, y_train, y_test = train_test_split(vectors, dataframe[class_heading], test_size=test_size, random_state=42)\\n    grid_search = GridSearchCV(RandomForestClassifier(),param_grid=GridParameters,cv=5,return_train_score=True,n_jobs=-1)\\n    grid_search.fit(X_train,y_train)  \\n    clf = RandomForestClassifier(max_features=grid_search.best_params_[\'max_features\'],\\n                                  max_depth=grid_search.best_params_[\'max_depth\'],\\n                                  n_estimators=grid_search.best_params_[\'n_estimators\'],\\n                                  min_samples_split=grid_search.best_params_[\'min_samples_split\'],\\n                                  min_samples_leaf=grid_search.best_params_[\'min_samples_leaf\'],\\n                                  bootstrap=grid_search.best_params_[\'bootstrap\'])\\n    clf.fit(X_train, y_train)    \\n    pred = clf.predict(X_test)\\n    f1_sc=metrics.accuracy_score(y_test, pred)\\n    joblib.dump(clf,model_path+\'/\'+\'classifier.pkl\')\\n    joblib.dump(vectorizer,model_path+\'/\'+\'vectorizer.pkl\')\\n\\n    print(\'model has been trained with accuracy of \',str(f1_sc))\\n\\n\"},\"category\":\"Sentiment analysis\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"max_features\":\"[\'sqrt\',\'auto\']\",\"model_name\":\"text1\",\"max_depth\":\"[10,None]\",\"test_size\":0.2,\"min_samples_split\":[5],\"vect_type\":\"None\",\"bootstrap\":\"True\",\"n_estimators\":\"[500,1000]\",\"class_heading\":\"class\",\"min_samples_leaf\":[1],\"text_heading\":\"text\"}}","DragNDropLite-47"
"DragNDropLite","Core","{\"formats\":{\"model_name\":\"text\",\"text\":\"text\"},\"classname\":\"random_forest_predict\",\"name\":\"random_forest_predict\",\"alias\":\"Random Forest Predict\",\"parentCategory\":\"156\",\"id\":48,\"codeGeneration\":{\"requirements\":[\"nltk\"],\"imports\":[\"import os, json, joblib\",\"import pandas as pd\",\"import codecs\",\"import numpy as np\",\"import nltk\",\"from nltk.corpus import stopwords\",\"from nltk.stem.wordnet import WordNetLemmatizer\"],\"script\":\"\\ndef random_forest_predict_<id>(dataset, model_name_param=\'test\', text_param=\'text\') :\\n    model_name = model_name_param\\n    text = text_param\\n    model_path=os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n    if model_path is None:\\n        raise Exception(\'Parameter Model path is empty\')\\n    \\n    if text is None:\\n        raise Exception(\'Text Input is required\')\\n\\n    vect=joblib.load(model_path+\'/\'+model_name+\'/\'+\'vectorizer.pkl\')\\n    classify=joblib.load(model_path+\'/\'+model_name+\'/\'+\'classifier.pkl\')\\n    def pre_process_text(text):\\n            #If using stemming...\\n            #stemmer = PorterStemmer()\\n            textArray=text.split()\\n            wnl = WordNetLemmatizer()\\n            processed_text = []\\n            for text in textArray:\\n                words_list = (str(text).lower()).split()\\n                final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words(\'english\')]\\n                #If using stemming...\\n                #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words(\'english\')]\\n                final_words_str = str((\' \'.join(final_words)))\\n                processed_text.append(final_words_str)   \\n            return \' \'.join(processed_text)\\n    if isinstance(text,str):\\n        out_prob=classify.predict_proba(vect.transform([text]))\\n        out_prob=np.round(out_prob[0],decimals=4)\\n        clas_li=classify.classes_\\n        out_dict={}\\n        for i in range(0,len(clas_li)):\\n            out_dict[clas_li[i]]=out_prob[i]\\n        pred_class=classify.predict(vect.transform([text]))\\n        # return {\'bestClass\':pred_class[0],\'allClassProab\':out_dict}\\n        print(\'text\',text,\'label\',pred_class[0])\\n        return {\'text\':text,\'label\':pred_class[0]}\\n    elif isinstance(text,list):\\n        result_list=[]\\n        for sentence in text:\\n            out_prob=classify.predict_proba(vect.transform([sentence]))\\n            out_prob=np.round(out_prob[0],decimals=4)\\n            clas_li=classify.classes_\\n            out_dict={}\\n            for i in range(0,len(clas_li)):\\n                out_dict[clas_li[i]]=out_prob[i]\\n            pred_class=classify.predict(vect.transform([sentence]))\\n            result_list.append({\'text \':sentence,\'label\':pred_class[0]}) \\n        print(result_list)\\n        return result_list\\n\\n\\n\"},\"category\":\"Sentiment analysis\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model_name\":\"test\",\"text\":\"text\"}}","DragNDropLite-48"
"DragNDropLite","Core","{\"formats\":{\"dropValues\":[\"bert\",\"xlnet\",\"random_forest\"],\"max_depth\":\"text\",\"test_size\":\"text\",\"min_samples_split\":\"text\",\"model_type\":\"dropValues\",\"bootstrap\":\"text\",\"n_estimators\":\"text\",\"class_heading\":\"text\",\"min_samples_leaf\":\"text\",\"test_ratio\":\"text\",\"text_heading\":\"text\",\"max_features\":\"text\",\"model_name\":\"text\",\"vect_type\":\"text\",\"text\":\"text\"},\"classname\":\"Sentiment_train\",\"name\":\"Sentiment_train\",\"alias\":\"Sentiment_train\",\"parentCategory\":\"156\",\"id\":50,\"codeGeneration\":{\"requirements\":[\"scikit-learn\",\"nltk\"],\"imports\":[\"import os\",\"import pickle\",\"import json\",\"import secrets\",\"import logging\",\"import joblib\",\"import numpy as np\",\"import pandas as pd\",\"from collections import OrderedDict\",\"from nltk.corpus import stopwords\",\"from threading import Thread\",\"from sklearn import metrics\",\"from nltk.stem.wordnet import WordNetLemmatizer\",\"from sklearn.preprocessing import LabelEncoder\",\"from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)\",\"from sklearn.ensemble import RandomForestClassifier\",\"from sklearn.model_selection import GridSearchCV, train_test_split\",\"from sklearn.utils import shuffle\"],\"script\":\"def Sentiment_train_<id>(dataset, test_size_param=0.2, max_depth_param=[10,None], min_samples_split_param=[5], model_type_param=[], n_estimators_param=[500,1000], bootstrap_param=True, class_heading_param=\'class\', min_samples_leaf_param=[1], text_heading_param=\'text\', test_ratio_param=0.1, max_features_param=[\'sqrt\',\'auto\'], model_name_param=\'test\', vect_type_param=None, text_param=\'text\'):\\n    model_name= model_type_param\\n    vect_type =  vect_type_param\\n    test_size =  test_size_param\\n    max_features = max_features_param\\n    n_estimators  = n_estimators_param\\n    max_depth=    max_depth_param\\n    min_samples_split = min_samples_split_param\\n    min_samples_leaf = min_samples_leaf_param\\n    bootstrap= bootstrap_param\\n    text_heading= text_heading_param\\n    class_heading=class_heading_param\\n    \\n    \\n    model_path =  os.path.join(os.environ[\'JOB_DIRECTORY\'],\'models\',\'classicmlpoc\')\\n    if model_path is None:\\n        raise Exception(\'Model path is a required parameter\')\\n    model_path = os.path.join(model_path,model_name)\\n\\n    if not os.path.isdir(model_path):\\n        os.makedirs(model_path)\\n\\n    if isinstance(max_features, str):\\n        max_features = (max_features)\\n    GridParameters = {\\n                \'max_features\': max_features,\\n                \'n_estimators\': n_estimators,\\n                \'max_depth\': max_depth,\\n                \'min_samples_split\': min_samples_split,\\n                \'min_samples_leaf\': min_samples_leaf,\\n                \'bootstrap\': [bootstrap]} \\n\\n    \\n    dataframe = pd.DataFrame(dataset)\\n\\n    if not all([True if i in dataframe.columns else False for i in [text_heading, class_heading]]):\\n        raise Exception(\'Column name mismatch.\')\\n\\n    if(vect_type==\'TFIDF\'):            \\n        vectorizer = TfidfVectorizer()\\n    else: \\n        vectorizer = CountVectorizer()             \\n    vectors = vectorizer.fit_transform(dataframe[text_heading])\\n    X_train, X_test, y_train, y_test = train_test_split(vectors, dataframe[class_heading], test_size=test_size, random_state=42)\\n    grid_search = GridSearchCV(RandomForestClassifier(),param_grid=GridParameters,cv=5,return_train_score=True,n_jobs=-1)\\n    grid_search.fit(X_train,y_train)  \\n    clf = RandomForestClassifier(max_features=grid_search.best_params_[\'max_features\'],\\n                                  max_depth=grid_search.best_params_[\'max_depth\'],\\n                                  n_estimators=grid_search.best_params_[\'n_estimators\'],\\n                                  min_samples_split=grid_search.best_params_[\'min_samples_split\'],\\n                                  min_samples_leaf=grid_search.best_params_[\'min_samples_leaf\'],\\n                                  bootstrap=grid_search.best_params_[\'bootstrap\'])\\n    clf.fit(X_train, y_train)    \\n    pred = clf.predict(X_test)\\n    f1_sc=metrics.accuracy_score(y_test, pred)\\n    joblib.dump(clf,model_path+\'/\'+\'classifier.pkl\')\\n    joblib.dump(vectorizer,model_path+\'/\'+\'vectorizer.pkl\')\\n\\n    print(\'model has been trained with accuracy of \',str(f1_sc))\\n\\n\"},\"category\":\"Sentiment analysis\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"dropValues\":\"\",\"max_depth\":\"[10,None]\",\"test_size\":0.2,\"min_samples_split\":[5],\"model_type\":[],\"bootstrap\":\"True\",\"n_estimators\":\"[500,1000]\",\"class_heading\":\"class\",\"min_samples_leaf\":[1],\"test_ratio\":0.1,\"text_heading\":\"text\",\"max_features\":\"[\'sqrt\',\'auto\']\",\"model_name\":\"test\",\"vect_type\":\"None\",\"text\":\"text\"}}","DragNDropLite-50"
"DragNDropLite","Core","{\"formats\":{\"dropValues\":[\"bert\",\"xlnet\",\"random_forest\"],\"batch_size\":\"text\",\"max_depth\":\"text\",\"test_size\":\"text\",\"min_samples_split\":\"text\",\"model_type\":\"dropValues\",\"__class_heading\":\"text\",\"bootstrap\":\"text\",\"n_estimators\":\"text\",\"num_workers\":\"text\",\"class_heading\":\"text\",\"num_labels\":\"text\",\"min_samples_leaf\":\"text\",\"test_ratio\":\"text\",\"text_heading\":\"text\",\"train_test_ratio\":\"text\",\"max_features\":\"text\",\"model_name\":\"text\",\"vect_type\":\"text\",\"max_seq_length\":\"text\",\"text\":\"text\",\"learning_rate\":\"text\",\"__text_heading\":\"text\"},\"classname\":\"Bert_Sentiment_train\",\"name\":\"Bert_Sentiment_train\",\"alias\":\"Bert Sentiment_train\",\"parentCategory\":\"156\",\"id\":51,\"codeGeneration\":{\"requirements\":[\"scikit-learn\",\"nltk\",\"lime\",\"progressbar2\",\"tensorflow\",\"imgkit\"],\"imports\":[\"import imgkit\",\"import lime\",\"import tensorflow\",\"import transformers\",\"from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW,get_linear_schedule_with_warmup,BertTokenizer,BertForSequenceClassification\",\"import torch\",\"from joblib import parallel_backend\",\"from joblib import Parallel, delayed\",\"import os\",\"import json\",\"import numpy as np\",\"import pandas as pd\",\"from sklearn.model_selection import train_test_split\",\"from sklearn.metrics import confusion_matrix, classification_report\",\"import sklearn\",\"from sklearn import metrics\",\"from sklearn.utils import shuffle\",\"from torch import nn, optim\",\"from tensorflow.keras.preprocessing.sequence import pad_sequences\",\"from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\",\"from torch.utils.data import Dataset, DataLoader\",\"import torch.nn.functional as F\",\"import logging, glob, imgkit, shutil, tempfile\",\"from threading import Thread\",\"from collections import defaultdict\",\"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\",\"import time\",\"import progressbar2\",\"widgets = [\' [\',progressbar.Timer(format= \'elapsed time: %(elapsed)s\'),\'] \',progressbar.Bar(\'*\'),\' (\',progressbar.ETA(), \') \', ]\",\"import math\",\"from lime.lime_text import LimeTextExplainer\",\"import joblib\",\"import codecs\",\"import nltk\",\"from nltk.corpus import stopwords\",\"from nltk.stem.wordnet import WordNetLemmatizer\",\"import pickle\",\"import secrets\",\"from collections import OrderedDict\",\"from threading import Thread\",\"from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)\",\"from sklearn.ensemble import RandomForestClassifier\",\"from sklearn.model_selection import GridSearchCV, train_test_split\",\"from sklearn.utils import shuffle\"],\"script\":\"def Bert_Sentiment_train_<id>(dataset, batch_size_param=8, test_size_param=0.2, max_depth_param=[10,None], min_samples_split_param=[5], model_type_param=[], __class_heading_param=\'class\', n_estimators_param=[500,1000], bootstrap_param=True, num_workers_param=4, class_heading_param=\'class\', num_labels_param=2, min_samples_leaf_param=[1], train_test_ratio_param=0.15, text_heading_param=\'text\', test_ratio_param=0.1, max_features_param=[\'sqrt\',\'auto\'], model_name_param=\'test\', vect_type_param=None, max_seq_length_param=512, text_param=\'text\', learning_rate_param=float(3e-5), __text_heading_param=\'text\'):\\n    class CustomDataset(Dataset):\\n\\n        def __init__(self, texts, labels, tokenizer, max_len):\\n            self.texts = texts\\n            self.labels = labels\\n            self.tokenizer = tokenizer\\n            self.max_len = max_len\\n\\n        def __len__(self):\\n            return len(self.texts)\\n\\n        def __getitem__(self, item):\\n            texts = str(self.texts[item])\\n            labels = self.labels[item]\\n\\n            encoding = self.tokenizer.encode_plus(\\n            texts,\\n            add_special_tokens=True,\\n            max_length=self.max_len,\\n            return_token_type_ids=False,\\n            pad_to_max_length=False,\\n            return_attention_mask=True,\\n            return_tensors=\'pt\',\\n            truncation = True\\n            )\\n\\n            input_ids = pad_sequences(encoding[\'input_ids\'], maxlen=self.max_len, dtype=torch.Tensor ,truncating=\'post\',padding=\'post\')\\n            input_ids = input_ids.astype(dtype = \'int64\')\\n            input_ids = torch.tensor(input_ids) \\n\\n            attention_mask = pad_sequences(encoding[\'attention_mask\'], maxlen=self.max_len, dtype=torch.Tensor ,truncating=\'post\',padding=\'post\')\\n            attention_mask = attention_mask.astype(dtype = \'int64\')\\n            attention_mask = torch.tensor(attention_mask)       \\n\\n            return {\\n            \'texts\': texts,\\n            \'input_ids\': input_ids,\\n            \'attention_mask\': attention_mask.flatten(),\\n            \'labels\': torch.tensor(labels, dtype=torch.long)\\n            }\\n    def create_data_loader(df, tokenizer, max_len, batch_size, num_workers):\\n        ds = CustomDataset(\\n            texts=np.array(df[__text_heading]),\\n            labels=np.array(df[__class_heading]),\\n            tokenizer=tokenizer,\\n            max_len=max_len\\n        )\\n\\n        return DataLoader(\\n            ds,\\n            batch_size=batch_size,\\n            num_workers=num_workers)\\n    def log_training( path, text):\\n            if not os.path.isdir(path):\\n                os.makedirs(path)\\n            with open(os.path.join(path,\'training_logs.txt\'),\'a+\') as f:\\n                f.write(text)\\n    def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples,max_len,batch_size):\\n            model = model.train()\\n            losses = []\\n            acc = 0\\n            counter = 0\\n\\n            bar = progressbar.ProgressBar(max_value=math.ceil(len(data_loader.dataset)/batch_size), widgets=widgets).start() \\n\\n            for d in data_loader:\\n                input_ids = d[\'input_ids\'].reshape(-1,max_len).to(device)\\n                attention_mask = d[\'attention_mask\'].to(device)\\n                targets = d[\'labels\'].to(device)\\n                outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\\n                loss = outputs[0]\\n                logits = outputs[1]\\n                # preds = preds.cpu().detach().numpy()\\n                _, prediction = torch.max(logits, dim=1)\\n                targets = targets.cpu().detach().numpy()\\n                prediction = prediction.cpu().detach().numpy()\\n                accuracy = metrics.accuracy_score(targets, prediction)\\n\\n                acc += accuracy\\n                losses.append(loss.item())\\n\\n                loss.backward()\\n\\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n                optimizer.step()\\n                scheduler.step()\\n                optimizer.zero_grad()\\n                counter = counter + 1\\n                bar.update(counter)\\n            return acc / counter, np.mean(losses)\\n\\n\\n    model_path =  r\'/app/jobs/models/classification\'\\n    pretrained_model_path =r\'/app/jobs/models/pretrained_models/BERT_sst5\'\\n\\n    model_name = model_type_param\\n    # learning_rate = float(3e-5)\\n    learning_rate = learning_rate_param\\n    __text_heading=__text_heading_param\\n    __class_heading=__class_heading_param\\n    max_seq_length =max_seq_length_param\\n    batch_size =batch_size_param\\n    num_workers =num_workers_param\\n    train_test_ratio =  train_test_ratio_param\\n    num_labels = num_labels_param\\n\\n    if model_name is None:\\n        raise Exception(\'Model name is undefined\')\\n\\n    model_save_path = os.path.join(model_path, model_name)\\n    if not os.path.isdir(model_save_path):\\n        os.makedirs(model_save_path)\\n    print(model_save_path,\'Training started \\\\n\')\\n    if file_path is None:\\n        raise Exception(\'File path cannot be empty\')\\n    check_point_dir =None\\n    if check_point_dir is not None:\\n        check_point_dir = os.path.join(check_point_dir,model_name)\\n        if not os.path.isdir(check_point_dir):\\n            os.makedirs(check_point_dir)\\n    # _, extension = os.path.splitext(file_path)\\n    # if extension == \'.csv\':\\n    #     __df  = pd.read_csv(file_path, index_col=None)\\n    # elif extension in [\'.xlsx\', \'.xls\']:\\n    #     __df = pd.read_excel(file_path, index_col=None)\\n    # else:\\n    #     raise Exception(\'File type not supported. Send a csv or xlsx file to train\')\\n\\n    __df=pd.DataFrame(dataset)\\n\\n    # self.__text_heading = parameters_dictionary.get(\'text_heading\',\'text\')\\n    # self.__class_heading = parameters_dictionary.get(\'class_heading\',\'class\')\\n    train_test_ratio =  0.15\\n    num_labels = 2\\n\\n    with open(os.path.join(model_save_path,model_name+\'_metadata.json\'), \'w\') as outfile: \\n        json.dump(model_details,outfile)\\n\\n    # model, tokenizer = load_train_model(base_model, num_labels, parameters_dictionary)\\n    model_tokenizer =  BertTokenizer\\n    model = BertForSequenceClassification\\n    tokenizer = getattr(model_tokenizer,\'from_pretrained\')(pretrained_model_path)\\n    model = getattr(model, \'from_pretrained\')(pretrained_model_path, num_labels = num_labels,ignore_mismatched_sizes=True)\\n    model.train()\\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\\n    model = model.to(device)\\n\\n    le = LabelEncoder()\\n    df=__df\\n    __text_heading=\'text\'\\n    __class_heading=\'class\'\\n    df[__class_heading] = le.fit_transform(__df[__class_heading])\\n    try:\\n        np.save(os.path.join(model_save_path,\'classes.npy\'), le.classes_)\\n    except Exception as e:\\n        print(exception(e))\\n    df = shuffle(df)\\n\\n    df_train, df_val = train_test_split(df, test_size=train_test_ratio, random_state=101)\\n    train_data_loader = create_data_loader(df_train, tokenizer, max_seq_length, batch_size, num_workers=0)\\n    val_data_loader = create_data_loader(df_val, tokenizer, max_seq_length, batch_size, num_workers=0)\\n    param_optimizer = list(model.named_parameters())\\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\\n    optimizer_grouped_parameters = [\\n                                    {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\\n                                    {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\':0.0}\\n    ]\\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\\n\\n    total_steps = len(train_data_loader) * epochs\\n\\n    scheduler = get_linear_schedule_with_warmup(\\n    optimizer,\\n    num_warmup_steps=0,\\n    num_training_steps=total_steps\\n    )\\n    history = defaultdict(list)\\n    best_accuracy = -1\\n    for epoch in range(epochs):\\n        print(f\'Epoch {epoch + 1}/{epochs}\')\\n        print( \'-\' * 10)\\n        log_training(model_save_path,f\'Epoch {epoch + 1}/{epochs} \\\\n\'+\'-\'*10+\'\\\\n\')\\n        train_acc, train_loss = train_epoch(\\n            model,\\n            train_data_loader,     \\n            optimizer, \\n            device, \\n            scheduler, \\n            len(df_train),\\n            max_len = max_seq_length,\\n            batch_size = batch_size\\n        )\\n\\n        print(f\'Train loss {train_loss} Train accuracy {train_acc}\')\\n        log_training(model_save_path,f\'Train loss {train_loss} Train accuracy {train_acc} \\\\n\')\\n\\n        val_acc, val_loss = eval_model(\\n            model,\\n            val_data_loader, \\n            device, \\n            len(df_val),\\n            max_len = max_seq_length,\\n            batch_size = batch_size\\n        )\\n\\n        print(f\'Val loss {val_loss} Val accuracy {val_acc}\')\\n        log_training(model_save_path,f\'Val loss {val_loss} Val accuracy {val_acc} \\\\n\')\\n\\n        history[\'train_acc\'].append(train_acc)\\n        history[\'train_loss\'].append(train_loss)\\n        history[\'val_acc\'].append(val_acc)\\n        history[\'val_loss\'].append(val_loss)\\n        if val_acc > best_accuracy:\\n            # torch.save(model.state_dict(), os.path.join(model_save_path, str(model_name) + \'.bin\'))\\n            model.save_pretrained(model_save_path)\\n            tokenizer.save_pretrained(model_save_path)\\n            model_details.update({\'history\': history, \'epoch\': epoch})\\n            model_details(model_save_path,model_name, \'modify\',{\'history\': history, \'epoch\': epoch, \'status\': f\'Trained {epoch}\', \'accuracy\': val_acc})\\n            best_accuracy = val_acc\\n\\n    model_details(model_save_path,model_name, \'modify\',{\'status\':\'Trained\'})\\n    print(\'MODEL TRAINED\')\\n    log_training(model_save_path,\'Model Trained \\\\n\')\\n\\n\"},\"category\":\"Sentiment analysis\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"dropValues\":\"\",\"batch_size\":8,\"max_depth\":\"[10,None]\",\"test_size\":0.2,\"min_samples_split\":[5],\"model_type\":[],\"__class_heading\":\"class\",\"bootstrap\":\"True\",\"n_estimators\":\"[500,1000]\",\"num_workers\":4,\"class_heading\":\"class\",\"num_labels\":2,\"min_samples_leaf\":[1],\"test_ratio\":0.1,\"text_heading\":\"text\",\"train_test_ratio\":0.15,\"max_features\":\"[\'sqrt\',\'auto\']\",\"model_name\":\"test\",\"vect_type\":\"None\",\"max_seq_length\":512,\"text\":\"text\",\"learning_rate\":\"float(3e-5)\",\"__text_heading\":\"text\"}}","DragNDropLite-51"
"DragNDropLite","Core","{\"formats\":{\"Select target feature\":\"text\",\"Select the feature required\":\"text\",\"Enter reference value\":\"text\"},\"classname\":\"DataDivision\",\"name\":\"DataDivision\",\"alias\":\"DataDivision\",\"parentCategory\":\"157\",\"id\":54,\"codeGeneration\":{\"requirements\":[\"whylogs\"],\"imports\":[\"import pandas as pd\",\"import whylogs as why\"],\"script\":\"\\ndef DataDivision_<id>(dataset, select_the_feature_required_param=[], select_target_feature_param=[], enter_reference_value_param=[]):\\n    print(\'dataset====\', dataset)\\n    wine = pd.DataFrame(dataset)\\n    cond_reference = (wine[select_the_feature_required_param]<=11)\\n    wine_reference = wine.loc[cond_reference]\\n\\n    cond_target = (wine[select_the_feature_required_param]>11)\\n    wine_target = wine.loc[cond_target]\\n\\n    \\n    result = why.log(pandas=wine_target)\\n    prof_view = result.view()\\n\\n    result_ref = why.log(pandas=wine_reference)\\n    prof_view_ref = result_ref.view()\\n\\n    return {\'prof_view\': prof_view, \'prof_view_ref\': prof_view_ref}\\n\\n\\n\"},\"category\":\"Data Profiling\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"model\"],\"attributes\":{\"Select target feature\":[],\"Select the feature required\":[],\"Enter reference value\":[]}}","DragNDropLite-54"
"DragNDropLite","Core","{\"formats\":{\"Generate the Drift Summary\":\"checkbox\",\"Select the feature required for histogram\":\"text\"},\"classname\":\"DriftReport\",\"name\":\"DriftReport\",\"alias\":\"DriftReport\",\"parentCategory\":\"157\",\"id\":55,\"codeGeneration\":{\"requirements\":[\"scikit-learn\",\"Ipython==7.34.0\",\"pybars3\",\"matplotlib\"],\"imports\":[\"import html\",\"import matplotlib.pyplot as plt\",\"import json\",\"import logging\",\"import os\",\"from typing import Any, Dict, List, Optional\",\"from IPython.core.display import HTML\",\"import whylogs.viz.drift.column_drift_algorithms as column_drift_algorithms\",\"from whylogs.api.usage_stats import emit_usage\",\"from whylogs.core.configs import SummaryConfig\",\"from whylogs.core.constraints import Constraints\",\"from whylogs.core.view.dataset_profile_view import DatasetProfileView\",\"from whylogs.migration.uncompound import _uncompound_dataset_profile\",\"from whylogs.viz.enums.enums import PageSpec, PageSpecEnum\",\"from whylogs.viz.utils.frequent_items_calculations import zero_padding_frequent_items\",\"from whylogs.viz.utils.html_template_utils import _get_compiled_template\",\"from whylogs.viz.utils.profile_viz_calculations import add_feature_statistics\",\"from whylogs.viz.utils.profile_viz_calculations import frequent_items_from_view\",\"from whylogs.viz.utils.profile_viz_calculations import generate_profile_summary\",\"from whylogs.viz.utils.profile_viz_calculations import generate_summaries_with_drift_score\",\"from whylogs.viz.utils.profile_viz_calculations import histogram_from_view\"],\"script\":\"class NotebookProfileVisualizerCustom:\\n    \'\'\'\\n    Visualize and compare profiles for drift detection, data quality, distribution comparison and feature statistics.\\n    NotebookProfileVisualizer enables visualization features for Jupyter Notebook environments, but also enables\\n    download\\n    of the generated reports as HTML files.\\n    Examples\\n    --------\\n    Create target and reference dataframes:\\n    .. code-block:: python\\n        import pandas as pd\\n        data_target = {\\n            \'animal\': [\'cat\', \'hawk\', \'snake\', \'cat\', \'snake\', \'cat\', \'cat\', \'snake\', \'hawk\',\'cat\'],\\n            \'legs\': [4, 2, 0, 4, 0, 4, 4, 0, 2, 4],\\n            \'weight\': [4.3, None, 2.3, 7.8, 3.7, 2.5, 5.5, 3.3, 0.6, 13.3],\\n        }\\n        data_reference = {\\n            \'animal\': [\'hawk\', \'hawk\', \'snake\', \'hawk\', \'snake\', \'snake\', \'cat\', \'snake\', \'hawk\',\'snake\'],\\n            \'legs\': [2, 2, 0, 2, 0, 0, 4, 0, 2, 0],\\n            \'weight\': [2.7, None, 1.2, 10.5, 2.2, 4.6, 3.8, 4.7, 0.6, 11.2],\\n        }\\n        target_df = pd.DataFrame(data_target)\\n        reference_df = pd.DataFrame(data_reference)\\n    Log data and create profile views:\\n    .. code-block:: python\\n        import whylogs as why\\n        results = why.log(pandas=target_df)\\n        prof_view = results.view()\\n        results_ref = why.log(pandas=reference_df)\\n        prof_view_ref = results_ref.view()\\n    Log data and create profile views:\\n    .. code-block:: python\\n        import whylogs as why\\n        results = why.log(pandas=target_df)\\n        prof_view = results.view()\\n        results_ref = why.log(pandas=reference_df)\\n        prof_view_ref = results_ref.view()\\n    Instantiate and set profile views:\\n    .. code-block:: python\\n        from whylogs.viz import NotebookProfileVisualizer\\n        visualization = NotebookProfileVisualizer()\\n        visualization.set_profiles(target_profile_view=prof_view,reference_profile_view=prof_view_ref)\\n    \'\'\'\\n\\n    _ref_view: Optional[DatasetProfileView]\\n    _target_view: DatasetProfileView\\n    _drift_map: Optional[Dict[str, column_drift_algorithms.ColumnDriftAlgorithm]] = None\\n\\n    @staticmethod\\n    def _display(template: str, page_spec: PageSpec, height: Optional[str]) -> \'HTML\':\\n        if not height:\\n            height = page_spec.height\\n        iframe = f\'\'\'<div></div><iframe srcdoc=\'{html.escape(template)}\' width=100% height={height}\\n        frameBorder=0></iframe>\'\'\'\\n        display = HTML(iframe)\\n        return display\\n\\n    def _display_distribution_chart(\\n        self,\\n        feature_name: str,\\n        difference: bool,\\n        cell_height: Optional[str] = None,\\n        config: Optional[SummaryConfig] = None,\\n    ) -> Optional[HTML]:\\n        if config is None:\\n            config = SummaryConfig()\\n        if difference:\\n            page_spec = PageSpecEnum.DIFFERENCED_CHART.value\\n        else:\\n            page_spec = PageSpecEnum.DISTRIBUTION_CHART.value\\n\\n        template = _get_compiled_template(page_spec.html)\\n        if self._target_view:\\n            target_profile_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n            reference_profile_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n\\n            target_column_profile_view = self._target_view.get_column(feature_name)\\n            if not target_column_profile_view:\\n                raise ValueError(\'ColumnProfileView for feature {} not found.\'.format(feature_name))\\n\\n            target_profile_features[feature_name][\'frequentItems\'] = frequent_items_from_view(\\n                target_column_profile_view, feature_name, config\\n            )\\n            if self._ref_view:\\n                ref_col_view = self._ref_view.get_column(feature_name)\\n                if not ref_col_view:\\n                    raise ValueError(\'ColumnProfileView for feature {} not found.\'.format(feature_name))\\n\\n                reference_profile_features[feature_name][\'frequentItems\'] = frequent_items_from_view(\\n                    ref_col_view, feature_name, config\\n                )\\n\\n                (\\n                    target_profile_features[feature_name][\'frequentItems\'],\\n                    reference_profile_features[feature_name][\'frequentItems\'],\\n                ) = zero_padding_frequent_items(\\n                    target_feature_items=target_profile_features[feature_name][\'frequentItems\'],\\n                    reference_feature_items=reference_profile_features[feature_name][\'frequentItems\'],\\n                )\\n            else:\\n                logger.warning(\'Reference profile not detected. Plotting only for target feature.\')\\n                reference_profile_features[feature_name][\'frequentItems\'] = [\\n                    {\'value\': x[\'value\'], \'estimate\': 0} for x in target_profile_features[feature_name][\'frequentItems\']\\n                ]  # Getting the same frequent items categories for target and adding 0 as estimate.\\n            distribution_chart = template(\\n                {\\n                    \'profile_from_whylogs\': json.dumps(target_profile_features),\\n                    \'reference_profile_from_whylogs\': json.dumps(reference_profile_features),\\n                }\\n            )\\n            result = self._display(distribution_chart, page_spec, cell_height)\\n            return result\\n\\n        else:\\n            logger.warning(\'This method has to get at least a target profile, with valid feature title\')\\n            return None\\n\\n    def _display_histogram_chart(self, feature_name: str, cell_height: Optional[str] = None) -> Optional[HTML]:\\n        page_spec = PageSpecEnum.DOUBLE_HISTOGRAM.value\\n        template = _get_compiled_template(page_spec.html)\\n        if self._target_view:\\n            target_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n            ref_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n\\n            target_col_view = self._target_view.get_column(feature_name)\\n            if not target_col_view:\\n                raise ValueError(f\'ColumnProfileView for feature {feature_name} not found.\')\\n\\n            target_histogram = histogram_from_view(target_col_view, feature_name)\\n            if self._ref_view:\\n                reference_column_profile_view = self._ref_view.get_column(feature_name)\\n                if not reference_column_profile_view:\\n                    raise ValueError(\'ColumnProfileView for feature {} not found.\'.format(feature_name))\\n                ref_histogram = histogram_from_view(reference_column_profile_view, feature_name)\\n            else:\\n                logger.warning(\'Reference profile not detected. Plotting only for target feature.\')\\n                ref_histogram = target_histogram.copy()\\n                ref_histogram[\'counts\'] = [\\n                    0 for _ in ref_histogram[\'counts\']\\n                ]  # To plot single profile, zero counts for non-existing profile.\\n\\n            ref_features[feature_name][\'histogram\'] = ref_histogram\\n            target_features[feature_name][\'histogram\'] = target_histogram\\n            if target_histogram[\'n\'] == 1:\\n                # in the degenerate case when the target is a single value, it will be hidden\\n                # so here we draw a vertical line, using the max (which is the observed value)\\n                target_features[feature_name][\'vertical_line\'] = target_histogram[\'max\']\\n            histogram_chart = template(\\n                {\\n                    \'profile_from_whylogs\': json.dumps(target_features),\\n                    \'reference_profile_from_whylogs\': json.dumps(ref_features),\\n                }\\n            )\\n            values = ref_features[feature_name][\'histogram\'][\'bins\'][:-1]\\n\\n            # Create histogram\\n            plt.figure(figsize=(12,4))\\n            plt.hist(values, bins=len(ref_features[feature_name][\'histogram\'][\'bins\'][:-1]), weights=ref_features[feature_name][\'histogram\'][\'counts\'], range=(ref_features[feature_name][\'histogram\'][\'start\'], ref_features[feature_name][\'histogram\'][\'end\']), rwidth=0.9)\\n            plt.xlabel(\'Value\')\\n            plt.ylabel(\'Frequency\')\\n            plt.title(\'Histogram\')\\n            plt.savefig(f\'/app/jobs/models/whylogs/{feature_name}_ref.png\')\\n\\n            values = target_features[feature_name][\'histogram\'][\'bins\'][:-1]\\n\\n            # Create histogram\\n            plt.figure(figsize=(12,4))\\n            plt.hist(values, bins=len(target_features[feature_name][\'histogram\'][\'bins\'][:-1]), weights=target_features[feature_name][\'histogram\'][\'counts\'], range=(target_features[feature_name][\'histogram\'][\'start\'], target_features[feature_name][\'histogram\'][\'end\']), rwidth=0.9)\\n            plt.xlabel(\'Value\')\\n            plt.ylabel(\'Frequency\')\\n            plt.title(\'Histogram\')\\n            plt.savefig(f\'/app/jobs/models/whylogs/{feature_name}_target.png\')\\n\\n            # with open (\'./target_features.json\', \'w\') as file:\\n            #   json.dump(target_features, file)\\n            # with open (\'./ref_features.json\', \'w\') as file:\\n            #   json.dump(ref_features, file)\\n            # print(\'target_features\', target_features)\\n            # print(\'ref_features\',ref_features)\\n\\n            return self._display(histogram_chart, page_spec, height=cell_height)\\n        else:\\n            logger.warning(\'This method has to get at least a target profile, with valid feature title\')\\n            return None\\n\\n    def add_drift_config(\\n        self, column_names: List[str], algorithm: column_drift_algorithms.ColumnDriftAlgorithm\\n    ) -> None:\\n        \'\'\'Add drift configuration.\\n        The algorithms and thresholds added through this method will be used to calculate drift scores in the summary_drift_report() method.\\n        If any drift configuration exists, the new configuration will overwrite the standard behavior when appliable.\\n        If a column has multiple configurations defined, the last one defined will be used.\\n        Parameters\\n        ----------\\n        config: DriftConfig, required\\n            Drift configuration.\\n        \'\'\'\\n        self._drift_map = {} if not self._drift_map else self._drift_map\\n        if not isinstance(algorithm, column_drift_algorithms.ColumnDriftAlgorithm):\\n            raise ValueError(\'Algorithm must be of class ColumnDriftAlgorithm.\')\\n        if not self._target_view or not self._ref_view:\\n            logger.error(\'Set target and reference profiles before adding drift configuration.\')\\n            raise ValueError\\n        if not algorithm:\\n            raise ValueError(\'Drift algorithm cannot be None.\')\\n        if not column_names:\\n            raise ValueError(\'Drift configuration must have at least one column name.\')\\n        if column_names:\\n            for column_name in column_names:\\n                if column_name not in self._target_view.get_columns().keys():\\n                    raise ValueError(f\'Column {column_name} not found in target profile.\')\\n                if column_name not in self._target_view.get_columns().keys():\\n                    raise ValueError(f\'Column {column_name} not found in reference profile.\')\\n        for column_name in column_names:\\n            if column_name in self._drift_map:\\n                logger.warning(f\'Overwriting existing drift configuration for column {column_name}.\')\\n            self._drift_map[column_name] = algorithm\\n\\n    def set_profiles(\\n        self, target_profile_view: DatasetProfileView, reference_profile_view: Optional[DatasetProfileView] = None\\n    ) -> None:\\n        \'\'\'Set profiles for Visualization/Comparison.\\n        Drift calculation is done if both target_profile and reference profile are passed.\\n        Parameters\\n        ----------\\n        target_profile_view: DatasetProfileView, required\\n            Target profile to visualize.\\n        reference_profile_view: DatasetProfileView, optional\\n            Reference, or baseline, profile to be compared against the target profile.\\n        \'\'\'\\n        self._target_view = _uncompound_dataset_profile(target_profile_view) if target_profile_view else None\\n        self._ref_view = _uncompound_dataset_profile(reference_profile_view) if reference_profile_view else None\\n\\n    def profile_summary(self, cell_height: Optional[str] = None) -> HTML:\\n        page_spec = PageSpecEnum.PROFILE_SUMMARY.value\\n        template = _get_compiled_template(page_spec.html)\\n\\n        try:\\n            profile_summary = generate_profile_summary(self._target_view, config=None)\\n            rendered_template = template(profile_summary)\\n            return self._display(rendered_template, page_spec, cell_height)\\n        except ValueError as e:\\n            logger.error(\'This method has to get target Dataset Profile View\')\\n            raise e\\n\\n    def summary_drift_report(self, height: Optional[str] = None) -> HTML:\\n        \'\'\'Generate drift report between target and reference profiles.\\n        KS is calculated if distribution metrics exists for said column.\\n        If not, Chi2 is calculated if frequent items, cardinality and count metric exists. If not, then no drift value is associated to the column.\\n        If feature is missing from any profile, it will not be included in the report.\\n        Both target_profile_view and reference_profile_view must be set previously with set_profiles.\\n        If custom drift behavior is desired, use add_drift_config before calling this method.\\n        Parameters\\n        ----------\\n        height: str, optional\\n            Preferred height, in pixels, for in-notebook visualization. Example:\\n            \'1000px\'. (Default is None)\\n        Returns\\n        -------\\n        HTML\\n            HTML Page of the given plot.\\n        Examples\\n        --------\\n        Generate Summary Drift Report (after setting profiles with set_profiles):\\n        .. code-block:: python\\n            from whylogs.viz.drift.column_drift_algorithms import Hellinger, ChiSquare\\n            from whylogs.viz import NotebookProfileVisualizer\\n            visualization = NotebookProfileVisualizer()\\n            visualization.set_profiles(target_profile_view=target_view, reference_profile_view=ref_view)\\n            visualization.add_drift_config(column_names=[\'weight\'], algorithm=Hellinger())\\n            visualization.add_drift_config(column_names=[\'legs\'], algorithm=ChiSquare())\\n            visualization.summary_drift_report()\\n        \'\'\'\\n        if not self._target_view or not self._ref_view:\\n            logger.error(\'This method has to get both target and reference profiles\')\\n            raise ValueError\\n        page_spec = PageSpecEnum.SUMMARY_REPORT.value\\n        template = _get_compiled_template(page_spec.html)\\n\\n        profiles_summary = generate_summaries_with_drift_score(\\n            self._target_view, self._ref_view, config=None, drift_map=self._drift_map\\n        )\\n        rendered_template = template(profiles_summary)\\n        summary_drift_report = self._display(rendered_template, page_spec, height)\\n        return summary_drift_report\\n\\n    def double_histogram(self, feature_name: str, cell_height: Optional[str] = None) -> HTML:\\n        \'\'\'Plot overlayed histograms for specified feature present in both target_profile and reference_profile.\\n        Applicable to numerical features only.\\n        If reference profile was not set, double_histogram will plot single histogram for target profile.\\n        Parameters\\n        ----------\\n        feature_name: str\\n            Name of the feature to generate histograms.\\n        cell_height: str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            \'1000px\'. (Default is None)\\n        Examples\\n        --------\\n        Generate double histogram plot for feature named weight (after setting profiles with set_profiles)\\n        .. code-block:: python\\n            visualization.double_histogram(feature_name=\'weight\')\\n        \'\'\'\\n        double_histogram = self._display_histogram_chart(feature_name, cell_height)\\n        return double_histogram\\n\\n    def distribution_chart(self, feature_name: str, cell_height: Optional[str] = None) -> HTML:\\n        \'\'\'Plot overlayed distribution charts for specified feature between two profiles.\\n        Applicable to categorical features.\\n        If reference profile was not set, distribution_chart will plot single chart for target profile.\\n        Parameters\\n        ----------\\n        feature_name : str\\n            Name of the feature to plot chart.\\n        cell_height : str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            cell_height=\'1000px\'. (Default is None)\\n        Returns\\n        -------\\n        HTML\\n            HTML Page of the given plot.\\n        Examples\\n        --------\\n        Generate distribution chart for animal feature (after setting profiles with set_profiles):\\n        .. code-block:: python\\n            visualization.distribution_chart(feature_name=\'animal\')\\n        \'\'\'\\n        difference = False\\n        distribution_chart = self._display_distribution_chart(feature_name, difference, cell_height)\\n        return distribution_chart\\n\\n    def difference_distribution_chart(self, feature_name: str, cell_height: Optional[str] = None) -> HTML:\\n        \'\'\'Plot overlayed distribution charts of differences between the categories of both profiles.\\n        Applicable to categorical features.\\n        Parameters\\n        ----------\\n        feature_name : str\\n            Name of the feature to plot chart.\\n        cell_height : str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            cell_height=\'1000px\'. (Default is None)\\n        Returns\\n        -------\\n        HTML\\n            HTML Page of the given plot.\\n        Examples\\n        --------\\n        Generate Difference Distribution Chart for feature named \'animal\':\\n        .. code-block:: python\\n            visualization.difference_distribution_chart(feature_name=\'animal\')\\n        \'\'\'\\n        difference = True\\n        difference_distribution_chart = self._display_distribution_chart(feature_name, difference, cell_height)\\n        return difference_distribution_chart\\n\\n    def constraints_report(self, constraints: Constraints, cell_height: Optional[str] = None) -> HTML:\\n        page_spec = PageSpecEnum.CONSTRAINTS_REPORT.value\\n        template = _get_compiled_template(page_spec.html)\\n        rendered_template = template(\\n            {\'constraints_report\': json.dumps(constraints.generate_constraints_report(with_summary=True))}\\n        )\\n        constraints_report = self._display(rendered_template, page_spec, cell_height)\\n        return constraints_report\\n\\n    def feature_statistics(\\n        self, feature_name: str, profile: str = \'reference\', cell_height: Optional[str] = None\\n    ) -> HTML:\\n        \'\'\'\\n        Generate a report for the main statistics of specified feature, for a given profile (target or reference).\\n        Statistics include overall metrics such as distinct and missing values, as well as quantile and descriptive\\n        statistics.\\n        If profile is not passed, the default is the reference profile.\\n        Parameters\\n        ----------\\n        feature_name: str\\n            Name of the feature to generate histograms.\\n        profile: str\\n            Profile to be used to generate the report. (Default is reference)\\n        cell_height: str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            cell_height=\'1000px\'. (Default is None)\\n        Examples\\n        --------\\n        Generate Difference Distribution Chart for feature named \'weight\', for target profile:\\n        .. code-block:: python\\n            visualization.feature_statistics(feature_name=\'weight\', profile=\'target\')\\n        \'\'\'\\n        page_spec = PageSpecEnum.FEATURE_STATISTICS.value\\n        template = _get_compiled_template(page_spec.html)\\n        if self._ref_view and profile.lower() == \'reference\':\\n            selected_profile_column = self._ref_view.get_column(feature_name)\\n        else:\\n            selected_profile_column = self._target_view.get_column(feature_name)\\n\\n        rendered_template = template(\\n            {\\n                \'profile_feature_statistics_from_whylogs\': json.dumps(\\n                    add_feature_statistics(feature_name, selected_profile_column)\\n                )\\n            }\\n        )\\n        feature_statistics = self._display(rendered_template, page_spec, cell_height)\\n        return feature_statistics\\n\\n    @staticmethod\\n    def write(\\n        rendered_html: HTML,\\n        preferred_path: Optional[str] = None,  # type: ignore\\n        html_file_name: Optional[str] = None,  # type: ignore\\n    ) -> None:\\n        \'\'\'Create HTML file for a given report.\\n        Parameters\\n        ----------\\n        rendered_html: HTML, optional\\n            Rendered HTML returned by a given report.\\n        preferred_path: str, optional\\n            Preferred path to write the HTML file.\\n        html_file_name: str, optional\\n            Name for the created HTML file. If none is passed, created HTML will be named ProfileVisualizer.html\\n        Examples\\n        --------\\n        Dowloads an HTML page named test.html into the current working directory, with feature statistics for weight feature for the target profile.\\n        .. code-block:: python\\n            import os\\n            visualization.write(\\n                rendered_html=visualization.feature_statistics(feature_name=\'weight\', profile=\'target\'),\\n                html_file_name=os.getcwd() + \'/test\',\\n            )\\n        \'\'\'\\n        if not html_file_name:\\n            html_file_name = \'ProfileVisualizer\'\\n        if preferred_path:\\n            full_path = os.path.join(os.path.expanduser(preferred_path), str(html_file_name) + \'.html\')\\n        else:\\n            full_path = os.path.join(os.pardir, \'html_reports\', str(html_file_name) + \'.html\')\\n\\n        with open(os.path.abspath(full_path), \'w\') as saved_html:\\n            saved_html.write(rendered_html.data)\\n\\n\\ndef DriftReport_<id>(feature_name, generate_the_drift_summary_param=[], select_the_feature_required_for_histogram_param=[]):\\n    visualization = NotebookProfileVisualizerCustom()\\n    visualization.set_profiles(target_profile_view=feature_name[\'prof_view\'], reference_profile_view=feature_name[\'prof_view_ref\'])\\n    # visualization.summary_drift_report()\\n    visualization.double_histogram(feature_name=select_the_feature_required_for_histogram_param)\\n    return {\'target\': f\'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_target.png\', \'ref\': f\'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_ref.png\'},{\'target\': f\'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_target.png\', \'ref\': f\'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_ref.png\'}\\n\\n\"},\"category\":\"Data Profiling\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out\"],\"attributes\":{\"Generate the Drift Summary\":[],\"Select the feature required for histogram\":[]}}","DragNDropLite-55"
"DragNDropLite","Core","{\"formats\":{\"image_path\":\"text\",\"model_path\":\"text\"},\"classname\":\"Predict\",\"name\":\"Predict\",\"alias\":\"Predict\",\"parentCategory\":\"154\",\"id\":79,\"codeGeneration\":{\"requirements\":[\"ultralytics\"],\"imports\":[\"from ultralytics import YOLO\",\"import os\",\"import shutil\"],\"script\":\"def Predict_<id>(dataset, image_path_param=\'/app/jobs/models/predict_image\', model_path_param=\'/app/jobs/models/classicmlpoc/predict_model__yolov8m.pt\'):\\n    image_path=image_path_param\\n    model_path=model_path_param\\n    os.chdir(\'/app/jobs/models/predict_image\')\\n    if os.path.exists(\'runs\'):\\n        shutil.rmtree(\'runs\')\\n    model = YOLO(model_path) \\n    results = model(source=image_path, save=True,save_txt=True,save_crop=True) \\n    print(\'classes=>\',results[0].names)\\n    print(\'speed=>\',results[0].speed)\\n    pred_output={}\\n    for result in results: # detection \\n        box_info=[]\\n        for box in result.boxes:\\n            box_cord = box.xyxy[0].tolist() # get box coordinates in (top, left, bottom, right) format\\n            box_class = int(box.cls.tolist()[0])  # get class of box\\n            box_cord.append(result.names[box_class])\\n            box_info.append(box_cord)\\n        pred_output[result.path]=box_info\\n    print(pred_output)\\n    return pred_output\\n\\n\\n\\n\"},\"category\":\"Cheque_detection\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"image_path\":\"/app/jobs/models/predict_image\",\"model_path\":\"/app/jobs/models/classicmlpoc/predict_model__yolov8m.pt\"}}","DragNDropLite-79"
"DragNDropLite","Core","{\"formats\":{},\"classname\":\"Ocr\",\"name\":\"Ocr\",\"alias\":\"Ocr\",\"parentCategory\":\"154\",\"id\":78,\"codeGeneration\":{\"requirements\":[\"easyocr\",\"matplotlib\",\"os\",\"opencv-python\"],\"imports\":[\"import easyocr\",\"import matplotlib.pyplot as plt\",\"import cv2\",\"import os\",\"from os import listdir\",\"from numpy import asarray\",\"from PIL import Image\"],\"script\":\"def Ocr_<id>(dataset):\\n    final_data={}\\n    for image in dataset:\\n        img=Image.open(image)\\n        boxes=dataset[image]\\n        img_data={}\\n        for box in boxes:\\n            area =tuple(box[:4])\\n            cropped_img = img.crop(area)\\n            cropped_img.show()\\n            cropped_img_arr=asarray(cropped_img)\\n            reader=easyocr.Reader([\'en\'],gpu=False)\\n            text=reader.readtext(cropped_img_arr,paragraph=\'True\')\\n            img_data[box[4]]=text\\n        final_data[image[-16:]]=img_data\\n    print(final_data)\\n    return final_data\\n\\n\\n\"},\"category\":\"Cheque_detection\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-78"
"DragNDropLite","Core","{\"formats\":{\"bucket\":\"text\",\"fileName\":\"text\",\"Destination_directory\":\"text\"},\"classname\":\"DownloadFileData\",\"name\":\"DownloadFileData\",\"alias\":\"DownloadFileData\",\"parentCategory\":\"154\",\"id\":93,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import requests\",\"import logging as logger\",\"import shutil\",\"import os\"],\"script\":\"\\ndef DownloadFileData_<id>(dataset, bucket_param=\'Demo\', filename_param=\'train_data\', destination_directory_param=\'models/classicmlpoc\'):\\n    import pandas as pd\\n    data={}\\n    print(\'before\',dataset)\\n    data[\'file_id\']=dataset[0][1]\\n    data[\'file_name\']=dataset[0][0]\\n    print(\'data..\',data)\\n    fileId, fileName, bucket = data[\'file_id\'], filename_param, bucket_param\\n    if(fileName!=data[\'file_name\']):\\n        raise Exception(\'file not found\') \\n    \\n    FILE_SERVER_URL = os.environ[\'FILE_SERVER_URL\']\\n    print(\'FILE_SERVER_URL: \', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ[\'FILE_SERVER_TOKEN\']\\n    print(\'FILE_SERVER_TOKEN: \', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ[\'JOB_DIRECTORY\'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + \'/api/lastcount/\' + fileId + \'?bucket=\' + bucket,\\n                                           headers={\'access-token\': FILE_SERVER_TOKEN},\\n                                           proxies={\'http\': \'\', \'https\': \'\'})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search(\'<Integer>(.*?)</Integer>\', totalChunksResponse.text).group(1)\\n            totalChunks = int(\'0\')\\n        logger.info(\'Total Chunks: \' + str(totalChunks + 1))\\n\\n        # create a folder \'chunkfiles\'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, \'chunkfiles\')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info(\'dir already exists...\')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model\'s chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            url = (FILE_SERVER_URL + \'/api/download/\' + fileId + \'/\') + \'file\' + \'?bucket=\' + bucket\\n            filedata = requests.get(url, headers={\'access-token\': FILE_SERVER_TOKEN}, proxies={\'http\': \'\', \'https\': \'\'})\\n            open(chunkfilePath + \'/\' + str(i), \'wb\').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, \'wb\') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, \'rb\')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n\"},\"category\":\"Cheque_detection\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"bucket\":\"Demo\",\"fileName\":\"train_data\",\"Destination_directory\":\"models/classicmlpoc\"}}","DragNDropLite-93"
"DragNDropLite","Core","{\"formats\":{\"bucket\":\"text\",\"fileName\":\"text\",\"Destination_directory\":\"text\",\"fileId\":\"text\"},\"classname\":\"DownloadFile\",\"name\":\"DownloadFile\",\"alias\":\"DownloadFile\",\"parentCategory\":\"146\",\"id\":12,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import requests\",\"import logging as logger\",\"import shutil\",\"import os\"],\"script\":\"\\ndef DownloadFile_<id>(bucket_param=\'Demo\', filename_param=\'your_model_name.pkl\', destination_directory_param=\'models/classicmlpoc\', fileid_param=\'\'):\\n    fileId, fileName, bucket = fileid_param, filename_param, bucket_param\\n    FILE_SERVER_URL = os.environ[\'FILE_SERVER_URL\']\\n    print(\'FILE_SERVER_URL: \', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ[\'FILE_SERVER_TOKEN\']\\n    print(\'FILE_SERVER_TOKEN: \', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ[\'JOB_DIRECTORY\'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + \'/api/lastcount/\' + fileId + \'?bucket=\' + bucket,\\n                                           headers={\'access-token\': FILE_SERVER_TOKEN},\\n                                           proxies={\'http\': \'\', \'https\': \'\'})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search(\'<Integer>(.*?)</Integer>\', totalChunksResponse.text).group(1)\\n            totalChunks = int(\'0\')\\n        logger.info(\'Total Chunks: \' + str(totalChunks + 1))\\n\\n        # create a folder \'chunkfiles\'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, \'chunkfiles\')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info(\'dir already exists...\')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model\'s chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            url = (FILE_SERVER_URL + \'/api/download/\' + fileId + \'/\') + \'file\' + \'?bucket=\' + bucket\\n            filedata = requests.get(url, headers={\'access-token\': FILE_SERVER_TOKEN}, proxies={\'http\': \'\', \'https\': \'\'})\\n            open(chunkfilePath + \'/\' + str(i), \'wb\').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, \'wb\') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, \'rb\')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n\"},\"category\":\"BaseConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"attributes\":{\"bucket\":\"Demo\",\"fileName\":\"your_model_name.pkl\",\"Destination_directory\":\"models/classicmlpoc\",\"fileId\":\"\"}}","DragNDropLite-12"
"DragNDropLite","Core","{\"formats\":{\"reference_value\":\"integer\",\"reference_feature\":\"text\"},\"classname\":\"Datasplit\",\"name\":\"Datasplit\",\"alias\":\"Datasplit\",\"parentCategory\":\"157\",\"id\":99,\"codeGeneration\":{\"requirements\":[\"whylogs\"],\"imports\":[\"import pandas as pd\",\"import whylogs as why\"],\"script\":\"def Datasplit_<id>(data,reference_feature_param=\'\',reference_value_param=0):\\n  wine = pd.DataFrame(data)\\n  cond_reference = (wine[reference_feature_param]<=int(reference_value_param))\\n  wine_reference = wine.loc[cond_reference]\\n  cond_target = (wine[reference_feature_param]>int(reference_value_param))\\n  wine_target = wine.loc[cond_target]\\n  return wine_reference,wine_target\\n\\n\\n\"},\"category\":\"Data Profiling\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"model\",\"out\"],\"attributes\":{\"reference_value\":0,\"reference_feature\":\"\"}}","DragNDropLite-99"
"DragNDropLite","Core","{\"formats\":{\"page_end_index\":\"integer\",\"page_start_index\":\"integer\",\"page_drop_indices\":\"list\"},\"classname\":\"pdf_to_text\",\"name\":\"pdf_to_text\",\"alias\":\"PDF to Text\",\"parentCategory\":\"166\",\"id\":100,\"codeGeneration\":{\"requirements\":[\"pymupdf\"],\"imports\":[\"import fitz\",\"import pandas as pd \"],\"script\":\"\\ndef pdf_to_text_<id>(dataset, page_start_index_param=0, page_end_index_param=-1, page_drop_indices_param=[]):\\n    page_drop_indices_param = [ss[\'name\'] for ss in page_drop_indices_param]\\n    text = \'\'\\n    if dataset.split(\'.\')[-1] == \'pdf\':\\n        doc = fitz.open(data)\\n        start = max(0, page_start_index_param)\\n        end = min(page_end_index_param, len(doc))\\n        for page in range(start, end):\\n            if page not in page_drop_indices_param:\\n                page1 = doc[page]\\n                text += page1.get_text(\'text\')\\n                text += \'\\\\n\'\\n    return text\\n\\n\\n\"},\"category\":\"Convertors\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"page_end_index\":-1,\"page_start_index\":0,\"page_drop_indices\":[]}}","DragNDropLite-100"
"DragNDropLite","Core","{\"formats\":{\"slide_drop_indices\":\"list\",\"slide_end_index\":\"integer\",\"slide_start_index\":\"integer\"},\"classname\":\"ppt_to_text\",\"name\":\"ppt_to_text\",\"alias\":\"PPT to Text\",\"parentCategory\":\"166\",\"id\":101,\"codeGeneration\":{\"requirements\":[\"python-pptx\"],\"imports\":[\"from pptx import Presentation\",\"import glob\"],\"script\":\"\\ndef ppt_to_text_<id>(dataset, slide_start_index_param=0, slide_end_index_param=-1, slide_drop_indices_param=[]):\\n    slide_drop_indices_param = [ss[\'name\'] for ss in slide_drop_indices_param]\\n    text = \'\'\\n    prs = Presentation(dataset)\\n    slides = prs.slides\\n    start = max(0, slide_start_index_param)\\n    end = min(slide_end_index_param, len(doc)) \\n    for slide_ind in range(start, end):\\n        if slide_ind not in slide_drop_indices_param:\\n            slide = slides[slide_ind]\\n            for shape in slide.shapes:\\n                if hasattr(shape, \'text\'):\\n                    text += shape.text\\n                    text += \'\\\\n\'\\n    return text\\n\\n\\n\"},\"category\":\"Convertors\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"slide_drop_indices\":[],\"slide_end_index\":-1,\"slide_start_index\":0}}","DragNDropLite-101"
"DragNDropLite","Core","{\"formats\":{},\"classname\":\"DataStatistics\",\"name\":\"DataStatistics\",\"alias\":\"DataStatistics\",\"parentCategory\":\"157\",\"id\":102,\"codeGeneration\":{\"requirements\":[\"whylogs\"],\"imports\":[\"import pandas as pd\",\"import whylogs as why\"],\"script\":\"def DataStatistics_<id>(wine_reference,wine_target):\\n  result = why.log(pandas=wine_target)\\n  prof_view = result.view()\\n  target = prof_view.to_pandas()\\n  target = target.to_dict(\'records\')\\n  result_ref = why.log(pandas=wine_reference)\\n  prof_view_ref = result_ref.view()\\n  reference = prof_view_ref.to_pandas()\\n  reference = reference.to_dict(\'records\')\\n  return target,reference\\n\\n\\n\"},\"category\":\"Data Profiling\",\"inputEndpoints\":[\"in\",\"in\"],\"outputEndpoints\":[\"model\",\"out\"],\"attributes\":{}}","DragNDropLite-102"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{},\"name\":\"Profanity\",\"alias\":\"Profanity\",\"parentCategory\":\"167\",\"id\":107,\"codeGeneration\":{\"requirements\":[\"response\",\"requests\"],\"imports\":[\"import sys\",\"import urllib3\",\"import requests\",\"import json\"],\"script\":\"def Profanity(inputText):\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/safety/profanity/censor\'\\r\\n    headers = {\'content-type\': \'application/json\'}\\r\\n    data = {\\r\\n    \'inputText\': inputText\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(\'response:\', \'\\\\n\', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-107"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{},\"name\":\"Explainability\",\"alias\":\"Explainability\",\"parentCategory\":\"167\",\"id\":108,\"codeGeneration\":{\"requirements\":[\"requests\",\"response\"],\"imports\":[\"import sys\",\"import requests\",\"import response\",\"import json\"],\"script\":\"def Explainability(input):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    import sys\\r\\n    inputText_param, explainerID_param = input[\'inputText\'], input[\'explainerID\']\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/explainability/local/explain\'\\r\\n    headers = {\'content-type\': \'application/json\'}\\r\\n    data = {\\r\\n       \'inputText\': inputText_param,\\r\\n       \'explainerID\': explainerID_param\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(\'response:\', \'\\\\n\', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-108"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{},\"name\":\"VerifyImage\",\"alias\":\"VerifyImage\",\"parentCategory\":\"167\",\"id\":113,\"codeGeneration\":{\"requirements\":[\"opencv-python\",\"requests\",\"response\"],\"imports\":[\"import requests\",\"import json\",\"import urllib3\",\"import cv2\",\"import sys\"],\"script\":\"def VerifyImage(image_payload):\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/verify\'\\r\\n    headers = {\'content-type\': \'multipart/form-data\'}\\r\\n    print(url)\\r\\n    with open(image_payload, \'rb\') as file:\\r\\n        response = requests.post(url, files={\'payload\': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-113"
"DragNDropLite","Core","{\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"classname\":\"PythonScriptConfig\",\"name\":\"Python Script\",\"alias\":\"Python Script\",\"parentCategory\":\"146\",\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"id\":114,\"category\":\"BaseConfig\",\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":\"\\rdef PythonScript( dataset):    #python-script Data\\r\\r    return dataset\"}}","DragNDropLite-114"
"DragNDropLite","Core","{\"attributes\":{},\"formats\":{}}","DragNDropLite-115"
"DragNDropLite","Core","{\"formats\":{\"bucket\":\"text\",\"val_data\":\"text\",\"train_data\":\"text\"},\"classname\":\"Prepare_Data\",\"name\":\"Prepare_Data\",\"alias\":\"Prepare_Data\",\"parentCategory\":\"168\",\"id\":117,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Prepare_Data(data,bucket_param,train_data_param,val_data_param):\\r\\n    context_length = 2048\\r\\n    global pretrained_model\\r\\n    base_path = os.getenv(\'AICLD_INPUT_ARTIFACTS_PATH\')\\r\\n    \\r\\n    train_dataset_file_path = Download_Data(bucket_param, train_data_param)\\r\\n    logger.info(\'train_dataset\')\\r\\n    val_dataset_file_path = Download_Data(bucket_param, val_data_param)\\r\\n    logger.info(\'Reading data files\')\\r\\n    #pretrained_model = Download_Data(\'aicloudprd\',\'foundational_model/codegen/models/Salesforce/codegen-350M-multi\')\\r\\n    #logger.info(\'pretrained_model\')\\r\\n    pretrained_model = os.getenv(\'AI_CLD_PRETRAINED_MDL_PATH\')\\r\\n    dir_content= os.listdir(pretrained_model)\\r\\n    logger.info(dir_content)\\r\\n    train_data = None\\r\\n    valid_data = None\\r\\n    if train_dataset_file_path.endswith(\'.json\') & val_dataset_file_path.endswith(\'.json\'):\\r\\n        train_data = [json.loads(line) for line in open(train_dataset_file_path, \'r\',encoding=\'utf-8\')]\\r\\n        valid_data = [json.loads(line) for line in open(val_dataset_file_path, \'r\',encoding=\'utf-8\')]\\r\\n    elif train_dataset_file_path.endswith(\'.csv\') & val_dataset_file_path.endswith(\'.csv\'):\\r\\n        train_data = pd.read_csv(train_dataset_file_path)\\r\\n        valid_data = pd.read_csv(val_dataset_file_path)\\r\\n        \\r\\n    #train_data = [json.loads(line) for line in open(train_dataset_file_path, \'r\',encoding=\'utf-8\')]\\r\\n    #valid_data = [json.loads(line) for line in open(val_dataset_file_path, \'r\',encoding=\'utf-8\')]\\r\\n    \\r\\n    ds_train = Dataset.from_pandas(pd.DataFrame(data=train_data))\\r\\n    ds_valid = Dataset.from_pandas(pd.DataFrame(data=valid_data))\\r\\n\\r\\n    logger.info(\'Creating raw_datasets .........\')\\r\\n\\r\\n    raw_datasets = DatasetDict(\\r\\n        {\\r\\n            \'train\': ds_train.shuffle(),\\r\\n            \'valid\': ds_valid.shuffle()\\r\\n        }\\r\\n    )\\r\\n    datalist = []\\r\\n    datalist.append(raw_datasets)\\r\\n    #datalist.append(pretrained_model)\\r\\n    return datalist\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"LLM_Model\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"bucket\":\"\",\"val_data\":\"\",\"train_data\":\"\"}}","DragNDropLite-117"
"DragNDropLite","Core","{\"formats\":{\"weight_decay\":\"int\",\"epoch\":\"int\",\"eval_step\":\"int\"},\"classname\":\"Train_Model\",\"name\":\"Train_Model\",\"alias\":\"Train_Model\",\"parentCategory\":\"168\",\"id\":118,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Train_Model(train_data_list,epoch_param,weight_decay_param,eval_step_param):\\r\\n    logger.info(\'in training stage\')\\r\\n    tokenized_datasets = train_data_list[0]\\r\\n    tokenizer = train_data_list[1]\\r\\n    #pretrained_model = train_data_list[2]\\r\\n    save_best_model_path = os.getenv(\'AICLD_MODEL_STORE_PATH\')+\'codegen-350M-java\'\\r\\n    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\\r\\n    model = AutoModelForCausalLM.from_pretrained(pretrained_model)\\r\\n    logger.info(\'Training started\')\\r\\n    resume_from_checkpoint=False\\r\\n    model_id=\'fft_2048\'\\r\\n    args = TrainingArguments(output_dir=\'codegen_dir_2048\', \\r\\n                             per_device_train_batch_size=1, \\r\\n                             per_device_eval_batch_size=1, \\r\\n                             evaluation_strategy=\'steps\', \\r\\n                             eval_steps=eval_step_param, \\r\\n                             logging_steps=100, \\r\\n                             gradient_accumulation_steps=4, \\r\\n                             num_train_epochs=epoch_param, \\r\\n                             weight_decay=weight_decay_param, \\r\\n                             warmup_steps=100, \\r\\n                             lr_scheduler_type=\'linear\', \\r\\n                             learning_rate=3e-5, \\r\\n                             save_strategy=\'steps\', \\r\\n                             save_total_limit = 1, \\r\\n                             fp16 = True, \\r\\n                             load_best_model_at_end=True, \\r\\n                             save_steps=500)\\r\\n                             #deepspeed = \'ds_config.json\') \\r\\n\\r\\n\\r\\n    trainer = Trainer(\\r\\n        model=model,\\r\\n        tokenizer=tokenizer,\\r\\n        args=args,\\r\\n        data_collator=data_collator,\\r\\n        train_dataset=tokenized_datasets[\'train\'],\\r\\n        eval_dataset=tokenized_datasets[\'valid\'],\\r\\n    )\\r\\n\\r\\n    if resume_from_checkpoint == \'True\':\\r\\n        logger.info(\'Training started from checkpoint\')\\r\\n        trainer.train(resume_from_checkpoint=True)\\r\\n        logger.info(\'Training completed\')\\r\\n    else:\\r\\n        logger.info(\'Training started\')\\r\\n        trainer.train()\\r\\n        logger.info(\'Training completed\')\\r\\n\\r\\n    os.mkdir(\'./trainer_logs\')\\r\\n    training_filename = \'./trainer_logs/train_logs.json\'\\r\\n    output_file = open(training_filename, \'w\', encoding=\'utf-8\')\\r\\n    for dic in trainer.state.log_history:\\r\\n\\r\\n        json.dump(dic, output_file) \\r\\n        output_file.write(\'\\\\n\') \\r\\n    logger.info(\'All logs saved at \',training_filename)\\r\\n\\r\\n    trainer.save_model(save_best_model_path)\\r\\n    logger.info(\'Finetuned model saved at \',save_best_model_path)\\r\\n    return save_best_model_path\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"LLM_Model\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"weight_decay\":0.1,\"epoch\":2,\"eval_step\":100}}","DragNDropLite-118"
"DragNDropLite","Core","{\"formats\":{\"model_version\":\"int\",\"model_name\":\"text\",\"user_id\":\"text\",\"image_uri\":\"text\",\"artifacts_uri\":\"text\",\"projectid\":\"text\",\"storagetype\":\"text\"},\"classname\":\"Register Model\",\"name\":\"Register Model\",\"alias\":\"Register Model\",\"parentCategory\":\"168\",\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import logging\",\"import requests\",\"import json\",\"from pathlib import Path\"],\"script\":\"logging_dir = \'/app/logs\'\\r\\nlog_filename = \'create.log\'\\r\\nPath(logging_dir).mkdir(parents=True, exist_ok=True)\\r\\nlog_filename = os.path.join(logging_dir, log_filename)\\r\\nlogging.basicConfig(filename=log_filename,\\r\\n                    format=\'%(asctime)s %(message)s\',\\r\\n                    filemode=\'w\')\\r\\nlogger = logging.getLogger()\\r\\nlogger.setLevel(logging.INFO)\\r\\n\\r\\ndef RegisterModel(user_id_param,model_name_param,model_version_param,projectid_param,image_uri_param,storagetype_param,artifacts_uri_param):\\r\\n\\r\\n    headers = {\\r\\n        \'accept\': \'application/json\',\\r\\n        \'Content-Type\': \'application/json\',\\r\\n    }\\r\\n    global model_version\\r\\n    model_version = model_version_param\\r\\n    headers[\'userId\'] = user_id_param\\r\\n    json_data = {\\r\\n        \'name\': \'\',\\r\\n        \'version\': \'\',\\r\\n        \'projectId\': \'\',\\r\\n        \'container\': {\\r\\n            \'imageUri\': \'\',\\r\\n            \'envVariables\': [], \\r\\n            \'ports\': [{\\r\\n                \'name\': \'containerport\',\\r\\n                \'value\': \'8080\'\\r\\n            }],\\r\\n        \'labels\': [{\\r\\n        \'name\': \'framework\',\\r\\n        \'value\': \'flask\'\\r\\n        }],\\r\\n        \'healthProbeUri\': \'/\'\\r\\n        },\\r\\n        \'artifacts\': {\\r\\n            \'storageType\': \'\',\\r\\n            \'uri\': \'\',\\r\\n        },    \\r\\n    }\\r\\n\\r\\n    json_data[\'name\'] = model_name_param\\r\\n    json_data[\'version\'] = model_version_param\\r\\n    json_data[\'projectId\'] = projectid_param\\r\\n    json_data[\'container\'][\'imageUri\'] = image_uri_param\\r\\n    json_data[\'artifacts\'][\'storageType\'] = storagetype_param\\r\\n    json_data[\'artifacts\'][\'uri\'] = artifacts_uri_param\\r\\n    json_data = json.dumps(json_data)\\r\\n    logger.info(\'model register payload\')\\r\\n    logger.info(json_data)\\r\\n    response = None\\r\\n    try:\\r\\n        response = requests.post(\'https://api-aicloud.ad.infosys.com/api/v1/models\',headers=headers,data=json_data,verify=False)\\r\\n    except Exception as e:\\r\\n        response = e\\r\\n        logger.info(response)\\r\\n    model_register_details = json.loads(response.text)\\r\\n    logger.info(\'model register info\')\\r\\n    logger.info(model_register_details)\\r\\n    model_id = model_register_details[\'data\'][\'id\']\\r\\n    logger.info(\'model_id\')\\r\\n    logger.info(model_id)\\r\\n    return model_id\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"id\":119,\"category\":\"LLM_Model\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model_version\":\"\",\"model_name\":\"\",\"user_id\":\"leapAccount01@infosys.com\",\"image_uri\":\"\",\"artifacts_uri\":\"\",\"projectid\":\"97dd455b57c64c6dba8bb5d0caa99dfd\",\"storagetype\":\"INFY_AICLD_NUTANIX\"}}","DragNDropLite-119"
"DragNDropLite","Core","{\"formats\":{\"user_id\":\"text\",\"endpoint_name\":\"text\",\"contexturi\":\"text\",\"projectid\":\"text\"},\"classname\":\"Create Endpoint\",\"name\":\"Create Endpoint\",\"alias\":\"Create Endpoint\",\"parentCategory\":\"169\",\"id\":120,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def CreateEndpoint(register_model_id,user_id_param,endpoint_name_param,contexturi_param,projectid_param):\\r\\n    headers = {\\r\\n        \'accept\': \'application/json\',\\r\\n        \'Content-Type\': \'application/json\',\\r\\n    }\\r\\n    headers[\'userId\'] = user_id_param\\r\\n    body = {}\\r\\n    body[\'name\'] = endpoint_name_param\\r\\n    body[\'contextUri\'] = contexturi_param\\r\\n    body[\'projectId\'] = projectid_param\\r\\n    body = json.dumps(body)\\r\\n    endpoint_model_ids = []\\r\\n    response = None\\r\\n    try:\\r\\n        response = requests.post(\'https://api-aicloud.ad.infosys.com/api/v1/endpoint\', headers=headers,data=body,verify=False)\\r\\n    except Exception as e:\\r\\n        response = e\\r\\n        logger.info(response)\\r\\n    endpoint_details = json.loads(response.text)\\r\\n    endpoint_id = endpoint_details[\'data\'][\'id\']\\r\\n    logger.info(\'endpoint_id\')\\r\\n    logger.info(endpoint_id)\\r\\n    endpoint_model_ids.append(register_model_id)\\r\\n    endpoint_model_ids.append(endpoint_id)\\r\\n    return endpoint_model_ids\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"Endpoint\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"user_id\":\"leapAccount01@infosys.com\",\"endpoint_name\":\"\",\"contexturi\":\"\",\"projectid\":\"97dd455b57c64c6dba8bb5d0caa99dfd\"}}","DragNDropLite-120"
"DragNDropLite","Core","{\"formats\":{\"minreplicacount\":\"int\",\"maxreplicacount\":\"int\",\"compute_maxqty\":\"int\",\"container_volume_size\":\"int\",\"compute_minqty\":\"int\",\"compute_type\":\"text\",\"compute_memory\":\"text\",\"servingframework\":\"text\"},\"classname\":\"Deploy Endpoint\",\"name\":\"Deploy Endpoint\",\"alias\":\"Deploy Endpoint\",\"parentCategory\":\"169\",\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def DeployEndpoint(endpoint_and_model_id,servingframework_param,minreplicacount_param,maxreplicacount_param,compute_type_param,compute_maxqty_param,compute_minqty_param,compute_memory_param,container_volume_size_param):\\r\\n\\r\\n    headers = {\\r\\n        \'accept\': \'application/json\',\\r\\n        \'Content-Type\': \'application/json\',\\r\\n    }\\r\\n\\r\\n    user_id_param=\'leapAccount01@infosys.com\'\\r\\n    headers[\'userId\'] = user_id_param\\r\\n    model_id = endpoint_and_model_id[0]\\r\\n    endpoint_id = endpoint_and_model_id[1]\\r\\n    json_data = {\\r\\n              \'endpointId\': \'\',\\r\\n              \'modelId\': \'\',\\r\\n              \'version\': \'\',\\r\\n              \'inferenceConfig\': {\\r\\n                \'servingFramework\': \'\',\\r\\n                \'inferenceSpec\': {\\r\\n                  \'minReplicaCount\': \'\',\\r\\n                  \'maxReplicaCount\': \'\',\\r\\n                  \'containerResourceConfig\': {\\r\\n                    \'computes\': [\\r\\n                      {\\r\\n                        \'type\': \'\',\\r\\n                        \'maxQty\': \'\',\\r\\n                        \'memory\': \'\',\\r\\n                        \'minQty\': \'\'\\r\\n                      }\\r\\n                    ],\\r\\n                    \'volumeSizeinGB\': \'\'\\r\\n                  },\\r\\n                  \'modelSpec\': [\\r\\n                              {\\r\\n                  \'modelUris\': {\\r\\n                    \'prefixUri\': \'/codegenexecute\',\\r\\n                    \'predictUri\': \'/swagger.json\'\\r\\n                  }\\r\\n                },\\r\\n                {\\r\\n                  \'modelUris\': {\\r\\n                    \'prefixUri\': \'/codegenexecute\',\\r\\n                    \'predictUri\': \'/predictions/\'\\r\\n                  }\\r\\n                }\\r\\n                  ]\\r\\n                }\\r\\n              }\\r\\n            }\\r\\n    json_data[\'endpointId\'] = endpoint_id\\r\\n    json_data[\'modelId\'] = model_id\\r\\n    json_data[\'version\'] = model_version\\r\\n    json_data[\'inferenceConfig\'][\'servingFramework\'] = servingframework_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'minReplicaCount\'] = minreplicacount_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'maxReplicaCount\'] = maxreplicacount_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'containerResourceConfig\'][\'computes\'][0][\'type\'] = compute_type_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'containerResourceConfig\'][\'computes\'][0][\'maxQty\'] = compute_maxqty_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'containerResourceConfig\'][\'computes\'][0][\'memory\'] = compute_memory_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'containerResourceConfig\'][\'computes\'][0][\'minQty\'] = compute_minqty_param\\r\\n    json_data[\'inferenceConfig\'][\'inferenceSpec\'][\'containerResourceConfig\'][\'volumeSizeinGB\'] = container_volume_size_param\\r\\n    json_data = json.dumps(json_data)\\r\\n    response = None\\r\\n    try:\\r\\n        response = requests.post(\'https://api-aicloud.ad.infosys.com/api/v1/endpoint/deploy\',headers=headers,data=json_data,verify=False)\\r\\n    except Exception as e:\\r\\n        response = e\\r\\n        logger.info(response)\\r\\n    response_data = json.loads(response.text)\\r\\n    logger.info(\'deployment_details\')\\r\\n    logger.info(response_data)\\r\\n    return response_data\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"id\":121,\"category\":\"Endpoint\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"minreplicacount\":\"1\",\"maxreplicacount\":1,\"compute_maxqty\":\"\",\"container_volume_size\":\"2\",\"compute_minqty\":\"\",\"compute_type\":\"GPU\",\"compute_memory\":\"20GB\",\"servingframework\":\"Custom\"}}","DragNDropLite-121"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{},\"name\":\"Analyze\",\"alias\":\"Analyze\",\"parentCategory\":\"167\",\"id\":122,\"codeGeneration\":{\"requirements\":[\"requests\",\"json\",\"urllib3\"],\"imports\":[\"\"],\"script\":\"def privacy_analyze(input):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    import sys\\r\\n    inputText = input[\'inputText\']\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/analyze\'\\r\\n    headers = {\'content-type\': \'application/json\'}\\r\\n    data = {\'inputText\': inputText}\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(\'response:\', \'\\\\n\', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-122"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{},\"name\":\"Anonymize\",\"alias\":\"Anonymize\",\"parentCategory\":\"167\",\"id\":123,\"codeGeneration\":{\"requirements\":[\"requests\",\"json\",\"urllib3\"],\"imports\":[],\"script\":\"def privacy_anonymize(input):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    import sys\\r\\n    inputText_param, piiEntitiesToBeRedacted_param, redactionType_param = input[\'inputText\'], input[\'piiEntitiesToBeRedacted\'], input[\'redactionType\']\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/anonymize\'\\r\\n    headers = {\'content-type\': \'application/json\'}\\r\\n    data = {\\r\\n        \'inputText\': inputText_param,\\r\\n        \'piiEntitiesToBeRedacted\': [\\r\\n            piiEntitiesToBeRedacted_param\\r\\n        ],\\r\\n        \'redactionType\': redactionType_param\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(\'response_anonymize:\', \'\\\\n\', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-123"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{},\"name\":\"Profanity Analyze\",\"alias\":\"Profanity Analyze\",\"parentCategory\":\"167\",\"id\":124,\"codeGeneration\":{\"requirements\":[\"requests\",\"json\",\"urllib3\"],\"imports\":[],\"script\":\"\\r\\ndef profanity_analyze(inputText):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/safety/profanity/analyze\'\\r\\n    headers = {\'content-type\': \'application/json\'}\\r\\n    data = {\\r\\n   \'inputText\': inputText\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(\'response:\', \'\\\\n\', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-124"
"DragNDropLite","Core","{\"subCategory\":\"\",\"formats\":{\"stride\":\"int\",\"max_length\":\"int\"},\"classname\":\" Prepare_Tokenized_Dataset\",\"name\":\"Prepare_Tokenized_Dataset\",\"alias\":\"Prepare_Tokenized_Dataset\",\"parentCategory\":\"168\",\"id\":125,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"\\r\\ndef Prepare_Tokenized_Dataset(datalist,stride_param,max_length_param):\\r\\n    raw_datasets = datalist[0]\\r\\n    #pretrained_model= datalist[1]\\r\\n    def tokenize(element):\\r\\n        stride = stride_param\\r\\n        max_length=max_length_param\\r\\n        outputs = tokenizer(\\r\\n            element[\'code\'],\\r\\n            truncation=True,\\r\\n            max_length=max_length,\\r\\n            return_overflowing_tokens=True,\\r\\n            return_length=True,\\r\\n            stride=stride,\\r\\n        )\\r\\n        input_batch = []\\r\\n        prev_chunk=[]\\r\\n        \\r\\n        not_first_chunk=False\\r\\n        for input_id in outputs.input_ids:\\r\\n            if len(input_id) == max_length:\\r\\n                input_batch.append(input_id)\\r\\n                prev_chunk=input_id\\r\\n                not_first_chunk=True\\r\\n            else:\\r\\n                if not_first_chunk:\\r\\n                    prev_chunk_last=prev_chunk[-(max_length-len(input_id)+stride):-(stride)]\\r\\n                    final_chunk=prev_chunk_last+input_id\\r\\n                    input_batch.append(final_chunk)\\r\\n                else:\\r\\n                    input_batch.append(input_id)\\r\\n                \\r\\n        return {\'input_ids\': input_batch}\\r\\n\\r\\n\\r\\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\\r\\n    tokenizer.pad_token = tokenizer.eos_token\\r\\n\\r\\n    logger.info(\'Creating tokenized_datasets .........\')\\r\\n\\r\\n    tokenized_datasets = raw_datasets.map(\\r\\n        tokenize, batched=True, remove_columns=raw_datasets[\'train\'].column_names\\r\\n    )\\r\\n    train_data_list = []\\r\\n    train_data_list.append(tokenized_datasets)\\r\\n    train_data_list.append(tokenizer)\\r\\n    #train_data_list.append(pretrained_model)\\r\\n    return train_data_list\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"LLM_Model\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"stride\":\"256\",\"max_length\":\"2048\"}}","DragNDropLite-125"
"DragNDropLite","Core","{\"formats\":{},\"name\":\"prompt\",\"alias\":\"prompt\",\"parentCategory\":\"171\",\"id\":128,\"codeGeneration\":{\"requirements\":[\"langchain[all]\"],\"imports\":[\"from langchain.prompts import (     ChatPromptTemplate,      MessagesPlaceholder,      SystemMessagePromptTemplate,      HumanMessagePromptTemplate )\",\"from langchain.chains import ConversationChain\",\"from langchain.chat_models import ChatVertexAI\",\"from langchain.memory import ConversationBufferMemory\"],\"script\":\"def prompt():\\n    prompt = ChatPromptTemplate.from_messages([\\n    SystemMessagePromptTemplate.from_template(\'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\'),\\n    MessagesPlaceholder(variable_name=\'history\'),\\n    HumanMessagePromptTemplate.from_template(\'{input}\')\\n    ])\\n    return prompt\\n\\n\\n\"},\"category\":\"LangChain\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-128"
"DragNDropLite","Core","{\"formats\":{\"model\":\"string\"},\"name\":\"LLM\",\"alias\":\"LLM\",\"parentCategory\":\"171\",\"id\":129,\"codeGeneration\":{\"requirements\":[\"langchain[all]\"],\"imports\":[\"from langchain.prompts import (     ChatPromptTemplate,      MessagesPlaceholder,      SystemMessagePromptTemplate,      HumanMessagePromptTemplate )\",\"from langchain.chains import ConversationChain\",\"from langchain.chat_models import ChatVertexAI\",\"from langchain.memory import ConversationBufferMemory\"],\"script\":\"def LLM(model_param=\'chat-bison\'):\\n    LLM = ChatVertexAI(model=model_param)\\n    return LLM\\n\\n\\n\"},\"category\":\"Langchain\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"model\":\"chat-bison\"}}","DragNDropLite-129"
"DragNDropLite","Core","{\"formats\":{\"return_messages\":\"string\"},\"name\":\"Memory\",\"alias\":\"Memory\",\"parentCategory\":\"171\",\"id\":130,\"codeGeneration\":{\"requirements\":[\"langchain[all]\"],\"imports\":[\"from langchain.prompts import (     ChatPromptTemplate,      MessagesPlaceholder,      SystemMessagePromptTemplate,      HumanMessagePromptTemplate )\",\"from langchain.chains import ConversationChain\",\"from langchain.chat_models import ChatVertexAI\",\"from langchain.memory import ConversationBufferMemory\"],\"script\":\"def Memory(return_messages_param = \'True\'):\\n    memory = ConversationBufferMemory(return_messages = return_messages_param)\\n    return memory\\n\\n\\n\"},\"category\":\"LangChain\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"return_messages\":\"True\"}}","DragNDropLite-130"
"DragNDropLite","Core","{\"formats\":{},\"name\":\"ImageAnalyzer\",\"alias\":\"ImageAnalyzer\",\"parentCategory\":\"167\",\"id\":131,\"codeGeneration\":{\"requirements\":[\"requests\",\"json\",\"urllib3\"],\"imports\":[\"import requests\",\"import json\",\"import urllib3\",\"import sys\"],\"script\":\"def ImageAnalyzer(image_payload):\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/analyze\'\\r\\n    headers = {\'content-type\': \'multipart/form-data\'}\\r\\n    print(url)\\r\\n    with open(image_payload, \'rb\') as file:\\r\\n        response = requests.post(url, files={\'payload\': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-131"
"DragNDropLite","Core","{\"formats\":{},\"name\":\"ImageAnonymizer\",\"alias\":\"ImageAnonymizer\",\"parentCategory\":\"167\",\"id\":132,\"codeGeneration\":{\"requirements\":[\"requests\",\"json\",\"urllib3\"],\"imports\":[\"import requests\",\"import json\",\"import urllib3\",\"import sys\"],\"script\":\"def ImageAnonymizer(image_payload):\\r\\n    url = \'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/anonymize\'\\r\\n    headers = {\'content-type\': \'multipart/form-data\'}\\r\\n    print(url)\\r\\n    with open(image_payload, \'rb\') as file:\\r\\n        response = requests.post(url, files={\'payload\': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\n\"},\"category\":\"RAI\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{}}","DragNDropLite-132"
"DragNDropLite","Core","{\"id\":133,\"name\":\"AdapterMethod\",\"alias\":\"AdapterMethod\",\"category\":\"\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{\"adaptermethod\":\"\"},\"formats\":{\"adaptermethod\":\"dropdown\"}}","DragNDropLite-133"
"DragNDropLite","Core","{\"formats\":{\"bucket\":\"text\",\"key\":\"text\"},\"classname\":\"Download_Data\",\"name\":\"Download_Data\",\"alias\":\"Download_Data\",\"parentCategory\":\"\",\"codeGeneration\":{\"requirements\":[\"boto3\",\"re\",\"botocore\",\"pathlib\",\"datasets\",\"transformers\",\"torch\",\"tqdm\",\"scikit-learn\"],\"imports\":[\"import os\",\"import pathlib\",\"import boto3\",\"import botocore\",\"import json\",\"import re\",\"from datasets import Dataset, DatasetDict\",\"from pathlib import Path\",\"import logging\",\"from collections import defaultdict\",\"from tqdm import tqdm\",\"from datasets import Dataset, DatasetDict\",\"from transformers import AutoTokenizer, AutoModelForCausalLM\",\"from transformers import DataCollatorForLanguageModeling\",\"from transformers import Trainer, TrainingArguments\",\"import torch\",\"import random\",\"from tqdm import tqdm\",\"import argparse\",\"import pandas as pd\",\"from sklearn.model_selection import train_test_split\",\"import gc\",\"import pickle as pkl\"],\"script\":\"logging_dir = \'/app/logs\'\\nlog_filename = \'create.log\'\\nPath(logging_dir).mkdir(parents=True, exist_ok=True)\\nlog_filename = os.path.join(logging_dir, log_filename)\\nlogging.basicConfig(filename=log_filename,\\n                    format=\'%(asctime)s %(message)s\',\\n                    filemode=\'w\')\\nlogger = logging.getLogger()\\nlogger.setLevel(logging.INFO)\\n\\ndef Download_Data(bucket_param, key_param):\\n    bucket = bucket_param\\n    key = key_param\\n    WORKING_DIRECTORY = \'dataset_file\'\\n    credentials = {\'secret_key\':\'g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\',\'endpoint\':\'https://10.82.53.110\',\'access_key\':\'GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\'}\\n    if not os.path.exists(WORKING_DIRECTORY):\\n        os.makedirs(WORKING_DIRECTORY)\\n    model_path = os.path.join(WORKING_DIRECTORY)\\n\\n    endpoint = credentials[\'endpoint\']\\n    access_key = credentials[\'access_key\']\\n    secret_key = credentials[\'secret_key\']\\n    s3 = boto3.resource(service_name=\'s3\', endpoint_url=endpoint, aws_access_key_id=access_key,\\n                        aws_secret_access_key=secret_key, verify=False)\\n    bucket_object = s3.Bucket(bucket)\\n    head, file_name = os.path.split(key)\\n    for my_bucket_object in bucket_object.objects.filter(Prefix=key):\\n        logger.info(my_bucket_object)\\n        object_save_path = (\\n            f\'{model_path}/{pathlib.Path(my_bucket_object.key).name}\'\\n        )\\n        bucket_object.download_file(my_bucket_object.key, object_save_path)\\n    \\n    dataset_file_path = os.path.join(WORKING_DIRECTORY,file_name)\\n    logger.info(dataset_file_path,\'download completed\')\\n    return dataset_file_path\\n\\n\\n\"},\"id\":134,\"category\":\"\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"attributes\":{\"bucket\":\"\",\"key\":\"\"}}","DragNDropLite-134"
"DragNDropLite","Core","{\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"MYSQLExtractorConfig\",\"name\":\"MYSQL Extractor\",\"alias\":\"MYSQL Extractor\",\"parentCategory\":\"1\",\"id\":135,\"codeGeneration\":{\"requirements\":[\"mysql-connector-python\"],\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"import json\",\"from leaputils import Security\"],\"script\":\"def MYSQLExtractor(dataset_param=\'\'):\\r    attributes = json.loads(dataset_param[\'attributes\'])\\r    datasource = json.loads(dataset_param[\'datasource\'][\'connectionDetails\'])\\r    def getConnection():\\r        username = datasource[\'userName\']\\r        password = Security.decrypt(datasource[\'password\'], dataset_param[\'datasource\'][\'salt\'])\\r        url = datasource[\'url\']\\r        host = urlparse(url[5:]).hostname\\r        port = urlparse(url[5:]).port\\r        database = urlparse(url[5:]).path.rsplit(\'/\', 1)[1]\\r        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port=port)\\r        return connection\\r    connection = getConnection()\\r\\r    query = attributes[\'Query\']  # self.mapQueryParams()\\r    cursor = connection.cursor(dictionary=True)\\r    cursor.execute(query)\\r    results = cursor.fetchall()\\r    return results\\r\\r\\r\\r\\n\"},\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"attributes\":{\"dataset\":\"\"}}","DragNDropLite-135"
"DragNDropLite","Core","{\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"MYSQLLoaderConfig\",\"name\":\"MYSQL Loader\",\"alias\":\"MYSQL Loader\",\"parentCategory\":\"2\",\"id\":136,\"codeGeneration\":{\"requirements\":[\"mysql-connector-python\"],\"imports\":[\"import mysql\",\"import json\",\"from leaputils import Security\",\"from urllib.parse import urlparse\"],\"script\":\"def MYSQLLoader(dataset, dataset_param=\'\'):\\r    attributes = json.loads(dataset_param[\'attributes\'])\\r    datasource = json.loads(dataset_param[\'datasource\'][\'connectionDetails\'])\\r    mode = attributes[\'writeMode\']\\r    url = datasource[\'url\']\\r    tablename = attributes[\'tableName\']\\r    username = datasource[\'userName\']\\r    password = Security.decrypt(datasource[\'password\'], dataset_param[\'datasource\'][\'salt\'])\\r    host = urlparse(url[5:]).hostname\\r    port = urlparse(url[5:]).port\\r    database = urlparse(url[5:]).path.rsplit(\'/\', 1)[1]\\r\\r    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\r    mycursor = cnx.cursor()\\r    if dataset != None and len(dataset) > 0:\\r        columnList = list(dataset[0].keys())\\r    if mode in \'overwrite\':\\r        mycursor.execute(\'Drop table IF EXISTS {0}\'.format(tablename))\\r\\r    # create table if not exists\\r    column_definition = \', \'.join([\'{0} TEXT\'.format(c) for c in columnList])\\r    createQuery = \' CREATE TABLE IF NOT EXISTS {0} ({1})\'.format(tablename, column_definition)\\r    mycursor.execute(createQuery)\\r    data = []\\r    for row in dataset:\\r        try:\\r            paramsDict = {}\\r            values = []\\r            for i in range(0, len(columnList)):\\r                paramsDict[columnList[i]] = row[columnList[i]]\\r                values.append(row[columnList[i]])\\r\\r            columns = \', \'.join(\'{0}\'.format(k) for k in paramsDict)\\r            duplicates = \', \'.join(\'{0}=VALUES({0})\'.format(k) for k in paramsDict)\\r            place_holders = \', \'.join(\'%s\'.format(k) for k in paramsDict)\\r\\r            query = \'INSERT INTO {0} ({1}) VALUES ({2})\'.format(tablename, columns, place_holders)\\r            if mode in (\'update\'):\\r                query = \'{0} ON DUPLICATE KEY UPDATE {1}\'.format(query, duplicates)\\r            data.append(values)\\r\\r        except Exception as e:\\r            logger.error(\'{0}:{1}\'.format(e, row))\\r    if (len(data) > 0):\\r        mycursor.executemany(query, data)\\r        cnx.commit()\\r\\r    mycursor.close()\\r    cnx.close()\\r\\r\\r\\r\\r\\r\\n\"},\"category\":\"DatasetLoaderConfig\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"attributes\":{\"dataset\":\"\"}}","DragNDropLite-136"
"DragNDropLite","Core","{\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"DatasetSourceConfig\",\"name\":\"DataSourcedict\",\"alias\":\"DataSourcedict\",\"parentCategory\":\"1\",\"id\":137,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def DataSourcedictREST(dataset_datasource_Url=\'\', dataset_datasource_AuthDetails_username=\'\', dataset_datasource_salt=\'\', dataset_datasource_AuthDetails_password=\'\'):\\r\\n    DSdict = {\\r\\n        \'Url\': dataset_datasource_Url,\\r\\n        \'salt\': dataset_datasource_salt,\\r\\n        \'AuthDetails\': {\\r\\n            \'username\': dataset_datasource_AuthDetails_username,\\r\\n            \'password\': dataset_datasource_AuthDetails_password\\r\\n        }\\r\\n    }\\r\\n    return DSdict\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"DatasetSourceConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"attributes\":{\"dataset\":\"\"}}","DragNDropLite-137"
"DragNDropLite","Core","{\"formats\":{\"dataset\":\"dropdown\"},\"classname\":\"DatasetExtractorConfig\",\"name\":\"MINIO Extractor\",\"alias\":\"MINIO Extractor\",\"parentCategory\":\"1\",\"codeGeneration\":{\"imports\":[\"from urllib.parse import urlparse\",\"from minio import Minio\"],\"requirments\":[\"minio\"],\"script\":\"\\n\\nd\\ne\\nf\\n \\nD\\na\\nt\\na\\ns\\ne\\nt\\nE\\nx\\nt\\nr\\na\\nc\\nt\\no\\nr\\nM\\nI\\nN\\nI\\nO\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\nu\\nr\\nl\\n=\\n\'\\n\'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\na\\nc\\nc\\ne\\ns\\ns\\nK\\ne\\ny\\n=\\n\'\\n\'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\ns\\ne\\nc\\nr\\ne\\nt\\nK\\ne\\ny\\n=\\n\'\\n\'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n=\\n\'\\n\'\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n=\\n\'\\n\'\\n)\\n:\\n\\n\\n \\n \\n \\n \\nU\\nR\\nL\\n \\n=\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\nu\\nr\\nl\\n\\n\\n \\n \\n \\n \\ns\\ne\\nc\\nu\\nr\\ne\\n \\n=\\n \\nT\\nr\\nu\\ne\\n \\ni\\nf\\n \\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\ns\\nc\\nh\\ne\\nm\\ne\\n \\n=\\n=\\n \\n\'\\nh\\nt\\nt\\np\\ns\\n\'\\n \\ne\\nl\\ns\\ne\\n \\nF\\na\\nl\\ns\\ne\\n\\n\\n \\n \\n \\n \\nc\\nl\\ni\\ne\\nn\\nt\\n \\n=\\nM\\ni\\nn\\ni\\no\\n(\\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\nh\\no\\ns\\nt\\nn\\na\\nm\\ne\\n+\\n\'\\n:\\n\'\\n+\\ns\\nt\\nr\\n(\\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\np\\no\\nr\\nt\\n)\\n,\\na\\nc\\nc\\ne\\ns\\ns\\n_\\nk\\ne\\ny\\n=\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\na\\nc\\nc\\ne\\ns\\ns\\nK\\ne\\ny\\n,\\ns\\ne\\nc\\nr\\ne\\nt\\n_\\nk\\ne\\ny\\n=\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\ns\\ne\\nc\\nr\\ne\\nt\\nK\\ne\\ny\\n,\\ns\\ne\\nc\\nu\\nr\\ne\\n=\\ns\\ne\\nc\\nu\\nr\\ne\\n)\\n\\n\\n \\n \\n \\n \\ni\\nf\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n.\\ns\\np\\nl\\ni\\nt\\n(\\n\'\\n.\\n\'\\n)\\n[\\n-\\n1\\n]\\n \\n=\\n=\\n \\n\'\\nc\\ns\\nv\\n\'\\n:\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\no\\nb\\nj\\n \\n=\\n \\nc\\nl\\ni\\ne\\nn\\nt\\n.\\ng\\ne\\nt\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n \\n=\\n \\np\\nd\\n.\\nr\\ne\\na\\nd\\n_\\nc\\ns\\nv\\n(\\no\\nb\\nj\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\nt\\nu\\nr\\nn\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n\\n\\n \\n \\n \\n \\ne\\nl\\ns\\ne\\n:\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n \\n=\\n \\n\'\\n.\\n/\\n\'\\n \\n+\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\ns\\nu\\nl\\nt\\n \\n=\\n \\nc\\nl\\ni\\ne\\nn\\nt\\n.\\nf\\ng\\ne\\nt\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n,\\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\nt\\nu\\nr\\nn\\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n\"},\"id\":138,\"category\":\"ExtractorConfig\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"attributes\":{\"dataset\":\"\"}}","DragNDropLite-138"
"DragNDropLite","Core","{\"formats\":{},\"name\":\" TransformerConfig\",\"alias\":\" TransformerConfig\",\"parentCategory\":\"143\",\"id\":143,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"\\n\\n\"},\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"attributes\":{}}","DragNDropLite-143"
"DragNDropLite","Core","{\"id\":146,\"name\":\"BaseConfig\",\"alias\":\"BaseConfig\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-146"
"DragNDropLite","Core","{\"formats\":{},\"name\":\"Model\",\"alias\":\"Model\",\"parentCategory\":\"\",\"id\":147,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"\\n\\n\"},\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"attributes\":{}}","DragNDropLite-147"
"DragNDropLite","Core","{\"id\":148,\"name\":\"Regression\",\"alias\":\"Regression\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-148"
"DragNDropLite","Core","{\"id\":149,\"name\":\" DocumentComprehension\",\"alias\":\" DocumentComprehension\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-149"
"DragNDropLite","Core","{\"id\":150,\"name\":\"Classification\",\"alias\":\"Classification\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-150"
"DragNDropLite","Core","{\"id\":155,\"name\":\" Semantic Similarity\",\"alias\":\" Semantic Similarity\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-155"
"DragNDropLite","Core","{\"id\":156,\"name\":\"Sentiment analysis\",\"alias\":\"Sentiment analysis\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-156"
"DragNDropLite","Core","{\"id\":157,\"name\":\"Data Profiling\",\"alias\":\"Data Profiling\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-157"
"DragNDropLite","Core","{\"id\":166,\"name\":\"Convertors\",\"alias\":\"Convertors\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-166"
"DragNDropLite","Core","{\"id\":167,\"name\":\"RAI\",\"alias\":\"RAI\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-167"
"DragNDropLite","Core","{\"id\":168,\"name\":\"LLM_Model\",\"alias\":\"LLM_Model\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-168"
"DragNDropLite","Core","{\"id\":169,\"name\":\"Endpoint\",\"alias\":\"Endpoint\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-169"
"DragNDropLite","Core","{\"id\":171,\"name\":\"LangChain\",\"alias\":\"LangChain\",\"category\":\"\",\"inputEndpoints\":[],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[],\"requirements\":[],\"script\":\"\"},\"parentCategory\":\"\",\"attributes\":{},\"formats\":{}}","DragNDropLite-171"
"DragNDropLite","Core","{\"formats\":{\"connections\":\"\"},\"classname\":\"Connection\",\"name\":\"Connection\",\"alias\":\"Connection\",\"parentCategory\":\"\",\"id\":172,\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"category\":\"Connection\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"attributes\":{\"connections\":\"\"}}","DragNDropLite-172"
"DragNDropLite","Core","{\"id\":78,\"name\":\"FlaskAPP\",\"category\":\"FlaskAPP\",\"parentCategory\":\"\",\"classname\":\"FlaskAPP\",\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"codeGeneration\":{\"imports\":[\"from flask import Flask\",\"from flask import jsonify\",\"from flask import request\",\"import openai\"],\"requirements\":[\"flask\",\"openai==0.28\"],\"script\":\"def FlaskAPP(port_param=5000,script_param=\'\'):\\r\\n    app.run(debug=True, host=\'0.0.0.0\', port = port_param)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"formats\":{\"port\":\"integer\",\"script\":\"textarea\"},\"alias\":\"FlaskAPP\",\"attributes\":{\"port\":\"\",\"script\":\"\"}}","DragNDropLiteCore-78"
