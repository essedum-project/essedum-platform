"created_by","created_date","deleted","description","job_id","json_content","lastmodifiedby","alias","lastmodifieddate","name","organization","type","version","tags","interfacetype","pipeline_metadata"
"lalith.basna@ad.infosys.com","2023-08-02 11:53:54.839000","","","NULL","{\"elements\":[{\"id\":\"WPCyu\",\"alias\":\"Combine Tools\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"combinetools\",\"requirements\":\"\",\"params\":[],\"script\":[\"def combinetools(tool1, tool2):\\r    #python-script Data\\r    tools = [\\r        tool1, tool2\\r        ]\\r\\r    return tools\\r\"]},\"position_x\":\"632\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ubKdP\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"bUnkk\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset3\",\"position\":\"BottomCenter\",\"elementId\":\"pgeDW\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\"},\"context\":[{\"name\":\"sales\",\"description\":\"useful for when a user is interested in various Neo4j information, \\n                       use-cases, or applications. A user is not asking for any debugging, but is only\\n                       interested in general advice for integrating and using Neo4j.\\n                       Input should be a fully formed question.\"},{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"name\":\"support\",\"description\":\"useful for when when a user asks to optimize or debug a Cypher statement or needs\"},{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"ChAYc\",\"alias\":\"Azure OpenAI GPT 35\",\"name\":\"Azure OpenAI GPT 35\",\"classname\":\"Azure OpenAI GPT 35 \",\"category\":\"LLM\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},\"position_x\":\"731\",\"position_y\":\"74\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"ubKdP\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"model_name\":\"text\",\"openai_api_type\":\"text\",\"deployment_name\":\"text\",\"openai_api_base\":\"text\",\"openai_api_version\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chat_models import AzureChatOpenAI\"],\"script\":\"def AzureOpenAIGPT35(deployment_name_param=\'\', \\r\\n                    model_name_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_version_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    #initialize LLM object\\r\\n    llm = AzureChatOpenAI(    \\r\\n        deployment_name=deployment_name_param, \\r\\n        model_name=model_name_param, \\r\\n        openai_api_key=openai_api_key_param,\\r\\n        openai_api_version = openai_api_version_param, \\r\\n        openai_api_base=openai_api_base_param,\\r\\n        openai_api_type=openai_api_type_param,\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n    return llm\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"ZAgSv\",\"alias\":\"Retrieval Chain\",\"name\":\"Retrieval Chain\",\"classname\":\"Retrieval Chain\",\"category\":\"Chain\",\"attributes\":{\"chain_type\":\"stuff\"},\"position_x\":\"354\",\"position_y\":\"73\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"bUnkk\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in2\",\"position\":\"TopCenter\",\"elementId\":\"BqdGO\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in3\",\"position\":\"BottomCenter\",\"elementId\":\"gtIwa\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"Tkhco\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\",\"in2\",\"in3\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"chain_type\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chains import RetrievalQA\"],\"script\":\"def RetrievalChain(llm, retriever_obj, prompt = None, chain_type_param=\'\',):\\r\\n    # create tool function\\r\\n    # initialize vectorstore retriever object\\r\\n    chain_type_kwargs = None\\r\\n    if prompt is not None:\\r\\n        chain_type_kwargs={\'prompt\': prompt}\\r\\n    timekeeping_policy = RetrievalQA.from_chain_type(\\r\\n        llm=llm,\\r\\n        chain_type=chain_type_param,\\r\\n        retriever=retriever_obj.as_retriever(),\\r\\n        chain_type_kwargs = chain_type_kwargs\\r\\n    )\\r\\n    return timekeeping_policy\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"GvhOK\",\"alias\":\"Retrieval Chain\",\"name\":\"Retrieval Chain\",\"classname\":\"Retrieval Chain\",\"category\":\"Chain\",\"attributes\":{\"chain_type\":\"stuff\"},\"position_x\":\"355\",\"position_y\":\"301\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"pgeDW\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in2\",\"position\":\"TopCenter\",\"elementId\":\"SJpTY\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in3\",\"position\":\"BottomCenter\",\"elementId\":\"MxgMe\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"MJzxN\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\",\"in2\",\"in3\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"chain_type\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chains import RetrievalQA\"],\"script\":\"def RetrievalChain(llm, retriever_obj, prompt = None, chain_type_param=\'\',):\\r\\n    # create tool function\\r\\n    # initialize vectorstore retriever object\\r\\n    chain_type_kwargs = None\\r\\n    if prompt is not None:\\r\\n        chain_type_kwargs={\'prompt\': prompt}\\r\\n    timekeeping_policy = RetrievalQA.from_chain_type(\\r\\n        llm=llm,\\r\\n        chain_type=chain_type_param,\\r\\n        retriever=retriever_obj.as_retriever(),\\r\\n        chain_type_kwargs = chain_type_kwargs\\r\\n    )\\r\\n    return timekeeping_policy\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"bUnkk\",\"alias\":\"Initialize Tool\",\"name\":\"Initialize Tool\",\"classname\":\"Initialize Tool\",\"category\":\"Tool\",\"attributes\":{\"name\":\"sales\",\"description\":\"useful for when a user is interested in various Neo4j information, \\n                       use-cases, or applications. A user is not asking for any debugging, but is only\\n                       interested in general advice for integrating and using Neo4j.\\n                       Input should be a fully formed question.\"},\"position_x\":\"537\",\"position_y\":\"73\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"ZAgSv\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"WPCyu\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"name\":\"text\",\"description\":\"textarea\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import Tool\"],\"script\":\"def InitializeTool(function_obj, name_param = \'\', description_param=\'\'):\\r\\n    tool =  Tool(\\r\\n                    name = name_param,\\r\\n                    func=function_obj.run,\\r\\n                    description = description_param\\r\\n                )\\r\\n    return tool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"pgeDW\",\"alias\":\"Initialize Tool\",\"name\":\"Initialize Tool\",\"classname\":\"Initialize Tool\",\"category\":\"Tool\",\"attributes\":{\"name\":\"support\",\"description\":\"useful for when when a user asks to optimize or debug a Cypher statement or needs\"},\"position_x\":\"534\",\"position_y\":\"301\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"GvhOK\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"WPCyu\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"name\":\"text\",\"description\":\"textarea\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import Tool\"],\"script\":\"def InitializeTool(function_obj, name_param = \'\', description_param=\'\'):\\r\\n    tool =  Tool(\\r\\n                    name = name_param,\\r\\n                    func=function_obj.run,\\r\\n                    description = description_param\\r\\n                )\\r\\n    return tool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"BqdGO\",\"alias\":\"Azure Searches\",\"name\":\"Azure Searches\",\"classname\":\"Azure Searches\",\"category\":\"vector\",\"attributes\":{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},\"position_x\":\"178\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"ZAgSv\",\"elementPosition\":\"TopCenter\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"EaHwR\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"vector_store_address\":\"text\",\"vector_store_password\":\"text\",\"index_name\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.vectorstores.azuresearch import AzureSearch\"],\"script\":\"def AzureSearches(embeddings, vector_store_address_param=\'\', vector_store_password_param=\'\', index_name_param=\'\'):\\r\\n    vector_store_address: str = vector_store_address_param\\r\\n    vector_store_password: str = vector_store_password_param\\r\\n\\r\\n    index_name: str = index_name_param\\r\\n    store: AzureSearch = AzureSearch(\\r\\n        azure_search_endpoint=vector_store_address,\\r\\n        azure_search_key=vector_store_password,\\r\\n        index_name=index_name,\\r\\n        embedding_function=embeddings.embed_query,\\r\\n    )\\r\\n\\r\\n    return store\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"}]},{\"id\":\"SJpTY\",\"alias\":\"Azure Searches\",\"name\":\"Azure Searches\",\"classname\":\"Azure Searches\",\"category\":\"vector\",\"attributes\":{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},\"position_x\":\"178\",\"position_y\":\"227\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"GvhOK\",\"elementPosition\":\"TopCenter\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"fGCrS\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"vector_store_address\":\"text\",\"vector_store_password\":\"text\",\"index_name\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.vectorstores.azuresearch import AzureSearch\"],\"script\":\"def AzureSearches(embeddings, vector_store_address_param=\'\', vector_store_password_param=\'\', index_name_param=\'\'):\\r\\n    vector_store_address: str = vector_store_address_param\\r\\n    vector_store_password: str = vector_store_password_param\\r\\n\\r\\n    index_name: str = index_name_param\\r\\n    store: AzureSearch = AzureSearch(\\r\\n        azure_search_endpoint=vector_store_address,\\r\\n        azure_search_key=vector_store_password,\\r\\n        index_name=index_name,\\r\\n        embedding_function=embeddings.embed_query,\\r\\n    )\\r\\n\\r\\n    return store\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"}]},{\"id\":\"ubKdP\",\"alias\":\"Initialize Agent\",\"name\":\"Initialize Agent\",\"classname\":\"Initialize Agent\",\"category\":\"Agent\",\"attributes\":{\"prefix_argument\":\"\",\"agent\":\"zero-shot-react-description\",\"max_execution_time\":\"180\",\"early_stopping_method\":\"generate\",\"verbose\":\"True\"},\"position_x\":\"826\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"WPCyu\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in2\",\"position\":\"TopCenter\",\"elementId\":\"ChAYc\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"KzioX\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in1\",\"in2\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"prefix_argument\":\"textarea\",\"agent\":\"text\",\"max_execution_time\":\"text\",\"early_stopping_method\":\"text\",\"verbose\":\"bool\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import AgentType\",\"from langchain.agents import initialize_agent\"],\"script\":\"def InitializeAgent(tools, llm, agent_param=\'\', max_execution_time_param=\'\', early_stopping_method_param=\'\', prefix_argument_param=\'\', verbose_param=\'\'):\\r\\n    # change the value of the prefix argument in the initialize_agent function. This will overwrite the default prompt template of the zero shot agent type\\r\\n    agent_kwargs = {\'prefix\': prefix_argument_param}\\r\\n    \\r\\n    \\r\\n    # initialize the LLM agent\\r\\n    agent = initialize_agent(tools, \\r\\n                             llm, \\r\\n                             agent=agent_param, \\r\\n                             verbose=verbose_param, \\r\\n                             max_execution_time=int(max_execution_time_param), \\r\\n                             early_stopping_method=early_stopping_method_param,\\r\\n                             agent_kwargs=agent_kwargs\\r\\n                             )\\r\\n    # agent = initialize_agent(tools, \\r\\n    #                          llm, \\r\\n    #                          agent=\'zero-shot-react-description\', \\r\\n    #                          verbose=True, \\r\\n    #                          agent_kwargs=agent_kwargs\\r\\n    #                          )\\r\\n    return agent\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"FunctionName\":\"combinetools\",\"requirements\":\"\",\"params\":[],\"script\":[\"def combinetools(tool1, tool2):\\r    #python-script Data\\r    tools = [\\r        tool1, tool2\\r        ]\\r\\r    return tools\\r\"]},{\"name\":\"sales\",\"description\":\"useful for when a user is interested in various Neo4j information, \\n                       use-cases, or applications. A user is not asking for any debugging, but is only\\n                       interested in general advice for integrating and using Neo4j.\\n                       Input should be a fully formed question.\"},{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"name\":\"support\",\"description\":\"useful for when when a user asks to optimize or debug a Cypher statement or needs\"},{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"gtIwa\",\"alias\":\"Prompt Templates\",\"name\":\"Prompt Templates\",\"classname\":\"Prompt Templates\",\"category\":\"Prompt\",\"attributes\":{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},\"position_x\":\"181\",\"position_y\":\"145\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"ZAgSv\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"input_var\":\"text\",\"validate_template\":\"bool\",\"templete\":\"textarea\",\"template_format\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.prompts import PromptTemplate\"],\"script\":\"def PromptTemplates(templete_param=\'\', input_var_param=\'\', validate_template_param=True, template_format_param=\'\'):\\r\\n    SUPPORT_PROMPT = PromptTemplate(\\r\\n        template=templete_param, \\r\\n        input_variables=input_var_param.split(\',\'),\\r\\n        validate_template=validate_template_param,\\r\\n        template_format=template_format_param,\\r\\n\\r\\n    )\\r\\n    return SUPPORT_PROMPT\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"MxgMe\",\"alias\":\"Prompt Templates\",\"name\":\"Prompt Templates\",\"classname\":\"Prompt Templates\",\"category\":\"Prompt\",\"attributes\":{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},\"position_x\":\"173\",\"position_y\":\"386\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"GvhOK\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"input_var\":\"text\",\"validate_template\":\"bool\",\"templete\":\"textarea\",\"template_format\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.prompts import PromptTemplate\"],\"script\":\"def PromptTemplates(templete_param=\'\', input_var_param=\'\', validate_template_param=True, template_format_param=\'\'):\\r\\n    SUPPORT_PROMPT = PromptTemplate(\\r\\n        template=templete_param, \\r\\n        input_variables=input_var_param.split(\',\'),\\r\\n        validate_template=validate_template_param,\\r\\n        template_format=template_format_param,\\r\\n\\r\\n    )\\r\\n    return SUPPORT_PROMPT\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"EaHwR\",\"alias\":\"Azure OpenAI text embedding ada 002\",\"name\":\"Azure OpenAI text embedding ada 002\",\"classname\":\"Azure OpenAI text embedding ada 002\",\"category\":\"Embedding\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},\"position_x\":\"0\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"BqdGO\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"openai_api_type\":\"text\",\"model\":\"text\",\"openai_api_base\":\"text\",\"deployment\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.embeddings.openai import OpenAIEmbeddings\"],\"script\":\"def AzureOpenAItextembeddingada002(deployment_param=\'\', \\r\\n                    model_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    # initialize embeddings object; for use with user query/input\\r\\n    embed = OpenAIEmbeddings(\\r\\n                    deployment=deployment_param,\\r\\n                    model=model_param,\\r\\n                    openai_api_key=openai_api_key_param,\\r\\n                    openai_api_base=openai_api_base_param,\\r\\n                    openai_api_type=openai_api_type_param,\\r\\n                )\\r\\n\\r\\n    return embed\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"fGCrS\",\"alias\":\"Azure OpenAI text embedding ada 002\",\"name\":\"Azure OpenAI text embedding ada 002\",\"classname\":\"Azure OpenAI text embedding ada 002\",\"category\":\"Embedding\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},\"position_x\":\"0\",\"position_y\":\"227\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"SJpTY\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"openai_api_type\":\"text\",\"model\":\"text\",\"openai_api_base\":\"text\",\"deployment\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.embeddings.openai import OpenAIEmbeddings\"],\"script\":\"def AzureOpenAItextembeddingada002(deployment_param=\'\', \\r\\n                    model_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    # initialize embeddings object; for use with user query/input\\r\\n    embed = OpenAIEmbeddings(\\r\\n                    deployment=deployment_param,\\r\\n                    model=model_param,\\r\\n                    openai_api_key=openai_api_key_param,\\r\\n                    openai_api_base=openai_api_base_param,\\r\\n                    openai_api_type=openai_api_type_param,\\r\\n                )\\r\\n\\r\\n    return embed\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"Tkhco\",\"alias\":\"Azure OpenAI GPT 35\",\"name\":\"Azure OpenAI GPT 35\",\"classname\":\"Azure OpenAI GPT 35 \",\"category\":\"LLM\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},\"position_x\":\"177\",\"position_y\":\"74\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"ZAgSv\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"model_name\":\"text\",\"openai_api_type\":\"text\",\"deployment_name\":\"text\",\"openai_api_base\":\"text\",\"openai_api_version\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chat_models import AzureChatOpenAI\"],\"script\":\"def AzureOpenAIGPT35(deployment_name_param=\'\', \\r\\n                    model_name_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_version_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    #initialize LLM object\\r\\n    llm = AzureChatOpenAI(    \\r\\n        deployment_name=deployment_name_param, \\r\\n        model_name=model_name_param, \\r\\n        openai_api_key=openai_api_key_param,\\r\\n        openai_api_version = openai_api_version_param, \\r\\n        openai_api_base=openai_api_base_param,\\r\\n        openai_api_type=openai_api_type_param,\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n    return llm\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"MJzxN\",\"alias\":\"Azure OpenAI GPT 35\",\"name\":\"Azure OpenAI GPT 35\",\"classname\":\"Azure OpenAI GPT 35 \",\"category\":\"LLM\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},\"position_x\":\"172\",\"position_y\":\"302\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"GvhOK\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"model_name\":\"text\",\"openai_api_type\":\"text\",\"deployment_name\":\"text\",\"openai_api_base\":\"text\",\"openai_api_version\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chat_models import AzureChatOpenAI\"],\"script\":\"def AzureOpenAIGPT35(deployment_name_param=\'\', \\r\\n                    model_name_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_version_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    #initialize LLM object\\r\\n    llm = AzureChatOpenAI(    \\r\\n        deployment_name=deployment_name_param, \\r\\n        model_name=model_name_param, \\r\\n        openai_api_key=openai_api_key_param,\\r\\n        openai_api_version = openai_api_version_param, \\r\\n        openai_api_base=openai_api_base_param,\\r\\n        openai_api_type=openai_api_type_param,\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n    return llm\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"KzioX\",\"alias\":\"Chat UI\",\"name\":\"Chat UI\",\"classname\":\"Chat UI\",\"category\":\"Chat UI\",\"attributes\":{\"prompt_text\":\"\\n<h2>Prompt Used </h2>\\n<h4>Neo4j Customer Support Bot:</h4> \\n<p>As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.</br>\\n<b>❓ Example Queries</b>\\n<ul>\\n    <li>Can you provide a cypher query to get all relationships between a specific set of nodes?</li>\\n    <li>Can you provide cypher query to get all nodes linked by a specific relationship?</li>\\n    <li>Can you provide cypher query to find nodes which have more incoming than outgoing connections of a particular kind?</li>\\n    <li>Can you provide cypher query to find all nodes connected to certain nodes in a directed graph?</li>\\n<ul>\\n</br>\\n<h4>Neo4j Marketing Bot:</h4> \\nAs a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.</br>\\n<b>❓ Example Queries</b>\\n<ul>\\n    <li>How are graph databases used in health care domain?</li>\\n</ul>\",\"markdown\":\"## Ask your Neo4j-related questions here.\",\"header\":\"? Neo4j Product Chatbot - Chain of Thought Demo\"},\"position_x\":\"842\",\"position_y\":\"307\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"ubKdP\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[],\"formats\":{\"prompt_text\":\"textarea\",\"markdown\":\"textarea\",\"header\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import streamlit as st\",\"import random\",\"from streamlit_chat import message\",\"import os\",\"from langchain.callbacks import StreamlitCallbackHandler\"],\"script\":\"from langchain.callbacks.base import BaseCallbackHandler\\r\\n\\r\\nclass StreamHandler(BaseCallbackHandler):\\r\\n    def __init__(self, initial_text=\'Start..\'):\\r\\n        self.text = initial_text\\r\\n\\r\\n    def on_text(self, text: str, **kwargs) -> None:\\r\\n        starter_text = \'Begin!\'\\r\\n        self.text = text\\r\\n        if (self.text.find(starter_text) > 0):\\r\\n            self.text = self.text[self.text.find(starter_text) + len(starter_text):]\\r\\n        st.session_state[\'thoughts\'][len(st.session_state[\'thoughts\']) - 1] = self.text\\r\\n        message(self.text, key=self.text + \'_chain_of_thoughts\')\\r\\n\\r\\ndef ChatUI(agent, header_param=\'? LLM HR Chatbot - Chain of Thought Demo\', markdown_param=\'\', prompt_text_param=\'\'):    \\r\\n    print(\'Initializing the streamlit app..\')\\r\\n    \\r\\n    st.header(header_param)\\r\\n    st.markdown(prompt_text_param, unsafe_allow_html=True)\\r\\n    st.markdown(markdown_param, unsafe_allow_html=True)   \\r\\n    \\r\\n    stream_handler1 = StreamHandler()\\r\\n\\r\\n    if \'past\' not in st.session_state:\\r\\n        st.session_state[\'past\'] = []\\r\\n    if \'generated\' not in st.session_state:\\r\\n        st.session_state[\'generated\'] = []\\r\\n    \\r\\n    if \'thoughts\' not in st.session_state:\\r\\n        st.session_state[\'thoughts\'] = []\\r\\n    \\r\\n    if \'input_message_key\' not in st.session_state:\\r\\n        st.session_state[\'input_message_key\'] = str(random.random())\\r\\n    \\r\\n    chat_container = st.container()\\r\\n    user_input = st.text_input(\'Type your message and press Enter to send.\', key=st.session_state[\'input_message_key\'])\\r\\n    \\r\\n    if st.button(\'Send\'):\\r\\n        try:\\r\\n            message(user_input, is_user=True, key=user_input + \'_user\')\\r\\n            st.session_state[\'thoughts\'].append(\'\')\\r\\n            response = agent.run(user_input, callbacks=[stream_handler1])\\r\\n        except Exception as e:\\r\\n            print(\'The traceback is: \', e)\\r\\n            response = \'Invalid input. Please try again\'\\r\\n        \\r\\n        \\r\\n        message(response, key=user_input)\\r\\n    \\r\\n        st.session_state[\'past\'].append(user_input)\\r\\n        st.session_state[\'generated\'].append(response)\\r\\n    \\r\\n        st.session_state[\'input_message_key\'] = str(random.random())\\r\\n    \\r\\n        st.experimental_rerun()\\r\\n    \\r\\n    if st.session_state[\'generated\']:\\r\\n        with chat_container:\\r\\n            for i in range(len(st.session_state[\'generated\'])):\\r\\n                message(st.session_state[\'past\'][i], is_user=True, key=str(i) + \'_user\')\\r\\n                message(st.session_state[\'thoughts\'][i], key=str(i) + \'_chain_of_thoughts\')\\r\\n                message(st.session_state[\'generated\'][i], key=str(i))   \\r\\n\\n\"},\"context\":[{\"prefix_argument\":\"\",\"agent\":\"zero-shot-react-description\",\"max_execution_time\":\"180\",\"early_stopping_method\":\"generate\",\"verbose\":\"True\"},{\"FunctionName\":\"combinetools\",\"requirements\":\"\",\"params\":[],\"script\":[\"def combinetools(tool1, tool2):\\r    #python-script Data\\r    tools = [\\r        tool1, tool2\\r        ]\\r\\r    return tools\\r\"]},{\"name\":\"sales\",\"description\":\"useful for when a user is interested in various Neo4j information, \\n                       use-cases, or applications. A user is not asking for any debugging, but is only\\n                       interested in general advice for integrating and using Neo4j.\\n                       Input should be a fully formed question.\"},{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\\na powerful graph database used for building various applications.\\nYou should answer user inquiries based on the context provided and avoid making up answers.\\nIf you don\'t know the answer, simply state that you don\'t know.\\nRemember to provide relevant information about Neo4j\'s features, benefits,\\nand use cases to assist the user in understanding its value for application development.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"name\":\"support\",\"description\":\"useful for when when a user asks to optimize or debug a Cypher statement or needs\"},{\"chain_type\":\"stuff\"},{\"vector_store_address\":\"https://search-ins.search.windows.net\",\"vector_store_password\":\"4FNKXW99u0BfMTXd2s1LNo10N6CxPafTwi7jLGa7cnAzSeB6HOL4\",\"index_name\":\"neo4j-sales-kb-index\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"input_var\":\"context,question\",\"validate_template\":\"True\",\"templete\":\"As a Neo4j Customer Support bot, you are here to assist with any issues \\na user might be facing with their graph database implementation and Cypher statements.\\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\\nIf the provided context doesn\'t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\\n\\n{context}\\n\\nQuestion: {question}\",\"template_format\":\"f-string\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"},{\"key\":\"runCommand\",\"value\":\"python -m streamlit run AIPATWN_33047_generatedCode.py --server.headless true --server.port 8501\"}]}","barad.vishal@ad.infosys.com","AITwin_ITSupport","2023-09-07 05:33:57","AIPATWN_33047","leo1311","Langchain","57","\"\"","chain","{\"55\":{\"taskId\":\"494ccd14-8991-4f61-bf60-70dfbc583082\"}}"
"admin","2023-07-20 05:24:34.033000","","","NULL","{\"elements\":[{\"id\":\"Klquq\",\"alias\":\"Combine Tools\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"combinetools\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef combinetools(tool1, tool2, tool3):\\r    #python-script Data\\r    tools = [\\r        tool1, tool2, tool3\\r        ]\\r\\r    return tools\\r\"]},\"position_x\":\"632\",\"position_y\":\"262\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"rMUZJ\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"FvHlX\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"lGXBM\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset3\",\"position\":\"BottomCenter\",\"elementId\":\"SEqJN\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\"},\"context\":[{\"name\":\"Timekeeping Policies\",\"description\":\"Useful for when you need to answer questions about employee timekeeping policies.\\n    \\n<user>: What is the policy on unused vacation leave?\\n<assistant>: I need to check the timekeeping policies to answer this question.\\n<assistant>: Action: Timekeeping Policies\\n<assistant>: Action Input: Vacation Leave Policy - Unused Leave\"},{\"chain_type\":\"stuff\"},{\"FunctionName\":\"vector_retriever\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef vector_retriever(vectorstore):\\r    return vectorstore.as_retriever()\\r\"]},{\"text_field\":\"text\"},{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"name\":\"Employee Data\",\"description\":\"Useful for when you need to answer questions about employee data stored in pandas dataframe \'df\'. \\nRun python pandas operations on \'df\' to help you get the right answer.\\n\'df\' has the following columns: [employee_id,name,position,organizational_unit,rank,hire_date,regularization_date,vacation_leave,sick_leave,basic_pay_in_phpemployment_status,supervisor]\\n            \\n<user>: How many Sick Leave do I have left?\\n<assistant>: df[df[\'name\'] == \'Alexander Verdad\'][\'sick_leave\']\\n<assistant>: You have n sick leaves left.\"},{},{\"file_path\":\"employee_data/employee_data.csv\",\"file_system\":\"file1\",\"credential\":\"Y/tGx+bNiee2MD/hJ+4kzZTRE0uTDTQqAUy5eeeKKI09jjfI3QTv50qiSz1dIc/VHxpCJxJ8o4kDolHSwWWGMA==\",\"account_url\":\"https://act1.dfs.core.windows.net/\",\"decode\":\"utf-8\"},{\"name\":\"Calculator\",\"description\":\"Useful when you need to do math operations or arithmetic.\"},{\"verbose\":\"True\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"KJEgj\",\"alias\":\"Calculator Tool\",\"name\":\"math_chain_tool_function\",\"classname\":\"math_chain_tool_function\",\"category\":\"Function Tools\",\"attributes\":{\"verbose\":\"True\"},\"position_x\":\"242\",\"position_y\":\"351\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"SEqJN\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"LkYJn\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"verbose\":\"bool\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain import LLMMathChain\"],\"script\":\"def math_chain_tool_function(llm, verbose_param=True):\\r\\n    # create tool function\\r\\n    tool_function = LLMMathChain.from_llm(llm=llm, verbose=verbose_param)\\r\\n    return tool_function\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"fqCLH\",\"alias\":\"Python REPL Tool\",\"name\":\"python_ast_repl_tool_function\",\"classname\":\"python_ast_repl_tool_function\",\"category\":\"Function Tools\",\"attributes\":{},\"position_x\":\"248\",\"position_y\":\"263\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"lGXBM\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"jAkfn\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.tools.python.tool import PythonAstREPLTool\"],\"script\":\"def python_ast_repl_tool_function(df):\\r\\n    # create tool function\\r\\n    tool_function = PythonAstREPLTool(locals={\'df\': df}) # set access of python_repl tool to the dataframe\\r\\n    return tool_function\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"file_path\":\"employee_data/employee_data.csv\",\"file_system\":\"file1\",\"credential\":\"Y/tGx+bNiee2MD/hJ+4kzZTRE0uTDTQqAUy5eeeKKI09jjfI3QTv50qiSz1dIc/VHxpCJxJ8o4kDolHSwWWGMA==\",\"account_url\":\"https://act1.dfs.core.windows.net/\",\"decode\":\"utf-8\"}]},{\"id\":\"jAkfn\",\"alias\":\"Azure Datalake\",\"name\":\"datalake\",\"classname\":\"datalake\",\"category\":\"Data\",\"attributes\":{\"file_path\":\"employee_data/employee_data.csv\",\"file_system\":\"file1\",\"credential\":\"Y/tGx+bNiee2MD/hJ+4kzZTRE0uTDTQqAUy5eeeKKI09jjfI3QTv50qiSz1dIc/VHxpCJxJ8o4kDolHSwWWGMA==\",\"account_url\":\"https://act1.dfs.core.windows.net/\",\"decode\":\"utf-8\"},\"position_x\":\"20\",\"position_y\":\"263\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"fqCLH\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"file_path\":\"text\",\"file_system\":\"text\",\"credential\":\"text\",\"account_url\":\"text\",\"decode\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from azure.storage.filedatalake import DataLakeServiceClient\",\"from io import StringIO\",\"import pandas as pd\"],\"script\":\"def datalake(account_url_param=\'\', credential_param=\'\', file_system_param=\'\', file_path_param=\'\', decode_param=\'\'):\\r\\n    # create employee data tool \\r\\n    client = DataLakeServiceClient( # authenticate to azure datalake\\r\\n                                  account_url=account_url_param,\\r\\n                                  credential=credential_param)\\r\\n                            \\r\\n    # azure data lake boilerplate to load from file system.  \\r\\n    file = client.get_file_system_client(file_system_param).get_file_client(file_path_param).download_file().readall().decode(decode_param) \\r\\n    \\r\\n    csv_file = StringIO(file) \\r\\n    #csv_file = \'/folder/employee_data.csv\'\\r\\n    df = pd.read_csv(csv_file) # load employee_data.csv as dataframe\\r\\n    return df\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"inLeK\",\"alias\":\"setup vectordb\",\"name\":\"setup_pinecode_vector\",\"classname\":\"setup_pinecode_vector\",\"category\":\"vector\",\"attributes\":{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},\"position_x\":\"4\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"GzXxI\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"environment\":\"text\",\"api_key\":\"text\",\"index_name\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import pinecone\"],\"script\":\"def setup_pinecode_vector(api_key_param=\'\', environment_param=\'\', index_name_param=\'\'):\\r\\n    # initialize pinecone client and connect to pinecone index\\r\\n    pinecone.init(\\r\\n            api_key=api_key_param,  \\r\\n            environment=environment_param \\r\\n    )\\r\\n\\r\\n    index_name = index_name_param\\r\\n    index = pinecone.Index(index_name) # connect to pinecone index\\r\\n    return index\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"GzXxI\",\"alias\":\"vector_store\",\"name\":\"vector_store\",\"classname\":\"vector_store\",\"category\":\"Data\",\"attributes\":{\"text_field\":\"text\"},\"position_x\":\"216\",\"position_y\":\"23\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in2\",\"position\":\"TopCenter\",\"elementId\":\"inLeK\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"GPJdl\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"DlDpJ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\",\"in2\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"text_field\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.vectorstores import Pinecone\"],\"script\":\"def vector_store(embed, index, text_field_param=\'\'):\\r\\n    vectorstore = Pinecone(\\r\\n        index, embed.embed_query, text_field_param\\r\\n    )\\r\\n    return vectorstore\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"}]},{\"id\":\"oMxKy\",\"alias\":\"Retrieval_QA\",\"name\":\"retrieval_qa_tool_function\",\"classname\":\"retrieval_qa_tool_function\",\"category\":\"Tool Function\",\"attributes\":{\"chain_type\":\"stuff\"},\"position_x\":\"241\",\"position_y\":\"166\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"FvHlX\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in2\",\"position\":\"TopCenter\",\"elementId\":\"GPJdl\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"BgPoi\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\",\"in2\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"chain_type\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chains import RetrievalQA\"],\"script\":\"def retrieval_qa_tool_function(llm, retriever_obj, chain_type_param=\'\'):\\r\\n    # create tool function\\r\\n    # initialize vectorstore retriever object\\r\\n    timekeeping_policy = RetrievalQA.from_chain_type(\\r\\n        llm=llm,\\r\\n        chain_type=chain_type_param,\\r\\n        retriever=retriever_obj,\\r\\n    )\\r\\n    return timekeeping_policy\\r\\n\\r\\n\\n\"},\"context\":[{\"FunctionName\":\"vector_retriever\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef vector_retriever(vectorstore):\\r    return vectorstore.as_retriever()\\r\"]},{\"text_field\":\"text\"},{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"GPJdl\",\"alias\":\"Vector DB Retriever\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"vector_retriever\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef vector_retriever(vectorstore):\\r    return vectorstore.as_retriever()\\r\"]},\"position_x\":\"406\",\"position_y\":\"24\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"GzXxI\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"oMxKy\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\"},\"context\":[{\"text_field\":\"text\"},{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"}]},{\"id\":\"HUCgG\",\"alias\":\"Azure OpenAI GPT 35\",\"name\":\"Azure OpenAI GPT 35\",\"classname\":\"Azure OpenAI GPT 35 \",\"category\":\"LLM\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},\"position_x\":\"765\",\"position_y\":\"96\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"rMUZJ\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"model_name\":\"text\",\"openai_api_type\":\"text\",\"deployment_name\":\"text\",\"openai_api_base\":\"text\",\"openai_api_version\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chat_models import AzureChatOpenAI\"],\"script\":\"def AzureOpenAIGPT35(deployment_name_param=\'\', \\r\\n                    model_name_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_version_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    #initialize LLM object\\r\\n    llm = AzureChatOpenAI(    \\r\\n        deployment_name=deployment_name_param, \\r\\n        model_name=model_name_param, \\r\\n        openai_api_key=openai_api_key_param,\\r\\n        openai_api_version = openai_api_version_param, \\r\\n        openai_api_base=openai_api_base_param,\\r\\n        openai_api_type=openai_api_type_param,\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n    return llm\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"FvHlX\",\"alias\":\"Initialize Tool\",\"name\":\"Initialize Tool\",\"classname\":\"Initialize Tool\",\"category\":\"Tool\",\"attributes\":{\"name\":\"Timekeeping Policies\",\"description\":\"Useful for when you need to answer questions about employee timekeeping policies.\\n    \\n<user>: What is the policy on unused vacation leave?\\n<assistant>: I need to check the timekeeping policies to answer this question.\\n<assistant>: Action: Timekeeping Policies\\n<assistant>: Action Input: Vacation Leave Policy - Unused Leave\"},\"position_x\":\"441\",\"position_y\":\"169\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"oMxKy\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"Klquq\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"name\":\"text\",\"description\":\"textarea\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import Tool\"],\"script\":\"def InitializeTool(function_obj, name_param = \'\', description_param=\'\'):\\r\\n    tool =  Tool(\\r\\n                    name = name_param,\\r\\n                    func=function_obj.run,\\r\\n                    description = description_param\\r\\n                )\\r\\n    return tool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"chain_type\":\"stuff\"},{\"FunctionName\":\"vector_retriever\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef vector_retriever(vectorstore):\\r    return vectorstore.as_retriever()\\r\"]},{\"text_field\":\"text\"},{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"lGXBM\",\"alias\":\"Initialize Tool\",\"name\":\"Initialize Tool\",\"classname\":\"Initialize Tool\",\"category\":\"Tool\",\"attributes\":{\"name\":\"Employee Data\",\"description\":\"Useful for when you need to answer questions about employee data stored in pandas dataframe \'df\'. \\nRun python pandas operations on \'df\' to help you get the right answer.\\n\'df\' has the following columns: [employee_id,name,position,organizational_unit,rank,hire_date,regularization_date,vacation_leave,sick_leave,basic_pay_in_phpemployment_status,supervisor]\\n            \\n<user>: How many Sick Leave do I have left?\\n<assistant>: df[df[\'name\'] == \'Alexander Verdad\'][\'sick_leave\']\\n<assistant>: You have n sick leaves left.\"},\"position_x\":\"430\",\"position_y\":\"265\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"fqCLH\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"Klquq\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"name\":\"text\",\"description\":\"textarea\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import Tool\"],\"script\":\"def InitializeTool(function_obj, name_param = \'\', description_param=\'\'):\\r\\n    tool =  Tool(\\r\\n                    name = name_param,\\r\\n                    func=function_obj.run,\\r\\n                    description = description_param\\r\\n                )\\r\\n    return tool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{},{\"file_path\":\"employee_data/employee_data.csv\",\"file_system\":\"file1\",\"credential\":\"Y/tGx+bNiee2MD/hJ+4kzZTRE0uTDTQqAUy5eeeKKI09jjfI3QTv50qiSz1dIc/VHxpCJxJ8o4kDolHSwWWGMA==\",\"account_url\":\"https://act1.dfs.core.windows.net/\",\"decode\":\"utf-8\"}]},{\"id\":\"SEqJN\",\"alias\":\"Initialize Tool\",\"name\":\"Initialize Tool\",\"classname\":\"Initialize Tool\",\"category\":\"Tool\",\"attributes\":{\"name\":\"Calculator\",\"description\":\"Useful when you need to do math operations or arithmetic.\"},\"position_x\":\"441\",\"position_y\":\"352\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"KJEgj\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"Klquq\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"name\":\"text\",\"description\":\"textarea\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import Tool\"],\"script\":\"def InitializeTool(function_obj, name_param = \'\', description_param=\'\'):\\r\\n    tool =  Tool(\\r\\n                    name = name_param,\\r\\n                    func=function_obj.run,\\r\\n                    description = description_param\\r\\n                )\\r\\n    return tool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"verbose\":\"True\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"rMUZJ\",\"alias\":\"Initialize Agent\",\"name\":\"Initialize Agent\",\"classname\":\"Initialize Agent\",\"category\":\"Agent\",\"attributes\":{\"prefix_argument\":\"You are friendly HR assistant. You are tasked to assist the current user: Alexander Verdad on questions related to HR. You have access to the following tools:\",\"agent\":\"zero-shot-react-description\",\"max_execution_time\":\"20\",\"early_stopping_method\":\"generate\",\"verbose\":\"True\"},\"position_x\":\"819\",\"position_y\":\"267\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"Klquq\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"in2\",\"position\":\"TopCenter\",\"elementId\":\"HUCgG\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"NeWdm\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in1\",\"in2\"],\"outputEndpoints\":[\"out1\"],\"formats\":{\"prefix_argument\":\"textarea\",\"agent\":\"text\",\"max_execution_time\":\"text\",\"early_stopping_method\":\"text\",\"verbose\":\"bool\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.agents import AgentType\",\"from langchain.agents import initialize_agent\"],\"script\":\"def InitializeAgent(tools, llm, agent_param=\'\', max_execution_time_param=\'\', early_stopping_method_param=\'\', prefix_argument_param=\'\', verbose_param=\'\'):\\r\\n    # change the value of the prefix argument in the initialize_agent function. This will overwrite the default prompt template of the zero shot agent type\\r\\n    agent_kwargs = {\'prefix\': prefix_argument_param}\\r\\n    \\r\\n    \\r\\n    # initialize the LLM agent\\r\\n    agent = initialize_agent(tools, \\r\\n                             llm, \\r\\n                             agent=agent_param, \\r\\n                             verbose=verbose_param, \\r\\n                             max_execution_time=int(max_execution_time_param), \\r\\n                             early_stopping_method=early_stopping_method_param,\\r\\n                             agent_kwargs=agent_kwargs\\r\\n                             )\\r\\n    # agent = initialize_agent(tools, \\r\\n    #                          llm, \\r\\n    #                          agent=\'zero-shot-react-description\', \\r\\n    #                          verbose=True, \\r\\n    #                          agent_kwargs=agent_kwargs\\r\\n    #                          )\\r\\n    return agent\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[{\"FunctionName\":\"combinetools\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef combinetools(tool1, tool2, tool3):\\r    #python-script Data\\r    tools = [\\r        tool1, tool2, tool3\\r        ]\\r\\r    return tools\\r\"]},{\"name\":\"Timekeeping Policies\",\"description\":\"Useful for when you need to answer questions about employee timekeeping policies.\\n    \\n<user>: What is the policy on unused vacation leave?\\n<assistant>: I need to check the timekeeping policies to answer this question.\\n<assistant>: Action: Timekeeping Policies\\n<assistant>: Action Input: Vacation Leave Policy - Unused Leave\"},{\"chain_type\":\"stuff\"},{\"FunctionName\":\"vector_retriever\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef vector_retriever(vectorstore):\\r    return vectorstore.as_retriever()\\r\"]},{\"text_field\":\"text\"},{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"name\":\"Employee Data\",\"description\":\"Useful for when you need to answer questions about employee data stored in pandas dataframe \'df\'. \\nRun python pandas operations on \'df\' to help you get the right answer.\\n\'df\' has the following columns: [employee_id,name,position,organizational_unit,rank,hire_date,regularization_date,vacation_leave,sick_leave,basic_pay_in_phpemployment_status,supervisor]\\n            \\n<user>: How many Sick Leave do I have left?\\n<assistant>: df[df[\'name\'] == \'Alexander Verdad\'][\'sick_leave\']\\n<assistant>: You have n sick leaves left.\"},{},{\"file_path\":\"employee_data/employee_data.csv\",\"file_system\":\"file1\",\"credential\":\"Y/tGx+bNiee2MD/hJ+4kzZTRE0uTDTQqAUy5eeeKKI09jjfI3QTv50qiSz1dIc/VHxpCJxJ8o4kDolHSwWWGMA==\",\"account_url\":\"https://act1.dfs.core.windows.net/\",\"decode\":\"utf-8\"},{\"name\":\"Calculator\",\"description\":\"Useful when you need to do math operations or arithmetic.\"},{\"verbose\":\"True\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"BgPoi\",\"alias\":\"Azure OpenAI GPT 35\",\"name\":\"Azure OpenAI GPT 35\",\"classname\":\"Azure OpenAI GPT 35 \",\"category\":\"LLM\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},\"position_x\":\"15\",\"position_y\":\"170\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"oMxKy\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"model_name\":\"text\",\"openai_api_type\":\"text\",\"deployment_name\":\"text\",\"openai_api_base\":\"text\",\"openai_api_version\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chat_models import AzureChatOpenAI\"],\"script\":\"def AzureOpenAIGPT35(deployment_name_param=\'\', \\r\\n                    model_name_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_version_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    #initialize LLM object\\r\\n    llm = AzureChatOpenAI(    \\r\\n        deployment_name=deployment_name_param, \\r\\n        model_name=model_name_param, \\r\\n        openai_api_key=openai_api_key_param,\\r\\n        openai_api_version = openai_api_version_param, \\r\\n        openai_api_base=openai_api_base_param,\\r\\n        openai_api_type=openai_api_type_param,\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n    return llm\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"LkYJn\",\"alias\":\"Azure OpenAI GPT 35\",\"name\":\"Azure OpenAI GPT 35\",\"classname\":\"Azure OpenAI GPT 35 \",\"category\":\"LLM\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},\"position_x\":\"27\",\"position_y\":\"349\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"KJEgj\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"model_name\":\"text\",\"openai_api_type\":\"text\",\"deployment_name\":\"text\",\"openai_api_base\":\"text\",\"openai_api_version\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.chat_models import AzureChatOpenAI\"],\"script\":\"def AzureOpenAIGPT35(deployment_name_param=\'\', \\r\\n                    model_name_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_version_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    #initialize LLM object\\r\\n    llm = AzureChatOpenAI(    \\r\\n        deployment_name=deployment_name_param, \\r\\n        model_name=model_name_param, \\r\\n        openai_api_key=openai_api_key_param,\\r\\n        openai_api_version = openai_api_version_param, \\r\\n        openai_api_base=openai_api_base_param,\\r\\n        openai_api_type=openai_api_type_param,\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n    return llm\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"DlDpJ\",\"alias\":\"Azure OpenAI text embedding ada 002\",\"name\":\"Azure OpenAI text embedding ada 002\",\"classname\":\"Azure OpenAI text embedding ada 002\",\"category\":\"Embedding\",\"attributes\":{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},\"position_x\":\"0\",\"position_y\":\"84\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"GzXxI\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\"],\"formats\":{\"openai_api_key\":\"text\",\"openai_api_type\":\"text\",\"model\":\"text\",\"openai_api_base\":\"text\",\"deployment\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"from langchain.embeddings.openai import OpenAIEmbeddings\"],\"script\":\"def AzureOpenAItextembeddingada002(deployment_param=\'\', \\r\\n                    model_param=\'\', \\r\\n                    openai_api_key_param=\'\', \\r\\n                    openai_api_base_param=\'\', \\r\\n                    openai_api_type_param=\'\'\\r\\n                    ):\\r\\n    # initialize embeddings object; for use with user query/input\\r\\n    embed = OpenAIEmbeddings(\\r\\n                    deployment=deployment_param,\\r\\n                    model=model_param,\\r\\n                    openai_api_key=openai_api_key_param,\\r\\n                    openai_api_base=openai_api_base_param,\\r\\n                    openai_api_type=openai_api_type_param,\\r\\n                )\\r\\n\\r\\n    return embed\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"context\":[]},{\"id\":\"NeWdm\",\"alias\":\"Chat UI\",\"name\":\"Chat UI\",\"classname\":\"Chat UI\",\"category\":\"Chat UI\",\"attributes\":{\"prompt_text\":\"<h3>Propmt Used : </h3>\\n    <p>You are friendly HR assistant. You are tasked to assist the current user: Alexander Verdad on questions related to HR. You have access to the following tools:</p>\\n<h3>Employee Data:</h3> \\n<p>Useful for when you need to answer questions about employee data stored in pandas dataframe \'df\'. Run python pandas operations on \'df\' to help you get the right answer.\\n\'df\' has the following columns: [employee_id,name,position,organizational_unit,rank,hire_date,regularization_date,vacation_leave,sick_leave,basic_pay_in_phpemployment_status,supervisor]</p>\\n\\n?: <b>How many Sick Leave do I have left?</b><br/>\\n?: df[df[\'name\'] == \'Alexander Verdad\'][\'sick_leave\']<br/>\\n?: You have n sick leaves left.<br/>\\nTimekeeping Policies: Useful for when you need to answer questions about employee timekeeping policies.<br/><br/>\\n\\n?: <b>What is the policy on unused vacation leave?</b><br/>\\n?: I need to check the timekeeping policies to answer this question.<br/>\\n?: Action: Timekeeping Policies<br/>\\n?: Action Input: Vacation Leave Policy - Unused Leave<br/>\\nCalculator: Useful when you need to do math operations or arithmetic.<br/><br/>\\n\\n<h4>Use the following format:</h4>\\n<b>Question</b>: the input question you must answer <br/>\\nThought: you should always think about what to do<br/>\\nAction: the action to take, should be one of [Employee Data, Timekeeping Policies, Calculator]<br/>\\nAction Input: the input to the action<br/>\\nObservation: the result of the action<br/>\\n... (this Thought/Action/Action Input/Observation can repeat N times)<br/>\\nThought: I now know the final answer<br/>\\n<b>Final Answer</b>: the final answer to the original input question </br></br>\",\"markdown\":\"<h3> Ask your HR-related questions here.</h3> <b>❓ Example Queries</b> <ul>      <li>Who am I and what\'s my employee ID?</li>      <li>How many unused vacation leaves do I have left and what is the policy on unused vacation leaves?</li>      <li>How much will I get paid if I encash my unused vacation leaves?</li>  </ul>\",\"header\":\"? LLM HR Chatbot - Chain of Thought Demo\"},\"position_x\":\"841\",\"position_y\":\"368\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in1\",\"position\":\"LeftMiddle\",\"elementId\":\"rMUZJ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[],\"formats\":{\"prompt_text\":\"textarea\",\"markdown\":\"text\",\"header\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import streamlit as st\",\"import random\",\"from streamlit_chat import message\",\"import os\",\"from langchain.callbacks import StreamlitCallbackHandler\"],\"script\":\"from langchain.callbacks.base import BaseCallbackHandler\\r\\n\\r\\nclass StreamHandler(BaseCallbackHandler):\\r\\n    def __init__(self, initial_text=\'Start..\'):\\r\\n        self.text = initial_text\\r\\n\\r\\n    def on_text(self, text: str, **kwargs) -> None:\\r\\n        starter_text = \'Begin!\'\\r\\n        self.text = text\\r\\n        if (self.text.find(starter_text) > 0):\\r\\n            self.text = self.text[self.text.find(starter_text) + len(starter_text):]\\r\\n        st.session_state[\'thoughts\'][len(st.session_state[\'thoughts\']) - 1] = self.text\\r\\n        message(self.text, key=self.text + \'_chain_of_thoughts\')\\r\\n\\r\\ndef ChatUI(agent, header_param=\'? LLM HR Chatbot - Chain of Thought Demo\', markdown_param=\'\', prompt_text_param=\'\'):    \\r\\n    print(\'Initializing the streamlit app..\')\\r\\n    \\r\\n    st.header(header_param)\\r\\n    st.markdown(prompt_text_param, unsafe_allow_html=True)\\r\\n    st.markdown(markdown_param, unsafe_allow_html=True)   \\r\\n    \\r\\n    stream_handler1 = StreamHandler()\\r\\n\\r\\n    if \'past\' not in st.session_state:\\r\\n        st.session_state[\'past\'] = []\\r\\n    if \'generated\' not in st.session_state:\\r\\n        st.session_state[\'generated\'] = []\\r\\n    \\r\\n    if \'thoughts\' not in st.session_state:\\r\\n        st.session_state[\'thoughts\'] = []\\r\\n    \\r\\n    if \'input_message_key\' not in st.session_state:\\r\\n        st.session_state[\'input_message_key\'] = str(random.random())\\r\\n    \\r\\n    chat_container = st.container()\\r\\n    user_input = st.text_input(\'Type your message and press Enter to send.\', key=st.session_state[\'input_message_key\'])\\r\\n    \\r\\n    if st.button(\'Send\'):\\r\\n        try:\\r\\n            message(user_input, is_user=True, key=user_input + \'_user\')\\r\\n            st.session_state[\'thoughts\'].append(\'\')\\r\\n            response = agent.run(user_input, callbacks=[stream_handler1])\\r\\n        except Exception as e:\\r\\n            print(\'The traceback is: \', e)\\r\\n            response = \'Invalid input. Please try again\'\\r\\n        \\r\\n        \\r\\n        message(response, key=user_input)\\r\\n    \\r\\n        st.session_state[\'past\'].append(user_input)\\r\\n        st.session_state[\'generated\'].append(response)\\r\\n    \\r\\n        st.session_state[\'input_message_key\'] = str(random.random())\\r\\n    \\r\\n        st.experimental_rerun()\\r\\n    \\r\\n    if st.session_state[\'generated\']:\\r\\n        with chat_container:\\r\\n            for i in range(len(st.session_state[\'generated\'])):\\r\\n                message(st.session_state[\'past\'][i], is_user=True, key=str(i) + \'_user\')\\r\\n                message(st.session_state[\'thoughts\'][i], key=str(i) + \'_chain_of_thoughts\')\\r\\n                message(st.session_state[\'generated\'][i], key=str(i))   \\r\\n\\n\"},\"context\":[{\"prefix_argument\":\"You are friendly HR assistant. You are tasked to assist the current user: Alexander Verdad on questions related to HR. You have access to the following tools:\",\"agent\":\"zero-shot-react-description\",\"max_execution_time\":\"20\",\"early_stopping_method\":\"generate\",\"verbose\":\"True\"},{\"FunctionName\":\"combinetools\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef combinetools(tool1, tool2, tool3):\\r    #python-script Data\\r    tools = [\\r        tool1, tool2, tool3\\r        ]\\r\\r    return tools\\r\"]},{\"name\":\"Timekeeping Policies\",\"description\":\"Useful for when you need to answer questions about employee timekeeping policies.\\n    \\n<user>: What is the policy on unused vacation leave?\\n<assistant>: I need to check the timekeeping policies to answer this question.\\n<assistant>: Action: Timekeeping Policies\\n<assistant>: Action Input: Vacation Leave Policy - Unused Leave\"},{\"chain_type\":\"stuff\"},{\"FunctionName\":\"vector_retriever\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef vector_retriever(vectorstore):\\r    return vectorstore.as_retriever()\\r\"]},{\"text_field\":\"text\"},{\"environment\":\"us-west1-gcp-free\",\"api_key\":\"b02bfd20-0885-4f0e-99ae-2873749d686d\",\"index_name\":\"tk-policy\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"openai_api_type\":\"azure\",\"model\":\"text-embedding-ada-002\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"deployment\":\"openaiada2\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"name\":\"Employee Data\",\"description\":\"Useful for when you need to answer questions about employee data stored in pandas dataframe \'df\'. \\nRun python pandas operations on \'df\' to help you get the right answer.\\n\'df\' has the following columns: [employee_id,name,position,organizational_unit,rank,hire_date,regularization_date,vacation_leave,sick_leave,basic_pay_in_phpemployment_status,supervisor]\\n            \\n<user>: How many Sick Leave do I have left?\\n<assistant>: df[df[\'name\'] == \'Alexander Verdad\'][\'sick_leave\']\\n<assistant>: You have n sick leaves left.\"},{},{\"file_path\":\"employee_data/employee_data.csv\",\"file_system\":\"file1\",\"credential\":\"Y/tGx+bNiee2MD/hJ+4kzZTRE0uTDTQqAUy5eeeKKI09jjfI3QTv50qiSz1dIc/VHxpCJxJ8o4kDolHSwWWGMA==\",\"account_url\":\"https://act1.dfs.core.windows.net/\",\"decode\":\"utf-8\"},{\"name\":\"Calculator\",\"description\":\"Useful when you need to do math operations or arithmetic.\"},{\"verbose\":\"True\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"},{\"openai_api_key\":\"85b968a4b5c84d849c99661788c2c1ed\",\"model_name\":\"gpt-35-turbo\",\"openai_api_type\":\"azure\",\"deployment_name\":\"gtp35turbo\",\"openai_api_base\":\"https://azureft.openai.azure.com/\",\"openai_api_version\":\"2023-03-15-preview\"}]},{\"id\":\"OScKF\",\"alias\":\"Chat UI\",\"name\":\"Chat UI\",\"classname\":\"Chat UI\",\"category\":\"Chat UI\",\"attributes\":{\"prompt_text\":\"<h3>Propmt Used : </h3>\\n    <p>You are friendly HR assistant. You are tasked to assist the current user: Alexander Verdad on questions related to HR. You have access to the following tools:</p>\\n<h3>Employee Data:</h3> \\n<p>Useful for when you need to answer questions about employee data stored in pandas dataframe \'df\'. Run python pandas operations on \'df\' to help you get the right answer.\\n\'df\' has the following columns: [employee_id,name,position,organizational_unit,rank,hire_date,regularization_date,vacation_leave,sick_leave,basic_pay_in_phpemployment_status,supervisor]</p>\\n\\n?: <b>How many Sick Leave do I have left?</b><br/>\\n?: df[df[\'name\'] == \'Alexander Verdad\'][\'sick_leave\']<br/>\\n?: You have n sick leaves left.<br/>\\nTimekeeping Policies: Useful for when you need to answer questions about employee timekeeping policies.<br/><br/>\\n\\n?: <b>What is the policy on unused vacation leave?</b><br/>\\n?: I need to check the timekeeping policies to answer this question.<br/>\\n?: Action: Timekeeping Policies<br/>\\n?: Action Input: Vacation Leave Policy - Unused Leave<br/>\\nCalculator: Useful when you need to do math operations or arithmetic.<br/><br/>\\n\\n<h4>Use the following format:</h4>\\n<b>Question</b>: the input question you must answer <br/>\\nThought: you should always think about what to do<br/>\\nAction: the action to take, should be one of [Employee Data, Timekeeping Policies, Calculator]<br/>\\nAction Input: the input to the action<br/>\\nObservation: the result of the action<br/>\\n... (this Thought/Action/Action Input/Observation can repeat N times)<br/>\\nThought: I now know the final answer<br/>\\n<b>Final Answer</b>: the final answer to the original input question </br></br>\",\"markdown\":\"<h3> Ask your HR-related questions here.</h3>\\n<b>❓ Example Queries</b>\\n<ul>\\n     <li>Who am I and what\'s my employee ID?</li>\\n     <li>How many unused vacation leaves do I have left and what is the policy on unused vacation leaves?</li>\\n     <li>How much will I get paid if I encash my unused vacation leaves?</li>\\n </ul>\",\"header\":\"? LLM HR Chatbot - Chain of Thought Demo\"},\"position_x\":\"646\",\"position_y\":\"85\",\"connectors\":[],\"inputEndpoints\":[\"in1\"],\"outputEndpoints\":[],\"formats\":{\"prompt_text\":\"textarea\",\"markdown\":\"textarea\",\"header\":\"text\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import streamlit as st\",\"import random\",\"from streamlit_chat import message\",\"import os\",\"from langchain.callbacks import StreamlitCallbackHandler\"],\"script\":\"from langchain.callbacks.base import BaseCallbackHandler\\r\\n\\r\\nclass StreamHandler(BaseCallbackHandler):\\r\\n    def __init__(self, initial_text=\'Start..\'):\\r\\n        self.text = initial_text\\r\\n\\r\\n    def on_text(self, text: str, **kwargs) -> None:\\r\\n        starter_text = \'Begin!\'\\r\\n        self.text = text\\r\\n        if (self.text.find(starter_text) > 0):\\r\\n            self.text = self.text[self.text.find(starter_text) + len(starter_text):]\\r\\n        st.session_state[\'thoughts\'][len(st.session_state[\'thoughts\']) - 1] = self.text\\r\\n        message(self.text, key=self.text + \'_chain_of_thoughts\')\\r\\n\\r\\ndef ChatUI(agent, header_param=\'? LLM HR Chatbot - Chain of Thought Demo\', markdown_param=\'\', prompt_text_param=\'\'):    \\r\\n    print(\'Initializing the streamlit app..\')\\r\\n    \\r\\n    st.header(header_param)\\r\\n    st.markdown(prompt_text_param, unsafe_allow_html=True)\\r\\n    st.markdown(markdown_param, unsafe_allow_html=True)   \\r\\n    \\r\\n    stream_handler1 = StreamHandler()\\r\\n\\r\\n    if \'past\' not in st.session_state:\\r\\n        st.session_state[\'past\'] = []\\r\\n    if \'generated\' not in st.session_state:\\r\\n        st.session_state[\'generated\'] = []\\r\\n    \\r\\n    if \'thoughts\' not in st.session_state:\\r\\n        st.session_state[\'thoughts\'] = []\\r\\n    \\r\\n    if \'input_message_key\' not in st.session_state:\\r\\n        st.session_state[\'input_message_key\'] = str(random.random())\\r\\n    \\r\\n    chat_container = st.container()\\r\\n    user_input = st.text_input(\'Type your message and press Enter to send.\', key=st.session_state[\'input_message_key\'])\\r\\n    \\r\\n    if st.button(\'Send\'):\\r\\n        try:\\r\\n            message(user_input, is_user=True, key=user_input + \'_user\')\\r\\n            st.session_state[\'thoughts\'].append(\'\')\\r\\n            response = agent.run(user_input, callbacks=[stream_handler1])\\r\\n        except Exception as e:\\r\\n            print(\'The traceback is: \', e)\\r\\n            response = \'Invalid input. Please try again\'\\r\\n        \\r\\n        \\r\\n        message(response, key=user_input)\\r\\n    \\r\\n        st.session_state[\'past\'].append(user_input)\\r\\n        st.session_state[\'generated\'].append(response)\\r\\n    \\r\\n        st.session_state[\'input_message_key\'] = str(random.random())\\r\\n    \\r\\n        st.experimental_rerun()\\r\\n    \\r\\n    if st.session_state[\'generated\']:\\r\\n        with chat_container:\\r\\n            for i in range(len(st.session_state[\'generated\'])):\\r\\n                message(st.session_state[\'past\'][i], is_user=True, key=str(i) + \'_user\')\\r\\n                message(st.session_state[\'thoughts\'][i], key=str(i) + \'_chain_of_thoughts\')\\r\\n                message(st.session_state[\'generated\'][i], key=str(i))   \\r\\n\\n\"},\"context\":[]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"minio\"},{\"key\":\"runCommand\",\"value\":\"python -m streamlit run AIPHR_CH75125_generatedCode.py --server.fileWatcherType none --server.headless true --server.port 8502\"}]}","madhanraj.muthu@ad.infosys.com","AITwin_HRSupport","2023-08-25 07:55:45","AIPHR_CH75125","leo1311","Langchain","334","NULL","chain","{\"333\":{\"taskId\":\"5127f049-26fc-431d-a0a2-2f3f0153f14b\"}}"
"poornasai.nagendra@ad.infosys.com","2023-07-26 07:28:32.796000","","","NULL","{\"elements\":[{\"id\":\"ZJsok\",\"alias\":\"Extract Words\",\"name\":\"Post Processing Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"Extract_words\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"a,an,the\",\"type\":\"Text\",\"alias\":\"a,an,the\",\"index\":\"1\"}],\"script\":[\"import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rnltk.download(\'punkt\')\\rnltk.download(\'stopwords\')\\rdef clean_text(text):   \\r    alphanumeric = \'\'    \\r    for character in text:\\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += \' \'    \\r    return alphanumeric  \\rdef tokenizer(text):       \\r    token = nltk.word_tokenize(text)        \\r    tokens = [t for t in token if t.isalpha()]        \\r    tokens = \' \'.join(tokens)        \\r    return tokens \\r\\rdef stopword_remover(tokens,custom_stopwords_param=\'\'):\\r    custom_stopwords_param = custom_stopwords_param.split(\',\')\\r    stopwords_nltk = set(stopwords.words(\'english\'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return \' \'.join(words)\\r    #return dataset  \\rdef lemmatize_text(text):       \\r    words = text.split() \\r    wordnet_lemmatizer = WordNetLemmatizer()\\r    words = [wordnet_lemmatizer.lemmatize(word, pos = \'v\') for word in words]        \\r    return \' \'.join(words)\\rdef remove_numbers(tokens):\\r    text = tokens.split()        \\r    no_num_text = [t for t in text if not(t.isdigit() or len(t)<=2)]      \\r    return no_num_text\\rdef Extract_words(dataset,custom_stopwords_param=\'\'): \\r    dataset = pd.DataFrame(dataset)    \\r    dataset[\'cleanText\'] = [clean_text(desc) for desc in dataset[\'shortdescription\']]\\r    #print(dataset)\\r    dataset[\'tokens\'] = dataset[\'cleanText\'].apply(tokenizer)\\r    #dataset = stopword_remover(dataset)\\r    dataset[\'cleanWords\'] = dataset[\'tokens\'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    #print(dataset)\\r    dataset[\'lemma_Text\'] = dataset[\'cleanWords\'].apply(lemmatize_text) \\r    dataset[\'filtered_column\'] = dataset[\'lemma_Text\'].apply(remove_numbers)\\r    dataset = dataset.iloc[:, [0,7]]\\r    #print(dataset)\\r    dataset = dataset.explode(\'filtered_column\')\\r    print(dataset)\\r    dataset = dataset.drop_duplicates()\\r    dataset = dataset.groupby([\'filtered_column\']).agg(numberList = pd.NamedAgg(column = \'NUMBER\',aggfunc=list))\\r    dataset[\'frequency\'] = dataset[\'numberList\'].apply(lambda x: len(x))\\r    dataset[\'numberList\'] = dataset[\'numberList\'].apply(lambda x: \',\'.join(x))\\r    dataset = dataset.to_dict(\'records\')\\r    print(dataset)\\r    return(dataset)\\r\\r\"]},\"position_x\":\"500\",\"position_y\":\"57\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"Dtpxn\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"XTmak\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\"],\"outputEndpoints\":[\"out1\",\"out2\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-09-11 07:40:02\",\"alias\":\"cleantext\",\"id\":2774,\"name\":\"LEACLNTX34353\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT NUMBER, CleanText AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE CleanText <> \\\\\\\" \\\\\\\" AND   CleanText IS NOT NULL \\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"Dtpxn\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-08-16 12:32:45\",\"alias\":\"InvertedIndex\",\"id\":2806,\"name\":\"LEAINVRT90611\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from leo1311_invertedindex order by frequency\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_invertedindex\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"699\",\"position_y\":\"50\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"ZJsok\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n\"},\"MYSQL\":{\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"Drop table IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"`{0}` TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"`{0}`\\\".format(k) for k in paramsDict)\\n            duplicates = \\\", \\\".join(\\\"{0}=VALUES({0})\\\".format(k) for k in paramsDict)\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON DUPLICATE KEY UPDATE {1}\\\".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\\"{0}:{1}\\\".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()\"},\"MSSQL\":{\"imports\":[\"from leap.core.iLoader import Loader\",\"from leap.utils.Utilities import Utilities\",\"import logging as logger\",\"from leap.utils import vault\",\"import pyodbc\",\"import re\",\"from datetime import datetime\",\"import os\"],\"script\":\"def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\\"<dataset.attributes.writeMode>\\\\\\\"\\n\\n    url=\\\\\\\"<dataset.datasource.connectionDetails.url>\\\\\\\"\\n\\n    tablename = \\\\\\\"<dataset.attributes.tableName>\\\\\\\"\\n\\n    username = \\\\\\\"<dataset.datasource.connectionDetails.userName>\\\\\\\"\\n\\n    password = Security.decrypt(\\\\\\\"<dataset.datasource.connectionDetails.password>\\\\\\\",\\\\\\\"<dataset.datasource.salt>\\\\\\\")\\n\\n    temp1 = self.url.split(\\\"//\\\")\\n\\n    temp2 = temp1[1].split(\\\";\\\")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\\"=\\\"))[1]\\n\\n    isTrusted = \\\"no\\\"\\n\\n    if username == \\\"\\\":\\n\\n    isTrusted = \\\"yes\\\"\\n\\n    regex = \\\\\\\"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\\"\\n\\n\\n    if(re.search(regex, server.split(\\\":\\\")[0])):\\n\\n        server=server.replace(\\\":\\\",\\\",\\\")\\n\\n\\n    connectionString = \\\\\\\"DRIVER={0};SERVER={1}; \\\\\\\"\\n\\n                       \\\\\\\"DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\\".format(\\n\\n        \\\"ODBC Driver 17 for SQL SERVER\\\", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\\"overwrite\\\\\\\":\\n\\n        cursor.execute(\\\\\\\"Drop table IF EXISTS {0}\\\\\\\".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\\", \\\\\\\".join([\\\\\\\"`{0}` TEXT\\\\\\\".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\\".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\\", \\\\\\\".join(\\\\\\\"`{0}`\\\\\\\".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\\", \\\\\\\".join(\\\\\\\"{0}=VALUES({0})\\\\\\\".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\\", \\\\\\\".join(\\\\\\\"%s\\\\\\\".format(k) for k in paramsDict)\\n\\n            query = \\\\\\\"INSERT INTO {0} ({1}) VALUES ({2})\\\\\\\".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\\"update\\\\\\\"):\\n\\n                query = \\\\\\\"{0} ON DUPLICATE KEY UPDATE {1}\\\\\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\\"{0}:{1}\\\\\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()\"},\"AWS\":{\"imports\":[\"import pandas as pd\",\"import pickle\",\"import os\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    filename = url.split(\'/\')[-1]\\n    extension = filename.split(\'.\')[-1]\\n\\n    data_directory = \\\"/opt/ml/processing/output\\\"\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\\"Saving data\\\")\\n    if extension == \'.csv\':\\n        dataset.to_csv(file_path)\\n    elif extension == \'pkl\':\\n        pickle.dumps(dataset, open(file_path, \'wb\'))\\n    else:\\n        with open(file_path, \'w\') as f:\\n            f.writelines(dataset)\\n\\n\"},\"POSTGRESQL\":{\"imports\":[\"import psycopg2\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"DROP TABLE IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"{0} TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\"CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"{0}\\\".format(k) for k in paramsDict.keys())\\n            duplicates = \\\", \\\".join(\\\"{0}=EXCLUDED.{0}\\\".format(k) for k in paramsDict.keys())\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON CONFLICT DO UPDATE SET {1}\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\\"{0}:{1}\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\"}},\"context\":[{\"FunctionName\":\"Extract_words\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"a,an,the\",\"type\":\"Text\",\"alias\":\"a,an,the\",\"index\":\"1\"}],\"script\":[\"import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rnltk.download(\'punkt\')\\rnltk.download(\'stopwords\')\\rdef clean_text(text):   \\r    alphanumeric = \'\'    \\r    for character in text:\\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += \' \'    \\r    return alphanumeric  \\rdef tokenizer(text):       \\r    token = nltk.word_tokenize(text)        \\r    tokens = [t for t in token if t.isalpha()]        \\r    tokens = \' \'.join(tokens)        \\r    return tokens \\r\\rdef stopword_remover(tokens,custom_stopwords_param=\'\'):\\r    custom_stopwords_param = custom_stopwords_param.split(\',\')\\r    stopwords_nltk = set(stopwords.words(\'english\'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return \' \'.join(words)\\r    #return dataset  \\rdef lemmatize_text(text):       \\r    words = text.split() \\r    wordnet_lemmatizer = WordNetLemmatizer()\\r    words = [wordnet_lemmatizer.lemmatize(word, pos = \'v\') for word in words]        \\r    return \' \'.join(words)\\rdef remove_numbers(tokens):\\r    text = tokens.split()        \\r    no_num_text = [t for t in text if not(t.isdigit() or len(t)<=2)]      \\r    return no_num_text\\rdef Extract_words(dataset,custom_stopwords_param=\'\'): \\r    dataset = pd.DataFrame(dataset)    \\r    dataset[\'cleanText\'] = [clean_text(desc) for desc in dataset[\'shortdescription\']]\\r    #print(dataset)\\r    dataset[\'tokens\'] = dataset[\'cleanText\'].apply(tokenizer)\\r    #dataset = stopword_remover(dataset)\\r    dataset[\'cleanWords\'] = dataset[\'tokens\'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    #print(dataset)\\r    dataset[\'lemma_Text\'] = dataset[\'cleanWords\'].apply(lemmatize_text) \\r    dataset[\'filtered_column\'] = dataset[\'lemma_Text\'].apply(remove_numbers)\\r    dataset = dataset.iloc[:, [0,7]]\\r    #print(dataset)\\r    dataset = dataset.explode(\'filtered_column\')\\r    print(dataset)\\r    dataset = dataset.drop_duplicates()\\r    dataset = dataset.groupby([\'filtered_column\']).agg(numberList = pd.NamedAgg(column = \'NUMBER\',aggfunc=list))\\r    dataset[\'frequency\'] = dataset[\'numberList\'].apply(lambda x: len(x))\\r    dataset[\'numberList\'] = dataset[\'numberList\'].apply(lambda x: \',\'.join(x))\\r    dataset = dataset.to_dict(\'records\')\\r    print(dataset)\\r    return(dataset)\\r\\r\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-09-11 07:40:02\",\"alias\":\"cleantext\",\"id\":2774,\"name\":\"LEACLNTX34353\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT NUMBER, CleanText AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE CleanText <> \\\\\\\" \\\\\\\" AND   CleanText IS NOT NULL \\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"XTmak\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-09-11 07:40:02\",\"alias\":\"cleantext\",\"id\":2774,\"name\":\"LEACLNTX34353\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT NUMBER, CleanText AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE CleanText <> \\\\\\\" \\\\\\\" AND   CleanText IS NOT NULL \\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"235\",\"position_y\":\"36\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ZJsok\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"servicenow\":{},\"MYSQL\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{}},\"context\":[]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}]}","admin","ExtractWords","2023-09-11 07:51:17","LEACLNTC74662","leo1311","DragNDropLite","263","NULL","pipeline","{\"262\":{\"taskId\":\"3c071b09-6ec1-4df8-b2a4-b4db3d410599\"}}"
"admin","2023-07-31 04:30:32.176000","","","NULL","{\"elements\":[{\"id\":\"xcDtt\",\"alias\":\"cleanTickets\",\"name\":\"Post Processing Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"cleanTickets\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import pandas as pd\\rimport nltk\\rnltk.download(\'stopwords\')\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rfrom datetime import datetime \\rdef alphaNum(text):   \\r    alphanumeric = \'\'    \\r    for character in text:\\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += \' \'    \\r    finalTokens = [t for t in alphanumeric.split(\' \') if not t.isnumeric()]\\r    return \' \'.join(finalTokens) \\r    \\rdef stopword_remover(tokens,custom_stopwords_param=\'\'):\\r    custom_stopwords_param = custom_stopwords_param.split(\',\')\\r    stopwords_nltk = set(stopwords.words(\'english\'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return \' \'.join(words)\\rdef lematize(text):\\r    #lematizer = WordNetLemmatizer()\\r    #tokens = text\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos=\'v\') for token in w_tokenizer]\\r    return \' \'.join(words)\\rdef cleanTickets(dataset,custom_stopwords_param=\'\'):\\r    #print(\'dataset main  \', dataset)\\r    dataset = pd.DataFrame(dataset)\\r    print(dataset)  \\r    dataset[\'alpha_text\'] = dataset[\'shortdescription\'].apply(alphaNum)\\r    #print(dataset)\\r    dataset[\'noStopword_text\'] = dataset[\'alpha_text\'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    #print(dataset)\\r    dataset[\'CleanText\'] = dataset[\'noStopword_text\'].apply(lematize)\\r    print(dataset)\\r    dataset[\'lastUpdated\'] = datetime.now() \\r    dataset = dataset[[\'number\', \'shortdescription\',\'CleanText\',\'lastUpdated\',\'group_by_field\']]\\r    dataset = dataset.to_dict(\'records\')\\r    return dataset\\r\"]},\"position_x\":\"452\",\"position_y\":\"33\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"AxniL\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"TnBwB\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\"],\"outputEndpoints\":[\"out1\",\"out2\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-08-08 06:34:08\",\"alias\":\"Clean_Tickets\",\"id\":2778,\"name\":\"LEACLN_T91544\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, shortdescription, configurationItem AS group_by_field FROM leo1311_tickets WHERE shortdescription <> \\\\\\\" \\\\\\\" AND shortdescription IS NOT NULL \\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_ticket\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"AxniL\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-08-16 12:15:08\",\"alias\":\"result_Tickets\",\"id\":2802,\"name\":\"LEARSLT_17026\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select number, clean_text, group_by_field, last_updated from leo1311_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"overwrite\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":\"\",\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null}},\"position_x\":\"686\",\"position_y\":\"116\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"xcDtt\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n\"},\"MYSQL\":{\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"Drop table IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"`{0}` TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"`{0}`\\\".format(k) for k in paramsDict)\\n            duplicates = \\\", \\\".join(\\\"{0}=VALUES({0})\\\".format(k) for k in paramsDict)\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON DUPLICATE KEY UPDATE {1}\\\".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\\"{0}:{1}\\\".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()\"},\"MSSQL\":{\"imports\":[\"from leap.core.iLoader import Loader\",\"from leap.utils.Utilities import Utilities\",\"import logging as logger\",\"from leap.utils import vault\",\"import pyodbc\",\"import re\",\"from datetime import datetime\",\"import os\"],\"script\":\"def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\\"<dataset.attributes.writeMode>\\\\\\\"\\n\\n    url=\\\\\\\"<dataset.datasource.connectionDetails.url>\\\\\\\"\\n\\n    tablename = \\\\\\\"<dataset.attributes.tableName>\\\\\\\"\\n\\n    username = \\\\\\\"<dataset.datasource.connectionDetails.userName>\\\\\\\"\\n\\n    password = Security.decrypt(\\\\\\\"<dataset.datasource.connectionDetails.password>\\\\\\\",\\\\\\\"<dataset.datasource.salt>\\\\\\\")\\n\\n    temp1 = self.url.split(\\\"//\\\")\\n\\n    temp2 = temp1[1].split(\\\";\\\")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\\"=\\\"))[1]\\n\\n    isTrusted = \\\"no\\\"\\n\\n    if username == \\\"\\\":\\n\\n    isTrusted = \\\"yes\\\"\\n\\n    regex = \\\\\\\"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\\"\\n\\n\\n    if(re.search(regex, server.split(\\\":\\\")[0])):\\n\\n        server=server.replace(\\\":\\\",\\\",\\\")\\n\\n\\n    connectionString = \\\\\\\"DRIVER={0};SERVER={1}; \\\\\\\"\\n\\n                       \\\\\\\"DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\\".format(\\n\\n        \\\"ODBC Driver 17 for SQL SERVER\\\", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\\"overwrite\\\\\\\":\\n\\n        cursor.execute(\\\\\\\"Drop table IF EXISTS {0}\\\\\\\".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\\", \\\\\\\".join([\\\\\\\"`{0}` TEXT\\\\\\\".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\\".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\\", \\\\\\\".join(\\\\\\\"`{0}`\\\\\\\".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\\", \\\\\\\".join(\\\\\\\"{0}=VALUES({0})\\\\\\\".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\\", \\\\\\\".join(\\\\\\\"%s\\\\\\\".format(k) for k in paramsDict)\\n\\n            query = \\\\\\\"INSERT INTO {0} ({1}) VALUES ({2})\\\\\\\".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\\"update\\\\\\\"):\\n\\n                query = \\\\\\\"{0} ON DUPLICATE KEY UPDATE {1}\\\\\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\\"{0}:{1}\\\\\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()\"},\"AWS\":{\"imports\":[\"import pandas as pd\",\"import pickle\",\"import os\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    filename = url.split(\'/\')[-1]\\n    extension = filename.split(\'.\')[-1]\\n\\n    data_directory = \\\"/opt/ml/processing/output\\\"\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\\"Saving data\\\")\\n    if extension == \'.csv\':\\n        dataset.to_csv(file_path)\\n    elif extension == \'pkl\':\\n        pickle.dumps(dataset, open(file_path, \'wb\'))\\n    else:\\n        with open(file_path, \'w\') as f:\\n            f.writelines(dataset)\\n\\n\"},\"POSTGRESQL\":{\"imports\":[\"import psycopg2\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"DROP TABLE IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"{0} TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\"CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"{0}\\\".format(k) for k in paramsDict.keys())\\n            duplicates = \\\", \\\".join(\\\"{0}=EXCLUDED.{0}\\\".format(k) for k in paramsDict.keys())\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON CONFLICT DO UPDATE SET {1}\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\\"{0}:{1}\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\"}},\"context\":[{\"FunctionName\":\"cleanTickets\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import pandas as pd\\rimport nltk\\rnltk.download(\'stopwords\')\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rfrom datetime import datetime \\rdef alphaNum(text):   \\r    alphanumeric = \'\'    \\r    for character in text:\\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += \' \'    \\r    finalTokens = [t for t in alphanumeric.split(\' \') if not t.isnumeric()]\\r    return \' \'.join(finalTokens) \\r    \\rdef stopword_remover(tokens,custom_stopwords_param=\'\'):\\r    custom_stopwords_param = custom_stopwords_param.split(\',\')\\r    stopwords_nltk = set(stopwords.words(\'english\'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return \' \'.join(words)\\rdef lematize(text):\\r    #lematizer = WordNetLemmatizer()\\r    #tokens = text\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos=\'v\') for token in w_tokenizer]\\r    return \' \'.join(words)\\rdef cleanTickets(dataset,custom_stopwords_param=\'\'):\\r    #print(\'dataset main  \', dataset)\\r    dataset = pd.DataFrame(dataset)\\r    print(dataset)  \\r    dataset[\'alpha_text\'] = dataset[\'shortdescription\'].apply(alphaNum)\\r    #print(dataset)\\r    dataset[\'noStopword_text\'] = dataset[\'alpha_text\'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    #print(dataset)\\r    dataset[\'CleanText\'] = dataset[\'noStopword_text\'].apply(lematize)\\r    print(dataset)\\r    dataset[\'lastUpdated\'] = datetime.now() \\r    dataset = dataset[[\'number\', \'shortdescription\',\'CleanText\',\'lastUpdated\',\'group_by_field\']]\\r    dataset = dataset.to_dict(\'records\')\\r    return dataset\\r\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-08-08 06:34:08\",\"alias\":\"Clean_Tickets\",\"id\":2778,\"name\":\"LEACLN_T91544\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, shortdescription, configurationItem AS group_by_field FROM leo1311_tickets WHERE shortdescription <> \\\\\\\" \\\\\\\" AND shortdescription IS NOT NULL \\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_ticket\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"TnBwB\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-08-08 06:34:08\",\"alias\":\"Clean_Tickets\",\"id\":2778,\"name\":\"LEACLN_T91544\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, shortdescription, configurationItem AS group_by_field FROM leo1311_tickets WHERE shortdescription <> \\\\\\\" \\\\\\\" AND shortdescription IS NOT NULL \\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_ticket\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"173\",\"position_y\":\"41\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"xcDtt\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"servicenow\":{},\"MYSQL\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{}},\"context\":[]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}]}","admin","cleanTickets","2023-09-19 06:11:56","LEACLNTC62252","leo1311","DragNDropLite","172","NULL","pipeline","{\"170\":{\"taskId\":\"ab22ce47-a188-458f-924b-18141a56defa\"}}"
"admin","2023-07-31 04:36:21.123000","","","NULL","{\"elements\":[{\"id\":\"ptuPQ\",\"alias\":\"Create Clusters\",\"name\":\"Post Processing Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"createClusters\",\"requirements\":\"\",\"params\":[],\"script\":[\"# importing required packages\\rfrom datetime import datetime\\rimport pandas as pd\\rdef createClusters(dataset):\\r    print(type(dataset))\\r    dataset = pd.DataFrame(dataset)\\r    sound_Df = dataset.groupby([\'group_by_field\',\'sound\']).agg(\\r    numberList = pd.NamedAgg(column=\'number\',aggfunc=list),\\r        textList = pd.NamedAgg(column=\'CleanText\',aggfunc=list)\\r    ).reset_index()\\r    sound_Df[\'numberListSize\'] = sound_Df[\'numberList\'].apply(len)\\r    sound_Df = sound_Df[sound_Df[\'numberList\'].apply(lambda x : len(x) >= 5)]\\r    sound_Df[\'cluster\'] = sound_Df[\'textList\'].apply(lambda x: x[0])\\r    sound_Df = sound_Df.drop(columns=[\'textList\'])\\r    sound_Df = sound_Df.explode(\'numberList\').reset_index(drop=True)\\r    sound_Df[\'lastUpdated\'] = datetime.now()\\r    sound_Df = sound_Df.rename(columns={\'numberList\':\'number\',\'cluster\': \'soundex_cluster\'})\\r    sound_Df = sound_Df.to_dict(\'records\')\\r    print(sound_Df)\\r    return sound_Df\\r\"]},\"position_x\":\"496\",\"position_y\":\"6\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out1\",\"position\":\"RightMiddle\",\"elementId\":\"EyGPQ\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"tzhfg\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\"],\"outputEndpoints\":[\"out1\",\"out2\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-09-11 08:21:16\",\"alias\":\"Tickets_cluster\",\"id\":2776,\"name\":\"LEATCKTS64318\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, CleanText, Soundex(CleanText) as sound, CASE WHEN group_by_field IS NULL THEN \\\\\\\"\\\\\\\" ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE CleanText is not null and  Soundex(CleanText) != \\\\\\\"\\\\\\\" and CleanText != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"EyGPQ\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-08-16 12:20:22\",\"alias\":\"Clusters_soundex\",\"id\":2804,\"name\":\"LEACLSTR41446\",\"description\":\"Tickets with soundex of Short description\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select  number , soundex_cluster, last_updated from leo1311_soundex\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"overwrite\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\" leo1311_soundex\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null}},\"position_x\":\"839\",\"position_y\":\"6\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"ptuPQ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{\"imports\":[\"from urllib.parse import urlparse\",\"import requests\",\"from requests.auth import HTTPBasicAuth\",\"from requests import auth\",\"from leaputils import Security\",\"import json\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\\"<dataset.datasource.connectionDetails.ConnectionType>\\\"\\n    auth_type = \\\"<dataset.datasource.connectionDetails.AuthType>\\\"\\n    auth_details = \\\"<dataset.datasource.connectionDetails.AuthDetails>\\\"\\n    test_dataset = \\\"<dataset.datasource.connectionDetails.testDataset>\\\"\\n    noProxy = \\\"<dataset.datasource.connectionDetails.noProxy>\\\"\\n    salt = \\\"<dataset.datasource.connectionDetails.salt>\\\"\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    method = \\\"<dataset.attributes.RequestMethod>\\\"\\n    path = \\\"<dataset.attributes.EndPoint>\\\"\\n    params = \\\"<dataset.attributes.QueryParams>\\\"\\n    headers = \\\"<dataset.attributes.Headers>\\\"\\n    requestBody = \\\"<dataset.attributes.Body>\\\"\\n    documentElement = \\\"<TransformationScript>\\\"\\n    \\n    if connection_type.lower() == \\\"apirequest\\\":\\n        URL = url\\n    elif connection_type.lower() == \\\"apispec\\\":\\n        URL = url + path\\n    logging.info(\\\"Connecting to URL {0}\\\".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != \'\' and hostname in os.environ.get(\\\"NO_PROXY\\\",\\\"\\\").split(\',\')) or (noProxy.lower() == \'true\'):\\n        logging.info(\\\"Removing Proxy\\\")\\n        PROXIES[\'http\'] = \'\'\\n        PROXIES[\'https\'] = \'\'\\n    auth_details=auth_details\\n    auth_token=\\\"\\\"\\n\\n    header_prefix = \\\"Bearer\\\"\\n    response = \\\"\\\"\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != \'\':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if headers != \'\':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\\"key\\\")] = item_object.get(\\\"value\\\")\\n\\n    if auth_type.lower() == \\\"basicauth\\\":\\n\\n        username = auth_details.get(\\\"username\\\")\\n        enc_password = auth_details.get(\\\"password\\\")\\n        password=enc_password\\n        if str(enc_password).startswith(\'enc\'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    elif auth_type.lower() == \\\"bearertoken\\\":\\n        auth_token = auth_details.get(\\\"authToken\\\")\\n\\n    elif auth_type.lower() == \\\"oauth\\\":\\n        auth_url = auth_details.get(\\\"authUrl\\\")\\n        auth_params = auth_details.get(\\\"authParams\\\")\\n        auth_headers = auth_details.get(\\\"authHeaders\\\")\\n        header_prefix = auth_details.get(\\\"HeaderPrefix\\\")\\n        auth_method = auth_details.get(\\\"authMethod\\\" , \\\"GET\\\")\\n        token_element = auth_details.get(\\\"tokenElement\\\", \\\"\\\")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n        if token_element!=\\\"\\\":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\\"noauth\\\":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    if auth_token!= \\\"\\\":\\n        HEADERS[\'Authorization\'] = header_prefix + \\\" \\\" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\\"CONNECT_TIMEOUT\\\",\\\"30\\\")), int(os.environ.get(\\\"READ_TIMEOUT\\\",\\\"30\\\"))))\\n\\n    logging.info(\\\"Response Code: {0}\\\".format(response.status_code))\\n\"},\"MYSQL\":{\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"Drop table IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"`{0}` TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"`{0}`\\\".format(k) for k in paramsDict)\\n            duplicates = \\\", \\\".join(\\\"{0}=VALUES({0})\\\".format(k) for k in paramsDict)\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON DUPLICATE KEY UPDATE {1}\\\".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\\"{0}:{1}\\\".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()\"},\"MSSQL\":{\"imports\":[\"from leap.core.iLoader import Loader\",\"from leap.utils.Utilities import Utilities\",\"import logging as logger\",\"from leap.utils import vault\",\"import pyodbc\",\"import re\",\"from datetime import datetime\",\"import os\"],\"script\":\"def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\\"<dataset.attributes.writeMode>\\\\\\\"\\n\\n    url=\\\\\\\"<dataset.datasource.connectionDetails.url>\\\\\\\"\\n\\n    tablename = \\\\\\\"<dataset.attributes.tableName>\\\\\\\"\\n\\n    username = \\\\\\\"<dataset.datasource.connectionDetails.userName>\\\\\\\"\\n\\n    password = Security.decrypt(\\\\\\\"<dataset.datasource.connectionDetails.password>\\\\\\\",\\\\\\\"<dataset.datasource.salt>\\\\\\\")\\n\\n    temp1 = self.url.split(\\\"//\\\")\\n\\n    temp2 = temp1[1].split(\\\";\\\")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\\"=\\\"))[1]\\n\\n    isTrusted = \\\"no\\\"\\n\\n    if username == \\\"\\\":\\n\\n    isTrusted = \\\"yes\\\"\\n\\n    regex = \\\\\\\"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\\"\\n\\n\\n    if(re.search(regex, server.split(\\\":\\\")[0])):\\n\\n        server=server.replace(\\\":\\\",\\\",\\\")\\n\\n\\n    connectionString = \\\\\\\"DRIVER={0};SERVER={1}; \\\\\\\"\\n\\n                       \\\\\\\"DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\\".format(\\n\\n        \\\"ODBC Driver 17 for SQL SERVER\\\", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\\"overwrite\\\\\\\":\\n\\n        cursor.execute(\\\\\\\"Drop table IF EXISTS {0}\\\\\\\".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\\", \\\\\\\".join([\\\\\\\"`{0}` TEXT\\\\\\\".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\\" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\\".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\\", \\\\\\\".join(\\\\\\\"`{0}`\\\\\\\".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\\", \\\\\\\".join(\\\\\\\"{0}=VALUES({0})\\\\\\\".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\\", \\\\\\\".join(\\\\\\\"%s\\\\\\\".format(k) for k in paramsDict)\\n\\n            query = \\\\\\\"INSERT INTO {0} ({1}) VALUES ({2})\\\\\\\".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\\"update\\\\\\\"):\\n\\n                query = \\\\\\\"{0} ON DUPLICATE KEY UPDATE {1}\\\\\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\\"{0}:{1}\\\\\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()\"},\"AWS\":{\"imports\":[\"import pandas as pd\",\"import pickle\",\"import os\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    url = \\\"<dataset.attributes.Url>\\\"\\n    filename = url.split(\'/\')[-1]\\n    extension = filename.split(\'.\')[-1]\\n\\n    data_directory = \\\"/opt/ml/processing/output\\\"\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\\"Saving data\\\")\\n    if extension == \'.csv\':\\n        dataset.to_csv(file_path)\\n    elif extension == \'pkl\':\\n        pickle.dumps(dataset, open(file_path, \'wb\'))\\n    else:\\n        with open(file_path, \'w\') as f:\\n            f.writelines(dataset)\\n\\n\"},\"POSTGRESQL\":{\"imports\":[\"import psycopg2\",\"from urllib.parse import urlparse\",\"from leaputils import Security\"],\"script\":\"\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\\"<dataset.attributes.writeMode>\\\"\\n    url=\\\"<dataset.datasource.connectionDetails.url>\\\"\\n    tablename = \\\"<dataset.attributes.tableName>\\\"\\n    username = \\\"<dataset.datasource.connectionDetails.userName>\\\"\\n    password = Security.decrypt(\\\"<dataset.datasource.connectionDetails.password>\\\",\\\"<dataset.datasource.salt>\\\")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\\"/\\\", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\"overwrite\\\":\\n        mycursor.execute(\\\"DROP TABLE IF EXISTS {0}\\\".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\\", \\\".join([\\\"{0} TEXT\\\".format(c) for c in columnList])\\n    createQuery = \\\"CREATE TABLE IF NOT EXISTS {0} ({1})\\\".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\", \\\".join(\\\"{0}\\\".format(k) for k in paramsDict.keys())\\n            duplicates = \\\", \\\".join(\\\"{0}=EXCLUDED.{0}\\\".format(k) for k in paramsDict.keys())\\n            place_holders = \\\", \\\".join(\\\"%s\\\".format(k) for k in paramsDict)\\n\\n            query = \\\"INSERT INTO {0} ({1}) VALUES ({2})\\\".format(tablename, columns, place_holders)\\n            if mode in (\\\"update\\\"):\\n                query = \\\"{0} ON CONFLICT DO UPDATE SET {1}\\\".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\\"{0}:{1}\\\".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\"}},\"context\":[{\"FunctionName\":\"createClusters\",\"requirements\":\"\",\"params\":[],\"script\":[\"# importing required packages\\rfrom datetime import datetime\\rimport pandas as pd\\rdef createClusters(dataset):\\r    print(type(dataset))\\r    dataset = pd.DataFrame(dataset)\\r    sound_Df = dataset.groupby([\'group_by_field\',\'sound\']).agg(\\r    numberList = pd.NamedAgg(column=\'number\',aggfunc=list),\\r        textList = pd.NamedAgg(column=\'CleanText\',aggfunc=list)\\r    ).reset_index()\\r    sound_Df[\'numberListSize\'] = sound_Df[\'numberList\'].apply(len)\\r    sound_Df = sound_Df[sound_Df[\'numberList\'].apply(lambda x : len(x) >= 5)]\\r    sound_Df[\'cluster\'] = sound_Df[\'textList\'].apply(lambda x: x[0])\\r    sound_Df = sound_Df.drop(columns=[\'textList\'])\\r    sound_Df = sound_Df.explode(\'numberList\').reset_index(drop=True)\\r    sound_Df[\'lastUpdated\'] = datetime.now()\\r    sound_Df = sound_Df.rename(columns={\'numberList\':\'number\',\'cluster\': \'soundex_cluster\'})\\r    sound_Df = sound_Df.to_dict(\'records\')\\r    print(sound_Df)\\r    return sound_Df\\r\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-09-11 08:21:16\",\"alias\":\"Tickets_cluster\",\"id\":2776,\"name\":\"LEATCKTS64318\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, CleanText, Soundex(CleanText) as sound, CASE WHEN group_by_field IS NULL THEN \\\\\\\"\\\\\\\" ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE CleanText is not null and  Soundex(CleanText) != \\\\\\\"\\\\\\\" and CleanText != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"tzhfg\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-09-11 08:21:16\",\"alias\":\"Tickets_cluster\",\"id\":2776,\"name\":\"LEATCKTS64318\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, CleanText, Soundex(CleanText) as sound, CASE WHEN group_by_field IS NULL THEN \\\\\\\"\\\\\\\" ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE CleanText is not null and  Soundex(CleanText) != \\\\\\\"\\\\\\\" and CleanText != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"leo1311_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"poornasai.nagendra@ad.infosys.com\",\"lastmodifieddate\":\"2023-07-28 05:55:39\",\"alias\":\"leo1311\",\"id\":524,\"name\":\"LEAL131Q55868\",\"description\":\"Leo data scource\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw==\",\"organization\":\"Leap\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-07-28 05:55:38\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null},\"backingDataset\":null,\"organization\":\"Leap\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"272\",\"position_y\":\"11\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ptuPQ\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"servicenow\":{},\"MYSQL\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{}},\"context\":[]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}]}","admin","Clustering soundex","2023-09-11 08:27:56","LEACLSTR70385","leo1311","DragNDropLite","81","NULL","pipeline","{\"81\":{\"taskId\":\"5fda415a-1423-4e53-b39b-9eef8d113a51\"}}"
