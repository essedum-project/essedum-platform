"type","org","plugindetails","pluginname"
"DragAndDrop","Core","{""id"": ""1"", ""name"": ""Count Vectorizer"", ""alias"": ""Count Vectorizer"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text"", ""vocabSize"": ""text""}, ""category"": ""FeatureExtractorConfig"", ""classname"": ""CountVectorizerFeatureExtractorConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """", ""vocabSize"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-1"
"DragAndDrop","Core","{""id"": ""2"", ""name"": ""TF"", ""alias"": ""TF"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""FeatureExtractorConfig"", ""classname"": ""TfFeatureExtractorConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-2"
"DragAndDrop","Core","{""id"": ""3"", ""name"": ""IDF"", ""alias"": ""IDF"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""FeatureExtractorConfig"", ""classname"": ""IdfFeatureExtractorConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-3"
"DragAndDrop","Core","{""id"": ""4"", ""name"": ""Python Script Transformer"", ""alias"": ""Python Script Transformer"", ""formats"": {""script"": ""textarea""}, ""category"": ""TransformerConfig"", ""classname"": ""PythonScriptTransformerConfig"", ""attributes"": {""script"": [""import logging\\r"", ""class CustomPythonClass():\\r"", "" def __main__(self, dataset):\\r"", "" \\r"", "" return dataset""]}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-4"
"DragAndDrop","Core","{""id"": ""5"", ""name"": ""Word Lemmetizer"", ""alias"": ""Word Lemmetizer"", ""formats"": {""tags"": ""textarea"", ""inputCol"": ""textarea"", ""outputCol"": ""textarea""}, ""category"": ""TransformerConfig"", ""classname"": ""WordLemmetizerTransformerConfig"", ""attributes"": {""tags"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-5"
"DragAndDrop","Core","{""id"": ""6"", ""name"": ""Word Stemmer"", ""alias"": ""Word Stemmer"", ""formats"": {""inputCol"": ""textarea"", ""outputCol"": ""textarea""}, ""category"": ""TransformerConfig"", ""classname"": ""WordStemmerTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-6"
"DragAndDrop","Core","{""id"": ""7"", ""name"": ""Naive Bayes"", ""alias"": ""Naive Bayes"", ""formats"": {""modelType"": ""text"", ""smoothing"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""NaiveBayesTransformerConfig"", ""attributes"": {""modelType"": """", ""smoothing"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-7"
"DragAndDrop","Core","{""id"": ""8"", ""name"": ""Tokenizer"", ""alias"": ""Tokenizer"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""TokenizerTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-8"
"DragAndDrop","Core","{""id"": ""9"", ""name"": ""Regex Tokenizer"", ""alias"": ""Regex Tokenizer"", ""formats"": {""pattern"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""RegexTokenizerTransformerConfig"", ""attributes"": {""pattern"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-9"
"DragAndDrop","Core","{""id"": ""10"", ""name"": ""Vector Assembler"", ""alias"": ""Vector Assembler"", ""formats"": {""inputCols"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""VectorAssemblerTransformerConfig"", ""attributes"": {""inputCols"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-10"
"DragAndDrop","Core","{""id"": ""11"", ""name"": ""Standard Scaler"", ""alias"": ""Standard Scaler"", ""formats"": {""withStd"": ""text"", ""inputCol"": ""text"", ""withMean"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""StandardScalerTransformerConfig"", ""attributes"": {""withStd"": """", ""inputCol"": """", ""withMean"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-11"
"DragAndDrop","Core","{""id"": ""12"", ""name"": ""PCA"", ""alias"": ""PCA"", ""formats"": {""K"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""PCATransformerConfig"", ""attributes"": {""K"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-12"
"DragAndDrop","Core","{""id"": ""13"", ""name"": ""K Means"", ""alias"": ""K Means"", ""formats"": {""K"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""KMeansTransformerConfig"", ""attributes"": {""K"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-13"
"DragAndDrop","Core","{""id"": ""14"", ""name"": ""LDA"", ""alias"": ""LDA"", ""formats"": {""K"": ""text"", ""inputCol"": ""text"", ""checkpointInterval"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""LDAConfig"", ""attributes"": {""K"": """", ""inputCol"": """", ""checkpointInterval"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-14"
"DragAndDrop","Core","{""id"": ""15"", ""name"": ""Model Sink"", ""alias"": ""Model Sink"", ""formats"": {""modelName"": ""text""}, ""category"": ""BaseConfig"", ""classname"": ""ModelSinkConfig"", ""attributes"": {""modelName"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-15"
"DragAndDrop","Core","{""id"": ""16"", ""name"": ""Model Source"", ""alias"": ""Model Source"", ""formats"": {""modelName"": ""text""}, ""category"": ""BaseConfig"", ""classname"": ""ModelSourceConfig"", ""attributes"": {""modelName"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-16"
"DragAndDrop","Core","{""id"": ""17"", ""name"": ""Post Processing Script"", ""alias"": ""Post Processing Script"", ""formats"": {""params"": ""textarea"", ""script"": ""textarea""}, ""category"": ""BaseConfig"", ""classname"": ""PostProcessingScriptConfig"", ""attributes"": {""params"": """", ""script"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out"", ""out2""]}","DragAndDrop-17"
"DragAndDrop","Core","{""id"": ""18"", ""name"": ""Pre Processing Script"", ""alias"": ""Pre Processing Script"", ""formats"": {""params"": ""textarea"", ""script"": ""textarea""}, ""category"": ""BaseConfig"", ""classname"": ""PreProcessingScriptConfig"", ""attributes"": {""params"": """", ""script"": """"}, ""inputEndpoints"": [""dataset1"", ""dataset2"", ""dataset3""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-18"
"DragAndDrop","Core","{""id"": ""19"", ""name"": ""Random Forest Classifier"", ""alias"": ""Random Forest Classifier"", ""formats"": {""labelCol"": ""text"", ""numTrees"": ""text"", ""featuresCol"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""RandomForestClassifierConfig"", ""attributes"": {""labelCol"": """", ""numTrees"": """", ""featuresCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-19"
"DragAndDrop","Core","{""id"": ""20"", ""name"": ""Stop Words Remover"", ""alias"": ""Stop Words Remover"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text"", ""stopWords"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""StopWordsRemoverTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """", ""stopWords"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-20"
"DragAndDrop","Core","{""id"": ""21"", ""name"": ""N Gram"", ""alias"": ""N Gram"", ""formats"": {""n"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""NgramTransformerConfig"", ""attributes"": {""n"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-21"
"DragAndDrop","Core","{""id"": ""22"", ""name"": ""Discrete Cosine Transform"", ""alias"": ""Discrete Cosine Transform"", ""formats"": {""inverse"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""DiscreteCosineTransformTransformerConfig"", ""attributes"": {""inverse"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-22"
"DragAndDrop","Core","{""id"": ""23"", ""name"": ""String Indexer"", ""alias"": ""String Indexer"", ""formats"": {""inputCols"": ""text"", ""outputCols"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""StringIndexerTransformerConfig"", ""attributes"": {""inputCols"": """", ""outputCols"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-23"
"DragAndDrop","Core","{""id"": ""24"", ""name"": ""Normalizer"", ""alias"": ""Normalizer"", ""formats"": {""p"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""NormalizerTransformerConfig"", ""attributes"": {""p"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-24"
"DragAndDrop","Core","{""id"": ""25"", ""name"": ""Min Max Scaler"", ""alias"": ""Min Max Scaler"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""MinMaxScalerTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-25"
"DragAndDrop","Core","{""id"": ""26"", ""name"": ""SQL Transformer"", ""alias"": ""SQL Transformer"", ""formats"": {""statement"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""SQLTransformerConfig"", ""attributes"": {""statement"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-26"
"DragAndDrop","Core","{""id"": ""27"", ""name"": ""Anomoly Detection"", ""alias"": ""Anomoly Detection"", ""formats"": {""inputCols"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""AnomolyDetectionConfig"", ""attributes"": {""inputCols"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-27"
"DragAndDrop","Core","{""id"": ""28"", ""name"": ""ARIMA"", ""alias"": ""ARIMA"", ""formats"": {""inputCols"": ""text"", ""seasonality"": ""text"", ""predictionCount"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""ARIMAModelConfig"", ""attributes"": {""inputCol"": """", ""seasonality"": """", ""predictionCount"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-28"
"DragAndDrop","Core","{""id"": ""29"", ""name"": ""ARIMA Grouped"", ""alias"": ""ARIMA Grouped"", ""formats"": {""groupCol"": ""text"", ""inputCols"": ""text"", ""predictionCount"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""ARIMAGroupedConfig"", ""attributes"": {""groupCol"": """", ""inputCol"": """", ""predictionCount"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-29"
"DragAndDrop","Core","{""id"": ""30"", ""name"": ""Clean Tickets"", ""alias"": ""Clean Tickets"", ""formats"": {""inputCols"": ""text"", ""outputCol"": ""text"", ""ambiguousWords"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""CleanTicketsConfig"", ""attributes"": {""inputCols"": """", ""outputCol"": ""clean_text"", ""ambiguousWords"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-30"
"DragAndDrop","Core","{""id"": ""31"", ""name"": ""Incident Clustering"", ""alias"": ""Incident Clustering"", ""formats"": {""vocabSize"": ""text"", ""TextColumn"": ""text"", ""saveModels"": ""checkbox"", ""ClusterCount"": ""text"", ""GroupByColumn"": ""text"", ""UniqueIdColumn"": ""text"", ""customStopWords"": ""text"", ""checkPointInterval"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""IncidentClusteringLDAConfig"", ""attributes"": {""vocabSize"": """", ""TextColumn"": """", ""saveModels"": """", ""ClusterCount"": """", ""GroupByColumn"": """", ""UniqueIdColumn"": """", ""customStopWords"": """", ""checkPointInterval"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""topics"", ""out""]}","DragAndDrop-31"
"DragAndDrop","Core","{""id"": ""32"", ""name"": ""Phishing Detection"", ""alias"": ""Phishing Detection"", ""formats"": {""inputCols"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""PhishingDetectionConfig"", ""attributes"": {""inputCols"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-32"
"DragAndDrop","Core","{""id"": ""33"", ""name"": ""Feature Selection"", ""alias"": ""Feature Selection"", ""formats"": {""labelCol"": ""text"", ""inputCols"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""FeatureSelectionConfig"", ""attributes"": {""labelCol"": """", ""inputCols"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-33"
"DragAndDrop","Core","{""id"": ""34"", ""name"": ""Regression"", ""alias"": ""Regression"", ""formats"": {""labelCol"": ""text"", ""inputCols"": ""text"", ""modelName"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""RegressionConfig"", ""attributes"": {""labelCol"": """", ""inputCols"": """", ""modelName"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-34"
"DragAndDrop","Core","{""id"": ""35"", ""name"": ""Classification"", ""alias"": ""Classification"", ""formats"": {""labelCol"": ""text"", ""inputCols"": ""text"", ""modelName"": ""text""}, ""category"": ""DomainSolutionConfig"", ""classname"": ""ClassificationConfig"", ""attributes"": {""labelCol"": """", ""inputCols"": """", ""modelName"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-35"
"DragAndDrop","Core","{""id"": ""36"", ""name"": ""Logistic Regression"", ""alias"": ""Logistic Regression"", ""formats"": {""labelCol"": ""text"", ""featuresCol"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""LogisticRegressionConfig"", ""attributes"": {""labelCol"": """", ""featuresCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-36"
"DragAndDrop","Core","{""id"": ""37"", ""name"": ""Decision Tree Classifier"", ""alias"": ""Decision Tree Classifier"", ""formats"": {""labelCol"": ""text"", ""featuresCol"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""DecisionTreeClassifierConfig"", ""attributes"": {""labelCol"": """", ""featuresCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-37"
"DragAndDrop","Core","{""id"": ""38"", ""name"": ""GBT Classifier"", ""alias"": ""GBT Classifier"", ""formats"": {""maxIter"": ""text"", ""labelCol"": ""text"", ""featuresCol"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""GBTClassifierConfig"", ""attributes"": {""maxIter"": """", ""labelCol"": """", ""featuresCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-38"
"DragAndDrop","Core","{""id"": ""39"", ""name"": ""Linear SVC"", ""alias"": ""Linear SVC"", ""formats"": {""maxIter"": ""text"", ""labelCol"": ""text"", ""regParam"": ""text"", ""featuresCol"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""LinearSVCConfig"", ""attributes"": {""maxIter"": """", ""labelCol"": """", ""regParam"": """", ""featuresCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-39"
"DragAndDrop","Core","{""id"": ""40"", ""name"": ""Linear Regression"", ""alias"": ""Linear Regression"", ""formats"": {""maxIter"": ""text"", ""labelCol"": ""text"", ""regParam"": ""text"", ""featuresCol"": ""text"", ""elasticNetParam"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""LinearRegressionConfig"", ""attributes"": {""maxIter"": """", ""labelCol"": """", ""regParam"": """", ""featuresCol"": """", ""elasticNetParam"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-40"
"DragAndDrop","Core","{""id"": ""41"", ""name"": ""Random Forest Regressor"", ""alias"": ""Random Forest Regressor"", ""formats"": {""labelCol"": ""text"", ""featuresCol"": ""text""}, ""category"": ""AnalyzerConfig"", ""classname"": ""RandomForestRegressorConfig"", ""attributes"": {""labelCol"": """", ""featuresCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-41"
"DragAndDrop","Core","{""id"": ""42"", ""name"": ""Dataset Extractor"", ""alias"": ""Dataset Dataset"", ""formats"": {""dataset"": ""dropdown"", ""applySchema"": ""checkbox"", ""isValidation"": ""checkbox"", ""samplingRatio"": ""text""}, ""category"": ""ExtractorConfig"", ""classname"": ""DatasetExtractorConfig"", ""attributes"": {""dataset"": """", ""applySchema"": false, ""isValidation"": """", ""samplingRatio"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-42"
"DragAndDrop","Core","{""id"": ""43"", ""name"": ""Word2Vec"", ""alias"": ""Word2Vec"", ""formats"": {""seed"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""FeatureExtractorConfig"", ""classname"": ""Word2VecFeatureExtractorConfig"", ""attributes"": {""seed"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-43"
"DragAndDrop","Core","{""id"": ""44"", ""name"": ""FeatureHasher"", ""alias"": ""FeatureHasher"", ""formats"": {""inputCols"": ""text"", ""outputCol"": ""text"", ""categoricalCols"": ""text""}, ""category"": ""FeatureExtractorConfig"", ""classname"": ""FeatureHasherFeatureExtractorConfig"", ""attributes"": {""inputCols"": """", ""outputCol"": """", ""categoricalCols"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-44"
"DragAndDrop","Core","{""id"": ""45"", ""name"": ""Binarizer"", ""alias"": ""Binarizer"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""BinarizerTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-45"
"DragAndDrop","Core","{""id"": ""46"", ""name"": ""PolynomialExpansion"", ""alias"": ""PolynomialExpansion"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""PolynomialExpansionConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-46"
"DragAndDrop","Core","{""id"": ""47"", ""name"": ""IndexToString"", ""alias"": ""IndexToString"", ""formats"": {""labels"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""IndexToStringTransformerConfig"", ""attributes"": {""labels"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-47"
"DragAndDrop","Core","{""id"": ""48"", ""name"": ""OneHotEncoderEstimator"", ""alias"": ""OneHotEncoderEstimator"", ""formats"": {""inputCols"": ""text"", ""outputCols"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""OneHotEncoderEstimatorTransformerConfig"", ""attributes"": {""inputCols"": """", ""outputCols"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-48"
"DragAndDrop","Core","{""id"": ""49"", ""name"": ""VectorIndexer"", ""alias"": ""VectorIndexer"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""VectorIndexerTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-49"
"DragAndDrop","Core","{""id"": ""50"", ""name"": ""MaxAbsScaler"", ""alias"": ""MaxAbsScaler"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""MaxAbsScalerTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-50"
"DragAndDrop","Core","{""id"": ""51"", ""name"": ""Bucketizer"", ""alias"": ""Bucketizer"", ""formats"": {""splits"": ""text"", ""inputCol"": ""text"", ""outputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""BucketizerTransformerConfig"", ""attributes"": {""splits"": """", ""inputCol"": """", ""outputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-51"
"DragAndDrop","Core","{""id"": ""52"", ""name"": ""ElementWiseProduct"", ""alias"": ""ElementWiseProduct"", ""formats"": {""inputCol"": ""text"", ""outputCol"": ""text"", ""scalingVec"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""ElementWiseProductTransformerConfig"", ""attributes"": {""inputCol"": """", ""outputCol"": """", ""scalingVec"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-52"
"DragAndDrop","Core","{""id"": ""53"", ""name"": ""VectorSizeHint"", ""alias"": ""VectorSizeHint"", ""formats"": {""size"": ""text"", ""inputCol"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""VectorSizeHintTransformerConfig"", ""attributes"": {""size"": """", ""inputCol"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-53"
"DragAndDrop","Core","{""id"": ""54"", ""name"": ""Dataset Loader"", ""alias"": ""Dataset Loader"", ""formats"": {""dataset"": ""dropdown"", ""applySchema"": ""checkbox""}, ""category"": ""LoaderConfig"", ""classname"": ""DatasetLoaderConfig"", ""attributes"": {""dataset"": """", ""applySchema"": false}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-54"
"DragAndDrop","Core","{""id"": ""55"", ""name"": ""Hive Extractor"", ""alias"": ""Hive Extractor"", ""formats"": {""query"": ""text""}, ""category"": ""ExtractorConfig"", ""classname"": ""HiveExtractorConfig"", ""attributes"": {""query"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-55"
"DragAndDrop","Core","{""id"": ""56"", ""name"": ""Hive Loader"", ""alias"": ""Hive Loader"", ""formats"": {""tableName"": ""text"", ""writeMode"": ""text""}, ""category"": ""LoaderConfig"", ""classname"": ""HiveLoaderConfig"", ""attributes"": {""tableName"": """", ""writeMode"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-56"
"DragAndDrop","Core","{""id"": ""57"", ""name"": ""XYZ_test"", ""alias"": ""XYZ_test"", ""formats"": {""id"": ""text"", ""arguments"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""XYZ_test"", ""attributes"": {""id"": """", ""arguments"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragAndDrop-57"
"DragNDropLite","Core","{""id"": 1, ""name"": ""Dataset Extractor"", ""alias"": ""Dataset Dataset"", ""formats"": {""dataset"": [""dropdown""]}, ""category"": ""ExtractorConfig"", ""classname"": ""DatasetExtractorConfig"", ""attributes"": {""dataset"": [""""]}, ""codeGeneration"": {""H2"": {}, ""AWS"": {}, ""REST"": {}, ""MSSQL"": {}, ""MYSQL"": {}, ""POSTGRESQL"": {}, ""servicenow"": {}}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-1"
"DragNDropLite","Core","{""id"": 2, ""name"": ""Dataset Loader"", ""alias"": ""Dataset Loader"", ""formats"": {""dataset"": ""dropdown""}, ""category"": ""LoaderConfig"", ""classname"": ""DatasetLoaderConfig"", ""attributes"": {""dataset"": """"}, ""codeGeneration"": {""AWS"": {""script"": ""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n"", ""imports"": [""import pandas as pd"", ""import pickle"", ""import os""]}, ""REST"": {""script"": ""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n"", ""imports"": [""from urllib.parse import urlparse"", ""import requests"", ""from requests.auth import HTTPBasicAuth"", ""from requests import auth"", ""from leaputils import Security"", ""import json""]}, ""MSSQL"": {""script"": ""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()"", ""imports"": [""from leap.core.iLoader import Loader"", ""from leap.utils.Utilities import Utilities"", ""import logging as logger"", ""from leap.utils import vault"", ""import pyodbc"", ""import re"", ""from datetime import datetime"", ""import os""]}, ""MYSQL"": {""script"": ""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()"", ""imports"": [""import mysql.connector"", ""from urllib.parse import urlparse"", ""from leaputils import Security""]}, ""POSTGRESQL"": {""script"": ""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()"", ""imports"": [""import psycopg2"", ""from urllib.parse import urlparse"", ""from leaputils import Security""]}}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": []}","DragNDropLite-2"
"DragNDropLite","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""ModelLoaderConfig"",""name"":""Model Loader"",""alias"":""Model Loader"",""parentCategory"":""141"",""id"":3,""codeGeneration"":{""REST"":{},""MYSQL"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""category"":""LoaderConfig"",""inputEndpoints"":[""in""],""outputEndpoints"":[],""attributes"":{""dataset"":""""}}","DragNDropLite-3"
"DragNDropLite","Core","{""id"": 4, ""name"": ""ScriptTransformer"", ""alias"": ""Script Transformer"", ""formats"": {""script"": ""textarea"", ""requirements"": ""text""}, ""category"": ""TransformerConfig"", ""classname"": ""ScriptTransformerConfig"", ""attributes"": {""script"": ""\\rdef ScriptTransformer_<id>( dataset):\\n    #pre-process Data\\r\\r    return dataset"", ""requirements"": """"}, ""codeGeneration"": {""script"": """", ""imports"": []}, ""inputEndpoints"": [""in1"", ""in2""], ""parentCategory"": """", ""outputEndpoints"": [""out1"", ""out2""]}","DragNDropLite-4"
"DragNDropLite","Core","{""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""classname"":""PreProcessingScriptConfig"",""name"":""Pre Processing Script"",""alias"":""Pre Processing Script"",""parentCategory"":""146"",""id"":5,""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n""},""category"":""BaseConfig"",""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""attributes"":{""FunctionName"":""PreProcessingScript"",""requirements"":"""",""params"":"""",""script"":""\\rdef PreProcessingScript_<id>( dataset):\\n    #pre-process Data\\r\\r    return dataset""}}","DragNDropLite-5"
"DragNDropLite","Core","{""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""classname"":""PostProcessingScriptConfig"",""name"":""Post Processing Script"",""alias"":""Post Processing Script"",""parentCategory"":""146"",""id"":6,""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""category"":""BaseConfig"",""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""attributes"":{""FunctionName"":""your_function_name"",""requirements"":"""",""params"":[],""script"":""def PostProcessingScript_<id>( dataset):    #Post-process Data    return dataset""}}","DragNDropLite-6"
"DragNDropLite","Core","{""formats"":{""modelName"":""text"",""experimentName"":""text""},""classname"":""SaveModel"",""name"":""SaveModel"",""alias"":""SaveModel"",""parentCategory"":""147"",""id"":7,""codeGeneration"":{""requirements"":[],""imports"":[""import pickle"",""from leaputils import FileServer"",""import requests"",""import hashlib"",""import shutil"",""import logging as logger"",""import os""],""script"":""\\ndef SaveModel_<id>(model, modelname_param='your_model_name.pkl', experimentname_param=''):\\n    #SaveModel\\n    modelPath = os.path.join(os.environ['JOB_DIRECTORY'],'models',experimentname_param)\\n    print('Saving Model at path:'+modelPath )\\n    logging.info('Saving Model at path:'+modelPath )\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    modelPath = os.path.join(modelPath, modelname_param)\\n    print('Saving Model at path:'+modelPath )\\n    if modelPath.split('.')[-1] == 'pkl':\\n        pickle.dump(model, open(modelPath, 'wb'))\\n        return modelPath\\n    else:\\n        model.write().overwrite().save(modelPath)\\n        return modelPath\\n\\n""},""category"":""ModelBaseConfig"",""inputEndpoints"":[""model""],""outputEndpoints"":[""modelPath""],""attributes"":{""modelName"":""your_model_name.pkl"",""experimentName"":""""}}","DragNDropLite-7"
"DragNDropLite","Core","{""formats"":{""Execution script"":""textarea"",""ModelName"":""text"",""Description"":""text"",""Bootstrap script"":""textarea"",""Bucket"":""text"",""ExperimentName"":""text"",""TypeOfModel"":""text""},""classname"":""PublishModelConfig"",""name"":""PublishModel"",""alias"":""PublishModel"",""parentCategory"":""147"",""id"":8,""codeGeneration"":{""requirements"":[],""imports"":[""import os.path"",""import shutil"",""import joblib"",""from leaputils import FileServer"",""import datetime"",""import pickle"",""import json""],""script"":""\\ndef PublishModel_<id>(model, execution_script_param='', description_param='', modelname_param='', bootstrap_script_param='', experimentname_param='', bucket_param='', typeofmodel_param=''):\\n    import torch\\n    #SaveModel\\n    modelPath = os.path.join(os.environ['JOB_DIRECTORY'],'models',experimentname_param)\\n    print('Saving Model at path:'+modelPath )\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    modelPath = os.path.join(modelPath)\\n    print('Saving Model at path:'+modelPath )\\n    if modelname_param.split('.')[-1] == 'pkl':\\n        pickle.dump(model, open(os.path.join(modelPath, modelname_param), 'wb'))\\n    elif modelname_param.split('.')[-1] == 'pt':\\n        torch.save(model,os.path.join(modelPath,modelname_param))\\n    elif modelname_param.split('.')[-1] == 'joblib':\\n        joblib.dump(model, open(os.path.join(modelPath,modelname_param), 'wb'))\\n    else:\\n        model.write().overwrite().save(os.path.join(modelPath,modelname_param))\\n    modelPath=os.path.join(modelPath,modelname_param)\\n    experimentName = experimentname_param\\n    bucket = bucket_param\\n    bootstrapScript = bootstrap_script_param\\n    executeScript = execution_script_param\\n    modelId = FileServer.generateFilId(bucket)\\n\\n    # zipFilePath = FileServer.zipFile(modelPath,os.path.join(os.environ['JOB_DIRECTORY'],'models'))\\n    # print(zipFilePath)\\n    fileId, fileName = FileServer.uploadFile(modelId,modelPath,bucket,False)\\n    apispec = {\\n          'openapi': '3.0.2',\\n          'info': {\\n            'version': '1',\\n            'title': experimentName\\n          },\\n          'servers': [\\n            {\\n              'url': os.environ['MODEL_SERVICE_URL']\\n            }\\n          ],\\n          'paths': {\\n            '/'+modelId: {\\n              'post': {\\n                'requestBody': {\\n                  'description': 'request',\\n                  'content': {\\n                    'application/json': {\\n                      'schema': {\\n                        '$ref': '#/components/schemas/input'\\n                      }\\n                    }\\n                  }\\n                },\\n                'responses': {\\n                  '200': {\\n                    'description': 'response',\\n                    'content': {\\n                      'application/json': {\\n                        'schema': {\\n                          'type': 'string'\\n                        }\\n                      }\\n                    }\\n                  },\\n                  'default': {\\n                    'description': 'error',\\n                    'content': {\\n                      'application/json': {\\n                        'schema': {\\n                          'type': 'string'\\n                        }\\n                      }\\n                    }\\n                  }\\n                }\\n              }\\n            }\\n          },\\n          'components': {\\n            'schemas': {\\n              'input': {\\n                'properties': {}\\n              }\\n            },\\n            'securitySchemes': {\\n              'bearerAuth': {\\n                'type': 'http',\\n                'scheme': 'bearer',\\n                'bearerFormat': 'JWT'\\n              }\\n            },\\n            'security': [\\n              {\\n                'bearerAuth': []\\n              }\\n            ]\\n          }\\n    }\\n    model = {}\\n    model['apispec']= json.dumps(apispec)\\n    model['error'] = 0\\n    model['fileId'] = modelId\\n    model['localupload'] =100\\n    model['status'] =0\\n    model['loadscript'] =bootstrapScript\\n    model['executionscript'] =executeScript\\n    model['metadata']=json.dumps({'type':'local','modeltype':'uploadmodel','createdtime':''+datetime.datetime.now().strftime('%m/%d/%Y, %H:%M:%S') +'','version':'1','framework':'','pushtocodestore':True,'public':True,'overwrite':True,'summary':'','taginfo':'','frameworkVersion':'','modelClassName':'','inferenceClassName':'','filePath':'','inputType':'','submittedBy':'admin','filename':''+fileName+''})\\n    model['modelname']= experimentName\\n    model['description']= description_param\\n    model['organization'] = bucket\\n    model['serverupload'] =100\\n    return [model]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""category"":""ModelBaseConfig"",""inputEndpoints"":[""model""],""outputEndpoints"":[""model"",""endpoint""],""attributes"":{""Execution script"":"""",""ModelName"":"""",""Description"":"""",""Bootstrap script"":"""",""Bucket"":"""",""ExperimentName"":"""",""TypeOfModel"":""""}}","DragNDropLite-8"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""fileName"":""text"",""Destination_directory"":""text"",""fileId"":""text""},""classname"":""ModelSourceConfig"",""name"":""ModelSource"",""alias"":""ModelSource"",""parentCategory"":""147"",""id"":9,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os""],""script"":""\\ndef ModelSource_<id>(bucket_param='Demo', filename_param='your_model_name.pkl', destination_directory_param='models/classicmlpoc', fileid_param=''):\\n    fileId, fileName, bucket = fileid_param, filename_param, bucket_param\\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    print('FILE_SERVER_URL: ', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    print('FILE_SERVER_TOKEN: ', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ['JOB_DIRECTORY'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + '/api/lastcount/' + fileId + '?bucket=' + bucket,\\n                                           headers={'access-token': FILE_SERVER_TOKEN},\\n                                           proxies={'http': '', 'https': ''})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search('<Integer>(.*?)</Integer>', totalChunksResponse.text).group(1)\\n            totalChunks = int('0')\\n        logger.info('Total Chunks: ' + str(totalChunks + 1))\\n\\n        # create a folder 'chunkfiles'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, 'chunkfiles')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info('dir already exists...')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model's chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            filenam = 'file' if fileId[0]=='f' else filename_param\\n            url = (FILE_SERVER_URL + '/api/download/' + fileId + '/') + filenam + '?bucket=' + bucket\\n            print(url)\\n            filedata = requests.get(url, headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n            open(chunkfilePath + '/' + str(i), 'wb').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, 'wb') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, 'rb')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n""},""category"":""ModelBaseConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""}}","DragNDropLite-9"
"DragNDropLite","Core","{""formats"":{""requirements"":""textarea"",""script"":""textarea""},""classname"":""ModelPredictScriptConfig"",""name"":""Model Predict Script"",""alias"":""Model Predict Script"",""parentCategory"":""147"",""id"":10,""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""category"":""ModelBaseConfig"",""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""attributes"":{""requirements"":"""",""script"":""\\rdef ModelPredictScript_<id>( dataset):\\n    #Model Predict Script\\r\\r    return dataset""}}","DragNDropLite-10"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""File"":""file""},""classname"":""UploadFile"",""name"":""UploadFile"",""alias"":""UploadFile"",""parentCategory"":""146"",""id"":11,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os"",""import hashlib""],""script"":""\\ndef UploadFile_<id>(path, bucket_param='Demo', file_param=''):\\n    bucket, totalcount = bucket_param, 1\\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    def checksum(path):\\n        filepath = path\\n        sha256_hash = hashlib.sha256()\\n        with open(filepath, 'rb') as f:\\n            # Read and update hash string value in blocks of 4K\\n            for byte_block in iter(lambda: f.read(4096), b''):\\n                sha256_hash.update(byte_block)\\n            checksum = sha256_hash.hexdigest()\\n        print('Generated checksum is : ', checksum)\\n        return checksum\\n\\n    def generateFilId(bucket):\\n        fileIdRes = requests.get(\\n            FILE_SERVER_URL + '/api/generate/fileid' + '?bucket=' + bucket + '&prefix=filedataset',\\n            headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n        if (fileIdRes.status_code == 200):\\n            fileId = fileIdRes.text\\n            print('Generated file id is : ', fileId)\\n            return fileId\\n        else:\\n            print('File id could not be generated')\\n\\n    fileid = generateFilId(bucket)\\n    print(fileid)\\n    checksum = checksum(path)\\n    totalCnt = totalcount\\n    files = [('file', ('file', open(path, 'rb'), 'application/octet-stream'))]\\n    try:\\n        uploadRes = requests.post(FILE_SERVER_URL + '/api/upload/' + fileid + '/true?bucket=' + bucket,\\n                                  headers={'access-token': FILE_SERVER_TOKEN}, data={'checksum': checksum,\\n                                                                                     'totalcount': totalCnt},\\n                                  files=files, proxies={'http': '', 'https': ''})\\n    except Exception as ex:\\n        print(ex)\\n    if (uploadRes.status_code == 200):\\n        print('File uploaded successfully')\\n        return uploadRes\\n    else:\\n        print('Error occurred while uploading a file')\\n        print(uploadRes.text)\\n\\n\\n""},""category"":""BaseConfig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""bucket"":""Demo"",""File"":""""}}","DragNDropLite-11"
"DragNDropLite","Core","{""formats"":{""tol"":""integer"",""l1_ratio"":""integer"",""copyX"":""integer"",""selection"":""text"",""alpha"":""integer"",""maxIter"":""integer"",""positive"":""integer"",""random_state"":""integer"",""precompute"":""integer"",""warm_start"":""integer"",""FitIntercept"":""integer""},""classname"":""ElasticnetR"",""name"":""ElasticnetR"",""alias"":""ElasticnetR"",""parentCategory"":""148"",""id"":16,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from sklearn.linear_model import ElasticNet"",""from sklearn.preprocessing import StandardScaler,PolynomialFeatures"",""from sklearn.pipeline import Pipeline""],""script"":""\\ndef ElasticnetR_<id>(dataset, tol_param=0.0001, l1_ratio_param=0.5, copyx_param=True, selection_param='cyclic', alpha_param=1.0, maxiter_param=1000, positive_param=True, random_state_param=None, precompute_param=False, warm_start_param=False, fitintercept_param=True):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputEN=[('scale',StandardScaler()),('polynomial', PolynomialFeatures(include_bias=False)),('model',ElasticNet(alpha=alpha_param, l1_ratio=l1_ratio_param, fit_intercept=fitintercept_param, precompute=precompute_param, copy_X=copyx_param, max_iter=maxiter_param, tol=tol_param, warm_start=warm_start_param, positive=positive_param, random_state=random_state_param, selection=selection_param))]\\n    pipeEN=Pipeline(InputEN)\\n\\n    pipeEN.fit(dataset['X_train'],dataset['Y_train'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeEN.score(dataset['X_train'],dataset['Y_train']))  \\n    \\n    return pipeEN\\n\\n\\n""},""category"":""Regression"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""tol"":""0.0001"",""l1_ratio"":""0.5"",""copyX"":""True"",""selection"":""cyclic"",""alpha"":""1.0"",""maxIter"":""1000"",""positive"":""True"",""random_state"":""None"",""precompute"":""False"",""warm_start"":""False"",""FitIntercept"":""True""}}","DragNDropLite-16"
"DragNDropLite","Core","{""formats"":{""criterion"":""text"",""ccp_alpha"":""integer"",""oob_score"":""integer"",""n_jobs"":""integer"",""max_depth"":""integer"",""min_samples_split"":""integer"",""bootstrap"":""integer"",""n_estimators"":""integer"",""random_state"":""integer"",""min_impurity_decrease"":""integer"",""min_weight_fraction_leaf"":""integer"",""warm_start"":""integer"",""min_samples_leaf"":""integer"",""verbose"":""integer"",""max_samples"":""integer"",""max_features"":""integer"",""max_leaf_nodes"":""integer""},""classname"":""RandomForestR"",""name"":""RandomForestR"",""alias"":""RandomForestR"",""parentCategory"":""148"",""id"":17,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from sklearn.ensemble import RandomForestRegressor"",""from sklearn.preprocessing import StandardScaler,PolynomialFeatures"",""from sklearn.pipeline import Pipeline""],""script"":""\\ndef RandomForestR_<id>(dataset, criterion_param='mse', ccp_alpha_param=0.0, oob_score_param=False, max_depth_param=None, min_samples_split_param=2, n_jobs_param=None, n_estimators_param=100, min_weight_fraction_leaf_param=0.0, min_impurity_decrease_param=0.0, bootstrap_param=True, random_state_param=None, min_samples_leaf_param=1, warm_start_param=False, verbose_param=0, max_samples_param=None, max_features_param=1.0, max_leaf_nodes_param=None):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    Input=[('scale',StandardScaler()),('polynomial', PolynomialFeatures(include_bias=False)),('model',RandomForestRegressor(n_estimators=n_estimators_param, criterion=criterion_param, max_depth=max_depth_param, min_samples_split=min_samples_split_param, min_samples_leaf=min_samples_leaf_param, min_weight_fraction_leaf=min_weight_fraction_leaf_param, max_features=max_features_param, max_leaf_nodes=max_leaf_nodes_param, min_impurity_decrease=min_impurity_decrease_param, bootstrap=bootstrap_param, oob_score=oob_score_param, n_jobs=n_jobs_param, random_state=random_state_param, verbose=verbose_param, warm_start=warm_start_param, ccp_alpha=ccp_alpha_param, max_samples=max_samples_param))]\\n    pipe=Pipeline(Input)\\n\\n    pipe.fit(dataset['X_train'],dataset['Y_train'])\\n    # pipe_pred_RFF = pipe.predict(X_test)\\n    print(pipe.score(dataset['X_train'],dataset['Y_train']))\\n\\n    return pipe\\n\\n\\n""},""category"":""Regression"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""criterion"":""mse"",""ccp_alpha"":""0.0"",""oob_score"":""False"",""n_jobs"":""None"",""max_depth"":""None"",""min_samples_split"":""2"",""bootstrap"":""True"",""n_estimators"":""100"",""random_state"":""None"",""min_impurity_decrease"":""0.0"",""min_weight_fraction_leaf"":""0.0"",""warm_start"":""False"",""min_samples_leaf"":""1"",""verbose"":""0"",""max_samples"":""None"",""max_features"":""1.0"",""max_leaf_nodes"":""None""}}","DragNDropLite-17"
"DragNDropLite","Core","{""formats"":{""tol"":""integer"",""epsilon"":""integer"",""shrinking"":""integer"",""C"":""integer"",""coef0"":""integer"",""cache_size"":""integer"",""kernel"":""text"",""degree"":""integer"",""max_iter"":""integer"",""gamma"":""text"",""verbose"":""integer""},""classname"":""SVR"",""name"":""SVR"",""alias"":""SVR"",""parentCategory"":""148"",""id"":18,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from sklearn.svm import SVR"",""from sklearn.preprocessing import StandardScaler,PolynomialFeatures"",""from sklearn.pipeline import Pipeline""],""script"":""\\ndef SVR_<id>(dataset, tol_param=0.001, epsilon_param=0.1, shrinking_param=True, c_param=1.0, coef0_param=0.0, cache_size_param=200, kernel_param='rbf', degree_param=3, max_iter_param=-1, gamma_param='scale', verbose_param=False):\\n    #Building and Training the Model and Displaying the Score of the Model\\n    InputSVR=[('scale',StandardScaler()),('polynomial', PolynomialFeatures(include_bias=False)),('model',SVR(kernel=kernel_param, degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1))]\\n    pipeSVR=Pipeline(InputSVR)\\n\\n    pipeSVR.fit(dataset['X_train'],dataset['Y_train'])\\n    # pipe_pred_LassoR = pipeLassoR.predict(X_test)\\n    print(pipeSVR.score(dataset['X_train'],dataset['Y_train']))\\n    print(((pipeSVR.predict(dataset['X_train']) - dataset['Y_train']) ** 2).mean())\\n    \\n    return pipeSVR\\n\\n\\n""},""category"":""Regression"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""tol"":""0.001"",""epsilon"":""0.1"",""shrinking"":""True"",""C"":""1.0"",""coef0"":""0.0"",""cache_size"":""200"",""kernel"":""rbf"",""degree"":""3"",""max_iter"":""-1"",""gamma"":""scale"",""verbose"":""False""}}","DragNDropLite-18"
"DragNDropLite","Core","{""formats"":{},""classname"":""InferenceR"",""name"":""InferenceR"",""alias"":""InferenceR"",""parentCategory"":""148"",""id"":19,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from leaputils import FileServer"",""import pickle"",""import os""],""script"":""\\ndef InferenceR_<id>(dataset, download_path):\\n    if isinstance(dataset,str):\\n        dataset, download_path = download_path, dataset\\n    print('download_path: ', download_path)\\n    # load the model\\n    unpickler = pickle.Unpickler(open(download_path, 'rb'))\\n    load_model = unpickler.load()\\n    print(load_model)\\n    pipe_pred = load_model.predict(dataset['X_test'])\\n    print(pipe_pred)\\n    \\n    # OUT\\n    if isinstance(pipe_pred,tuple):\\n        pipe_pred, prediction_results = pipe_pred\\n        output=pd.DataFrame({'Id':dataset['dataset_id']})\\n        output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\n        print('out', output.to_dict('records'))\\n    else:\\n        output=pd.DataFrame({'Id':dataset['dataset_id'],'Result':pipe_pred})\\n    os.remove(download_path)\\n    return output.to_dict('records')\\n\\n\\n""},""category"":""Regression"",""inputEndpoints"":[""in"",""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-19"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""PYTHON_JOB_TEMP"":""text""},""classname"":""FetchFile"",""name"":""FetchFile"",""alias"":""FetchFile"",""parentCategory"":""149"",""id"":20,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os""],""script"":""def FetchFile_<id>(dataset, bucket_param='Demo', python_job_temp_param='docqna/files'):\\n    fileId =  dataset[0].get('file_id')\\n    fileName = dataset[0].get('file_name')\\n    bucket = bucket_param\\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    print('FILE_SERVER_URL: ', dataset)\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    print('FILE_SERVER_TOKEN: ', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ['JOB_DIRECTORY'], python_job_temp_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + '/api/lastcount/' + fileId + '?bucket=' + bucket,\\n                                           headers={'access-token': FILE_SERVER_TOKEN},\\n                                           proxies={'http': '', 'https': ''})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search('(.*?)', totalChunksResponse.text).group(1)\\n            totalChunks = int('0')\\n        logger.info('Total Chunks: ' + str(totalChunks + 1))\\n\\n        # create a folder 'chunkfiles'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, 'chunkfiles')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info('dir already exists...')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model's chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            url = (FILE_SERVER_URL + '/api/download/' + fileId + '/') + 'file' + '?bucket=' + bucket\\n            filedata = requests.get(url, headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n            open(chunkfilePath + '/' + str(i), 'wb').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, 'wb') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, 'rb')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n""},""category"":""DocumentComprehension"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out""],""attributes"":{""bucket"":""Demo"",""PYTHON_JOB_TEMP"":""docqna/files""}}","DragNDropLite-20"
"DragNDropLite","Core","{""formats"":{""question"":""text"",""prompt"":""text""},""classname"":""ProcessQuery"",""name"":""ProcessQuery"",""alias"":""ProcessQuery"",""parentCategory"":""149"",""id"":21,""codeGeneration"":{""requirements"":[],""imports"":[""import re"",""import numpy as np"",""import torch"",""from transformers import DonutProcessor, VisionEncoderDecoderModel"",""from PIL import Image""],""script"":""def ProcessQuery_<id>(dataset, question_param='what is the identification number present?', prompt_param='<s_docvqa><s_question>{user_input}</s_question><s_answer>'):\\n    def read_img_to_3d_array(img_path):\\n        img = Image.open(img_path)\\n        img_array = np.asarray(img)\\n        if len(img_array.shape) == 3:\\n            #img_array = img_array[:, :, :3]\\n            pass\\n        else:\\n            assert len(img_array.shape) == 2\\n            h, w = img_array.shape\\n            img_array = img_array.reshape([h, w, 1])\\n            img_array = np.concatenate([img_array] * 3, axis = -1)\\n        assert len(img_array.shape) == 3\\n        img_array = img_array[:, :, :3]\\n        return img_array\\n\\n    def process_document(image, question,task_prompt):\\n        processor = DonutProcessor.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\\n        model = VisionEncoderDecoderModel.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n        model.to(device)\\n        # prepare encoder inputs\\n        pixel_values = processor(image, return_tensors='pt').pixel_values\\n        # prepare decoder inputs\\n        \\n        prompt = task_prompt.replace('{user_input}', question)\\n        decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors='pt').input_ids\\n        # generate answer\\n        outputs = model.generate(\\n            pixel_values.to(device),\\n            decoder_input_ids=decoder_input_ids.to(device),\\n            max_length=model.decoder.config.max_position_embeddings,\\n            early_stopping=True,\\n            pad_token_id=processor.tokenizer.pad_token_id,\\n            eos_token_id=processor.tokenizer.eos_token_id,\\n            use_cache=True,\\n            num_beams=1,\\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\\n            return_dict_in_generate=True,\\n        )\\n        # postprocess\\n        sequence = processor.batch_decode(outputs.sequences)[0]\\n        sequence = sequence.replace(processor.tokenizer.eos_token, '').replace(processor.tokenizer.pad_token, '')\\n        sequence = re.sub(r'<.*?>', '', sequence, count=1).strip()  # remove first task start token\\n        return processor.token2json(sequence)\\n    task_prompt = prompt_param\\n    input_img = dataset\\n    question = question_param\\n    image  = read_img_to_3d_array(input_img)\\n    print(dataset,type(dataset))\\n    res = process_document(image, question,task_prompt)\\n    print(res)\\n    return [res]\\n\\n""},""category"":""DocumentComprehension"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out""],""attributes"":{""question"":""what is the identification number present?"",""prompt"":""<s_docvqa><s_question>{user_input}</s_question><s_answer>""}}","DragNDropLite-21"
"DragNDropLite","Core","{""formats"":{""K"":""text""},""classname"":""KNearestNeighbor"",""name"":""KNearestNeighbor"",""alias"":""KNearestNeighbor"",""parentCategory"":""150"",""id"":22,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from sklearn.neighbors import KNeighborsClassifier"",""from sklearn.preprocessing import LabelEncoder""],""script"":""\\ndef KNearestNeighbor_<id>(dataset, k_param=5):\\n    knn = KNeighborsClassifier(n_neighbors=k_param)\\n    knn.fit(dataset[0],dataset[1])\\n    return knn\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""K"":""5""}}","DragNDropLite-22"
"DragNDropLite","Core","{""formats"":{""hyper_parameter"":""text"",""model_name"":""text"",""label_heading"":""text"",""kernel"":""text"",""penalty"":""text"",""gamma"":""text"",""test_ratio"":""text"",""text_heading"":""text""},""classname"":""SVM"",""name"":""SVM"",""alias"":""SVM Train"",""parentCategory"":""150"",""id"":25,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle"",""import json"",""import secrets"",""import logging"",""import numpy as np"",""import pandas as pd"",""from collections import OrderedDict"",""from sklearn.pipeline import Pipeline"",""from sklearn.preprocessing import LabelEncoder"",""from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)"",""from sklearn.naive_bayes import MultinomialNB"",""from sklearn.svm import LinearSVC, SVC"",""from sklearn.model_selection import GridSearchCV, train_test_split"",""from sklearn.utils import shuffle"",""from sklearn.metrics import accuracy_score""],""script"":""\\ndef SVM_<id>(dataset, hyper_parameter_param='default', model_name_param='svmtest', label_heading_param='label', kernel_param='rbf', penalty_param=1, gamma_param='scale', test_ratio_param=0.2, text_heading_param='text'):\\n    df = pd.DataFrame(dataset)\\n    test_ratio = test_ratio_param\\n    text_heading = text_heading_param\\n    label_heading = label_heading_param\\n    model_name = model_name_param\\n    hyper_parameter = hyper_parameter_param\\n    kernel = kernel_param\\n    penalty = penalty_param\\n    gamma = gamma_param\\n    if model_name is None:\\n        raise Exception('Model name is empty')\\n    save_path = os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n    if not save_path :\\n        raise Exception('Save path is a required parameter')\\n    save_path = os.path.join(save_path,model_name)\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    df = df.replace('', np.nan)\\n    df = df.dropna()\\n    train_df, test_df = train_test_split(df, test_size=test_ratio, random_state=42)\\n    Train_X = train_df[text_heading]\\n    Train_Y = train_df[label_heading]\\n    label_list = list(df[label_heading].unique())\\n    encoder = LabelEncoder()\\n    encoder.fit(label_list)\\n    np.save(os.path.join(save_path, 'classes.npy'), encoder.classes_)\\n\\n    \\n    if hyper_parameter.lower() == 'default':\\n        clf_model = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('SVM', SVC(probability=True))])\\n        pkl_filename = os.path.join(save_path, 'default_'+model_name+'_.pkl')\\n        clf_model = clf_model.fit(Train_X, Train_Y)\\n        with open(pkl_filename, 'wb') as file:\\n            pickle.dump(clf_model, file)\\n    elif hyper_parameter.lower() == 'gridsearch':\\n        hyper_param_dict = OrderedDict()\\n        hyper_param_dict['kernel'] = ['linear']\\n\\n        hyper_param_dict['gamma'] = [0.1]\\n\\n        hyper_param_dict['penalty'] = [1]\\n\\n        hyper_param_dict['K_folds'] = 2\\n\\n        kernel = hyper_param_dict['kernel']\\n        gamma = hyper_param_dict['gamma']\\n        penalty = hyper_param_dict['penalty']\\n        K_folds = hyper_param_dict['K_folds']\\n\\n        SVM_kernel_ = [ele for ele in kernel if ele.lower()!='linear']\\n        param_grid = []\\n        if len(SVM_kernel_) == len(kernel):\\n            param_grid = [{ 'SVM__kernel': kernel, \\n                            'SVM__gamma': gamma,\\n                            'SVM__C': penalty}]\\n\\n        else:\\n            param_grid = [{ 'SVM__kernel': kernel, \\n                            'SVM__gamma': gamma,\\n                            'SVM__C': penalty},\\n                            {   'SVM__kernel': ['linear'], \\n                                'SVM__C': penalty\\n                            }]\\n\\n        svc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('SVM', SVC(probability=True))])\\n        grid = GridSearchCV(svc_clf, param_grid, n_jobs=-1, cv=K_folds, scoring='accuracy') \\n        grid.fit(Train_X, Train_Y)\\n        pkl_filename = os.path.join(save_path, model_name+'grid_model.pkl' )\\n        with open(pkl_filename, 'wb') as file:\\n            pickle.dump(grid, file)\\n    elif hyper_parameter.lower() == 'manual':\\n        if isinstance(kernel, list):\\n            kernel = kernel[0]\\n        if isinstance(penalty, list):\\n            penalty = penalty[0]\\n        if not isinstance(gamma, str):\\n            raise Exception('Gamma should be str i.e. scale')\\n        clf_model = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('SVM', SVC(C= penalty , kernel=kernel, gamma=gamma , probability=True))])\\n        pkl_filename = os.path.join(save_path,'manual'+model_name+'.pkl')\\n        model_details.update({'kernel':kernel, 'gamma':gamma, 'penalty':penalty})\\n        clf_model = clf_model.fit(Train_X, Train_Y)\\n        with open(pkl_filename, 'wb') as file:\\n            pickle.dump(clf_model, file)\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""hyper_parameter"":""default"",""model_name"":""svmtest"",""label_heading"":""label"",""kernel"":""rbf"",""penalty"":1,""gamma"":""scale"",""test_ratio"":0.2,""text_heading"":""text""}}","DragNDropLite-25"
"DragNDropLite","Core","{""formats"":{},""classname"":""LogisticRegression"",""name"":""LogisticRegression"",""alias"":""LogisticRegression"",""parentCategory"":""150"",""id"":26,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from sklearn.linear_model import LogisticRegression"",""from sklearn.preprocessing import LabelEncoder""],""script"":""\\ndef LogisticRegression_<id>(dataset):\\n    model = LogisticRegression()\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-26"
"DragNDropLite","Core","{""formats"":{},""classname"":""DecisionTree"",""name"":""DecisionTree"",""alias"":""DecisionTree"",""parentCategory"":""150"",""id"":27,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import pandas as pd"",""import numpy as np"",""from sklearn.tree import DecisionTreeClassifier"",""from sklearn.preprocessing import LabelEncoder""],""script"":""\\ndef DecisionTree_<id>(dataset):\\n    model = DecisionTreeClassifier(random_state = 1)\\n    model.fit(dataset[0],dataset[1])\\n    return model\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-27"
"DragNDropLite","Core","{""formats"":{""model_name"":""text"",""test_size"":""text"",""alpha"":""text"",""vect_type"":""text"",""clf_type"":""text"",""class_heading"":""text"",""text_heading"":""text""},""classname"":""naive_bayes"",""name"":""naive_bayes"",""alias"":""Naive Bayes"",""parentCategory"":""150"",""id"":28,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os, json"",""import pandas as pd"",""import codecs"",""import nltk"",""from pandas import DataFrame"",""from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"",""from sklearn.naive_bayes import MultinomialNB"",""from sklearn import metrics"",""from sklearn.model_selection import train_test_split"",""import joblib"",""from sklearn.linear_model import LogisticRegression"",""import numpy as np"",""from threading import Thread"",""from sklearn.pipeline import make_pipeline""],""script"":""\\ndef naive_bayes_<id>(dataset, model_name_param='test', test_size_param=0.2, vect_type_param='TFIDF', alpha_param=0.01, clf_type_param='model', class_heading_param='class', text_heading_param='text'):\\n\\tdataframe = pd.DataFrame(dataset)\\n\\tmodel_name = model_name_param\\n\\tif model_name is None:\\n\\t\\traise Exception('Model name is empty')\\n\\tmodel_path  = '/app/jobs/models/classicmlpoc'\\n\\tmodel_path = os.path.join(model_path,model_name_param)\\n\\n\\tif not os.path.isdir(model_path):\\n\\t\\tos.makedirs(model_path)\\n\\tvect_type = vect_type_param\\n\\tif(vect_type=='TFIDF'):            \\n\\t\\tvectorizer = TfidfVectorizer()\\n\\telse: \\n\\t\\tvectorizer = CountVectorizer()             \\n\\tvectors = vectorizer.fit_transform(dataframe[text_heading_param])\\n\\tX_train, X_test, y_train, y_test = train_test_split(vectors, dataframe[class_heading_param], test_size=test_size_param, random_state=42)\\n\\tclf_type = clf_type_param\\n\\tif clf_type=='naive bayes':\\n\\t\\tclf = MultinomialNB(alpha=alpha_param)\\n\\telse: \\n\\t\\tclf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')  \\n\\tclf.fit(X_train, y_train)    \\n\\tpred = clf.predict(X_test)\\n\\tf1_sc=metrics.accuracy_score(y_test, pred)\\n\\tjoblib.dump(clf,model_path+'/'+'classifier.pkl')\\n\\tjoblib.dump(vectorizer,model_path+'/'+'vectorizer.pkl')\\n\\t\\n\\treturn clf\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_name"":""test"",""test_size"":""0.2"",""alpha"":""0.01"",""vect_type"":""TFIDF"",""clf_type"":""model"",""class_heading"":""class"",""text_heading"":""text""}}","DragNDropLite-28"
"DragNDropLite","Core","{""formats"":{""model_name"":""text"",""text"":""text""},""classname"":""naive_bayes_predict"",""name"":""naive_bayes_predict"",""alias"":""Naive Bayes Predict"",""parentCategory"":""150"",""id"":29,""codeGeneration"":{""requirements"":[""nltk""],""imports"":[""import os, json, joblib"",""import pandas as pd"",""import codecs"",""import numpy as np"",""import nltk"",""from nltk.corpus import stopwords"",""from nltk.stem.wordnet import WordNetLemmatizer""],""script"":""\\ndef naive_bayes_predict_<id>(dataset, model_name_param='test', text_param='text') :\\n    model_name = model_name_param\\n    text = text_param\\n    model_path  = '/app/jobs/models/classicmlpoc'\\n    model_path = os.path.join(model_path,model_name)\\n    if model_path is None:\\n        raise Exception('Parameter Model path is empty')\\n    \\n    if text is None:\\n        raise Exception('Text Input is required')\\n\\n    vect=joblib.load(model_path+'/'+'vectorizer.pkl')\\n    classify=joblib.load(model_path+'/'+'classifier.pkl')\\n    def pre_process_text(text):\\n            #If using stemming...\\n            #stemmer = PorterStemmer()\\n            textArray=text.split()\\n            wnl = WordNetLemmatizer()\\n            processed_text = []\\n            for text in textArray:\\n                words_list = (str(text).lower()).split()\\n                final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words('english')]\\n                #If using stemming...\\n                #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words('english')]\\n                final_words_str = str((' '.join(final_words)))\\n                processed_text.append(final_words_str)   \\n            return ' '.join(processed_text)\\n    if isinstance(text,str):\\n        out_prob=classify.predict_proba(vect.transform([text]))\\n        out_prob=np.round(out_prob[0],decimals=4)\\n        clas_li=classify.classes_\\n        out_dict={}\\n        for i in range(0,len(clas_li)):\\n            out_dict[clas_li[i]]=out_prob[i]\\n        pred_class=classify.predict(vect.transform([text]))\\n        # return {'bestClass':pred_class[0],'allClassProab':out_dict}\\n        print('text',text,'label',pred_class[0])\\n        return {'text':text,'label':pred_class[0]}\\n    elif isinstance(text,list):\\n        result_list=[]\\n        for sentence in text:\\n            out_prob=classify.predict_proba(vect.transform([sentence]))\\n            out_prob=np.round(out_prob[0],decimals=4)\\n            clas_li=classify.classes_\\n            out_dict={}\\n            for i in range(0,len(clas_li)):\\n                out_dict[clas_li[i]]=out_prob[i]\\n            pred_class=classify.predict(vect.transform([sentence]))\\n            result_list.append({'text ':sentence,'label':pred_class[0]}) \\n        print(result_list)\\n        return result_list\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_name"":""test"",""text"":""text""}}","DragNDropLite-29"
"DragNDropLite","Core","{""formats"":{""model_name"":""text"",""class_heading"":""text"",""test_ratio"":""text"",""text_heading"":""text""},""classname"":""MNB"",""name"":""MNB"",""alias"":""MNB"",""parentCategory"":""150"",""id"":30,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle"",""import json"",""import secrets"",""import logging"",""import numpy as np"",""import pandas as pd"",""from sklearn.pipeline import Pipeline"",""from sklearn.preprocessing import LabelEncoder"",""from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)"",""from sklearn.naive_bayes import MultinomialNB"",""from sklearn.model_selection import train_test_split""],""script"":""\\ndef MNB_<id>(dataset, model_name_param='test', class_heading_param='class', text_heading_param='text', test_ratio_param=0.1):\\n    df = pd.DataFrame(dataset)\\n    model_name=model_name_param\\n    test_ratio = test_ratio_param\\n    if model_name is None:\\n        raise Exception('Model name is empty')\\n    save_path  = os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n    save_path = os.path.join(save_path,model_name)\\n    os.makedirs(save_path, exist_ok=True)\\n\\n    df = df.replace('', np.nan)\\n    df = df.dropna()\\n    train_df, test_df = train_test_split(df, test_size=test_ratio, random_state=42)\\n    Train_X = train_df[text_heading_param]\\n    Train_Y = train_df[class_heading_param]\\n    label_list = list(df[class_heading_param].unique())\\n    encoder = LabelEncoder()\\n    encoder.fit(label_list)\\n    np.save(os.path.join(save_path, 'classes.npy'), encoder.classes_)\\n\\n    clf_model = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\\n    clf_model = clf_model.fit(Train_X, Train_Y)\\n    pkl_filename = os.path.join(save_path, model_name+'.pkl')\\n    with open(pkl_filename, 'wb') as file:\\n        pickle.dump(clf_model, file)\\n    return save_path\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_name"":""test"",""class_heading"":""class"",""test_ratio"":0.1,""text_heading"":""text""}}","DragNDropLite-30"
"DragNDropLite","Core","{""formats"":{""model_name"":""text"",""text"":""text""},""classname"":""MNB_predict"",""name"":""MNB_predict"",""alias"":""MNB Predict"",""parentCategory"":""150"",""id"":31,""codeGeneration"":{""requirements"":[],""imports"":[""import os"",""import pickle""],""script"":""\\ndef MNB_predict_<id>(dataset, model_name_param='test', text_param='text'):\\n    model_name = model_name_param\\n    pklfilename = model_name+'.pkl'\\n    texts = text_param\\n    texts = [texts]\\n    model_path = os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n    model_path = os.path.join(model_path,model_name,pklfilename)\\n    with open(model_path, 'rb') as file:\\n        pickle_model = pickle.load(file)\\n    result = pickle_model.predict(texts).tolist()\\n    return result\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_name"":""test"",""text"":""text""}}","DragNDropLite-31"
"DragNDropLite","Core","{""formats"":{""dropValues1"":[""naive_bayes"",""logistic_regression"",""mnb"",""svm""],""dropValues"":[""classification"",""regression""],""task"":""dropValues"",""model_name"":""text"",""model_type"":""dropValues1"",""text"":""text""},""classname"":""Model_Inference"",""name"":""Model_Inference"",""alias"":""Model_Inference"",""parentCategory"":""150"",""id"":32,""codeGeneration"":{""requirements"":[""scikit-learn"",""nltk""],""imports"":[""import pandas as pd"",""import numpy as np"",""from leaputils import FileServer"",""import pickle, joblib"",""import os,json,codecs,nltk"",""from nltk.corpus import stopwords"",""from nltk.stem.wordnet import WordNetLemmatizer""],""script"":""\\ndef Model_Inference_<id>(dataset, download_path, task_param=[], model_name_param='text', model_type_param=[], text_param='text'):\\n    task = task_param\\n    model_type = model_type_param\\n    model_name = model_name_param\\n    text = text_param\\n    if task == 'regression':\\n        if isinstance(dataset,str):\\n            dataset, download_path = download_path, dataset\\n        print('download_path: ', download_path)\\n    # load the model\\\\n    \\n        unpickler = pickle.Unpickler(open(download_path, 'rb'))\\n        load_model = unpickler.load()\\n        print(load_model)\\n        pipe_pred = load_model.predict(dataset['X_test'])\\n        print(pipe_pred)\\n        if isinstance(pipe_pred,tuple):\\n            pipe_pred, prediction_results = pipe_pred\\n            output=pd.DataFrame({'Id':dataset['dataset_id']})\\n            output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\n            print('out', output.to_dict('records'))\\n        else:\\n            output=pd.DataFrame({'Id':dataset['dataset_id'],'Result':pipe_pred})\\n            os.remove(download_path)\\n        return output.to_dict('records')\\n    elif task == 'classification':\\n        if model_type in ['naive_bayes','logistic_regression']:\\n            model_path = os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n            model_path = os.path.join(model_path,model_name)\\n            vect=joblib.load(model_path+'/'+'vectorizer.pkl')\\n            classify=joblib.load(model_path+'/'+'classifier.pkl')\\n\\n            def pre_process_text(text):\\n                    #If using stemming...\\n                    #stemmer = PorterStemmer()\\n                    textArray=text.split()\\n                    wnl = WordNetLemmatizer()\\n                    processed_text = []\\n                    for text in textArray:\\n                        words_list = (str(text).lower()).split()\\n                        final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words('english')]\\n                        #If using stemming...\\n                        #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words('english')]\\n                        final_words_str = str((' '.join(final_words)))\\n                        processed_text.append(final_words_str)   \\n                    return ' '.join(processed_text)\\n            if isinstance(text,str):\\n                out_prob=classify.predict_proba(vect.transform([text]))\\n                out_prob=np.round(out_prob[0],decimals=4)\\n                clas_li=classify.classes_\\n                out_dict={}\\n                for i in range(0,len(clas_li)):\\n                    out_dict[clas_li[i]]=out_prob[i]\\n                pred_class=classify.predict(vect.transform([text]))\\n                # return {'bestClass':pred_class[0],'allClassProab':out_dict}\\n                print('text',text,'label',pred_class[0])\\n                #return {'text':text,'label':pred_class[0]}\\n            elif isinstance(text,list):\\n                result_list=[]\\n                for sentence in text:\\n                    out_prob=classify.predict_proba(vect.transform([sentence]))\\n                    out_prob=np.round(out_prob[0],decimals=4)\\n                    clas_li=classify.classes_\\n                    out_dict={}\\n                    for i in range(0,len(clas_li)):\\n                        out_dict[clas_li[i]]=out_prob[i]\\n                    pred_class=classify.predict(vect.transform([sentence]))\\n                    result_list.append({'text ':sentence,'label':pred_class[0]}) \\n                print(result_list)\\n                #return result_list\\n        elif algo.lower() in ['mnb','svm']:\\n            model_path = os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n            model_path = os.path.join(model_path,model_name,model_name+'.pkl')\\n            texts = text_param\\n            texts = [texts]\\n            with open(model_path, 'rb') as file:\\n                pickle_model = pickle.load(file)\\n            result = pickle_model.predict(texts).tolist()\\n            print(result)\\n\\n\\n""},""category"":""Classification"",""inputEndpoints"":[""in"",""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues1"":"""",""dropValues"":"""",""task"":[],""model_name"":""text"",""model_type"":[],""text"":""text""}}","DragNDropLite-32"
"DragNDropLite","Core","{""formats"":{""childpath"":""text"",""retriever"":""text"",""retriever_type"":""text"",""name"":""text"",""files"":""text"",""store_type"":""text""},""classname"":""Ingestion"",""name"":""Ingestion"",""alias"":""Ingestion"",""parentCategory"":""151"",""id"":33,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import os"",""import json""],""script"":""\\ndef Ingestion_<id>(path, childpath_param=/, retriever_param='dense_passage_retriever', retriever_type_param='text', name_param='text', files_param='text', store_type_param='text'):\\n    url = 'http://10.177.28.36:6199/haystack/document/store/create/store_type_param/retriever_type_param/name_param'\\n    data = {'retriever':retriever_param}\\n    file_names = files_param.split(',')\\n    files = []\\n    for file in file_names:\\n        name = path+ childpath_param + file\\n        with open(name, 'rb') as file:\\n            files.append(('files', file.read()))\\n    resp = requests.post(url=url, files=files, params=data)\\n\\n\\n""},""category"":""Haystack"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""childpath"":""/"",""retriever"":""dense_passage_retriever"",""retriever_type"":""text"",""name"":""text"",""files"":""text"",""store_type"":""text""}}","DragNDropLite-33"
"DragNDropLite","Core","{""formats"":{""retriever"":""text"",""retriever_type"":""text"",""store_name"":""text"",""store_type"":""text""},""classname"":""GenerativeQA"",""name"":""GenerativeQA"",""alias"":""GenerativeQA"",""parentCategory"":""151"",""id"":34,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import os"",""import json""],""script"":""\\ndef GenerativeQA_<id>(dataset, retriever_param='dense_passage_retriever', retriever_type_param='text', store_name_param='text', store_type_param='text'):\\n    url = 'http://10.177.28.36:6199/haystack/pipeline/generative_qa/store_type_param/retriever_type_param/store_name_param'\\n    out_list = []\\n    for row in dataset:\\n        retri = row['retriever']\\n        ques= row['questions']\\n        data = json.dumps({\\n            'retriever': retri,\\n            'questions': ques\\n        })\\n        print(data)\\n        resp = requests.get(url=url, params=data)\\n        res=resp.json()\\n        print(res)\\n        out = {}\\n        for i, r in enumerate(res['results']):\\n            out['result_'+str(i+1)] = r\\n            out['status_code'] = res['code']\\n\\n        out_list.append(out)\\n    return out_list\\n\\n\\n""},""category"":""Haystack"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""retriever"":""dense_passage_retriever"",""retriever_type"":""text"",""store_name"":""text"",""store_type"":""text""}}","DragNDropLite-34"
"DragNDropLite","Core","{""formats"":{""defaults"":""text"",""OutputFeatures"":""list"",""preprocessing"":""text"",""preprocessing_split_column"":""text"",""model_type"":""text"",""preprocessing_split_probabilities"":""list"",""preprocessing_undersample_majority"":""text"",""backend"":""text"",""preprocessing_sample_ratio"":""text"",""preprocessing_split_type"":""text"",""preprocessing_oversample_minority"":""text""},""classname"":""LudwigConfig"",""name"":""LudwigConfig"",""alias"":""LudwigConfig"",""parentCategory"":""152"",""id"":35,""codeGeneration"":{""requirements"":[""ludwig""],""imports"":[""import pandas as pd""],""script"":""\\ndef LudwigConfig_<id>(dataset, outputfeatures_param='', defaults_param=False, preprocessing_param=False, preprocessing_split_column_param=None, model_type_param='ecd', preprocessing_split_probabilities_param=None, preprocessing_undersample_majority_param=None, backend_param=False, preprocessing_sample_ratio_param=1.0, preprocessing_split_type_param='random', preprocessing_oversample_minority_param=None):\\n    train_df = dataset['train_df']\\n    output = outputfeatures_param\\n    out_features = []\\n    for out in output:\\n        out_features.append(out['name'])\\n    cols = list(set(train_df.columns) - set(out_features)) # add\\n    features = train_df[cols]\\n\\n    #extract categorical features\\n    categorical_features = []\\n    for p in features:\\n        if train_df[p].dtype == 'object':\\n            categorical_features.append(p)\\n    print('categorical features:', categorical_features, '\\\\n')\\n\\n    # get numerical features\\n    numerical_features = list(set(features) - set(categorical_features))\\n    print('numerical features:', numerical_features, '\\\\n')\\n\\n    # template for config\\n    config = {'model_type': model_type_param, 'input_features':[], 'output_features': [], 'trainer':{}}\\n\\n    # setup input features for categorical features\\n    for p in categorical_features:\\n        a_feature = {\\n            'name': p.replace(' ','_'), \\n            'type': 'category'\\n        }\\n        config['input_features'].append(a_feature)\\n\\n    # setup input features for numerical features\\n    for p in numerical_features:\\n        a_feature = {\\n            'name': p.replace(' ', '_'), \\n            'type': 'number'\\n        }\\n        config['input_features'].append(a_feature)\\n\\n    # set up output variable\\n    output = outputfeatures_param\\n    for out in output:\\n        config['output_features'].append({'name': out['name'], 'type':out['value']})\\n        \\n    if config['model_type'] == 'ecd':\\n        # config['combiner'] = None # add\\n        pass\\n        \\n    if preprocessing_param:\\n        config['preprocessing'] = {\\n            'sample_ratio': preprocessing_sample_ratio_param,\\n            'oversample_minority': preprocessing_oversample_minority_param,\\n            'undersample_majority': preprocessing_undersample_majority_param,\\n            'split':{\\n                'type': preprocessing_split_type_param\\n            }\\n        }\\n    \\n    if preprocessing_split_column_param != 'None': # add\\n        config['preprocessing']['split']['column'] = preprocessing_split_column_param\\n\\n    if preprocessing_param and preprocessing_split_type_param != 'fixed': # add\\n        prob = preprocessing_split_probabilities_param\\n        config['preprocessing']['split']['probabilities'] = [pr['name'] for pr in prob]\\n    \\n\\n    # set default preprocessing and encoder for numerical features\\n    if defaults_param:\\n        config['defaults'] = { # add\\n            'number': {\\n                'preprocessing': {\\n                    'missing_value_strategy': 'fill_with_mean', \\n                    'normalization': 'zscore'\\n                },\\n                'encoder': {\\n                    'type': 'dense',\\n                    'num_layers': 2\\n                },\\n            },\\n            'category': {\\n                'encoder': {\\n                    'type': 'sparse'\\n                },\\n                'decoder': {\\n                    'top_k': 2\\n                },\\n                'loss': {\\n                    'confidence_penalty': 0.1  \\n                }\\n            }\\n        }\\n\\n    # set up trainer\\n    if config['model_type'] == 'gbm':\\n      config['trainer'] = {\\n             'bagging_fraction': 0.8,\\n             'bagging_freq': 1,\\n             'bagging_seed': 3,\\n             'boosting_rounds_per_checkpoint': 50,\\n             'boosting_type': 'gbdt',\\n             'cat_l2': 10.0,\\n             'cat_smooth': 10.0,\\n             'cegb_penalty_split': 0.0,\\n             'cegb_tradeoff': 1.0,\\n             'drop_rate': 0.1,\\n             'drop_seed': 4,\\n             'early_stop': 5,\\n             'eval_batch_size': 1048576,\\n             'evaluate_training_set': False,\\n             'extra_seed': 6,\\n             'extra_trees': False,\\n             'feature_fraction': 0.75,\\n             'feature_fraction_bynode': 1.0,\\n             'feature_fraction_seed': 2,\\n             'feature_pre_filter': True,\\n             'lambda_l1': 0.25,\\n             'lambda_l2': 0.2,\\n             'learning_rate': 0.03,\\n             'linear_lambda': 0.0,\\n             'max_bin': 255,\\n             'max_cat_threshold': 32,\\n             'max_cat_to_onehot': 4,\\n             'max_delta_step': 0.0,\\n             'max_depth': 18,\\n             'max_drop': 50,\\n             'min_data_in_leaf': 20,\\n             'min_data_per_group': 100,\\n             'min_gain_to_split': 0.03,\\n             'min_sum_hessian_in_leaf': 0.001,\\n             'neg_bagging_fraction': 1.0,\\n             'num_boost_round': 1000,\\n             'num_leaves': 82,\\n             'other_rate': 0.1,\\n             'path_smooth': 0.0,\\n             'pos_bagging_fraction': 1.0,\\n             'skip_drop': 0.5,\\n             'top_rate': 0.2,\\n             'tree_learner': 'serial',\\n             'uniform_drop': False,\\n             'validation_field': None,\\n             'validation_metric': None,\\n             'verbose': -1,\\n             'xgboost_dart_mode': False}\\n    else:\\n      config['trainer'] = {'batch_size': 'auto', # add\\n             'bucketing_field': None,\\n             'checkpoints_per_epoch': 0,\\n             'early_stop': 5,\\n             'epochs': 100,\\n             'eval_batch_size': None,\\n             'evaluate_training_set': False,\\n             'gradient_clipping': {'clipglobalnorm': 0.5,\\n                                   'clipnorm': None,\\n                                   'clipvalue': None},\\n             'increase_batch_size_eval_metric': 'loss',\\n             'increase_batch_size_eval_split': 'training',\\n             'increase_batch_size_on_plateau': 0,\\n             'increase_batch_size_on_plateau_patience': 5,\\n             'increase_batch_size_on_plateau_rate': 2.0,\\n             'learning_rate': 0.001,\\n             'learning_rate_scaling': 'linear',\\n             'learning_rate_scheduler': {'decay': None,\\n                                         'decay_rate': 0.96,\\n                                         'decay_steps': 10000,\\n                                         'reduce_eval_metric': 'loss',\\n                                         'reduce_eval_split': 'training',\\n                                         'reduce_on_plateau': 0,\\n                                         'reduce_on_plateau_patience': 10,\\n                                         'reduce_on_plateau_rate': 0.1,\\n                                         'staircase': False,\\n                                         'warmup_evaluations': 0,\\n                                         'warmup_fraction': 0.0},\\n             'max_batch_size': 1099511627776,\\n             'optimizer': {'amsgrad': False,\\n                           'betas': [0.9, 0.999],\\n                           'eps': 1e-08,\\n                           'type': 'adam',\\n                           'weight_decay': 0.0},\\n             'regularization_lambda': 0.0,\\n             'regularization_type': 'l2',\\n             'should_shuffle': True,\\n             'steps_per_checkpoint': 0,\\n             'train_steps': None,\\n             'use_mixed_precision': False,\\n             'validation_field': None,\\n             'validation_metric': None}\\n\\n\\n    if backend_param:\\n        config['backend'] = {\\n            'type': '<backend_type>',\\n            'cache_dir': '<backend_cache_dir>',\\n            'cache_credentials': '<backend_cache_credentials>',\\n            'processor': {\\n                'type': '<backend_processor_type>'\\n            },\\n            'trainer': {\\n                'strategy': '<backend_trainer_strategy>'\\n            },\\n            'loader': {\\n                'fully_executed': '<backend_loader_fully_executed>',\\n                'window_size_bytes': '<backend_loader_window_size_bytes>'\\n            }\\n        }\\n\\n    print('config:', config)\\n    return {'config': config, 'train_df': dataset['train_df'], 'test_df': dataset['test_df']}\\n\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""defaults"":""False"",""OutputFeatures"":"""",""preprocessing"":""False"",""preprocessing_split_column"":""None"",""model_type"":""ecd"",""preprocessing_split_probabilities"":""None"",""preprocessing_undersample_majority"":""None"",""backend"":""False"",""preprocessing_sample_ratio"":""1.0"",""preprocessing_split_type"":""random"",""preprocessing_oversample_minority"":""None""}}","DragNDropLite-35"
"DragNDropLite","Core","{""formats"":{""image_name"":""text"",""model_name"":""text""},""classname"":""ObjDectInference"",""name"":""ObjDectInference"",""alias"":""ObjDectInference"",""parentCategory"":""153"",""id"":36,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from PIL import Image"",""from ultralytics import YOLO""],""script"":""def ObjDectInference_<id>(data, image_name_param='/app/jobs/models/legacy_AI/Object_detection/predict_images/sample_nv.jpg', model_name_param='/app/jobs/models/legacy_AI/Object_detection/trained_models/best_6Feb.pt'):\\n    from ultralytics import YOLO\\n    from PIL import Image\\n    import cv2\\n    \\n    model_path=model_name_param\\n    #model_path=r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\best.pt'\\n\\n    image_path=image_name_param\\n    #image_path=r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\mix_1_shelf_1061_warp_affine.png'\\n\\n    model = YOLO(model_path)  # path1\\n    im1 = Image.open(image_path) # path 2\\n\\n    results = model.predict(source=im1, save=True,save_txt=True)  # save plotted images\\n    \\n    print('names:',['cheerios','cocacola','oreo','weikfiedpasta'])\\n\\n    for result in results:\\n        # detection\\n        print('box with xyxy format (N, 4)=>',result.boxes.xyxy)  # box with xyxy format, (N, 4)\\n        print('box with xywh format (N, 4)=>',result.boxes.xywh)  # box with xywh format, (N, 4)\\n        print('box with xyxy format but normalized, (N, 4)=>',result.boxes.xyxyn)  # box with xyxy format but normalized, (N, 4)\\n        print('box with xywh format but normalized, (N, 4)=>',result.boxes.xywhn)  # box with xywh format but normalized, (N, 4)\\n        print('confidence score, (N, 1)=>',result.boxes.conf)   # confidence score, (N, 1)\\n        print('cls (N, 1)=>',result.boxes.cls)\\n    \\n    return results\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""image_name"":""/app/jobs/models/legacy_AI/Object_detection/predict_images/sample_nv.jpg"",""model_name"":""/app/jobs/models/legacy_AI/Object_detection/trained_models/best_6Feb.pt""}}","DragNDropLite-36"
"DragNDropLite","Core","{""formats"":{""val_path"":""text"",""classes"":""text"",""train_path"":""text"",""yml_name"":""text""},""classname"":""ObjDetectCustomDataYml"",""name"":""ObjDetectCustomDataYml"",""alias"":""ObjDetectCustomDataYml"",""parentCategory"":""154"",""id"":37,""codeGeneration"":{""requirements"":[],""imports"":[""from ultralytics import YOLO"",""import os""],""script"":""def ObjDetectCustomDataYml_<id>(dataset, val_path_param='/app/jobs/models/cheque_detection_image/val', classes_param='IssueBank,ReceiverName,AcNo,Amt,ChqNo,DateIss,Sign', train_path_param='/app/jobs/models/cheque_detection_image/train', yml_name_param='custom_data.yml'):\\n    \\n    yml_name = yml_name_param     # custom_data.yml\\n    train_path = train_path_param # '/app/jobs/models/legacy_AI/Object_detection/custom_data/train'\\n    val_path = val_path_param     # '/app/jobs/models/legacy_AI/Object_detection/custom_data/val'\\n    classes = classes_param       # 'class1,class2,class3'\\n\\n    yml_path= '/app/jobs/models/cheque_detection_image/'+yml_name\\n\\n    classes = classes.split(',')\\n    n_classes = len(classes)\\n\\n    L = [\\n        'train : {0}\\\\n'.format(train_path),\\n        'val : {0}\\\\n\\\\n'.format(val_path),\\n        'nc : {0}\\\\n'.format(n_classes),\\n        'names : {0}'.format(classes)\\n    ]\\n\\n    with open(yml_path,'w') as file1:\\n        file1.writelines(L)\\n        \\n    return yml_path\\n\\n""},""category"":""Cheque_detection"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""val_path"":""/app/jobs/models/cheque_detection_image/val"",""classes"":""IssueBank,ReceiverName,AcNo,Amt,ChqNo,DateIss,Sign"",""train_path"":""/app/jobs/models/cheque_detection_image/train"",""yml_name"":""custom_data.yml""}}","DragNDropLite-37"
"DragNDropLite","Core","{""formats"":{""default_yolo_model"":""text"",""Batch_Size"":""Integer"",""no_of_epochs"":""Integer""},""classname"":""ObjDetectTrain"",""name"":""ObjDetectTrain"",""alias"":""ObjDetectTrain"",""parentCategory"":""154"",""id"":38,""codeGeneration"":{""requirements"":[""protoc""],""imports"":[""from ultralytics import YOLO"",""import os"",""import shutil""],""script"":""def ObjDetectTrain_<id>(yml_path, default_yolo_model_param='yolov8n.pt', batch_size_param=4, no_of_epochs_param=2):\\n    from ultralytics import YOLO\\n    import os\\n    import shutil\\n    \\n    n_epoch= no_of_epochs_param\\n    n_batch= batch_size_param\\n    default_model =default_yolo_model_param\\n    \\n    if os.path.exists('runs'):\\n        shutil.rmtree('runs')\\n    # Load a model\\n    model = YOLO(default_model)  # load a pretrained model (recommended for training)\\n\\n    # Train the model\\n    try:\\n        model.train(data=yml_path, epochs=n_epoch, imgsz=640,batch=n_batch)\\n    except Exception as e:\\n        print('We got error in train model=>',e)\\n\\n    f=os.listdir('runs/detect')\\n\\n    print(f)\\n\\n    for x in f:\\n        if x.startswith('train'):\\n            print(x)\\n            run=x\\n    print('train output is in=>','runs/detect/'+run)\\n\\n    train_folder=os.listdir('runs/detect/'+run)\\n    print('train folder=>',train_folder)\\n\\n    weights=os.listdir('runs/detect/'+run+'/weights')\\n    print('Weights=>',weights)\\n    if len(weights)>0:\\n        weights.sort()\\n        return weights[0]\\n    else:\\n        raise Exception('No Model Found')\\n\\n\\n\\n\\n\\n""},""category"":""Cheque_detection"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""default_yolo_model"":""yolov8n.pt"",""Batch_Size"":4,""no_of_epochs"":2}}","DragNDropLite-38"
"DragNDropLite","Core","{""formats"":{""image_path"":""text"",""model_path"":""text""},""classname"":""ImgClassificationInference"",""name"":""ImgClassificationInference"",""alias"":""ImgClassificationInference"",""parentCategory"":""153"",""id"":39,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from ultralytics import YOLO""],""script"":""def ImgClassificationInference_<id>(dataset, image_path_param='/app/jobs/models/legacy_AI/Image_Classification/sample_image/sample_imag.png', model_path_param='/app/jobs/models/legacy_AI/Image_Classification/pretrained_models/best_14march.pt'):\\n    from ultralytics import YOLO\\n\\n    #rootpath='/legacyAi/object_detection'\\n    #model_path='models/best.pt'\\n    model_path=model_path_param # /app/jobs/models/legacy_AI/Image_Classification/pretrained_models/best.pt\\n    #image_path='predict_img/sample.jpg'\\n    image_path=image_path_param # /app/jobs/models/legacy_AI/Image_Classification/sample_image/sample.png\\n    \\n    model = YOLO(model_path)  # path1\\n    #im1 = Image.open(image_path) # path 2\\n    results = model(source=image_path, save=True,save_txt=True)  # save plotted images\\n\\n    print('classes=>',results[0].names)\\n    print('prob=>',results[0].probs)\\n    print('speed=>',results[0].speed)\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""image_path"":""/app/jobs/models/legacy_AI/Image_Classification/sample_image/sample_imag.png"",""model_path"":""/app/jobs/models/legacy_AI/Image_Classification/pretrained_models/best_14march.pt""}}","DragNDropLite-39"
"DragNDropLite","Core","{""formats"":{""Image_Zip_Path"":""text"",""Root_Path_For_Working"":""text"",""model_path"":""text""},""classname"":""ImgClassifyBatchInference"",""name"":""ImgClassifyBatchInference"",""alias"":""ImgClassifyBatchInference"",""parentCategory"":""153"",""id"":40,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from ultralytics import YOLO""],""script"":""def ImgClassifyBatchInference_<id>(dataset, image_zip_path_param='/app/jobs/models/legacy_AI/Image_Classification/custom_data/cat_dog_test.zip', root_path_for_working_param='/app/jobs/models/legacy_AI/Image_Classification/cat_dog_test', model_path_param='/app/jobs/models/legacy_AI/Image_Classification/CatDogData/runs/classify/train/weights/best.pt'):\\n    from ultralytics import YOLO\\n    import shutil\\n    import zipfile\\n    import string\\n\\n    # Define the input zip file and json file paths\\n    zip_path = image_zip_path_param # r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\cat_dog_dataset_old\\\\cat_dog_dataset\\\\new_data.zip'\\n    \\n    # Set the root path to 'my_project' directory\\n    root_path=root_path_for_working_param   #'D:/INFCAT/Legacy_DL/Object Detection'\\n    \\n    path = root_path\\n    if not os.path.exists(path):\\n        os.makedirs(path)\\n    else:\\n        shutil.rmtree(path)           # Removes all the subdirectories!\\n        os.makedirs(path)\\n    \\n    os.chdir(root_path)\\n    \\n    model_path=model_path_param # /app/jobs/models/legacy_AI/Image_Classification/pretrained_models/best.pt\\n    #image_path='<Image_path_OR_folder_path>' # /app/jobs/models/legacy_AI/Image_Classification/sample_image/sample.png\\n    \\n    # Extract the zip file to a new directory with the same name as the zip file\\n    zip_dir = os.path.splitext(zip_path)[0]\\n    if not os.path.exists(zip_dir):\\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n            zip_ref.extractall(zip_dir)\\n    \\n    if len(os.listdir(zip_dir))==1:\\n        zip_dir=zip_dir+'/' + zip_dir.split('/')[-1]\\n    model = YOLO(model_path)  # path1\\n    \\n    results = model(source=zip_dir, save=True,save_txt=True)  # save plotted images\\n    \\n    def process_input_list(input_list):\\n        output_list = []\\n        for input_dict in input_list:\\n            output_dict = {}\\n            #current_dir = os.getcwd()\\n            #relative_path = os.path.relpath(input_dict.path , current_dir)\\n            path_dict=input_dict.path.split('/')[-2:]\\n            if len(path_dict)<2:\\n                path_dict=input_dict.path.split('\\\\\\\\')[-2:]\\n            relative_path = '/'.join(path_dict)\\n            output_dict['filePath'] = input_dict.path #relative_path\\n            output_dict['probs'] = {}\\n            max_index=int(input_dict.probs.argmax())\\n            output_dict['result']=input_dict.names[max_index]\\n            for i in range(len(input_dict.names)):\\n                output_dict['probs'][str(input_dict.names[i])]=float(input_dict.probs[i])\\n            output_list.append(output_dict)\\n        return output_list\\n\\n    # example usage\\n    input_list = results\\n\\n    output_list = process_input_list(input_list)\\n    \\n    for out in output_list:\\n        print(out)\\n    \\n    import json\\n    output = {'Results': output_list}\\n\\n    #print(output)\\n    with open('results.json', 'w') as outfile:\\n        json.dump(output, outfile)\\n        \\n    return output\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""Image_Zip_Path"":""/app/jobs/models/legacy_AI/Image_Classification/custom_data/cat_dog_test.zip"",""Root_Path_For_Working"":""/app/jobs/models/legacy_AI/Image_Classification/cat_dog_test"",""model_path"":""/app/jobs/models/legacy_AI/Image_Classification/CatDogData/runs/classify/train/weights/best.pt""}}","DragNDropLite-40"
"DragNDropLite","Core","{""formats"":{""Image_Zip_Path"":""text"",""Root_Path_For_Working"":""text"",""model_path"":""text""},""classname"":""ImgClassifyBatchInferenceVer2"",""name"":""ImgClassifyBatchInferenceVer2"",""alias"":""ImgClassifyBatchInferenceVer2"",""parentCategory"":""153"",""id"":41,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from ultralytics import YOLO""],""script"":""def ImgClassifyBatchInferenceVer2_<id>(dataset0,dataset1,dataset2, root_path_for_working_param='/app/jobs/models/legacy_AI/Image_Classification/cat_dog_test'):\\n    from ultralytics import YOLO\\n    import shutil\\n    import zipfile\\n    import string\\n\\n    # Define the input zip file and json file paths\\n    zip_path = [fn for fn in [dataset0,dataset1,dataset2] if str(fn).endswith('.zip')][0] #'<Image_Zip_Path>' # r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\cat_dog_dataset_old\\\\cat_dog_dataset\\\\new_data.zip'\\n    \\n    # Set the root path to 'my_project' directory\\n    root_path=root_path_for_working_param   #'D:/INFCAT/Legacy_DL/Object Detection'\\n    \\n    path = root_path\\n    if not os.path.exists(path):\\n        os.makedirs(path)\\n    else:\\n        shutil.rmtree(path)           # Removes all the subdirectories!\\n        os.makedirs(path)\\n    \\n    os.chdir(root_path)\\n    \\n    model_path=[fn for fn in [dataset0,dataset1,dataset2] if str(fn).endswith('.pt')][0] #'<model_path>' # /app/jobs/models/legacy_AI/Image_Classification/pretrained_models/best.pt\\n    #image_path='<Image_path_OR_folder_path>' # /app/jobs/models/legacy_AI/Image_Classification/sample_image/sample.png\\n    \\n    # Extract the zip file to a new directory with the same name as the zip file\\n    zip_dir = os.path.splitext(zip_path)[0]\\n    if not os.path.exists(zip_dir):\\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n            zip_ref.extractall(zip_dir)\\n    \\n    #if len(os.listdir(zip_dir))==1:\\n        #zip_dir=zip_dir+'/' + zip_dir.split('/')[-1]\\n    model = YOLO(model_path)  # path1\\n    \\n    results = model(source=zip_dir, save=True,save_txt=True)  # save plotted images\\n    \\n    def process_input_list(input_list):\\n        output_list = []\\n        for input_dict in input_list:\\n            output_dict = {}\\n            #current_dir = os.getcwd()\\n            #relative_path = os.path.relpath(input_dict.path , current_dir)\\n            path_dict=input_dict.path.split('/')[-2:]\\n            if len(path_dict)<2:\\n                path_dict=input_dict.path.split('\\\\\\\\')[-2:]\\n            relative_path = '/'.join(path_dict)\\n            output_dict['filePath'] = input_dict.path #relative_path\\n            #output_dict['names'] = input_dict.names\\n            #output_dict['probs'] = input_dict.probs\\n            max_index=int(input_dict.probs.argmax())\\n            output_dict['result']=input_dict.names[max_index]\\n            #for i in range(len(input_dict.names)):\\n                #output_dict['probs'][str(input_dict.names[i])]=float(input_dict.probs[i])\\n            output_list.append(output_dict)\\n        return output_list\\n\\n    # example usage\\n    input_list = results\\n\\n    output_list = process_input_list(input_list)\\n    \\n    for out in output_list:\\n        print(out)\\n    \\n    import json\\n    output = {'Results': output_list}\\n\\n    #print(output)\\n    with open('results.json', 'w') as outfile:\\n        json.dump(output, outfile)\\n        \\n    return output_list\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in"",""model_path"",""Image_Zip_Path""],""outputEndpoints"":[""out""],""attributes"":{""Image_Zip_Path"":"""",""Root_Path_For_Working"":""/app/jobs/models/legacy_AI/Image_Classification/cat_dog_test"",""model_path"":""""}}","DragNDropLite-41"
"DragNDropLite","Core","{""formats"":{""default_yolo_model"":""text"",""Batch_Size"":""Integer"",""no_of_epochs"":""Integer""},""classname"":""ImgClassificationTrain"",""name"":""ImgClassificationTrain"",""alias"":""ImgClassificationTrain"",""parentCategory"":""153"",""id"":42,""codeGeneration"":{""requirements"":[],""imports"":[""from ultralytics import YOLO"",""import os""],""script"":""def ImgClassificationTrain_<id>(dataset_path, default_yolo_model_param='yolov8n-cls.pt', batch_size_param=4, no_of_epochs_param=2):\\n    from ultralytics import YOLO\\n    import os\\n    \\n    n_epoch= no_of_epochs_param\\n    n_batch= batch_size_param\\n    default_model =default_yolo_model_param # yolov8n-cls.pt\\n    #dataset_path = '<dataset_path>' # /app/jobs/models/legacy_AI/Image_Classification/custom_data\\n    \\n    \\n    # Load a model\\n    #model = YOLO('yolov8n.yaml')  # build a new model from scratch\\n    model = YOLO(default_model)  # load a pretrained model (recommended for training)\\n\\n    # Train the model\\n    model.train(data=dataset_path, epochs=n_epoch, imgsz=64,batch=n_batch)\\n\\n    f=os.listdir('runs/classify')\\n\\n    print(f)\\n\\n    for x in f:\\n        if x.startswith('train'):\\n            print(x)\\n            run=x\\n    print('train output is in=>','runs/classify/'+run)\\n\\n    train_folder=os.listdir('runs/classify/'+run)\\n    print('train folder=>',train_folder)\\n\\n    weights=os.listdir('runs/classify/'+run+'/weights')\\n    print('Weights=>',weights)\\n    loaded_model = YOLO(os.path.join(dataset_path,'runs/classify/train/weights/best.pt'))\\n    return loaded_model\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""default_yolo_model"":""yolov8n-cls.pt"",""Batch_Size"":4,""no_of_epochs"":2}}","DragNDropLite-42"
"DragNDropLite","Core","{""formats"":{""question"":""text"",""target_class"":""text"",""prompt"":""text""},""classname"":""DocQnaBatchInf"",""name"":""DocQnaBatchInf"",""alias"":""DocQnaBatchInf"",""parentCategory"":""153"",""id"":43,""codeGeneration"":{""requirements"":[],""imports"":[""import re"",""import numpy as np"",""import torch"",""from transformers import DonutProcessor, VisionEncoderDecoderModel"",""from PIL import Image""],""script"":""def DocQnaBatchInf_<id>( dataset, question_param='what is the identification number present?', target_class_param='Eviction', prompt_param='<s_docvqa><s_question>{user_input}</s_question><s_answer>'):\\n    #pre-process Data\\n    import torch\\n    import logging as logger\\n    from transformers import DonutProcessor, VisionEncoderDecoderModel\\n    from PIL import Image\\n    import os\\n    import numpy as np\\n    import mysql.connector\\n    import requests\\n    from urllib.parse import urlparse\\n    import re\\n    \\n    def read_img_to_3d_array(img_path):\\n        img = Image.open(img_path)\\n        img_array = np.asarray(img)\\n        if len(img_array.shape) == 3:\\n            #img_array = img_array[:, :, :3]\\n            pass\\n        else:\\n            assert len(img_array.shape) == 2\\n            h, w = img_array.shape\\n            img_array = img_array.reshape([h, w, 1])\\n            img_array = np.concatenate([img_array] * 3, axis = -1)\\n        assert len(img_array.shape) == 3\\n        img_array = img_array[:, :, :3]\\n        return img_array\\n\\n    def process_document(image, question,task_prompt):\\n        processor = DonutProcessor.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\\n        model = VisionEncoderDecoderModel.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n        model.to(device)\\n        # prepare encoder inputs\\n        pixel_values = processor(image, return_tensors='pt').pixel_values\\n        # prepare decoder inputs\\n        \\n        prompt = task_prompt.replace('{user_input}', question)\\n        decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors='pt').input_ids\\n        # generate answer\\n        outputs = model.generate(\\n            pixel_values.to(device),\\n            decoder_input_ids=decoder_input_ids.to(device),\\n            max_length=model.decoder.config.max_position_embeddings,\\n            early_stopping=True,\\n            pad_token_id=processor.tokenizer.pad_token_id,\\n            eos_token_id=processor.tokenizer.eos_token_id,\\n            use_cache=True,\\n            num_beams=1,\\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\\n            return_dict_in_generate=True,\\n        )\\n        # postprocess\\n        sequence = processor.batch_decode(outputs.sequences)[0]\\n        sequence = sequence.replace(processor.tokenizer.eos_token, '').replace(processor.tokenizer.pad_token, '')\\n        sequence = re.sub(r'', '', sequence, count=1).strip()  # remove first task start token\\n        return processor.token2json(sequence)\\n    \\n    \\n    results = dataset\\n    print(results)\\n    \\n    answers=[]\\n    for res in results:\\n        if res['result']==target_class_param:\\n            task_prompt = prompt_param\\n            input_img = res['filePath']\\n            question = question_param\\n            image  = read_img_to_3d_array(input_img)\\n            #print(dataset,type(dataset))\\n            print('File=>',input_img)\\n            #for que in question:\\n            res1 = process_document(image, question,task_prompt)\\n            print('question=>',question)\\n            print('answer=>',res1)\\n            answers.append(res1)\\n            \\n    print('OUTPUT=>')\\n    print(answers)\\n            \\n    return answers\\n\\n\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out""],""attributes"":{""question"":""what is the identification number present?"",""target_class"":""Eviction"",""prompt"":""<s_docvqa><s_question>{user_input}</s_question><s_answer>""}}","DragNDropLite-43"
"DragNDropLite","Core","{""formats"":{""Split_Ratio"":""text"",""Image_Zip_Path"":""text"",""Root_Path_For_Working"":""text"",""Dataset_Name"":""text"",""Image_Annotation_Json_Path"":""text""},""classname"":""PreprocessImgClassification"",""name"":""PreprocessImgClassification"",""alias"":""PreprocessImgClassification"",""parentCategory"":""153"",""id"":44,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from ultralytics import YOLO""],""script"":""def PreprocessImgClassification_<id>(dataset, split_ratio_param=0.8, image_zip_path_param='/app/jobs/models/legacy_AI/Image_Classification/custom_data/cat_dog_dataset.zip', root_path_for_working_param='/app/jobs/models/legacy_AI/Image_Classification', dataset_name_param='CatDogData', image_annotation_json_path_param='/app/jobs/models/legacy_AI/Image_Classification/custom_data/cat_dog_anno.json'):\\n    import os\\n    import shutil\\n    import random\\n    import json\\n    import zipfile\\n    import string\\n\\n    # Define the input zip file and json file paths\\n    zip_path = image_zip_path_param # r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\cat_dog_dataset_old\\\\cat_dog_dataset\\\\new_data.zip'\\n    json_path = image_annotation_json_path_param #r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\cat_dog_anno.json'\\n\\n    dataset_name=dataset_name_param\\n    split_ratio = split_ratio_param\\n    split_ratio = float(split_ratio)\\n\\n    # Set the root path to 'my_project' directory\\n    root_path=root_path_for_working_param   #'D:/INFCAT/Legacy_DL/Object Detection'\\n    os.chdir(root_path)\\n    \\n    #length = 10\\n    #chars = string.ascii_lowercase + string.digits\\n    #random_string = ''.join(random.choice(chars) for _ in range(length))\\n    #dataset_name=dataset_name+random_string\\n    #os.makedirs(dataset_name)\\n    \\n    path = dataset_name\\n    if not os.path.exists(path):\\n        os.makedirs(path)\\n    else:\\n        shutil.rmtree(path)           # Removes all the subdirectories!\\n        os.makedirs(path)\\n\\n    new_root_path=os.path.join(root_path,dataset_name)\\n    os.chdir(new_root_path)\\n\\n    # Define the output directory paths\\n    output_dir = new_root_path\\n    train_dir = os.path.join(output_dir, 'train')\\n    test_dir = os.path.join(output_dir, 'test')\\n\\n    # Create the output directories\\n    if not os.path.exists(train_dir):\\n        os.makedirs(train_dir)\\n    if not os.path.exists(test_dir):\\n        os.makedirs(test_dir)\\n\\n    # Extract the zip file to a new directory with the same name as the zip file\\n    zip_dir = os.path.splitext(zip_path)[0]\\n    if not os.path.exists(zip_dir):\\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n            zip_ref.extractall(zip_dir)\\n\\n    # Load the labels from the json file\\n    with open(json_path, 'r') as f:\\n        labels = json.load(f)['labelsArray']\\n\\n    # Split the images into train and test directories\\n    for label in set([l['label'] for l in labels]):\\n        label_train_dir = os.path.join(train_dir, label)\\n        label_test_dir = os.path.join(test_dir, label)\\n        if not os.path.exists(label_train_dir):\\n            os.makedirs(label_train_dir)\\n        if not os.path.exists(label_test_dir):\\n            os.makedirs(label_test_dir)\\n        label_images = [l for l in labels if l['label'] == label]\\n        random.shuffle(label_images)\\n        train_count = int(len(label_images) * split_ratio)\\n        for i, l in enumerate(label_images):\\n            src_path = os.path.join(zip_dir, '/'.join(l['filePath'].split('/')[-1:]))\\n            if i < train_count:\\n                dst_dir = label_train_dir\\n            else:\\n                dst_dir = label_test_dir\\n            dst_path = os.path.join(dst_dir, os.path.basename( '/'.join(l['filePath'].split('/')[-1:])))\\n            shutil.copy(src_path, dst_path)\\n            \\n    return new_root_path\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""Split_Ratio"":""0.8"",""Image_Zip_Path"":""/app/jobs/models/legacy_AI/Image_Classification/custom_data/cat_dog_dataset.zip"",""Root_Path_For_Working"":""/app/jobs/models/legacy_AI/Image_Classification"",""Dataset_Name"":""CatDogData"",""Image_Annotation_Json_Path"":""/app/jobs/models/legacy_AI/Image_Classification/custom_data/cat_dog_anno.json""}}","DragNDropLite-44"
"DragNDropLite","Core","{""formats"":{""Split_Ratio"":""text"",""Image_Zip_Path"":""text"",""Root_Path_For_Working"":""text"",""Dataset_Name"":""text"",""Image_Annotation_Json_Path"":""text""},""classname"":""PreprocessImgClassificationVer2"",""name"":""PreprocessImgClassificationVer2"",""alias"":""PreprocessImgClassificationVer2"",""parentCategory"":""153"",""id"":45,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from ultralytics import YOLO""],""script"":""def PreprocessImgClassificationVer2_<id>(dataset0,dataset1,dataset2, split_ratio_param=0.8, root_path_for_working_param='/app/jobs/models/legacy_AI/Image_Classification', dataset_name_param='CatDogData'):\\n    import os\\n    import shutil\\n    import random\\n    import json\\n    import zipfile\\n    import string\\n\\n    # Define the input zip file and json file paths\\n    zip_path = [fn for fn in [dataset0,dataset1,dataset2] if str(fn).endswith('.zip')][0] #'<Image_Zip_Path>' # r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\cat_dog_dataset_old\\\\cat_dog_dataset\\\\new_data.zip'\\n    json_path = [fn for fn in [dataset0,dataset1,dataset2] if str(fn).endswith('.json')][0] #'<Image_Annotation_Json_Path>' #r'D:\\\\INFCAT\\\\Legacy_DL\\\\Object Detection\\\\cat_dog_anno.json'\\n\\n    dataset_name=dataset_name_param\\n    split_ratio = split_ratio_param\\n    split_ratio = float(split_ratio)\\n\\n    # Set the root path to 'my_project' directory\\n    root_path=root_path_for_working_param   #'D:/INFCAT/Legacy_DL/Object Detection'\\n    os.chdir(root_path)\\n    \\n    #length = 10\\n    #chars = string.ascii_lowercase + string.digits\\n    #random_string = ''.join(random.choice(chars) for _ in range(length))\\n    #dataset_name=dataset_name+random_string\\n    #os.makedirs(dataset_name)\\n    \\n    path = dataset_name\\n    if not os.path.exists(path):\\n        os.makedirs(path)\\n    else:\\n        shutil.rmtree(path)           # Removes all the subdirectories!\\n        os.makedirs(path)\\n\\n    new_root_path=os.path.join(root_path,dataset_name)\\n    os.chdir(new_root_path)\\n\\n    # Define the output directory paths\\n    output_dir = new_root_path\\n    train_dir = os.path.join(output_dir, 'train')\\n    test_dir = os.path.join(output_dir, 'test')\\n\\n    # Create the output directories\\n    if not os.path.exists(train_dir):\\n        os.makedirs(train_dir)\\n    if not os.path.exists(test_dir):\\n        os.makedirs(test_dir)\\n\\n    # Extract the zip file to a new directory with the same name as the zip file\\n    zip_dir = os.path.splitext(zip_path)[0]\\n    if not os.path.exists(zip_dir):\\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\\n            zip_ref.extractall(zip_dir)\\n\\n    # Load the labels from the json file\\n    with open(json_path, 'r') as f:\\n        labels = json.load(f)['labelsArray']\\n\\n    # Split the images into train and test directories\\n    for label in set([l['label'] for l in labels]):\\n        label_train_dir = os.path.join(train_dir, label)\\n        label_test_dir = os.path.join(test_dir, label)\\n        if not os.path.exists(label_train_dir):\\n            os.makedirs(label_train_dir)\\n        if not os.path.exists(label_test_dir):\\n            os.makedirs(label_test_dir)\\n        label_images = [l for l in labels if l['label'] == label]\\n        random.shuffle(label_images)\\n        train_count = int(len(label_images) * split_ratio)\\n        for i, l in enumerate(label_images):\\n            src_path = os.path.join(zip_dir, '/'.join(l['filePath'].split('/')[-1:]))\\n            if i < train_count:\\n                dst_dir = label_train_dir\\n            else:\\n                dst_dir = label_test_dir\\n            dst_path = os.path.join(dst_dir, os.path.basename( '/'.join(l['filePath'].split('/')[-1:])))\\n            shutil.copy(src_path, dst_path)\\n            \\n    return new_root_path\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in"",""Image_Zip_Path"",""Annotation_Json_Path""],""outputEndpoints"":[""Dir_path""],""attributes"":{""Split_Ratio"":""0.8"",""Image_Zip_Path"":"""",""Root_Path_For_Working"":""/app/jobs/models/legacy_AI/Image_Classification"",""Dataset_Name"":""CatDogData"",""Image_Annotation_Json_Path"":""""}}","DragNDropLite-45"
"DragNDropLite","Core","{""formats"":{""sentence"":""text"",""model_name"":""text"",""display_flag"":""text""},""classname"":""Semantic_Similarity_predict"",""name"":""Semantic_Similarity_predict"",""alias"":""Semantic Similarity predict"",""parentCategory"":""155"",""id"":46,""codeGeneration"":{""requirements"":[""tensorflow_hub"",""protoc==3.1"",""tensorflow""],""imports"":[""import tensorflow_hub as hub"",""import numpy as np"",""import tensorflow as tf"",""from tensorflow.python.keras import backend as K""],""script"":""def Semantic_Similarity_predict_<id>(dataset, sentence_param='text', model_name_param='text', display_flag_param='text'):\\n\\n\\n\\n    def calculate_score(embed,sentences_pair, display_flag):\\n        embeddings = embed(sentences_pair)\\n        correlation_matrix = np.inner(embeddings, embeddings)\\n        matrix = np.round_(correlation_matrix, 2)\\n        if len(sentences_pair) == 2 and display_flag == 'score':\\n            return {'score':matrix[0,1]}\\n        return {'score':matrix}\\n    sess = K.get_session()\\n    sentence = sentence_param\\n    sentences=[i for i in sentence]\\n    #     sentences=[{''}]\\n    if sentences is None:\\n        raise Exception('Sentence parameter is empty')\\n    if not isinstance(sentences,list):\\n        raise Exception(f'Expected sentences to be of type list, but received as {type(sentences)}')\\n    display_flag = display_flag_param\\n    model_path = r'/app/jobs/models/pretrained_models/universal-sentence-encoder-large_5'\\n    #     if model_path is not None:\\n    #         embed = embed\\n    #         print('----------------',embed)\\n    #     else:\\n    embed= hub.load(model_path)\\n    model_path, embed = model_path, embed\\n    if isinstance(sentences[0],list):\\n        __result = []\\n        for sentence_pair in sentences:\\n            __result.append(calculate_score(embed, sentence_pair, display_flag))\\n    else:\\n        __result = calculate_score(embed, sentences, display_flag)\\n    print(__result)\\n\\n    # except Exception as error:\\n    #     raise Exception(error)\\n\\n""},""category"":""Semantic Similarity"",""inputEndpoints"":[""in"",""in""],""outputEndpoints"":[""out""],""attributes"":{""sentence"":""text"",""model_name"":""text"",""display_flag"":""text""}}","DragNDropLite-46"
"DragNDropLite","Core","{""formats"":{""max_features"":""text"",""model_name"":""text"",""max_depth"":""text"",""test_size"":""text"",""min_samples_split"":""text"",""vect_type"":""text"",""bootstrap"":""text"",""n_estimators"":""text"",""class_heading"":""text"",""min_samples_leaf"":""text"",""text_heading"":""text""},""classname"":""sentiment_randomforest"",""name"":""sentiment_randomforest"",""alias"":""sentiment_randomforest"",""parentCategory"":""156"",""id"":47,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle"",""import json"",""import secrets"",""import logging"",""import joblib"",""import numpy as np"",""import pandas as pd"",""from collections import OrderedDict"",""from nltk.corpus import stopwords"",""from threading import Thread"",""from sklearn import metrics"",""from nltk.stem.wordnet import WordNetLemmatizer"",""from sklearn.preprocessing import LabelEncoder"",""from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)"",""from sklearn.ensemble import RandomForestClassifier"",""from sklearn.model_selection import GridSearchCV, train_test_split"",""from sklearn.utils import shuffle""],""script"":""def sentiment_randomforest_<id>(dataset, max_features_param=['sqrt','auto'], model_name_param='text1', test_size_param=0.2, max_depth_param=[10,None], min_samples_split_param=[5], vect_type_param=None, n_estimators_param=[500,1000], bootstrap_param=True, class_heading_param='class', min_samples_leaf_param=[1], text_heading_param='text'):\\n    model_name= model_name_param\\n    vect_type =  vect_type_param\\n    test_size =  test_size_param\\n    max_features = max_features_param\\n    n_estimators  = n_estimators_param\\n    max_depth=    max_depth_param\\n    min_samples_split = min_samples_split_param\\n    min_samples_leaf = min_samples_leaf_param\\n    bootstrap= bootstrap_param\\n    text_heading= text_heading_param\\n    class_heading=class_heading_param\\n    \\n    \\n    model_path =  os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n    print(model_path)\\n    if model_path is None:\\n        raise Exception('Model path is a required parameter')\\n    model_path = os.path.join(model_path,model_name)\\n\\n    if not os.path.isdir(model_path):\\n        os.makedirs(model_path)\\n\\n    if isinstance(max_features, str):\\n        max_features = (max_features)\\n    GridParameters = {\\n                'max_features': max_features,\\n                'n_estimators': n_estimators,\\n                'max_depth': max_depth,\\n                'min_samples_split': min_samples_split,\\n                'min_samples_leaf': min_samples_leaf,\\n                'bootstrap': [bootstrap]} \\n\\n    \\n    dataframe = pd.DataFrame(dataset)\\n\\n    if not all([True if i in dataframe.columns else False for i in [text_heading, class_heading]]):\\n        raise Exception('Column name mismatch.')\\n\\n    if(vect_type=='TFIDF'):            \\n        vectorizer = TfidfVectorizer()\\n    else: \\n        vectorizer = CountVectorizer()             \\n    vectors = vectorizer.fit_transform(dataframe[text_heading])\\n    X_train, X_test, y_train, y_test = train_test_split(vectors, dataframe[class_heading], test_size=test_size, random_state=42)\\n    grid_search = GridSearchCV(RandomForestClassifier(),param_grid=GridParameters,cv=5,return_train_score=True,n_jobs=-1)\\n    grid_search.fit(X_train,y_train)  \\n    clf = RandomForestClassifier(max_features=grid_search.best_params_['max_features'],\\n                                  max_depth=grid_search.best_params_['max_depth'],\\n                                  n_estimators=grid_search.best_params_['n_estimators'],\\n                                  min_samples_split=grid_search.best_params_['min_samples_split'],\\n                                  min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\\n                                  bootstrap=grid_search.best_params_['bootstrap'])\\n    clf.fit(X_train, y_train)    \\n    pred = clf.predict(X_test)\\n    f1_sc=metrics.accuracy_score(y_test, pred)\\n    joblib.dump(clf,model_path+'/'+'classifier.pkl')\\n    joblib.dump(vectorizer,model_path+'/'+'vectorizer.pkl')\\n\\n    print('model has been trained with accuracy of ',str(f1_sc))\\n\\n""},""category"":""Sentiment analysis"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""max_features"":""['sqrt','auto']"",""model_name"":""text1"",""max_depth"":""[10,None]"",""test_size"":0.2,""min_samples_split"":[5],""vect_type"":""None"",""bootstrap"":""True"",""n_estimators"":""[500,1000]"",""class_heading"":""class"",""min_samples_leaf"":[1],""text_heading"":""text""}}","DragNDropLite-47"
"DragNDropLite","Core","{""formats"":{""model_name"":""text"",""text"":""text""},""classname"":""random_forest_predict"",""name"":""random_forest_predict"",""alias"":""Random Forest Predict"",""parentCategory"":""156"",""id"":48,""codeGeneration"":{""requirements"":[""nltk""],""imports"":[""import os, json, joblib"",""import pandas as pd"",""import codecs"",""import numpy as np"",""import nltk"",""from nltk.corpus import stopwords"",""from nltk.stem.wordnet import WordNetLemmatizer""],""script"":""\\ndef random_forest_predict_<id>(dataset, model_name_param='test', text_param='text') :\\n    model_name = model_name_param\\n    text = text_param\\n    model_path=os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n    if model_path is None:\\n        raise Exception('Parameter Model path is empty')\\n    \\n    if text is None:\\n        raise Exception('Text Input is required')\\n\\n    vect=joblib.load(model_path+'/'+model_name+'/'+'vectorizer.pkl')\\n    classify=joblib.load(model_path+'/'+model_name+'/'+'classifier.pkl')\\n    def pre_process_text(text):\\n            #If using stemming...\\n            #stemmer = PorterStemmer()\\n            textArray=text.split()\\n            wnl = WordNetLemmatizer()\\n            processed_text = []\\n            for text in textArray:\\n                words_list = (str(text).lower()).split()\\n                final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words('english')]\\n                #If using stemming...\\n                #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words('english')]\\n                final_words_str = str((' '.join(final_words)))\\n                processed_text.append(final_words_str)   \\n            return ' '.join(processed_text)\\n    if isinstance(text,str):\\n        out_prob=classify.predict_proba(vect.transform([text]))\\n        out_prob=np.round(out_prob[0],decimals=4)\\n        clas_li=classify.classes_\\n        out_dict={}\\n        for i in range(0,len(clas_li)):\\n            out_dict[clas_li[i]]=out_prob[i]\\n        pred_class=classify.predict(vect.transform([text]))\\n        # return {'bestClass':pred_class[0],'allClassProab':out_dict}\\n        print('text',text,'label',pred_class[0])\\n        return {'text':text,'label':pred_class[0]}\\n    elif isinstance(text,list):\\n        result_list=[]\\n        for sentence in text:\\n            out_prob=classify.predict_proba(vect.transform([sentence]))\\n            out_prob=np.round(out_prob[0],decimals=4)\\n            clas_li=classify.classes_\\n            out_dict={}\\n            for i in range(0,len(clas_li)):\\n                out_dict[clas_li[i]]=out_prob[i]\\n            pred_class=classify.predict(vect.transform([sentence]))\\n            result_list.append({'text ':sentence,'label':pred_class[0]}) \\n        print(result_list)\\n        return result_list\\n\\n\\n""},""category"":""Sentiment analysis"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_name"":""test"",""text"":""text""}}","DragNDropLite-48"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""Concat"",""Sequence Concat"",""TabNet"",""Transformer"",""TabTransformer"",""Comparator""],""Combiner Type"":""dropValues""},""classname"":""CombinerAlgorithm"",""name"":""CombinerAlgorithm"",""alias"":""CombinerAlgorithm"",""parentCategory"":""152"",""id"":49,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def CombinerAlgorithm_<id>(model_details,combiner_type_param=[],dropout_param=0,num_fc_layers_param=0,output_size_param=256,norm_param=0,activation_param='relu',flatten_inputs_param=False ,residual_param=False,use_bias_param=True,bias_initializer_param='zeros',weights_initializer_param='xavier_uniform',norm_params_param=0,fc_layers_param=0,main_sequence_feature_param=0,reduce_output_param=0,size_param=32,num_steps_param=3,num_total_blocks_param=4,num_shared_blocks_param=2,relaxation_factor_param=1.5,bn_epsilon_param=0.001,bn_momentum_param=0.05,bn_virtual_bs_param=1024,sparsity_param=0.0001,entmax_mode_param='sparsemax',entmax_alpha_param=1.5,fc_dropout_param=0,transformer_output_size_param=256,hidden_size_param=256,num_layers_param=1, num_heads_param=8,fc_activation_param='relu',fc_residual_param=False,embed_input_feature_name_param=0,entity_1_param=0,entity_2_param=0):\\n    Combiner_Type=combiner_type_param\\n    if(Combiner_Type.lower() == 'concat'):\\n        combiner={\\n            'type': 'concat',\\n            'dropout':dropout_param,\\n            'num_fc_layers': num_fc_layers_param,\\n            'output_size': output_size_param,\\n            'norm': norm_param,\\n            'activation': activation_param,\\n            'flatten_inputs': flatten_inputs_param,\\n            'residual': residual_param,\\n            'use_bias':use_bias_param,\\n            'bias_initializer':bias_initializer_param,\\n            'weights_initializer': weights_initializer_param,\\n            'norm_params': norm_params_param,\\n            'fc_layers': fc_layers_param,\\n        }\\n\\n        model_details.update({'combiner':combiner})\\n        return model_details\\n    elif Combiner_Type == 'Sequence Concat':\\n        combiner={\\n            'type': 'sequence_concat',\\n            'main_sequence_feature':main_sequence_feature_param, \\n            'reduce_output': reduce_output_param\\n        }\\n        model_details.update({'combiner':combiner})\\n        return model_details\\n    elif Combiner_Type == 'tabnet':\\n     \\n        combiner={\\n            'type': 'tabnet',\\n            'size': size_param,\\n            'dropout': dropout_param,\\n            'output_size':output_size_param,\\n            'num_steps': num_steps_param,\\n            'num_total_blocks': num_total_blocks_param,\\n            'num_shared_blocks': num_shared_blocks_param,\\n            'relaxation_factor': relaxation_factor_param,\\n            'bn_epsilon': bn_epsilon_param,\\n            'bn_momentum':bn_momentum_param,\\n            'bn_virtual_bs': bn_virtual_bs_param,\\n            'sparsity': sparsity_param,\\n            'entmax_mode': entmax_mode_param,\\n            'entmax_alpha': entmax_alpha_param\\n             \\n        }\\n\\n        model_details.update({'combiner':combiner})\\n        return model_details\\n    \\n    \\n    elif Combiner_Type == 'transformer':\\n        combiner={\\n            'type': 'transformer',\\n            'dropout': dropout_param,\\n            'num_fc_layers': num_fc_layers_param,\\n            'output_size': output_size_param,\\n            'norm': norm_param,\\n            'fc_dropout': fc_dropout_param,\\n            'transformer_output_size': transformer_output_size_param,\\n            'hidden_size': hidden_size_param,\\n            'num_layers': num_layers_param,\\n            'num_heads': num_heads_param,\\n            'use_bias': use_bias_param,\\n            'bias_initializer':bias_initializer_param,\\n            'weights_initializer': weights_initializer_param,\\n            'norm_params': norm_params_param,\\n            'fc_layers': fc_layers_param,\\n            'fc_activation':fc_activation_param,\\n            'fc_residual': fc_residual_param,\\n            'reduce_output':reduce_output_param\\n        }\\n        model_details.update({'combiner':combiner})\\n        return model_details\\n\\n    elif Combiner_Type  == 'tabtransformer':\\n\\n        combiner={\\n            \\n            'type': 'tabtransformer',\\n            'dropout':dropout_param,\\n            'num_fc_layers': num_fc_layers_param,\\n            'output_size': output_size_param,\\n            'norm': norm_param,\\n            'fc_dropout': fc_dropout_param,\\n            'embed_input_feature_name':embed_input_feature_name_param,\\n            'transformer_output_size': transformer_output_size_param,\\n            'hidden_size': hidden_size_param,\\n            'num_layers': num_layers_param,\\n            'num_heads': num_heads_param,\\n            'use_bias': use_bias_param,\\n            'bias_initializer': bias_initializer_param,\\n            'weights_initializer':weights_initializer_param,\\n            'norm_params': norm_params_param,\\n            'fc_layers':fc_layers_param,\\n            'fc_activation': fc_activation_param,\\n            'fc_residual': fc_residual_param,\\n            'reduce_output': reduce_output_param\\n             \\n        }\\n        model_details.update({'combiner':combiner})\\n        return model_details\\n    \\n    elif Combiner_Type  == 'comparator':\\n\\n        combiner={\\n            'type': 'comparator',\\n            'entity_1': entity_1_param,\\n            'entity_2': entity_2_param,\\n            'dropout': dropout_param,\\n            'num_fc_layers': num_fc_layers_param,\\n            'output_size': output_size_param,\\n            'norm':  norm_param,\\n            'activation': activation_param,\\n            'use_bias':  use_bias_param,\\n            'bias_initializer':  bias_initializer_param,\\n            'weights_initializer': weights_initializer_param,\\n            'norm_params':  norm_params_param,\\n            'fc_layers':   fc_layers_param\\n        }\\n        model_details.update({'combiner':combiner})\\n        return model_details\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""Combiner Type"":[]}}","DragNDropLite-49"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""bert"",""xlnet"",""random_forest""],""max_depth"":""text"",""test_size"":""text"",""min_samples_split"":""text"",""model_type"":""dropValues"",""bootstrap"":""text"",""n_estimators"":""text"",""class_heading"":""text"",""min_samples_leaf"":""text"",""test_ratio"":""text"",""text_heading"":""text"",""max_features"":""text"",""model_name"":""text"",""vect_type"":""text"",""text"":""text""},""classname"":""Sentiment_train"",""name"":""Sentiment_train"",""alias"":""Sentiment_train"",""parentCategory"":""156"",""id"":50,""codeGeneration"":{""requirements"":[""scikit-learn"",""nltk""],""imports"":[""import os"",""import pickle"",""import json"",""import secrets"",""import logging"",""import joblib"",""import numpy as np"",""import pandas as pd"",""from collections import OrderedDict"",""from nltk.corpus import stopwords"",""from threading import Thread"",""from sklearn import metrics"",""from nltk.stem.wordnet import WordNetLemmatizer"",""from sklearn.preprocessing import LabelEncoder"",""from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)"",""from sklearn.ensemble import RandomForestClassifier"",""from sklearn.model_selection import GridSearchCV, train_test_split"",""from sklearn.utils import shuffle""],""script"":""def Sentiment_train_<id>(dataset, test_size_param=0.2, max_depth_param=[10,None], min_samples_split_param=[5], model_type_param=[], n_estimators_param=[500,1000], bootstrap_param=True, class_heading_param='class', min_samples_leaf_param=[1], text_heading_param='text', test_ratio_param=0.1, max_features_param=['sqrt','auto'], model_name_param='test', vect_type_param=None, text_param='text'):\\n    model_name= model_type_param\\n    vect_type =  vect_type_param\\n    test_size =  test_size_param\\n    max_features = max_features_param\\n    n_estimators  = n_estimators_param\\n    max_depth=    max_depth_param\\n    min_samples_split = min_samples_split_param\\n    min_samples_leaf = min_samples_leaf_param\\n    bootstrap= bootstrap_param\\n    text_heading= text_heading_param\\n    class_heading=class_heading_param\\n    \\n    \\n    model_path =  os.path.join(os.environ['JOB_DIRECTORY'],'models','classicmlpoc')\\n    if model_path is None:\\n        raise Exception('Model path is a required parameter')\\n    model_path = os.path.join(model_path,model_name)\\n\\n    if not os.path.isdir(model_path):\\n        os.makedirs(model_path)\\n\\n    if isinstance(max_features, str):\\n        max_features = (max_features)\\n    GridParameters = {\\n                'max_features': max_features,\\n                'n_estimators': n_estimators,\\n                'max_depth': max_depth,\\n                'min_samples_split': min_samples_split,\\n                'min_samples_leaf': min_samples_leaf,\\n                'bootstrap': [bootstrap]} \\n\\n    \\n    dataframe = pd.DataFrame(dataset)\\n\\n    if not all([True if i in dataframe.columns else False for i in [text_heading, class_heading]]):\\n        raise Exception('Column name mismatch.')\\n\\n    if(vect_type=='TFIDF'):            \\n        vectorizer = TfidfVectorizer()\\n    else: \\n        vectorizer = CountVectorizer()             \\n    vectors = vectorizer.fit_transform(dataframe[text_heading])\\n    X_train, X_test, y_train, y_test = train_test_split(vectors, dataframe[class_heading], test_size=test_size, random_state=42)\\n    grid_search = GridSearchCV(RandomForestClassifier(),param_grid=GridParameters,cv=5,return_train_score=True,n_jobs=-1)\\n    grid_search.fit(X_train,y_train)  \\n    clf = RandomForestClassifier(max_features=grid_search.best_params_['max_features'],\\n                                  max_depth=grid_search.best_params_['max_depth'],\\n                                  n_estimators=grid_search.best_params_['n_estimators'],\\n                                  min_samples_split=grid_search.best_params_['min_samples_split'],\\n                                  min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\\n                                  bootstrap=grid_search.best_params_['bootstrap'])\\n    clf.fit(X_train, y_train)    \\n    pred = clf.predict(X_test)\\n    f1_sc=metrics.accuracy_score(y_test, pred)\\n    joblib.dump(clf,model_path+'/'+'classifier.pkl')\\n    joblib.dump(vectorizer,model_path+'/'+'vectorizer.pkl')\\n\\n    print('model has been trained with accuracy of ',str(f1_sc))\\n\\n""},""category"":""Sentiment analysis"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""max_depth"":""[10,None]"",""test_size"":0.2,""min_samples_split"":[5],""model_type"":[],""bootstrap"":""True"",""n_estimators"":""[500,1000]"",""class_heading"":""class"",""min_samples_leaf"":[1],""test_ratio"":0.1,""text_heading"":""text"",""max_features"":""['sqrt','auto']"",""model_name"":""test"",""vect_type"":""None"",""text"":""text""}}","DragNDropLite-50"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""bert"",""xlnet"",""random_forest""],""batch_size"":""text"",""max_depth"":""text"",""test_size"":""text"",""min_samples_split"":""text"",""model_type"":""dropValues"",""__class_heading"":""text"",""bootstrap"":""text"",""n_estimators"":""text"",""num_workers"":""text"",""class_heading"":""text"",""num_labels"":""text"",""min_samples_leaf"":""text"",""test_ratio"":""text"",""text_heading"":""text"",""train_test_ratio"":""text"",""max_features"":""text"",""model_name"":""text"",""vect_type"":""text"",""max_seq_length"":""text"",""text"":""text"",""learning_rate"":""text"",""__text_heading"":""text""},""classname"":""Bert_Sentiment_train"",""name"":""Bert_Sentiment_train"",""alias"":""Bert Sentiment_train"",""parentCategory"":""156"",""id"":51,""codeGeneration"":{""requirements"":[""scikit-learn"",""nltk"",""lime"",""progressbar2"",""tensorflow"",""imgkit""],""imports"":[""import imgkit"",""import lime"",""import tensorflow"",""import transformers"",""from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW,get_linear_schedule_with_warmup,BertTokenizer,BertForSequenceClassification"",""import torch"",""from joblib import parallel_backend"",""from joblib import Parallel, delayed"",""import os"",""import json"",""import numpy as np"",""import pandas as pd"",""from sklearn.model_selection import train_test_split"",""from sklearn.metrics import confusion_matrix, classification_report"",""import sklearn"",""from sklearn import metrics"",""from sklearn.utils import shuffle"",""from torch import nn, optim"",""from tensorflow.keras.preprocessing.sequence import pad_sequences"",""from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler"",""from torch.utils.data import Dataset, DataLoader"",""import torch.nn.functional as F"",""import logging, glob, imgkit, shutil, tempfile"",""from threading import Thread"",""from collections import defaultdict"",""from sklearn.preprocessing import LabelEncoder, OneHotEncoder"",""import time"",""import progressbar2"",""widgets = [' [',progressbar.Timer(format= 'elapsed time: %(elapsed)s'),'] ',progressbar.Bar('*'),' (',progressbar.ETA(), ') ', ]"",""import math"",""from lime.lime_text import LimeTextExplainer"",""import joblib"",""import codecs"",""import nltk"",""from nltk.corpus import stopwords"",""from nltk.stem.wordnet import WordNetLemmatizer"",""import pickle"",""import secrets"",""from collections import OrderedDict"",""from threading import Thread"",""from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)"",""from sklearn.ensemble import RandomForestClassifier"",""from sklearn.model_selection import GridSearchCV, train_test_split"",""from sklearn.utils import shuffle""],""script"":""def Bert_Sentiment_train_<id>(dataset, batch_size_param=8, test_size_param=0.2, max_depth_param=[10,None], min_samples_split_param=[5], model_type_param=[], __class_heading_param='class', n_estimators_param=[500,1000], bootstrap_param=True, num_workers_param=4, class_heading_param='class', num_labels_param=2, min_samples_leaf_param=[1], train_test_ratio_param=0.15, text_heading_param='text', test_ratio_param=0.1, max_features_param=['sqrt','auto'], model_name_param='test', vect_type_param=None, max_seq_length_param=512, text_param='text', learning_rate_param=float(3e-5), __text_heading_param='text'):\\n    class CustomDataset(Dataset):\\n\\n        def __init__(self, texts, labels, tokenizer, max_len):\\n            self.texts = texts\\n            self.labels = labels\\n            self.tokenizer = tokenizer\\n            self.max_len = max_len\\n\\n        def __len__(self):\\n            return len(self.texts)\\n\\n        def __getitem__(self, item):\\n            texts = str(self.texts[item])\\n            labels = self.labels[item]\\n\\n            encoding = self.tokenizer.encode_plus(\\n            texts,\\n            add_special_tokens=True,\\n            max_length=self.max_len,\\n            return_token_type_ids=False,\\n            pad_to_max_length=False,\\n            return_attention_mask=True,\\n            return_tensors='pt',\\n            truncation = True\\n            )\\n\\n            input_ids = pad_sequences(encoding['input_ids'], maxlen=self.max_len, dtype=torch.Tensor ,truncating='post',padding='post')\\n            input_ids = input_ids.astype(dtype = 'int64')\\n            input_ids = torch.tensor(input_ids) \\n\\n            attention_mask = pad_sequences(encoding['attention_mask'], maxlen=self.max_len, dtype=torch.Tensor ,truncating='post',padding='post')\\n            attention_mask = attention_mask.astype(dtype = 'int64')\\n            attention_mask = torch.tensor(attention_mask)       \\n\\n            return {\\n            'texts': texts,\\n            'input_ids': input_ids,\\n            'attention_mask': attention_mask.flatten(),\\n            'labels': torch.tensor(labels, dtype=torch.long)\\n            }\\n    def create_data_loader(df, tokenizer, max_len, batch_size, num_workers):\\n        ds = CustomDataset(\\n            texts=np.array(df[__text_heading]),\\n            labels=np.array(df[__class_heading]),\\n            tokenizer=tokenizer,\\n            max_len=max_len\\n        )\\n\\n        return DataLoader(\\n            ds,\\n            batch_size=batch_size,\\n            num_workers=num_workers)\\n    def log_training( path, text):\\n            if not os.path.isdir(path):\\n                os.makedirs(path)\\n            with open(os.path.join(path,'training_logs.txt'),'a+') as f:\\n                f.write(text)\\n    def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples,max_len,batch_size):\\n            model = model.train()\\n            losses = []\\n            acc = 0\\n            counter = 0\\n\\n            bar = progressbar.ProgressBar(max_value=math.ceil(len(data_loader.dataset)/batch_size), widgets=widgets).start() \\n\\n            for d in data_loader:\\n                input_ids = d['input_ids'].reshape(-1,max_len).to(device)\\n                attention_mask = d['attention_mask'].to(device)\\n                targets = d['labels'].to(device)\\n                outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\\n                loss = outputs[0]\\n                logits = outputs[1]\\n                # preds = preds.cpu().detach().numpy()\\n                _, prediction = torch.max(logits, dim=1)\\n                targets = targets.cpu().detach().numpy()\\n                prediction = prediction.cpu().detach().numpy()\\n                accuracy = metrics.accuracy_score(targets, prediction)\\n\\n                acc += accuracy\\n                losses.append(loss.item())\\n\\n                loss.backward()\\n\\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n                optimizer.step()\\n                scheduler.step()\\n                optimizer.zero_grad()\\n                counter = counter + 1\\n                bar.update(counter)\\n            return acc / counter, np.mean(losses)\\n\\n\\n    model_path =  r'/app/jobs/models/classification'\\n    pretrained_model_path =r'/app/jobs/models/pretrained_models/BERT_sst5'\\n\\n    model_name = model_type_param\\n    # learning_rate = float(3e-5)\\n    learning_rate = learning_rate_param\\n    __text_heading=__text_heading_param\\n    __class_heading=__class_heading_param\\n    max_seq_length =max_seq_length_param\\n    batch_size =batch_size_param\\n    num_workers =num_workers_param\\n    train_test_ratio =  train_test_ratio_param\\n    num_labels = num_labels_param\\n\\n    if model_name is None:\\n        raise Exception('Model name is undefined')\\n\\n    model_save_path = os.path.join(model_path, model_name)\\n    if not os.path.isdir(model_save_path):\\n        os.makedirs(model_save_path)\\n    print(model_save_path,'Training started \\\\n')\\n    if file_path is None:\\n        raise Exception('File path cannot be empty')\\n    check_point_dir =None\\n    if check_point_dir is not None:\\n        check_point_dir = os.path.join(check_point_dir,model_name)\\n        if not os.path.isdir(check_point_dir):\\n            os.makedirs(check_point_dir)\\n    # _, extension = os.path.splitext(file_path)\\n    # if extension == '.csv':\\n    #     __df  = pd.read_csv(file_path, index_col=None)\\n    # elif extension in ['.xlsx', '.xls']:\\n    #     __df = pd.read_excel(file_path, index_col=None)\\n    # else:\\n    #     raise Exception('File type not supported. Send a csv or xlsx file to train')\\n\\n    __df=pd.DataFrame(dataset)\\n\\n    # self.__text_heading = parameters_dictionary.get('text_heading','text')\\n    # self.__class_heading = parameters_dictionary.get('class_heading','class')\\n    train_test_ratio =  0.15\\n    num_labels = 2\\n\\n    with open(os.path.join(model_save_path,model_name+'_metadata.json'), 'w') as outfile: \\n        json.dump(model_details,outfile)\\n\\n    # model, tokenizer = load_train_model(base_model, num_labels, parameters_dictionary)\\n    model_tokenizer =  BertTokenizer\\n    model = BertForSequenceClassification\\n    tokenizer = getattr(model_tokenizer,'from_pretrained')(pretrained_model_path)\\n    model = getattr(model, 'from_pretrained')(pretrained_model_path, num_labels = num_labels,ignore_mismatched_sizes=True)\\n    model.train()\\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n    model = model.to(device)\\n\\n    le = LabelEncoder()\\n    df=__df\\n    __text_heading='text'\\n    __class_heading='class'\\n    df[__class_heading] = le.fit_transform(__df[__class_heading])\\n    try:\\n        np.save(os.path.join(model_save_path,'classes.npy'), le.classes_)\\n    except Exception as e:\\n        print(exception(e))\\n    df = shuffle(df)\\n\\n    df_train, df_val = train_test_split(df, test_size=train_test_ratio, random_state=101)\\n    train_data_loader = create_data_loader(df_train, tokenizer, max_seq_length, batch_size, num_workers=0)\\n    val_data_loader = create_data_loader(df_val, tokenizer, max_seq_length, batch_size, num_workers=0)\\n    param_optimizer = list(model.named_parameters())\\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\\n    optimizer_grouped_parameters = [\\n                                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\\n                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\\n    ]\\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\\n\\n    total_steps = len(train_data_loader) * epochs\\n\\n    scheduler = get_linear_schedule_with_warmup(\\n    optimizer,\\n    num_warmup_steps=0,\\n    num_training_steps=total_steps\\n    )\\n    history = defaultdict(list)\\n    best_accuracy = -1\\n    for epoch in range(epochs):\\n        print(f'Epoch {epoch + 1}/{epochs}')\\n        print( '-' * 10)\\n        log_training(model_save_path,f'Epoch {epoch + 1}/{epochs} \\\\n'+'-'*10+'\\\\n')\\n        train_acc, train_loss = train_epoch(\\n            model,\\n            train_data_loader,     \\n            optimizer, \\n            device, \\n            scheduler, \\n            len(df_train),\\n            max_len = max_seq_length,\\n            batch_size = batch_size\\n        )\\n\\n        print(f'Train loss {train_loss} Train accuracy {train_acc}')\\n        log_training(model_save_path,f'Train loss {train_loss} Train accuracy {train_acc} \\\\n')\\n\\n        val_acc, val_loss = eval_model(\\n            model,\\n            val_data_loader, \\n            device, \\n            len(df_val),\\n            max_len = max_seq_length,\\n            batch_size = batch_size\\n        )\\n\\n        print(f'Val loss {val_loss} Val accuracy {val_acc}')\\n        log_training(model_save_path,f'Val loss {val_loss} Val accuracy {val_acc} \\\\n')\\n\\n        history['train_acc'].append(train_acc)\\n        history['train_loss'].append(train_loss)\\n        history['val_acc'].append(val_acc)\\n        history['val_loss'].append(val_loss)\\n        if val_acc > best_accuracy:\\n            # torch.save(model.state_dict(), os.path.join(model_save_path, str(model_name) + '.bin'))\\n            model.save_pretrained(model_save_path)\\n            tokenizer.save_pretrained(model_save_path)\\n            model_details.update({'history': history, 'epoch': epoch})\\n            model_details(model_save_path,model_name, 'modify',{'history': history, 'epoch': epoch, 'status': f'Trained {epoch}', 'accuracy': val_acc})\\n            best_accuracy = val_acc\\n\\n    model_details(model_save_path,model_name, 'modify',{'status':'Trained'})\\n    print('MODEL TRAINED')\\n    log_training(model_save_path,'Model Trained \\\\n')\\n\\n""},""category"":""Sentiment analysis"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""batch_size"":8,""max_depth"":""[10,None]"",""test_size"":0.2,""min_samples_split"":[5],""model_type"":[],""__class_heading"":""class"",""bootstrap"":""True"",""n_estimators"":""[500,1000]"",""num_workers"":4,""class_heading"":""class"",""num_labels"":2,""min_samples_leaf"":[1],""test_ratio"":0.1,""text_heading"":""text"",""train_test_ratio"":0.15,""max_features"":""['sqrt','auto']"",""model_name"":""test"",""vect_type"":""None"",""max_seq_length"":512,""text"":""text"",""learning_rate"":""float(3e-5)"",""__text_heading"":""text""}}","DragNDropLite-51"
"DragNDropLite","Core","{""formats"":{""allow_parallel_threads"":""text"",""gpu_memory_limit"":""text"",""gpus"":""text"",""PYTHON_JOB_TEMP"":""text"",""callbacks"":""text"",""backend"":""text""},""classname"":""Ludwig"",""name"":""Ludwig"",""alias"":""Ludwig"",""parentCategory"":""152"",""id"":52,""codeGeneration"":{""requirements"":[""scikit-learn==1.1.3"",""ludwig"",""hummingbird.ml"",""lightgbm==3.3.5""],""imports"":[""import pandas as pd"",""import numpy as np"",""from ludwig.api import LudwigModel""],""script"":""\\ndef Ludwig_<id>(dataset, allow_parallel_threads_param=True, gpu_memory_limit_param=None, gpus_param=None, python_job_temp_param='models/classicmlpoc', callbacks_param=None, backend_param=None):\\n    print(os.getcwd())\\n    modelPath = os.path.join(os.environ['JOB_DIRECTORY'],python_job_temp_param)\\n    if not os.path.exists(modelPath):\\n        os.makedirs(modelPath)\\n    os.chdir(modelPath)\\n    print(os.getcwd())\\n    model = LudwigModel(config=dataset['config'], logging_level=logging.INFO, backend=backend_param, gpus=gpus_param, gpu_memory_limit=gpu_memory_limit_param, allow_parallel_threads=allow_parallel_threads_param, callbacks=callbacks_param)\\n    # Trains the model. This cell might take a few minutes.\\n    train_stats, preprocessed_data, output_directory = model.train(training_set=dataset['train_df'],\\n                                                               test_set=dataset['test_df'])\\n    \\n    print('train_stats', train_stats)\\n    print('preprocessed_data', preprocessed_data)\\n    print('output_directory', output_directory)\\n    return model\\n\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""model""],""attributes"":{""allow_parallel_threads"":""True"",""gpu_memory_limit"":""None"",""gpus"":""None"",""PYTHON_JOB_TEMP"":""models/classicmlpoc"",""callbacks"":""None"",""backend"":""None""}}","DragNDropLite-52"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""select"",""ecd"",""gbm""],""model_type"":""dropValues""},""classname"":""ModelType"",""name"":""ModelType"",""alias"":""ModelType"",""parentCategory"":""152"",""id"":53,""codeGeneration"":{""requirements"":[""scikit-learn"",""ludwig""],""imports"":[""import pandas as pd"",""import numpy as np"",""from ludwig.api import LudwigModel""],""script"":""def ModelType_<id>(model_type_param=[]):\\n    model_details={} \\n    model_type= 'ecd' if model_type_param =='select' else model_type_param\\n    model_details.update({'model_type': model_type})\\n    return model_details\\n\\n""},""category"":""Ludwig"",""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""model_type"":[]}}","DragNDropLite-53"
"DragNDropLite","Core","{""formats"":{""Select target feature"":""text"",""Select the feature required"":""text"",""Enter reference value"":""text""},""classname"":""DataDivision"",""name"":""DataDivision"",""alias"":""DataDivision"",""parentCategory"":""157"",""id"":54,""codeGeneration"":{""requirements"":[""whylogs""],""imports"":[""import pandas as pd"",""import whylogs as why""],""script"":""\\ndef DataDivision_<id>(dataset, select_the_feature_required_param=[], select_target_feature_param=[], enter_reference_value_param=[]):\\n    print('dataset====', dataset)\\n    wine = pd.DataFrame(dataset)\\n    cond_reference = (wine[select_the_feature_required_param]<=11)\\n    wine_reference = wine.loc[cond_reference]\\n\\n    cond_target = (wine[select_the_feature_required_param]>11)\\n    wine_target = wine.loc[cond_target]\\n\\n    \\n    result = why.log(pandas=wine_target)\\n    prof_view = result.view()\\n\\n    result_ref = why.log(pandas=wine_reference)\\n    prof_view_ref = result_ref.view()\\n\\n    return {'prof_view': prof_view, 'prof_view_ref': prof_view_ref}\\n\\n\\n""},""category"":""Data Profiling"",""inputEndpoints"":[""in""],""outputEndpoints"":[""model""],""attributes"":{""Select target feature"":[],""Select the feature required"":[],""Enter reference value"":[]}}","DragNDropLite-54"
"DragNDropLite","Core","{""formats"":{""Generate the Drift Summary"":""checkbox"",""Select the feature required for histogram"":""text""},""classname"":""DriftReport"",""name"":""DriftReport"",""alias"":""DriftReport"",""parentCategory"":""157"",""id"":55,""codeGeneration"":{""requirements"":[""scikit-learn"",""Ipython==7.34.0"",""pybars3"",""matplotlib""],""imports"":[""import html"",""import matplotlib.pyplot as plt"",""import json"",""import logging"",""import os"",""from typing import Any, Dict, List, Optional"",""from IPython.core.display import HTML"",""import whylogs.viz.drift.column_drift_algorithms as column_drift_algorithms"",""from whylogs.api.usage_stats import emit_usage"",""from whylogs.core.configs import SummaryConfig"",""from whylogs.core.constraints import Constraints"",""from whylogs.core.view.dataset_profile_view import DatasetProfileView"",""from whylogs.migration.uncompound import _uncompound_dataset_profile"",""from whylogs.viz.enums.enums import PageSpec, PageSpecEnum"",""from whylogs.viz.utils.frequent_items_calculations import zero_padding_frequent_items"",""from whylogs.viz.utils.html_template_utils import _get_compiled_template"",""from whylogs.viz.utils.profile_viz_calculations import add_feature_statistics"",""from whylogs.viz.utils.profile_viz_calculations import frequent_items_from_view"",""from whylogs.viz.utils.profile_viz_calculations import generate_profile_summary"",""from whylogs.viz.utils.profile_viz_calculations import generate_summaries_with_drift_score"",""from whylogs.viz.utils.profile_viz_calculations import histogram_from_view""],""script"":""class NotebookProfileVisualizerCustom:\\n    '''\\n    Visualize and compare profiles for drift detection, data quality, distribution comparison and feature statistics.\\n    NotebookProfileVisualizer enables visualization features for Jupyter Notebook environments, but also enables\\n    download\\n    of the generated reports as HTML files.\\n    Examples\\n    --------\\n    Create target and reference dataframes:\\n    .. code-block:: python\\n        import pandas as pd\\n        data_target = {\\n            'animal': ['cat', 'hawk', 'snake', 'cat', 'snake', 'cat', 'cat', 'snake', 'hawk','cat'],\\n            'legs': [4, 2, 0, 4, 0, 4, 4, 0, 2, 4],\\n            'weight': [4.3, None, 2.3, 7.8, 3.7, 2.5, 5.5, 3.3, 0.6, 13.3],\\n        }\\n        data_reference = {\\n            'animal': ['hawk', 'hawk', 'snake', 'hawk', 'snake', 'snake', 'cat', 'snake', 'hawk','snake'],\\n            'legs': [2, 2, 0, 2, 0, 0, 4, 0, 2, 0],\\n            'weight': [2.7, None, 1.2, 10.5, 2.2, 4.6, 3.8, 4.7, 0.6, 11.2],\\n        }\\n        target_df = pd.DataFrame(data_target)\\n        reference_df = pd.DataFrame(data_reference)\\n    Log data and create profile views:\\n    .. code-block:: python\\n        import whylogs as why\\n        results = why.log(pandas=target_df)\\n        prof_view = results.view()\\n        results_ref = why.log(pandas=reference_df)\\n        prof_view_ref = results_ref.view()\\n    Log data and create profile views:\\n    .. code-block:: python\\n        import whylogs as why\\n        results = why.log(pandas=target_df)\\n        prof_view = results.view()\\n        results_ref = why.log(pandas=reference_df)\\n        prof_view_ref = results_ref.view()\\n    Instantiate and set profile views:\\n    .. code-block:: python\\n        from whylogs.viz import NotebookProfileVisualizer\\n        visualization = NotebookProfileVisualizer()\\n        visualization.set_profiles(target_profile_view=prof_view,reference_profile_view=prof_view_ref)\\n    '''\\n\\n    _ref_view: Optional[DatasetProfileView]\\n    _target_view: DatasetProfileView\\n    _drift_map: Optional[Dict[str, column_drift_algorithms.ColumnDriftAlgorithm]] = None\\n\\n    @staticmethod\\n    def _display(template: str, page_spec: PageSpec, height: Optional[str]) -> 'HTML':\\n        if not height:\\n            height = page_spec.height\\n        iframe = f'''<div></div><iframe srcdoc='{html.escape(template)}' width=100% height={height}\\n        frameBorder=0></iframe>'''\\n        display = HTML(iframe)\\n        return display\\n\\n    def _display_distribution_chart(\\n        self,\\n        feature_name: str,\\n        difference: bool,\\n        cell_height: Optional[str] = None,\\n        config: Optional[SummaryConfig] = None,\\n    ) -> Optional[HTML]:\\n        if config is None:\\n            config = SummaryConfig()\\n        if difference:\\n            page_spec = PageSpecEnum.DIFFERENCED_CHART.value\\n        else:\\n            page_spec = PageSpecEnum.DISTRIBUTION_CHART.value\\n\\n        template = _get_compiled_template(page_spec.html)\\n        if self._target_view:\\n            target_profile_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n            reference_profile_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n\\n            target_column_profile_view = self._target_view.get_column(feature_name)\\n            if not target_column_profile_view:\\n                raise ValueError('ColumnProfileView for feature {} not found.'.format(feature_name))\\n\\n            target_profile_features[feature_name]['frequentItems'] = frequent_items_from_view(\\n                target_column_profile_view, feature_name, config\\n            )\\n            if self._ref_view:\\n                ref_col_view = self._ref_view.get_column(feature_name)\\n                if not ref_col_view:\\n                    raise ValueError('ColumnProfileView for feature {} not found.'.format(feature_name))\\n\\n                reference_profile_features[feature_name]['frequentItems'] = frequent_items_from_view(\\n                    ref_col_view, feature_name, config\\n                )\\n\\n                (\\n                    target_profile_features[feature_name]['frequentItems'],\\n                    reference_profile_features[feature_name]['frequentItems'],\\n                ) = zero_padding_frequent_items(\\n                    target_feature_items=target_profile_features[feature_name]['frequentItems'],\\n                    reference_feature_items=reference_profile_features[feature_name]['frequentItems'],\\n                )\\n            else:\\n                logger.warning('Reference profile not detected. Plotting only for target feature.')\\n                reference_profile_features[feature_name]['frequentItems'] = [\\n                    {'value': x['value'], 'estimate': 0} for x in target_profile_features[feature_name]['frequentItems']\\n                ]  # Getting the same frequent items categories for target and adding 0 as estimate.\\n            distribution_chart = template(\\n                {\\n                    'profile_from_whylogs': json.dumps(target_profile_features),\\n                    'reference_profile_from_whylogs': json.dumps(reference_profile_features),\\n                }\\n            )\\n            result = self._display(distribution_chart, page_spec, cell_height)\\n            return result\\n\\n        else:\\n            logger.warning('This method has to get at least a target profile, with valid feature title')\\n            return None\\n\\n    def _display_histogram_chart(self, feature_name: str, cell_height: Optional[str] = None) -> Optional[HTML]:\\n        page_spec = PageSpecEnum.DOUBLE_HISTOGRAM.value\\n        template = _get_compiled_template(page_spec.html)\\n        if self._target_view:\\n            target_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n            ref_features: Dict[str, Dict[str, Any]] = {feature_name: {}}\\n\\n            target_col_view = self._target_view.get_column(feature_name)\\n            if not target_col_view:\\n                raise ValueError(f'ColumnProfileView for feature {feature_name} not found.')\\n\\n            target_histogram = histogram_from_view(target_col_view, feature_name)\\n            if self._ref_view:\\n                reference_column_profile_view = self._ref_view.get_column(feature_name)\\n                if not reference_column_profile_view:\\n                    raise ValueError('ColumnProfileView for feature {} not found.'.format(feature_name))\\n                ref_histogram = histogram_from_view(reference_column_profile_view, feature_name)\\n            else:\\n                logger.warning('Reference profile not detected. Plotting only for target feature.')\\n                ref_histogram = target_histogram.copy()\\n                ref_histogram['counts'] = [\\n                    0 for _ in ref_histogram['counts']\\n                ]  # To plot single profile, zero counts for non-existing profile.\\n\\n            ref_features[feature_name]['histogram'] = ref_histogram\\n            target_features[feature_name]['histogram'] = target_histogram\\n            if target_histogram['n'] == 1:\\n                # in the degenerate case when the target is a single value, it will be hidden\\n                # so here we draw a vertical line, using the max (which is the observed value)\\n                target_features[feature_name]['vertical_line'] = target_histogram['max']\\n            histogram_chart = template(\\n                {\\n                    'profile_from_whylogs': json.dumps(target_features),\\n                    'reference_profile_from_whylogs': json.dumps(ref_features),\\n                }\\n            )\\n            values = ref_features[feature_name]['histogram']['bins'][:-1]\\n\\n            # Create histogram\\n            plt.figure(figsize=(12,4))\\n            plt.hist(values, bins=len(ref_features[feature_name]['histogram']['bins'][:-1]), weights=ref_features[feature_name]['histogram']['counts'], range=(ref_features[feature_name]['histogram']['start'], ref_features[feature_name]['histogram']['end']), rwidth=0.9)\\n            plt.xlabel('Value')\\n            plt.ylabel('Frequency')\\n            plt.title('Histogram')\\n            plt.savefig(f'/app/jobs/models/whylogs/{feature_name}_ref.png')\\n\\n            values = target_features[feature_name]['histogram']['bins'][:-1]\\n\\n            # Create histogram\\n            plt.figure(figsize=(12,4))\\n            plt.hist(values, bins=len(target_features[feature_name]['histogram']['bins'][:-1]), weights=target_features[feature_name]['histogram']['counts'], range=(target_features[feature_name]['histogram']['start'], target_features[feature_name]['histogram']['end']), rwidth=0.9)\\n            plt.xlabel('Value')\\n            plt.ylabel('Frequency')\\n            plt.title('Histogram')\\n            plt.savefig(f'/app/jobs/models/whylogs/{feature_name}_target.png')\\n\\n            # with open ('./target_features.json', 'w') as file:\\n            #   json.dump(target_features, file)\\n            # with open ('./ref_features.json', 'w') as file:\\n            #   json.dump(ref_features, file)\\n            # print('target_features', target_features)\\n            # print('ref_features',ref_features)\\n\\n            return self._display(histogram_chart, page_spec, height=cell_height)\\n        else:\\n            logger.warning('This method has to get at least a target profile, with valid feature title')\\n            return None\\n\\n    def add_drift_config(\\n        self, column_names: List[str], algorithm: column_drift_algorithms.ColumnDriftAlgorithm\\n    ) -> None:\\n        '''Add drift configuration.\\n        The algorithms and thresholds added through this method will be used to calculate drift scores in the `summary_drift_report()` method.\\n        If any drift configuration exists, the new configuration will overwrite the standard behavior when appliable.\\n        If a column has multiple configurations defined, the last one defined will be used.\\n        Parameters\\n        ----------\\n        config: DriftConfig, required\\n            Drift configuration.\\n        '''\\n        self._drift_map = {} if not self._drift_map else self._drift_map\\n        if not isinstance(algorithm, column_drift_algorithms.ColumnDriftAlgorithm):\\n            raise ValueError('Algorithm must be of class ColumnDriftAlgorithm.')\\n        if not self._target_view or not self._ref_view:\\n            logger.error('Set target and reference profiles before adding drift configuration.')\\n            raise ValueError\\n        if not algorithm:\\n            raise ValueError('Drift algorithm cannot be None.')\\n        if not column_names:\\n            raise ValueError('Drift configuration must have at least one column name.')\\n        if column_names:\\n            for column_name in column_names:\\n                if column_name not in self._target_view.get_columns().keys():\\n                    raise ValueError(f'Column {column_name} not found in target profile.')\\n                if column_name not in self._target_view.get_columns().keys():\\n                    raise ValueError(f'Column {column_name} not found in reference profile.')\\n        for column_name in column_names:\\n            if column_name in self._drift_map:\\n                logger.warning(f'Overwriting existing drift configuration for column {column_name}.')\\n            self._drift_map[column_name] = algorithm\\n\\n    def set_profiles(\\n        self, target_profile_view: DatasetProfileView, reference_profile_view: Optional[DatasetProfileView] = None\\n    ) -> None:\\n        '''Set profiles for Visualization/Comparison.\\n        Drift calculation is done if both `target_profile` and `reference profile` are passed.\\n        Parameters\\n        ----------\\n        target_profile_view: DatasetProfileView, required\\n            Target profile to visualize.\\n        reference_profile_view: DatasetProfileView, optional\\n            Reference, or baseline, profile to be compared against the target profile.\\n        '''\\n        self._target_view = _uncompound_dataset_profile(target_profile_view) if target_profile_view else None\\n        self._ref_view = _uncompound_dataset_profile(reference_profile_view) if reference_profile_view else None\\n\\n    def profile_summary(self, cell_height: Optional[str] = None) -> HTML:\\n        page_spec = PageSpecEnum.PROFILE_SUMMARY.value\\n        template = _get_compiled_template(page_spec.html)\\n\\n        try:\\n            profile_summary = generate_profile_summary(self._target_view, config=None)\\n            rendered_template = template(profile_summary)\\n            return self._display(rendered_template, page_spec, cell_height)\\n        except ValueError as e:\\n            logger.error('This method has to get target Dataset Profile View')\\n            raise e\\n\\n    def summary_drift_report(self, height: Optional[str] = None) -> HTML:\\n        '''Generate drift report between target and reference profiles.\\n        KS is calculated if distribution metrics exists for said column.\\n        If not, Chi2 is calculated if frequent items, cardinality and count metric exists. If not, then no drift value is associated to the column.\\n        If feature is missing from any profile, it will not be included in the report.\\n        Both target_profile_view and reference_profile_view must be set previously with `set_profiles`.\\n        If custom drift behavior is desired, use `add_drift_config` before calling this method.\\n        Parameters\\n        ----------\\n        height: str, optional\\n            Preferred height, in pixels, for in-notebook visualization. Example:\\n            `'1000px'`. (Default is None)\\n        Returns\\n        -------\\n        HTML\\n            HTML Page of the given plot.\\n        Examples\\n        --------\\n        Generate Summary Drift Report (after setting profiles with `set_profiles`):\\n        .. code-block:: python\\n            from whylogs.viz.drift.column_drift_algorithms import Hellinger, ChiSquare\\n            from whylogs.viz import NotebookProfileVisualizer\\n            visualization = NotebookProfileVisualizer()\\n            visualization.set_profiles(target_profile_view=target_view, reference_profile_view=ref_view)\\n            visualization.add_drift_config(column_names=['weight'], algorithm=Hellinger())\\n            visualization.add_drift_config(column_names=['legs'], algorithm=ChiSquare())\\n            visualization.summary_drift_report()\\n        '''\\n        if not self._target_view or not self._ref_view:\\n            logger.error('This method has to get both target and reference profiles')\\n            raise ValueError\\n        page_spec = PageSpecEnum.SUMMARY_REPORT.value\\n        template = _get_compiled_template(page_spec.html)\\n\\n        profiles_summary = generate_summaries_with_drift_score(\\n            self._target_view, self._ref_view, config=None, drift_map=self._drift_map\\n        )\\n        rendered_template = template(profiles_summary)\\n        summary_drift_report = self._display(rendered_template, page_spec, height)\\n        return summary_drift_report\\n\\n    def double_histogram(self, feature_name: str, cell_height: Optional[str] = None) -> HTML:\\n        '''Plot overlayed histograms for specified feature present in both `target_profile` and `reference_profile`.\\n        Applicable to numerical features only.\\n        If reference profile was not set, `double_histogram` will plot single histogram for target profile.\\n        Parameters\\n        ----------\\n        feature_name: str\\n            Name of the feature to generate histograms.\\n        cell_height: str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            `'1000px'`. (Default is None)\\n        Examples\\n        --------\\n        Generate double histogram plot for feature named `weight` (after setting profiles with `set_profiles`)\\n        .. code-block:: python\\n            visualization.double_histogram(feature_name='weight')\\n        '''\\n        double_histogram = self._display_histogram_chart(feature_name, cell_height)\\n        return double_histogram\\n\\n    def distribution_chart(self, feature_name: str, cell_height: Optional[str] = None) -> HTML:\\n        '''Plot overlayed distribution charts for specified feature between two profiles.\\n        Applicable to categorical features.\\n        If reference profile was not set, `distribution_chart` will plot single chart for target profile.\\n        Parameters\\n        ----------\\n        feature_name : str\\n            Name of the feature to plot chart.\\n        cell_height : str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            `cell_height='1000px'`. (Default is None)\\n        Returns\\n        -------\\n        HTML\\n            HTML Page of the given plot.\\n        Examples\\n        --------\\n        Generate distribution chart for `animal` feature (after setting profiles with `set_profiles`):\\n        .. code-block:: python\\n            visualization.distribution_chart(feature_name='animal')\\n        '''\\n        difference = False\\n        distribution_chart = self._display_distribution_chart(feature_name, difference, cell_height)\\n        return distribution_chart\\n\\n    def difference_distribution_chart(self, feature_name: str, cell_height: Optional[str] = None) -> HTML:\\n        '''Plot overlayed distribution charts of differences between the categories of both profiles.\\n        Applicable to categorical features.\\n        Parameters\\n        ----------\\n        feature_name : str\\n            Name of the feature to plot chart.\\n        cell_height : str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            `cell_height='1000px'`. (Default is None)\\n        Returns\\n        -------\\n        HTML\\n            HTML Page of the given plot.\\n        Examples\\n        --------\\n        Generate Difference Distribution Chart for feature named 'animal':\\n        .. code-block:: python\\n            visualization.difference_distribution_chart(feature_name='animal')\\n        '''\\n        difference = True\\n        difference_distribution_chart = self._display_distribution_chart(feature_name, difference, cell_height)\\n        return difference_distribution_chart\\n\\n    def constraints_report(self, constraints: Constraints, cell_height: Optional[str] = None) -> HTML:\\n        page_spec = PageSpecEnum.CONSTRAINTS_REPORT.value\\n        template = _get_compiled_template(page_spec.html)\\n        rendered_template = template(\\n            {'constraints_report': json.dumps(constraints.generate_constraints_report(with_summary=True))}\\n        )\\n        constraints_report = self._display(rendered_template, page_spec, cell_height)\\n        return constraints_report\\n\\n    def feature_statistics(\\n        self, feature_name: str, profile: str = 'reference', cell_height: Optional[str] = None\\n    ) -> HTML:\\n        '''\\n        Generate a report for the main statistics of specified feature, for a given profile (target or reference).\\n        Statistics include overall metrics such as distinct and missing values, as well as quantile and descriptive\\n        statistics.\\n        If `profile` is not passed, the default is the reference profile.\\n        Parameters\\n        ----------\\n        feature_name: str\\n            Name of the feature to generate histograms.\\n        profile: str\\n            Profile to be used to generate the report. (Default is `reference`)\\n        cell_height: str, optional\\n            Preferred cell height, in pixels, for in-notebook visualization. Example:\\n            `cell_height='1000px'`. (Default is None)\\n        Examples\\n        --------\\n        Generate Difference Distribution Chart for feature named 'weight', for target profile:\\n        .. code-block:: python\\n            visualization.feature_statistics(feature_name='weight', profile='target')\\n        '''\\n        page_spec = PageSpecEnum.FEATURE_STATISTICS.value\\n        template = _get_compiled_template(page_spec.html)\\n        if self._ref_view and profile.lower() == 'reference':\\n            selected_profile_column = self._ref_view.get_column(feature_name)\\n        else:\\n            selected_profile_column = self._target_view.get_column(feature_name)\\n\\n        rendered_template = template(\\n            {\\n                'profile_feature_statistics_from_whylogs': json.dumps(\\n                    add_feature_statistics(feature_name, selected_profile_column)\\n                )\\n            }\\n        )\\n        feature_statistics = self._display(rendered_template, page_spec, cell_height)\\n        return feature_statistics\\n\\n    @staticmethod\\n    def write(\\n        rendered_html: HTML,\\n        preferred_path: Optional[str] = None,  # type: ignore\\n        html_file_name: Optional[str] = None,  # type: ignore\\n    ) -> None:\\n        '''Create HTML file for a given report.\\n        Parameters\\n        ----------\\n        rendered_html: HTML, optional\\n            Rendered HTML returned by a given report.\\n        preferred_path: str, optional\\n            Preferred path to write the HTML file.\\n        html_file_name: str, optional\\n            Name for the created HTML file. If none is passed, created HTML will be named `ProfileVisualizer.html`\\n        Examples\\n        --------\\n        Dowloads an HTML page named `test.html` into the current working directory, with feature statistics for `weight` feature for the target profile.\\n        .. code-block:: python\\n            import os\\n            visualization.write(\\n                rendered_html=visualization.feature_statistics(feature_name='weight', profile='target'),\\n                html_file_name=os.getcwd() + '/test',\\n            )\\n        '''\\n        if not html_file_name:\\n            html_file_name = 'ProfileVisualizer'\\n        if preferred_path:\\n            full_path = os.path.join(os.path.expanduser(preferred_path), str(html_file_name) + '.html')\\n        else:\\n            full_path = os.path.join(os.pardir, 'html_reports', str(html_file_name) + '.html')\\n\\n        with open(os.path.abspath(full_path), 'w') as saved_html:\\n            saved_html.write(rendered_html.data)\\n\\n\\ndef DriftReport_<id>(feature_name, generate_the_drift_summary_param=[], select_the_feature_required_for_histogram_param=[]):\\n    visualization = NotebookProfileVisualizerCustom()\\n    visualization.set_profiles(target_profile_view=feature_name['prof_view'], reference_profile_view=feature_name['prof_view_ref'])\\n    # visualization.summary_drift_report()\\n    visualization.double_histogram(feature_name=select_the_feature_required_for_histogram_param)\\n    return {'target': f'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_target.png', 'ref': f'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_ref.png'},{'target': f'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_target.png', 'ref': f'/app/jobs/models/whylogs/{select_the_feature_required_for_histogram_param}_ref.png'}\\n\\n""},""category"":""Data Profiling"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out"",""out""],""attributes"":{""Generate the Drift Summary"":[],""Select the feature required for histogram"":[]}}","DragNDropLite-55"
"DragNDropLite","Core","{""formats"":{""encoder_reduce_output"":""text"",""preprocessing_word_tokenizer"":""text"",""level"":""text"",""name"":""text"",""preprocessing_tokenizer"":""text"",""type"":""text"",""encoder_trainable"":""text"",""preprocessing_missing_value_strategy"":""text"",""encoder_type"":""text""},""classname"":""InputFeatures"",""name"":""InputFeatures"",""alias"":""InputFeatures"",""parentCategory"":""152"",""id"":56,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""\\ndef InputFeatures_<id>(model_details, encoder_reduce_output_param=None, preprocessing_word_tokenizer_param='space', level_param='word', name_param='title', preprocessing_tokenizer_param='space', type_param='text', preprocessing_missing_value_strategy_param='fill_with_mean', encoder_trainable_param=True, encoder_type_param='bert'):\\n    name=name_param\\n    type=type_param\\n    level=level_param\\n    word_tokenizer= preprocessing_word_tokenizer_param\\n    missing_value_strategy=preprocessing_missing_value_strategy_param\\n    tokenizer=preprocessing_tokenizer_param\\n    encoder_type=encoder_type_param\\n    reduce_output=encoder_reduce_output_param\\n    trainable=encoder_trainable_param\\n    input_features=[{\\n        'name': name,\\n        'type': type,\\n        'level':  level,\\n        'preprocessing':{\\n            'word_tokenizer': word_tokenizer,\\n            # 'missing_value_strategy': missing_value_strategy,\\n            'tokenizer': tokenizer,\\n        },\\n        'encoder':{\\n            'type':encoder_type,\\n            'reduce_output':reduce_output,\\n            'trainable':trainable\\n        }\\n    }]\\n\\n    model_details.update({'input_features':input_features})\\n    return model_details\\n \\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""encoder_reduce_output"":""None"",""preprocessing_word_tokenizer"":""space"",""level"":""word"",""name"":""title"",""preprocessing_tokenizer"":""space"",""type"":""text"",""encoder_trainable"":""True"",""preprocessing_missing_value_strategy"":""fill_with_mean"",""encoder_type"":""bert""}}","DragNDropLite-56"
"DragNDropLite","Core","{""formats"":{""decoder_type"":""text"",""decoder_output_size"":""text"",""dependencies_coarse_class"":""text"",""name"":""text"",""decoder_cell_type"":""text"",""decoder_num_fc_layers"":""text"",""type"":""text"",""loss_weight"":""text"",""decoder_num_layers"":""text"",""decoder_max_sequence_length"":""text""},""classname"":""OutputFeatures"",""name"":""OutputFeatures"",""alias"":""OutputFeatures"",""parentCategory"":""152"",""id"":57,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""\\ndef OutputFeatures_<id>(model_details, decoder_type_param='generator', decoder_output_size_param=64, dependencies_coarse_class_param='coarse_class', name_param='class', decoder_cell_type_param='lstm', decoder_num_fc_layers_param=2, type_param='category', decoder_num_layers_param=2, loss_weight_param=1.5, decoder_max_sequence_length_param=256):\\n\\n    name=name_param\\n    type=type_param\\n    decoder_type=decoder_type_param\\n    decoder_cell_type=decoder_cell_type_param\\n    decoder_num_layers=decoder_num_layers_param\\n    decoder_max_sequence_length=decoder_max_sequence_length_param\\n    decoder_num_fc_layers= decoder_num_fc_layers_param\\n    decoder_output_size=decoder_output_size_param\\n    loss_weight =loss_weight_param\\n    dependencies_coarse_class=dependencies_coarse_class_param\\n    output_features=[{\\n        'name':name,\\n        'type': type,\\n        # 'decoder': {\\n        #     'type': decoder_type,\\n        #     'cell_type': decoder_cell_type,\\n        #     'num_layers': decoder_num_layers,\\n        #     'max_sequence_length': decoder_max_sequence_length\\n        #     # 'num_fc_layers': decoder_num_fc_layers,\\n        #     # 'output_size': decoder_output_size\\n        # },\\n        'loss': {\\n            'weight': loss_weight\\n        },\\n        # 'dependencies': [\\n           # dependencies_coarse_class\\n        # ]\\n    }]\\n\\n    model_details.update({'output_features':output_features})\\n    return model_details\\n\\n\\n \\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""decoder_type"":""generator"",""decoder_output_size"":64,""dependencies_coarse_class"":""coarse_class"",""name"":""class"",""decoder_cell_type"":""lstm"",""decoder_num_fc_layers"":2,""type"":""category"",""loss_weight"":1.5,""decoder_num_layers"":2,""decoder_max_sequence_length"":256}}","DragNDropLite-57"
"DragNDropLite","Core","{""formats"":{""transformer_output_size"":""text"",""hidden_size"":""text"",""num_heads"":""text"",""bn_virtual_bs"":""text"",""weights_initializer"":""text"",""residual"":""text"",""sparsity"":""text"",""num_fc_layers"":""text"",""num_total_blocks"":""text"",""type"":""text"",""norm"":""text"",""num_steps"":""text"",""entmax_alpha"":""text"",""relaxation_factor"":""text"",""entmax_mode"":""text"",""num_layers"":""text"",""bn_momentum"":""text"",""reduce_output"":""text"",""main_sequence_feature"":""text"",""entity_1"":""text"",""entity_2"":""text"",""output_size"":""text"",""fc_layers"":""text"",""fc_activation"":""text"",""bias_initializer"":""text"",""use_bias"":""text"",""num_shared_blocks"":""text"",""fc_dropout"":""text"",""dropout"":""text"",""norm_params"":""text"",""bn_epsilon"":""text"",""flatten_inputs"":""text"",""activation"":""text"",""fc_residual"":""text"",""embed_input_feature_name"":""text""},""classname"":""Combiner"",""name"":""Combiner"",""alias"":""Combiner"",""parentCategory"":""152"",""id"":58,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""transformer_output_size"":256,""hidden_size"":256,""num_heads"":8,""bn_virtual_bs"":1024,""weights_initializer"":""xavier_uniform"",""residual"":false,""sparsity"":0.0001,""num_fc_layers"":0,""num_total_blocks"":4,""type"":""concat"",""norm"":null,""num_steps"":3,""entmax_alpha"":1.5,""relaxation_factor"":1.5,""entmax_mode"":""sparsemax"",""num_layers"":1,""bn_momentum"":0.05,""reduce_output"":""concat"",""main_sequence_feature"":null,""entity_1"":null,""entity_2"":null,""output_size"":256,""fc_layers"":null,""fc_activation"":""relu"",""bias_initializer"":""zeros"",""use_bias"":true,""num_shared_blocks"":2,""fc_dropout"":0,""dropout"":0,""norm_params"":null,""bn_epsilon"":0.001,""flatten_inputs"":false,""activation"":""relu"",""fc_residual"":false,""embed_input_feature_name"":null}}","DragNDropLite-58"
"DragNDropLite","Core","{""formats"":{""split_type"":""dropValues3"",""dropValues3"":[""select"",""random"",""fixed"",""stratify"",""hash""],""split_column"":""text"",""sample_ratio"":""text"",""oversample_minority"":""text"",""undersample_majority"":""text"",""split_probabilities"":""text""},""classname"":""LudwigPreprocessing"",""name"":""LudwigPreprocessing"",""alias"":""LudwigPreprocessing"",""parentCategory"":""152"",""id"":59,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""\\ndef LudwigPreprocessing_<id>(model_details, split_type_param=[], split_column_param='split', sample_ratio_param=1, oversample_minority_param=0.5, undersample_majority_param=0.3, split_probabilities_param=0.7):\\n\\n    sample_ratio = sample_ratio_param\\n    sample_ratio =oversample_minority_param\\n    undersample_majority =undersample_majority_param\\n    split_type = 'random' if split_type_param=='select' else split_type_param\\n    split_probabilities=split_probabilities_param\\n    split_column =split_column_param\\n\\n    preprocessing= {\\n        'sample_ratio': sample_ratio,\\n        'oversample_minority':sample_ratio,\\n        'undersample_majority':undersample_majority,\\n        'split':{\\n            'type': split_type,\\n            'probabilities':split_probabilities,\\n            'column': split_column\\n        }\\n       \\n    },\\n    model_details.update({'preprocessing':preprocessing})\\n    return model_details\\n \\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""split_type"":[],""dropValues3"":"""",""split_column"":""split"",""sample_ratio"":1,""oversample_minority"":0.5,""undersample_majority"":0.3,""split_probabilities"":0.7}}","DragNDropLite-59"
"DragNDropLite","Core","{""formats"":{""text_loss_type"":""text"",""text_decoder_embedding_size"":""text"",""text_encoder_num_filters"":""text"",""text_encoder_embedding_size"":""text"",""text_decoder_type"":""text"",""text_decoder_bias_initializer"":""text"",""text_loss_confidence_penalty"":""text"",""text_encoder_type"":""text"",""text_preprocessing_most_common"":""text""},""classname"":""Defaults"",""name"":""Defaults"",""alias"":""Defaults"",""parentCategory"":""152"",""id"":60,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""def Defaults_<id>(model_details, text_decoder_embedding_size_param=128, text_loss_type_param='softmax_cross_entropy', text_encoder_embedding_size_param=128, text_encoder_num_filters_param=512, text_decoder_type_param='generator', text_decoder_bias_initializer_param='he_normal', text_loss_confidence_penalty_param=0.1, text_preprocessing_most_common_param=10000, text_encoder_type_param='stacked_cnn'):\\n\\n    text_preprocessing_most_common=text_preprocessing_most_common_param\\n    text_encoder_type=text_encoder_type_param\\n    text_encoder_embedding_size=text_encoder_embedding_size_param\\n    text_encoder_num_filters=text_encoder_num_filters_param\\n    text_decoder_type=text_decoder_type_param\\n    text_decoder_embedding_size=text_decoder_embedding_size_param\\n    text_decoder_bias_initializer='<text_decoder_num_filters>'\\n    text_loss_type=text_loss_type_param\\n    text_loss_confidence_penalty=text_loss_confidence_penalty_param\\n        \\n    defaults={\\n       'defaults':{\\n            'text':{\\n                    'preprocessing':{ \\n                            'most_common': text_preprocessing_most_common\\n                    },\\n                    \\n                    'encoder':{\\n                            'type': text_encoder_type,\\n                            'embedding_size': text_encoder_embedding_size,\\n                            'num_filters': text_encoder_num_filters\\n                    },\\n                        \\n                    'decoder':{\\n                            'type':text_decoder_type,\\n                            'embedding_size': text_decoder_embedding_size,\\n                            'bias_initializer': text_decoder_bias_initializer\\n                    },\\n                        \\n                    'loss':{\\n                            'type': text_loss_type,\\n                            'confidence_penalty':text_loss_confidence_penalty\\n                    }\\n            }\\n        }\\n    }\\n\\n    model_details.update({'defaults':defaults})\\n    return model_details\\n\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""text_loss_type"":""softmax_cross_entropy"",""text_decoder_embedding_size"":128,""text_encoder_num_filters"":512,""text_encoder_embedding_size"":128,""text_decoder_type"":""generator"",""text_decoder_bias_initializer"":""he_normal"",""text_loss_confidence_penalty"":0.1,""text_encoder_type"":""stacked_cnn"",""text_preprocessing_most_common"":10000}}","DragNDropLite-60"
"DragNDropLite","Core","{""formats"":{""use_mixed_precision"":""text"",""increase_batch_size_eval_split"":""text"",""increase_batch_size_on_plateau_patience"":""text"",""train_steps"":null,""learning_rate_scheduler_staircase"":""text"",""learning_rate_scheduler_decay_rate"":""text"",""learning_rate_scheduler_decay_steps"":""text"",""gradient_clipping_clipglobalnorm"":""text"",""validation_metric"":""text"",""optimizer_weight_decay"":""text"",""optimizer_amsgrad"":""text"",""optimizer_eps"":""text"",""learning_rate_scheduler_warmup_fraction"":""text"",""learning_rate_scheduler_warmup_evaluations"":""text"",""should_shuffle"":""text"",""bucketing_field"":""text"",""evaluate_training_set"":""text"",""max_batch_size"":""text"",""increase_batch_size_eval_metric"":""text"",""regularization_lambda"":""text"",""increase_batch_size_on_plateau"":""text"",""batch_size"":""text"",""learning_rate_scheduler_reduce_eval_split"":""text"",""early_stop"":""text"",""optimizer_betas"":""text"",""learning_rate_scheduler_reduce_on_plateau_patience"":""text"",""learning_rate_scheduler_reduce_on_plateau"":""text"",""eval_batch_size"":""text"",""increase_batch_size_on_plateau_rate"":""text"",""learning_rate_scaling"":""text"",""steps_per_checkpoint"":""text"",""gradient_clipping_clipvalue"":""text"",""learning_rate_scheduler_reduce_eval_metric"":""text"",""regularization_type"":""text"",""checkpoints_per_epoch"":""text"",""optimizer_type"":""text"",""validation_field"":""text"",""epochs"":""text"",""learning_rate"":""text"",""gradient_clipping_clipnorm"":""text"",""learning_rate_scheduler_decay"":""text"",""learning_rate_scheduler_reduce_on_plateau_rate"":""text""},""classname"":""Trainer"",""name"":""Trainer"",""alias"":""Trainer"",""parentCategory"":""152"",""id"":61,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""def Trainer_<id>(model_details, use_mixed_precision_param=False, increase_batch_size_eval_split_param='training', increase_batch_size_on_plateau_patience_param=5, train_steps_param=None, learning_rate_scheduler_staircase_param=False, learning_rate_scheduler_decay_rate_param=0.96, learning_rate_scheduler_decay_steps_param=10000, gradient_clipping_clipglobalnorm_param=0.5, optimizer_weight_decay_param=0, validation_metric_param=None, optimizer_amsgrad_param=False, optimizer_eps_param=1e-08, learning_rate_scheduler_warmup_evaluations_param=0, learning_rate_scheduler_warmup_fraction_param=0, should_shuffle_param=True, evaluate_training_set_param=False, bucketing_field_param=None, max_batch_size_param=1099511627776, increase_batch_size_eval_metric_param='loss', regularization_lambda_param=0, increase_batch_size_on_plateau_param=0, batch_size_param='auto', learning_rate_scheduler_reduce_eval_split_param='training', early_stop_param=5, optimizer_betas_param=[0.9, 0.999], learning_rate_scheduler_reduce_on_plateau_patience_param=10, learning_rate_scheduler_reduce_on_plateau_param=0, eval_batch_size_param=None, increase_batch_size_on_plateau_rate_param=2, learning_rate_scaling_param='linear', steps_per_checkpoint_param=0, gradient_clipping_clipvalue_param=None, learning_rate_scheduler_reduce_eval_metric_param='loss', regularization_type_param=l2, checkpoints_per_epoch_param=0, optimizer_type_param='adam', validation_field_param=None, learning_rate_param=0.001, epochs_param=5, gradient_clipping_clipnorm_param=None, learning_rate_scheduler_decay_param=None, learning_rate_scheduler_reduce_on_plateau_rate_param=0.1):\\n    learning_rate=learning_rate_param\\n    epochs=epochs_param\\n    batch_size=batch_size_param\\n    early_stop=early_stop_param\\n    optimizer_type=optimizer_type_param\\n    optimizer_betas=optimizer_betas_param\\n    optimizer_eps=optimizer_eps_param\\n    optimizer_weight_decay=optimizer_weight_decay_param\\n    optimizer_amsgrad=optimizer_amsgrad_param\\n    regularization_type=regularization_type_param\\n    use_mixed_precision=use_mixed_precision_param\\n    checkpoints_per_epoch=checkpoints_per_epoch_param\\n    regularization_lambda=regularization_lambda_param\\n    train_steps=train_steps_param\\n    steps_per_checkpoint=steps_per_checkpoint_param\\n    max_batch_size=max_batch_size_param\\n    eval_batch_size=eval_batch_size_param\\n    evaluate_training_set=evaluate_training_set_param\\n    validation_field=validation_field_param\\n    validation_metric=validation_metric_param\\n    should_shuffle=should_shuffle_param\\n    increase_batch_size_on_plateau=increase_batch_size_on_plateau_param\\n    increase_batch_size_on_plateau_patience=increase_batch_size_on_plateau_patience_param\\n    increase_batch_size_on_plateau_rate=increase_batch_size_on_plateau_rate_param\\n    increase_batch_size_eval_metric=increase_batch_size_eval_metric_param\\n    increase_batch_size_eval_split=increase_batch_size_eval_split_param\\n    gradient_clipping_clipglobalnorm=gradient_clipping_clipglobalnorm_param\\n    gradient_clipping_clipnorm=gradient_clipping_clipnorm_param\\n    gradient_clipping_clipvalue=gradient_clipping_clipvalue_param\\n       \\n    learning_rate_scaling=learning_rate_scaling_param\\n    bucketing_field=bucketing_field_param\\n    learning_rate_scheduler_decay=learning_rate_scheduler_decay_param\\n    learning_rate_scheduler_decay_rate=learning_rate_scheduler_decay_rate_param\\n    learning_rate_scheduler_decay_steps=learning_rate_scheduler_decay_steps_param\\n    learning_rate_scheduler_staircase=learning_rate_scheduler_staircase_param\\n    learning_rate_scheduler_reduce_on_plateau=learning_rate_scheduler_reduce_on_plateau_param\\n    learning_rate_scheduler_reduce_on_plateau_patience=learning_rate_scheduler_reduce_on_plateau_patience_param\\n    learning_rate_scheduler_reduce_on_plateau_rate=learning_rate_scheduler_reduce_on_plateau_rate_param\\n    learning_rate_scheduler_warmup_evaluations=learning_rate_scheduler_warmup_evaluations_param\\n    learning_rate_scheduler_warmup_fraction=learning_rate_scheduler_warmup_fraction_param\\n    learning_rate_scheduler_reduce_eval_metric=learning_rate_scheduler_reduce_eval_metric_param\\n    learning_rate_scheduler_reduce_eval_split=learning_rate_scheduler_reduce_eval_split_param\\n    trainer= {\\n        'learning_rate': learning_rate,\\n        'epochs': epochs,\\n        'batch_size': batch_size,\\n        'early_stop':early_stop,\\n        'optimizer':{\\n            'type': optimizer_type,\\n            'betas':optimizer_betas,\\n            'eps': optimizer_eps,\\n            'weight_decay': optimizer_weight_decay,\\n            'amsgrad': optimizer_amsgrad\\n            },\\n        'regularization_type': regularization_type,\\n        'use_mixed_precision': use_mixed_precision,\\n        'checkpoints_per_epoch':checkpoints_per_epoch,\\n        'regularization_lambda':regularization_lambda,\\n        'train_steps': train_steps,\\n        'steps_per_checkpoint':steps_per_checkpoint ,\\n        'max_batch_size': max_batch_size,\\n        'eval_batch_size':eval_batch_size,\\n        'evaluate_training_set':evaluate_training_set,\\n        'validation_field':validation_field,\\n        'validation_metric': validation_metric,\\n        'should_shuffle':should_shuffle,\\n        'increase_batch_size_on_plateau':increase_batch_size_on_plateau,\\n        'increase_batch_size_on_plateau_patience':increase_batch_size_on_plateau_patience,\\n        'increase_batch_size_on_plateau_rate': increase_batch_size_on_plateau_rate,\\n        'increase_batch_size_eval_metric': increase_batch_size_eval_metric,\\n        'increase_batch_size_eval_split': increase_batch_size_eval_split,\\n        'gradient_clipping':\\n           { 'clipglobalnorm': gradient_clipping_clipglobalnorm,\\n            'clipnorm': gradient_clipping_clipnorm,\\n            'clipvalue': gradient_clipping_clipvalue\\n           },\\n        'learning_rate_scaling':learning_rate_scaling,\\n        'bucketing_field':bucketing_field,\\n        'learning_rate_scheduler':\\n            {'decay': learning_rate_scheduler_decay,\\n            'decay_rate': learning_rate_scheduler_decay_rate,\\n            'decay_steps': learning_rate_scheduler_decay_steps,\\n            'staircase':learning_rate_scheduler_staircase,\\n            'reduce_on_plateau': learning_rate_scheduler_reduce_on_plateau,\\n            'reduce_on_plateau_patience':learning_rate_scheduler_reduce_on_plateau_patience ,\\n            'reduce_on_plateau_rate':learning_rate_scheduler_reduce_on_plateau_rate,\\n            'warmup_evaluations':learning_rate_scheduler_warmup_evaluations,\\n            'warmup_fraction': learning_rate_scheduler_warmup_fraction,\\n            'reduce_eval_metric': learning_rate_scheduler_reduce_eval_metric,\\n            'reduce_eval_split': learning_rate_scheduler_reduce_eval_split\\n           }\\n    }\\n    model_details.update({'trainer':trainer})\\n    return model_details   \\n\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""use_mixed_precision"":false,""increase_batch_size_eval_split"":""training"",""increase_batch_size_on_plateau_patience"":5,""train_steps"":null,""learning_rate_scheduler_staircase"":false,""learning_rate_scheduler_decay_rate"":0.96,""learning_rate_scheduler_decay_steps"":10000,""gradient_clipping_clipglobalnorm"":0.5,""validation_metric"":null,""optimizer_weight_decay"":0,""optimizer_amsgrad"":""False"",""optimizer_eps"":1e-8,""learning_rate_scheduler_warmup_fraction"":0,""learning_rate_scheduler_warmup_evaluations"":0,""should_shuffle"":true,""bucketing_field"":null,""evaluate_training_set"":false,""max_batch_size"":""1099511627776"",""increase_batch_size_eval_metric"":""loss"",""regularization_lambda"":0,""increase_batch_size_on_plateau"":0,""batch_size"":""auto"",""learning_rate_scheduler_reduce_eval_split"":""training"",""early_stop"":5,""optimizer_betas"":[0.9,0.999],""learning_rate_scheduler_reduce_on_plateau_patience"":10,""learning_rate_scheduler_reduce_on_plateau"":0,""eval_batch_size"":null,""increase_batch_size_on_plateau_rate"":2,""learning_rate_scaling"":""linear"",""steps_per_checkpoint"":""0"",""gradient_clipping_clipvalue"":null,""learning_rate_scheduler_reduce_eval_metric"":""loss"",""regularization_type"":""l2"",""checkpoints_per_epoch"":""0"",""optimizer_type"":""adam"",""validation_field"":null,""epochs"":5,""learning_rate"":0.001,""gradient_clipping_clipnorm"":null,""learning_rate_scheduler_decay"":null,""learning_rate_scheduler_reduce_on_plateau_rate"":0.1}}","DragNDropLite-61"
"DragNDropLite","Core","{""formats"":{""neg_bagging_fraction"":""text"",""max_depth"":""text"",""pos_bagging_fraction"":""text"",""cat_smooth"":""text"",""other_rate"":""text"",""xgboost_dart_mode"":""dropValues1"",""feature_pre_filter"":""dropValues1"",""min_data_in_leaf"":""text"",""evaluate_training_set"":""dropValues1"",""lambda_l1"":""text"",""max_delta_step"":""text"",""lambda_l2"":""text"",""add"":""tree"",""dropValues1"":[""select"",""True"",""False""],""dropValues"":[""select"",""gbdt"",""dart""],""dropValues2"":[""select"",""serial"",""feature"",""data"",""voting""],""bagging_freq"":""text"",""drop_seed"":""text"",""feature_fraction_seed"":""text"",""max_drop"":""text"",""cegb_penalty_split"":""text"",""eval_batch_size"":""text"",""cat_l2"":""text"",""tree_learner"":""dropValues2"",""bagging_seed"":""text"",""validation_field"":""text"",""cegb_tradeoff"":""text"",""num_boost_round"":""text"",""extra_trees"":""dropValues1"",""feature_fraction"":""text"",""path_smooth"":""text"",""min_data_per_group"":""text"",""boosting_type"":""dropValues"",""validation_metric"":""text"",""min_sum_hessian_in_leaf"":""text"",""boosting_rounds_per_checkpoint"":""text"",""extra_seed"":""text"",""drop_rate"":""text"",""num_leaves"":""text"",""max_cat_threshold"":""text"",""top_rate"":""text"",""feature_fraction_bynode"":""text"",""early_stop"":""text"",""verbose"":""text"",""min_gain_to_split"":""text"",""uniform_drop"":""dropValues1"",""bagging_fraction"":""text"",""max_bin"":""text"",""linear_lambda"":""text"",""skip_drop"":""text"",""learning_rate"":""text"",""max_cat_to_onehot"":""text""},""classname"":""TrainerGBM"",""name"":""TrainerGBM"",""alias"":""TrainerGBM"",""parentCategory"":""152"",""id"":62,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""def TrainerGBM_<id>(model_details, neg_bagging_fraction_param=1, max_depth_param=18, pos_bagging_fraction_param=1, cat_smooth_param=10, other_rate_param=0.1, xgboost_dart_mode_param=[], feature_pre_filter_param=[], min_data_in_leaf_param=20, evaluate_training_set_param=[], lambda_l1_param=0.25, max_delta_step_param=0, lambda_l2_param=0.2, add_param=[], bagging_freq_param=1, feature_fraction_seed_param=2, drop_seed_param=4, max_drop_param=50, cegb_penalty_split_param=0, eval_batch_size_param=1048576, cat_l2_param=10, tree_learner_param=[], bagging_seed_param=3, validation_field_param=None, cegb_tradeoff_param=1, num_boost_round_param=1000, extra_trees_param=[], feature_fraction_param=0.75, min_data_per_group_param=100, path_smooth_param=0, boosting_type_param=[], validation_metric_param=None, min_sum_hessian_in_leaf_param=0.001, boosting_rounds_per_checkpoint_param=50, extra_seed_param=6, drop_rate_param=0.1, num_leaves_param=82, max_cat_threshold_param=32, top_rate_param=0.2, feature_fraction_bynode_param=1, early_stop_param=5, min_gain_to_split_param=0.03, verbose_param=-1, bagging_fraction_param=0.8, uniform_drop_param=[], max_bin_param=255, linear_lambda_param=0, learning_rate_param=0.03, skip_drop_param=0.5, max_cat_to_onehot_param=4):\\n    \\n    learning_rate=learning_rate_param\\n    early_stop=early_stop_param\\n    max_depth= max_depth_param\\n    boosting_type= 'gbdt' if boosting_type_param=='select' else boosting_type_param   ## gbdt, dart.\\n    bagging_fraction= bagging_fraction_param\\n    feature_fraction= feature_fraction_param\\n    extra_trees= False if extra_trees_param=='select' else bool(extra_trees_param)   \\n    lambda_l1= lambda_l1_param\\n    lambda_l2= lambda_l2_param\\n    drop_rate= drop_rate_param\\n    tree_learner= 'serial' if tree_learner_param=='select' else tree_learner_param   ##serial, feature, data, voting.\\n    boosting_rounds_per_checkpoint= boosting_rounds_per_checkpoint_param\\n    num_boost_round= num_boost_round_param\\n    num_leaves= num_leaves_param\\n    min_data_in_leaf= min_data_in_leaf_param\\n    pos_bagging_fraction= pos_bagging_fraction_param\\n    neg_bagging_fraction= neg_bagging_fraction_param\\n    bagging_freq= bagging_freq_param\\n    bagging_seed= bagging_seed_param\\n    feature_fraction_bynode= feature_fraction_bynode_param\\n    feature_fraction_seed= feature_fraction_seed_param\\n    extra_seed= extra_seed_param\\n    linear_lambda= linear_lambda_param\\n    max_drop= max_drop_param\\n    skip_drop= skip_drop_param\\n    uniform_drop= False if uniform_drop_param=='select' else bool(uniform_drop_param)  \\n    drop_seed= drop_seed_param\\n    eval_batch_size= eval_batch_size_param\\n    evaluate_training_set= False if evaluate_training_set_param=='select' else bool(evaluate_training_set_param)  \\n    validation_field= validation_field_param\\n    validation_metric= validation_metric_param\\n    min_sum_hessian_in_leaf= min_sum_hessian_in_leaf_param\\n    max_delta_step= max_delta_step_param\\n    min_gain_to_split= min_gain_to_split_param\\n    xgboost_dart_mode=False if xgboost_dart_mode_param=='select' else bool(xgboost_dart_mode_param)   \\n    top_rate= top_rate_param\\n    other_rate= other_rate_param\\n    min_data_per_group= min_data_per_group_param\\n    max_cat_threshold= max_cat_threshold_param\\n    cat_l2= cat_l2_param\\n    cat_smooth= cat_smooth_param\\n    max_cat_to_onehot= max_cat_to_onehot_param\\n    cegb_tradeoff= cegb_tradeoff_param\\n    cegb_penalty_split= cegb_penalty_split_param\\n    path_smooth= path_smooth_param\\n    verbose= verbose_param\\n    max_bin= max_bin_param\\n    feature_pre_filter= True if feature_pre_filter_param=='select' else bool(feature_pre_filter_param) \\n     \\n    trainer_GBM = {\\n        'learning_rate': learning_rate,\\n        'early_stop': early_stop,\\n        'max_depth': max_depth,\\n        'boosting_type': boosting_type,\\n        'bagging_fraction': bagging_fraction,\\n        'feature_fraction': feature_fraction,\\n        'extra_trees':extra_trees,\\n        'lambda_l1': lambda_l1,\\n        'lambda_l2': lambda_l2,\\n        'drop_rate': drop_rate,\\n        'tree_learner': tree_learner,\\n        'boosting_rounds_per_checkpoint': boosting_rounds_per_checkpoint,\\n        'num_boost_round': num_boost_round,\\n        'num_leaves': num_leaves,\\n        'min_data_in_leaf': min_data_in_leaf,\\n        'pos_bagging_fraction':pos_bagging_fraction,\\n        'neg_bagging_fraction': neg_bagging_fraction,\\n        'bagging_freq': bagging_freq,\\n        'bagging_seed': bagging_seed,\\n        'feature_fraction_bynode': feature_fraction_bynode,\\n        'feature_fraction_seed': feature_fraction_seed,\\n        'extra_seed': extra_seed,\\n        'linear_lambda': linear_lambda,\\n        'max_drop': max_drop,\\n        'skip_drop':skip_drop,\\n        'uniform_drop': uniform_drop,\\n        'drop_seed': drop_seed,\\n        'eval_batch_size': eval_batch_size,\\n        'evaluate_training_set': evaluate_training_set,\\n        'validation_field': validation_field,\\n        'validation_metric': validation_metric,\\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\\n        'max_delta_step': max_delta_step,\\n        'min_gain_to_split': min_gain_to_split,\\n        'xgboost_dart_mode':xgboost_dart_mode,\\n        'top_rate':top_rate,\\n        'other_rate': other_rate,\\n        'min_data_per_group': min_data_per_group,\\n        'max_cat_threshold':max_cat_threshold,\\n        'cat_l2': cat_l2,\\n        'cat_smooth': cat_smooth,\\n        'max_cat_to_onehot': max_cat_to_onehot,\\n        'cegb_tradeoff': cegb_tradeoff,\\n        'cegb_penalty_split':cegb_penalty_split,\\n        'path_smooth': path_smooth,\\n        'verbose': verbose,\\n        'max_bin': max_bin,\\n        'feature_pre_filter': feature_pre_filter\\n    }\\n    model_details.update({'trainer':trainer_GBM})\\n    return model_details\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""neg_bagging_fraction"":1,""max_depth"":18,""pos_bagging_fraction"":1,""cat_smooth"":10,""other_rate"":0.1,""xgboost_dart_mode"":[],""feature_pre_filter"":[],""min_data_in_leaf"":20,""evaluate_training_set"":[],""lambda_l1"":0.25,""max_delta_step"":0,""lambda_l2"":0.2,""add"":[],""dropValues1"":"""",""dropValues"":"""",""dropValues2"":"""",""bagging_freq"":1,""drop_seed"":4,""feature_fraction_seed"":2,""max_drop"":50,""cegb_penalty_split"":0,""eval_batch_size"":1048576,""cat_l2"":10,""tree_learner"":[],""bagging_seed"":3,""validation_field"":null,""cegb_tradeoff"":1,""num_boost_round"":1000,""extra_trees"":[],""feature_fraction"":0.75,""path_smooth"":0,""min_data_per_group"":100,""boosting_type"":[],""validation_metric"":null,""min_sum_hessian_in_leaf"":0.001,""boosting_rounds_per_checkpoint"":50,""extra_seed"":6,""drop_rate"":0.1,""num_leaves"":82,""max_cat_threshold"":32,""top_rate"":0.2,""feature_fraction_bynode"":1,""early_stop"":5,""verbose"":-1,""min_gain_to_split"":0.03,""uniform_drop"":[],""bagging_fraction"":0.8,""max_bin"":255,""linear_lambda"":0,""skip_drop"":0.5,""learning_rate"":0.03,""max_cat_to_onehot"":4}}","DragNDropLite-62"
"DragNDropLite","Core","{""formats"":{""parameters_combiner_num_fc_layers_lower"":""text"",""parameters_combiner_num_fc_layers_space"":""text"",""parameters_combiner_num_fc_layers_upper"":""text"",""goal"":""text"",""executor_scheduler_type"":""text"",""parameters_title_encoder_bidirectional_categories"":""text"",""parameters_section_encoder_embedding_size"":""text"",""parameters_title_encoder_bidirectional_space"":""text"",""output_feature"":""text"",""executor_num_samples"":""text"",""parameters_trainer_learning_rate_upper"":""text"",""parameters_trainer_learning_rate_lower"":""text"",""parameters_title_encoder_cell_type_space"":""text"",""executor_type"":""text"",""search_alg_type"":""text"",""parameters_trainer_optimizer_type"":""text"",""split"":""text"",""parameters_title_encoder_cell_type_values"":""text"",""metric"":""text"",""parameters_title_encoder_num_layers"":""text"",""parameters_trainer_learning_rate_space"":""text"",""parameters_preprocessing_text_vocab_size"":""text""},""classname"":""Hyperopt"",""name"":""Hyperopt"",""alias"":""Hyperopt"",""parentCategory"":""152"",""id"":63,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""def Hyperopt_<id>(model_details, parameters_combiner_num_fc_layers_space_param='randint', parameters_combiner_num_fc_layers_lower_param=2, parameters_combiner_num_fc_layers_upper_param=6, goal_param='minimize', parameters_title_encoder_bidirectional_categories_param=True, executor_scheduler_type_param='fifo', parameters_title_encoder_bidirectional_space_param='choice', parameters_section_encoder_embedding_size_param='', output_feature_param='combined', parameters_trainer_learning_rate_upper_param=0.1, executor_num_samples_param='', parameters_trainer_learning_rate_lower_param=0.001, parameters_title_encoder_cell_type_space_param='grid_search', parameters_trainer_optimizer_type_param='', search_alg_type_param='variant_generator', executor_type_param='ray', split_param='validation', parameters_title_encoder_cell_type_values_param='rnn', metric_param='loss', parameters_title_encoder_num_layers_param='', parameters_trainer_learning_rate_space_param='loguniform', parameters_preprocessing_text_vocab_size_param=''):\\n    goal=goal_param\\n    output_feature='<>'\\n    metric=metric_param\\n    split=split_param\\n    parameters_title_encoder_cell_type_space=parameters_title_encoder_cell_type_space_param\\n    parameters_title_encoder_cell_type_values=parameters_title_encoder_cell_type_values_param\\n    parameters_title_encoder_bidirectional_space=parameters_title_encoder_bidirectional_space_param\\n    parameters_title_encoder_bidirectional_categories=parameters_title_encoder_bidirectional_categories_param\\n    parameters_title_encoder_num_layers=parameters_title_encoder_num_layers_param\\n    parameters_combiner_num_fc_layers_space=parameters_combiner_num_fc_layers_space_param\\n    parameters_combiner_num_fc_layers_lower=parameters_combiner_num_fc_layers_lower_param\\n    parameters_combiner_num_fc_layers_upper=parameters_combiner_num_fc_layers_upper_param\\n    parameters_section_encoder_embedding_size=parameters_section_encoder_embedding_size_param\\n    parameters_preprocessing_text_vocab_size=parameters_preprocessing_text_vocab_size_param\\n    parameters_trainer_learning_rate_space=parameters_trainer_learning_rate_space_param \\n    parameters_trainer_learning_rate_lower=parameters_trainer_learning_rate_lower_param\\n    parameters_trainer_learning_rate_upper=parameters_trainer_learning_rate_upper_param\\n    parameters_trainer_optimizer_type=parameters_trainer_optimizer_type_param\\n    search_alg_type=search_alg_type_param\\n    executor_type=executor_type_param\\n    executor_num_samples=executor_num_samples_param\\n    executor_scheduler_type=executor_scheduler_type_param\\n\\n    hyperopt={\\n        'goal':  goal,\\n        'output_feature': output_feature,\\n        'metric': metric,\\n        'split': split,\\n        'parameters':{\\n            'title.encoder.cell_type': { \\n                'space':parameters_title_encoder_cell_type_space,\\n                'values': parameters_title_encoder_cell_type_values\\n            },\\n            'title.encoder.bidirectional':{\\n                'space': parameters_title_encoder_bidirectional_space,\\n                'categories': parameters_title_encoder_bidirectional_categories\\n            },\\n            'title.encoder.num_layers': parameters_title_encoder_num_layers,\\n            'combiner.num_fc_layers': { \\n                'space':parameters_combiner_num_fc_layers_space,\\n                'lower': parameters_combiner_num_fc_layers_lower,\\n                'upper': parameters_combiner_num_fc_layers_upper\\n            },\\n            'section.encoder.embedding_size':parameters_section_encoder_embedding_size,\\n            'preprocessing.text.vocab_size':parameters_preprocessing_text_vocab_size,\\n            'trainer.learning_rate': {\\n                'space': parameters_trainer_learning_rate_space,\\n                'lower': parameters_trainer_learning_rate_lower,\\n                'upper': parameters_trainer_learning_rate_upper\\n            },\\n            'trainer.optimizer.type': parameters_trainer_optimizer_type\\n            \\n        },\\n        'search_alg':{\\n        'type': search_alg_type\\n        },\\n        'executor':{\\n            'type': executor_type,\\n            'num_samples': executor_num_samples,\\n            'scheduler':{\\n            'type': executor_scheduler_type \\n            }\\n        }\\n    }\\n\\n    model_details.update({'hyperopt':hyperopt})\\n    return model_details\\n\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""parameters_combiner_num_fc_layers_lower"":2,""parameters_combiner_num_fc_layers_space"":""randint"",""parameters_combiner_num_fc_layers_upper"":6,""goal"":""minimize"",""executor_scheduler_type"":""fifo"",""parameters_title_encoder_bidirectional_categories"":""True"",""parameters_section_encoder_embedding_size"":"""",""parameters_title_encoder_bidirectional_space"":""choice"",""output_feature"":""combined"",""executor_num_samples"":"""",""parameters_trainer_learning_rate_upper"":0.1,""parameters_trainer_learning_rate_lower"":0.001,""parameters_title_encoder_cell_type_space"":""grid_search"",""executor_type"":""ray"",""search_alg_type"":""variant_generator"",""parameters_trainer_optimizer_type"":"""",""split"":""validation"",""parameters_title_encoder_cell_type_values"":""rnn"",""metric"":""loss"",""parameters_title_encoder_num_layers"":"""",""parameters_trainer_learning_rate_space"":""loguniform"",""parameters_preprocessing_text_vocab_size"":""""}}","DragNDropLite-63"
"DragNDropLite","Core","{""formats"":{""processor_parallelism"":""text"",""processor_type"":""text"",""loader_fully_executed"":""text"",""trainer_resources_per_worker_CPU"":""text"",""trainer_use_gpu"":""text"",""loader_window_size_bytes"":""text"",""trainer_num_workers"":""text"",""type"":""text"",""trainer_strategy"":""text"",""cache_dir"":""text"",""trainer_resources_per_worker_GPU"":""text"",""processor_persist"":""text"",""cache_credentials"":""text""},""classname"":""Backend"",""name"":""Backend"",""alias"":""Backend"",""parentCategory"":""152"",""id"":64,""codeGeneration"":{""requirements"":[""scikit-learn""],""imports"":[""import os"",""import pickle""],""script"":""def Backend_<id>(model_details, processor_parallelism_param=100, processor_type_param='dask', loader_fully_executed_param='false', trainer_resources_per_worker_cpu_param=2, trainer_use_gpu_param=True, loader_window_size_bytes_param=500000000, trainer_num_workers_param=4, type_param='ray', trainer_strategy_param='horovod', cache_dir_param='s3://my_bucket/cache', trainer_resources_per_worker_gpu_param=1, processor_persist_param='true', cache_credentials_param='/home/user/.credentials.json'):\\n    type=type_param\\n    cache_dir=cache_dir_param\\n    cache_credentials=cache_credentials_param\\n    processor_type=processor_type_param\\n    processor_parallelism=processor_parallelism_param\\n    processor_persist=processor_persist_param\\n    trainer_strategy=trainer_strategy_param,\\n    trainer_use_gpu=trainer_use_gpu_param\\n    trainer_num_workers=trainer_num_workers_param\\n    trainer_resources_per_worker_CPU=trainer_resources_per_worker_cpu_param\\n    trainer_resources_per_worker_GPU=trainer_resources_per_worker_gpu_param\\n    loader_fully_executed=loader_fully_executed_param\\n    loader_window_size_bytes=loader_window_size_bytes_param\\n     \\n    backend={\\n        'type': type,\\n        'cache_dir': cache_dir,\\n        'cache_credentials':   cache_credentials,\\n        'processor':{\\n            'type': processor_type,\\n            'parallelism': processor_parallelism,\\n            'persist':  processor_persist\\n        },\\n        'trainer':{\\n            'strategy': trainer_strategy,\\n            'use_gpu':  trainer_use_gpu,\\n            'num_workers': trainer_num_workers,\\n            'resources_per_worker':\\n               { 'CPU': trainer_resources_per_worker_CPU,\\n                 'GPU':  trainer_resources_per_worker_GPU\\n               }\\n        },\\n        'loader': {\\n            'fully_executed': loader_fully_executed,\\n            'window_size_bytes':  loader_window_size_bytes\\n        }\\n    }\\n\\n    model_details.update({'backend':backend})\\n    return model_details\\n\\n""},""category"":""Ludwig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""processor_parallelism"":100,""processor_type"":""dask"",""loader_fully_executed"":""false"",""trainer_resources_per_worker_CPU"":2,""trainer_use_gpu"":true,""loader_window_size_bytes"":500000000,""trainer_num_workers"":4,""type"":""ray"",""trainer_strategy"":""horovod"",""cache_dir"":""s3://my_bucket/cache"",""trainer_resources_per_worker_GPU"":1,""processor_persist"":""true"",""cache_credentials"":""/home/user/.credentials.json""}}","DragNDropLite-64"
"DragNDropLite","Core","{""formats"":{""Split_Ratio"":""text"",""Dataset_Name"":""text""},""classname"":""PreprocessImgClassification_Test"",""name"":""PreprocessImgClassification_Test"",""alias"":""PreprocessImgClassification_Test"",""parentCategory"":""153"",""id"":65,""codeGeneration"":{""requirements"":[],""imports"":[""import numpy as np"",""from ultralytics import YOLO""],""script"":""def PreprocessImgClassification_Test_<id>(data_folder_path, split_ratio_param=0.8, dataset_name_param='Eviction'):\\n    import os\\n    import shutil\\n    import random\\n    import json\\n    import zipfile\\n    import string\\n    import pathlib\\n\\n    # Define the input zip file and json file paths\\n    zip_path =data_folder_path #/app/jobs/docqna/files/Eviction_data.zip\\n    existGDBPath = pathlib.Path(zip_path)\\n    basepath,name = os.path.split(os.path.abspath(existGDBPath))\\n    print (basepath)\\n    json_path = os.path.join(basepath,'Eviction_data.json')\\n    with open(json_path, 'r') as f:\\n        labels = json.load(f)['labelsArray']\\n    print('######################',labels)\\n    zip_file_path = os.path.join(basepath,'Eviction_dataset.zip')\\n    zip_dir = os.path.splitext(zip_file_path)[0] \\n    print('#################### Zip dir Path: ',zip_dir) #/app/jobs/docqna/files/Eviction_Data\\n    print(os.listdir('.'))\\n    if not os.path.exists(zip_dir):\\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\\n                    zip_ref.extractall(zip_dir)\\n    dataset_name=dataset_name_param\\n    split_ratio = split_ratio_param\\n    split_ratio = float(split_ratio)\\n\\n    # Set the root path to 'my_project' directory\\n    root_path='/app/jobs/docqna/files'   #'D:/INFCAT/Legacy_DL/Object Detection'\\n    os.chdir(root_path)\\n    print('#################### root_path location Contents: ',os.listdir('.'))\\n\\n    path = dataset_name\\n    if not os.path.exists(path):\\n        os.makedirs(path)\\n    else:\\n        shutil.rmtree(path)           # Removes all the subdirectories!\\n        os.makedirs(path)\\n\\n    new_root_path=os.path.join(root_path,dataset_name)\\n    os.chdir(new_root_path) # /app/jobs/docqna/files/Eviction\\n    \\n    # Define the output directory paths\\n    output_dir = new_root_path+'/data'\\n    train_dir = os.path.join(output_dir, 'train')\\n    test_dir = os.path.join(output_dir, 'test')\\n\\n    # Create the output directories\\n    if not os.path.exists(train_dir):\\n        os.makedirs(train_dir)\\n    if not os.path.exists(test_dir):\\n        os.makedirs(test_dir)\\n    print('#################### new_root_path location Contents: ',os.listdir('.'))\\n    # Extract the zip file to a new directory with the same name as the zip file\\n    \\n    print('#################### data_dir location Contents: ',os.listdir('./data_dir'))\\n    # Load the labels from the json file\\n    #with open(json_path, 'r') as f:\\n    #    labels = json.load(f)['labelsArray']\\n\\n    # Split the images into train and test directories\\n    for label in set([l['label'] for l in labels]):\\n        label_train_dir = os.path.join(train_dir, label)\\n        label_test_dir = os.path.join(test_dir, label)\\n        if not os.path.exists(label_train_dir):\\n            os.makedirs(label_train_dir)\\n        if not os.path.exists(label_test_dir):\\n            os.makedirs(label_test_dir)\\n        label_images = [l for l in labels if l['label'] == label]\\n        random.shuffle(label_images)\\n        train_count = int(len(label_images) * split_ratio)\\n        for i, l in enumerate(label_images):\\n            src_path = os.path.join(zip_dir,l['filePath'])\\n            if i < train_count:\\n                dst_dir = label_train_dir\\n            else:\\n                dst_dir = label_test_dir\\n            dst_path = os.path.join(dst_dir, os.path.basename(l['filePath']))\\n            shutil.copy(src_path, dst_path)\\n            \\n    return new_root_path\\n\\n""},""category"":""Legacy_AI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""Split_Ratio"":""0.8"",""Dataset_Name"":""Eviction""}}","DragNDropLite-65"
"DragNDropLite","Core","{""formats"":{""image_path"":""text"",""model_path"":""text""},""classname"":""Predict"",""name"":""Predict"",""alias"":""Predict"",""parentCategory"":""154"",""id"":79,""codeGeneration"":{""requirements"":[""ultralytics""],""imports"":[""from ultralytics import YOLO"",""import os"",""import shutil""],""script"":""def Predict_<id>(dataset, image_path_param='/app/jobs/models/predict_image', model_path_param='/app/jobs/models/classicmlpoc/predict_model__yolov8m.pt'):\\n    image_path=image_path_param\\n    model_path=model_path_param\\n    os.chdir('/app/jobs/models/predict_image')\\n    if os.path.exists('runs'):\\n        shutil.rmtree('runs')\\n    model = YOLO(model_path) \\n    results = model(source=image_path, save=True,save_txt=True,save_crop=True) \\n    print('classes=>',results[0].names)\\n    print('speed=>',results[0].speed)\\n    pred_output={}\\n    for result in results: # detection \\n        box_info=[]\\n        for box in result.boxes:\\n            box_cord = box.xyxy[0].tolist() # get box coordinates in (top, left, bottom, right) format\\n            box_class = int(box.cls.tolist()[0])  # get class of box\\n            box_cord.append(result.names[box_class])\\n            box_info.append(box_cord)\\n        pred_output[result.path]=box_info\\n    print(pred_output)\\n    return pred_output\\n\\n\\n\\n""},""category"":""Cheque_detection"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""image_path"":""/app/jobs/models/predict_image"",""model_path"":""/app/jobs/models/classicmlpoc/predict_model__yolov8m.pt""}}","DragNDropLite-79"
"DragNDropLite","Core","{""formats"":{},""classname"":""ModelTrain"",""name"":""ModelTrain"",""alias"":""ModelTrain_"",""parentCategory"":""158"",""id"":81,""codeGeneration"":{""requirements"":[""sklearn""],""imports"":[""from sklearn.linear_model import LogisticRegression""],""script"":""\\ndef ModelTrain_<id>(training_df):\\n    reg = LogisticRegression()\\n    X_train = training_df['X_train']\\n    y_train = training_df['y_train']\\n    reg.fit(X=X_train[sorted(X_train)], y=y_train)\\n    joblib.dump(reg, open('model1.joblib', 'wb'))\\n    reg1=joblib.load('model1.joblib')\\n    print('reg1s', reg1)\\n    # model_dump=dump(value=reg, filename='<Filename>')\\n    return reg\\n\\n\\n\\n\\n""},""category"":""Feast"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-81"
"DragNDropLite","Core","{""formats"":{},""classname"":""IngestFeatureOnline"",""name"":""IngestFeatureOnline"",""alias"":""IngestFeatureOnline"",""parentCategory"":""158"",""id"":83,""codeGeneration"":{""requirements"":[],""imports"":[""from feast import FeatureStore"",""from datetime import datetime""],""script"":""\\ndef IngestFeatureOnline_<id>(dataset):\\n    os.chdir('jobs/models/feature_repo/feature_repo')\\n    store= FeatureStore(repo_path='')\\n    result=store.materialize_incremental(end_date = datetime.now())\\n    return {'result': result}\\n\\n\\n\\n\\n""},""category"":""Feast"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-83"
"DragNDropLite","Core","{""formats"":{""patient_id_param"":""integer"",""Features"":""text""},""classname"":""GetOnlineFeature"",""name"":""GetOnlineFeature"",""alias"":""GetOnlineFeature"",""parentCategory"":""158"",""id"":85,""codeGeneration"":{""requirements"":[],""imports"":[""from feast import FeatureStore"",""import pandas as pd"",""from joblib import load""],""script"":""\\ndef GetOnlineFeature_<id>(result, features_param=[], patient_id_param=[]):\\n    store= FeatureStore(repo_path='')\\n    features = features_param.split(',') # ['glucose','BMI'] # glucose,BMI\\n    features = ['predictors_df_feature_view:' + feature for feature in features]\\n    entity_rows = [{'patient_id': id} for id in patient_id_param]\\n\\n    feature = store.get_online_features(features=features, entity_rows=entity_rows).to_dict()\\n\\n    features_df = pd.DataFrame.from_dict(data=feature)\\n    return {'features_df': features_df}\\n\\n\\n""},""category"":""Feast"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out"",""out""],""attributes"":{""patient_id_param"":[],""Features"":[]}}","DragNDropLite-85"
"DragNDropLite","Core","{""formats"":{""Features"":""text""},""classname"":""InferenceModel"",""name"":""InferenceModel"",""alias"":""InferenceModel"",""parentCategory"":""158"",""id"":86,""codeGeneration"":{""requirements"":[],""imports"":[""import pandas as pd"",""from joblib import load""],""script"":""\\ndef InferenceModel_<id>(path, features_df):\\n    print('before_path',path)\\n    reg = load(path)\\n    predictions = reg.predict(features_df[sorted(features_df.drop('patient_id', axis=1))])\\n    return {'predictions': predictions}\\n\\n\\n""},""category"":""Feast"",""inputEndpoints"":[""dataset1"",""in""],""outputEndpoints"":[""out""],""attributes"":{""Features"":""""}}","DragNDropLite-86"
"DragNDropLite","Core","{""formats"":{},""classname"":""Ocr"",""name"":""Ocr"",""alias"":""Ocr"",""parentCategory"":""154"",""id"":78,""codeGeneration"":{""requirements"":[""easyocr"",""matplotlib"",""os"",""opencv-python""],""imports"":[""import easyocr"",""import matplotlib.pyplot as plt"",""import cv2"",""import os"",""from os import listdir"",""from numpy import asarray"",""from PIL import Image""],""script"":""def Ocr_<id>(dataset):\\n    final_data={}\\n    for image in dataset:\\n        img=Image.open(image)\\n        boxes=dataset[image]\\n        img_data={}\\n        for box in boxes:\\n            area =tuple(box[:4])\\n            cropped_img = img.crop(area)\\n            cropped_img.show()\\n            cropped_img_arr=asarray(cropped_img)\\n            reader=easyocr.Reader(['en'],gpu=False)\\n            text=reader.readtext(cropped_img_arr,paragraph='True')\\n            img_data[box[4]]=text\\n        final_data[image[-16:]]=img_data\\n    print(final_data)\\n    return final_data\\n\\n\\n""},""category"":""Cheque_detection"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-78"
"DragNDropLite","Core","{""formats"":{""overwrite_existing_files"":""text"",""urls"":""list"",""crawler_depth"":""integer"",""id_hash_keys"":""list"",""webdriver_options"":""list"",""output_dir"":""text"",""crawler_naming_function"":""text"",""extract_hidden_text"":""text"",""file_path_meta_field_name"":""text"",""loading_wait_time"":""integer"",""filter_urls"":""list""},""classname"":""Crawler"",""name"":""Crawler"",""alias"":""Crawler"",""parentCategory"":""159"",""id"":95,""codeGeneration"":{""requirements"":[""farm-haystack[all]""],""imports"":[""from haystack.nodes import Crawler""],""script"":""def Crawler_<id>(*compo_op, overwrite_existing_files_param=True, urls_param=None, crawler_depth_param=1, id_hash_keys_param=None, webdriver_options_param=None, output_dir_param=None, crawler_naming_function_param=None, extract_hidden_text_param=True, file_path_meta_field_name_param=None, loading_wait_time_param=None, filter_urls_param=None):\\n    \\n    filter_urls_list = list()\\n    if filter_urls_param != None:\\n        for i in filter_urls_param:\\n            filter_urls_list.append(i['value'])\\n        \\n    urls_list = list()\\n    if urls_param != None:\\n        for i in urls_param:\\n            urls_list.append(i['value'])\\n\\n    id_hash_keys_list = list()\\n    if id_hash_keys_param != None:\\n        for i in id_hash_keys_param:\\n            id_hash_keys_list.append(i['value'])        \\n\\n    webdriver_options_list = list()\\n    if webdriver_options_param != None:\\n        for i in webdriver_options_param:\\n            webdriver_options_list.append(i['value'])        \\n            \\n    crawler = Crawler(\\n                        urls = None if urls_param == None else urls_list,\\n                        crawler_depth = crawler_depth_param,\\n                        filter_urls =  None if filter_urls_param == None else filter_urls_list,\\n                        id_hash_keys = None if id_hash_keys_param == None else id_hash_keys_list,\\n                        extract_hidden_text = extract_hidden_text_param,\\n                        loading_wait_time = None if loading_wait_time_param == None else loading_wait_time_param,\\n                        output_dir = None if output_dir_param == 'None' else output_dir_param,\\n                        overwrite_existing_files = overwrite_existing_files_param,\\n                        file_path_meta_field_name = None if file_path_meta_field_name_param == 'None' else file_path_meta_field_name_param,\\n                        crawler_naming_function = None if crawler_naming_function_param == 'None' else crawler_naming_function_param,\\n                        webdriver_options = None if webdriver_options_param == None else webdriver_options_list\\n                      )\\n    \\n    component_name = crawler.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(crawler, component_name, ['Query'])\\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(crawler, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""overwrite_existing_files"":""True"",""urls"":""None"",""crawler_depth"":1,""id_hash_keys"":""None"",""webdriver_options"":""None"",""output_dir"":""None"",""crawler_naming_function"":""None"",""extract_hidden_text"":""True"",""file_path_meta_field_name"":""None"",""loading_wait_time"":""None"",""filter_urls"":""None""}}","DragNDropLite-95"
"DragNDropLite","Core","{""formats"":{},""classname"":""get_documents"",""name"":""get_documents"",""alias"":""get_documents"",""parentCategory"":""160"",""id"":89,""codeGeneration"":{""requirements"":[],""imports"":[""import os"",""from haystack.document_stores import ElasticsearchDocumentStore""],""script"":""\\ndef get_documents_<id>(document_store, ):\\n    print('Fetching the list of documents')\\n    ELASTIC_INDEX='isaiml_knowledgesearch_index-000001'\\n    return document_store.get_all_documents(index=ELASTIC_INDEX)\\n\\n\\n""},""category"":""RFP_Elastic_API"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-89"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""Outcome""],""Features"":""text"",""LabelColumn"":""dropValues""},""classname"":""GenerateTrainDataset"",""name"":""GenerateTrainDataset"",""alias"":""GenerateTrainDataset"",""parentCategory"":""158"",""id"":84,""codeGeneration"":{""requirements"":[""feast-postgres"",""scikit-learn""],""imports"":[""import os"",""from feast import FeatureStore"",""import pandas as pd"",""from sklearn.model_selection import train_test_split""],""script"":""\\ndef GenerateTrainDataset_<id>(entity_df, features_param=[], labelcolumn_param=[]):\\n    os.chdir('jobs/models/feature_repo/feature_repo')\\n    store= FeatureStore(repo_path='')\\n    features = features_param.split(',') # ['glucose','BMI'] # glucose,BMI\\n    # [{'name': , 'key':, 'randomname':}]\\n    features = ['predictors_df_feature_view:' + feature for feature in features]\\n    #entity_df = pd.read_parquet(path='data/target_df.parquet')\\n    training_df = store.get_historical_features(\\n    entity_df = entity_df,\\n        features = features\\n    ).to_df()\\n\\n    y = training_df[labelcolumn_param]\\n    X = training_df.drop(\\n        labels=['Outcome', 'event_timestamp', 'patient_id'], \\n        axis=1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(X, \\n                                                        y, \\n                                                        stratify=y)\\n    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\\n\\n\\n""},""category"":""Feast"",""inputEndpoints"":[""dataset1""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""Features"":[],""LabelColumn"":[]}}","DragNDropLite-84"
"DragNDropLite","Core","{""formats"":{""query"":""text""},""classname"":""HiveExtractorConfig"",""name"":""TEST"",""alias"":""TEST"",""parentCategory"":""161"",""id"":90,""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""category"":""ExtractorConfig"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""""}}","DragNDropLite-90"
"DragNDropLite","Core","{""formats"":{},""classname"":""get_start"",""name"":""get_start"",""alias"":""get_start"",""parentCategory"":""160"",""id"":91,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def get_start_<id>(, ):\\n    return 'started'\\n\\n""},""category"":""RFP_Elastic_API"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-91"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""fileName"":""text"",""Destination_directory"":""text""},""classname"":""DownloadFileData"",""name"":""DownloadFileData"",""alias"":""DownloadFileData"",""parentCategory"":""154"",""id"":93,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os""],""script"":""\\ndef DownloadFileData_<id>(dataset, bucket_param='Demo', filename_param='train_data', destination_directory_param='models/classicmlpoc'):\\n    import pandas as pd\\n    data={}\\n    print('before',dataset)\\n    data['file_id']=dataset[0][1]\\n    data['file_name']=dataset[0][0]\\n    print('data..',data)\\n    fileId, fileName, bucket = data['file_id'], filename_param, bucket_param\\n    if(fileName!=data['file_name']):\\n        raise Exception('file not found') \\n    \\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    print('FILE_SERVER_URL: ', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    print('FILE_SERVER_TOKEN: ', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ['JOB_DIRECTORY'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + '/api/lastcount/' + fileId + '?bucket=' + bucket,\\n                                           headers={'access-token': FILE_SERVER_TOKEN},\\n                                           proxies={'http': '', 'https': ''})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search('<Integer>(.*?)</Integer>', totalChunksResponse.text).group(1)\\n            totalChunks = int('0')\\n        logger.info('Total Chunks: ' + str(totalChunks + 1))\\n\\n        # create a folder 'chunkfiles'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, 'chunkfiles')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info('dir already exists...')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model's chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            url = (FILE_SERVER_URL + '/api/download/' + fileId + '/') + 'file' + '?bucket=' + bucket\\n            filedata = requests.get(url, headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n            open(chunkfilePath + '/' + str(i), 'wb').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, 'wb') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, 'rb')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n""},""category"":""Cheque_detection"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""bucket"":""Demo"",""fileName"":""train_data"",""Destination_directory"":""models/classicmlpoc""}}","DragNDropLite-93"
"DragNDropLite","Core","{""id"": 94, ""name"": ""ObjDetectTrain"", ""alias"": ""ObjDetectTrain"", ""formats"": {""Batch_Size"": ""Integer"", ""no_of_epochs"": ""Integer"", ""default_yolo_model"": ""text""}, ""category"": ""Cheque_detection"", ""classname"": ""ObjDetectTrain"", ""attributes"": {""Batch_Size"": 4, ""no_of_epochs"": 2, ""default_yolo_model"": ""yolov8n.pt""}, ""codeGeneration"": {""script"": ""def ObjDetectTrain_<id>(yml_path, default_yolo_model_param='yolov8n.pt', batch_size_param=4, no_of_epochs_param=2):\\n    from ultralytics import YOLO\\n    import os\\n    import shutil\\n    \\n    n_epoch= no_of_epochs_param\\n    n_batch= batch_size_param\\n    default_model =default_yolo_model_param\\n    \\n    if os.path.exists('runs'):\\n        shutil.rmtree('runs')\\n    # Load a model\\n    model = YOLO(default_model)  # load a pretrained model (recommended for training)\\n\\n    # Train the model\\n    try:\\n        model.train(data=yml_path, epochs=n_epoch, imgsz=640,batch=n_batch)\\n    except Exception as e:\\n        print('We got error in train model=>',e)\\n\\n    f=os.listdir('runs/detect')\\n\\n    print(f)\\n\\n    for x in f:\\n        if x.startswith('train'):\\n            print(x)\\n            run=x\\n    print('train output is in=>','runs/detect/'+run)\\n\\n    train_folder=os.listdir('runs/detect/'+run)\\n    print('train folder=>',train_folder)\\n\\n    weights=os.listdir('runs/detect/'+run+'/weights')\\n    print('Weights=>',weights)\\n    if len(weights)>0:\\n        weights.sort()\\n        return weights[0]\\n    else:\\n        raise Exception('No Model Found')"", ""imports"": [""from ultralytics import YOLO"", ""import os"", ""import shutil""], ""requirements"": [""protoc""]}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-94"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""fileName"":""text"",""Destination_directory"":""text"",""fileId"":""text""},""classname"":""DownloadFile"",""name"":""DownloadFile"",""alias"":""DownloadFile"",""parentCategory"":""146"",""id"":12,""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os""],""script"":""\\ndef DownloadFile_<id>(bucket_param='Demo', filename_param='your_model_name.pkl', destination_directory_param='models/classicmlpoc', fileid_param=''):\\n    fileId, fileName, bucket = fileid_param, filename_param, bucket_param\\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    print('FILE_SERVER_URL: ', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    print('FILE_SERVER_TOKEN: ', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ['JOB_DIRECTORY'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + '/api/lastcount/' + fileId + '?bucket=' + bucket,\\n                                           headers={'access-token': FILE_SERVER_TOKEN},\\n                                           proxies={'http': '', 'https': ''})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search('<Integer>(.*?)</Integer>', totalChunksResponse.text).group(1)\\n            totalChunks = int('0')\\n        logger.info('Total Chunks: ' + str(totalChunks + 1))\\n\\n        # create a folder 'chunkfiles'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, 'chunkfiles')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info('dir already exists...')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model's chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            url = (FILE_SERVER_URL + '/api/download/' + fileId + '/') + 'file' + '?bucket=' + bucket\\n            filedata = requests.get(url, headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n            open(chunkfilePath + '/' + str(i), 'wb').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, 'wb') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, 'rb')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n""},""category"":""BaseConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""}}","DragNDropLite-12"
"DragNDropLite","Core","{""formats"":{""remove_numeric_tables"":""text"",""id_hash_keys"":""list"",""valid_languages"":""list""},""classname"":""ImageToTextConverter"",""name"":""ImageToTextConverter"",""alias"":""ImageToTextConverter"",""parentCategory"":""159"",""id"":96,""codeGeneration"":{""requirements"":[""farm-haystack[all]"",""faiss-cpu==1.7.3""],""imports"":[""from haystack.nodes import ImageToTextConverter""],""script"":""def ImageToTextConverter_<id>(*compo_op, remove_numeric_tables_param=False, id_hash_keys_param=None, valid_languages_param=None):\\n    \\n    valid_languages_list = list()\\n    if valid_languages_param != None:\\n        for i in valid_languages_param:\\n            valid_languages_list.append(i['value'])\\n\\n    id_hash_keys_list = list()\\n    if id_hash_keys_param != None:\\n        for i in id_hash_keys_param:\\n            id_hash_keys_list.append(i['value'])        \\n             \\n    converter = ImageToTextConverter(\\n                                    remove_numeric_tables = remove_numeric_tables_param, \\n                                    valid_languages = None if valid_languages_param == None else valid_languages_list,\\n                                    id_hash_keys = None if id_hash_keys_param == None else id_hash_keys_list\\n                                    )\\n    \\n    component_name = converter.__class__.__name__\\n    \\n    if index_pipe.components == {}:\\n            index_pipe.add_node(converter, component_name, ['File'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(converter, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""remove_numeric_tables"":""False"",""id_hash_keys"":""None"",""valid_languages"":""None""}}","DragNDropLite-96"
"DragNDropLite","Core","{""formats"":{""use_auth_token"":""text"",""model_kwargs"":""text"",""default_prompt_template"":""text"",""debug"":""text"",""output_variable"":""text"",""api_key"":""text"",""devices"":""text"",""stop_words"":""text"",""use_gpu"":""text"",""top_k"":""integer"",""model_name_or_path"":""text"",""max_length"":""integer""},""classname"":""PromptNode"",""name"":""PromptNode"",""alias"":""PromptNode"",""parentCategory"":""159"",""id"":71,""codeGeneration"":{""requirements"":[""datasets""],""imports"":[""import boto3"",""import os"",""from haystack.nodes import PromptNode""],""script"":""def PromptNode_<id>(*compo_op, use_auth_token_param=None, default_prompt_template_param=None, model_kwargs_param=None, debug_param=False, output_variable_param=None, api_key_param=None, devices_param=None, stop_words_param=None, use_gpu_param=None, top_k_param=1, model_name_or_path_param=google/flan-t5-base, max_length_param=100):\\n    \\n    stop_words_list = list()\\n    if stop_words_param != None:\\n        for i in stop_words_param:\\n            stop_words_list.append(i['value']) \\n            \\n    queries_list = list()\\n    if <queries> != None:\\n        for i in <queries>:\\n            queries_list.append(i['value'])             \\n             \\n    if len(compo_op) > 1:\\n        prompt_node = PromptNode(\\n                                model_name_or_path = 'google/flan-t5-large',\\n                                default_prompt_template = compo_op[1],\\n                                output_variable = None if output_variable_param == 'None' else output_variable_param,\\n                                max_length = max_length_param,\\n                                api_key = None if api_key_param == 'None' else api_key_param,\\n                                use_auth_token =  None if use_auth_token_param == 'None' else use_auth_token_param,\\n                                use_gpu = use_gpu_param,\\n                                stop_words = None if stop_words_param == None else stop_words_list,\\n                                top_k = top_k_param\\n                                )\\n    else:\\n        prompt_node = PromptNode(\\n                                model_name_or_path = 'google/flan-t5-large',\\n                                default_prompt_template = default_prompt_template,\\n                                output_variable = None if output_variable_param == 'None' else output_variable_param,\\n                                max_length = max_length_param,\\n                                api_key = None if api_key_param == 'None' else api_key_param,\\n                                use_auth_token =  None if use_auth_token_param == 'None' else use_auth_token_param,\\n                                use_gpu = use_gpu_param,\\n                                stop_words = None if stop_words_param == None else stop_words_list,\\n                                top_k = top_k_param\\n                                )\\n    \\n    component_name = prompt_node.__class__.__name__\\n    \\n    if index_pipe.components == {}:\\n            index_pipe.add_node(prompt_node, component_name, ['Query'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(prompt_node, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    glob_dict['input_queries'] = queries_list\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_PromptsAndLLMs_Components"",""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[""out""],""attributes"":{""use_auth_token"":""None"",""model_kwargs"":""None"",""default_prompt_template"":""None"",""debug"":""False"",""output_variable"":""None"",""api_key"":""None"",""devices"":""None"",""stop_words"":""None"",""use_gpu"":""None"",""top_k"":1,""model_name_or_path"":""google/flan-t5-base"",""max_length"":""100""}}","DragNDropLite-71"
"DragNDropLite","Core","{""formats"":{""output_parser"":""text"",""prompt_text"":""text"",""name"":""text""},""classname"":""PromptTemplate"",""name"":""PromptTemplate"",""alias"":""PromptTemplate"",""parentCategory"":""159"",""id"":72,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import PromptTemplate""],""script"":""def PromptTemplate_<id>(*compo_op, output_parser_param=None, prompt_text_param='', name_param=''):\\n    lfqa_prompt = PromptTemplate(\\n                            name = name_param,\\n                            prompt_text = ''prompt_text_param'',\\n                            output_parser =  None if output_parser_param == 'None' else output_parser_param\\n                            )\\n    component_name = retriever.__class__.__name__\\n\\n    \\n    return component_name, lfqa_prompt\\n\\n""},""category"":""HayStack_PromptsAndLLMs_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out1"",""out2""],""attributes"":{""output_parser"":""None"",""prompt_text"":"""",""name"":""""}}","DragNDropLite-72"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""BM25Okapi"",""BM25L"",""BM25Plus""],""devices"":""text"",""use_gpu"":""text"",""embedding_dim"":""integer"",""index"":""text"",""progress_bar"":""text"",""bm25_tokenization_regex"":""text"",""duplicate_documents"":""text"",""bm25_algorithm"":""dropValues"",""similarity"":""text"",""label_index"":""text"",""embedding_field"":""text"",""bm25_parameters"":""text"",""scoring_batch_size"":""integer"",""return_embedding"":""text"",""use_bm25"":""text""},""classname"":""InMemoryDocumentStore"",""name"":""InMemoryDocumentStore"",""alias"":""InMemoryDocumentStore"",""parentCategory"":""162"",""id"":73,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.document_stores import InMemoryDocumentStore""],""script"":""def InMemoryDocumentStore_<id>(*compo_op, devices_param=None, use_gpu_param=True, embedding_dim_param=786, index_param='document', progress_bar_param=True, bm25_tokenization_regex_param='(?u)\\\\b\\\\w\\\\w+\\\\b', duplicate_documents_param='overwrite', bm25_algorithm_param=BM25Okapi, label_index_param='label', embedding_field_param='embedding', similarity_param='dot_product', scoring_batch_size_param=500000, bm25_parameters_param=None, return_embedding_param=False, use_bm25_param=False):\\n    document_store = InMemoryDocumentStore(use_bm25 = use_bm25_param)\\n    component_name = document_store.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(document_store, component_name, ['Query'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(document_store, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    glob_dict['document_store'] = document_store\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DocumentStore"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""devices"":""None"",""use_gpu"":""True"",""embedding_dim"":786,""index"":""document"",""progress_bar"":""True"",""bm25_tokenization_regex"":""(?u)\\\\b\\\\w\\\\w+\\\\b"",""duplicate_documents"":""overwrite"",""bm25_algorithm"":""BM25Okapi"",""similarity"":""dot_product"",""label_index"":""label"",""embedding_field"":""embedding"",""bm25_parameters"":""None"",""scoring_batch_size"":500000,""return_embedding"":""False"",""use_bm25"":""False""}}","DragNDropLite-73"
"DragNDropLite","Core","{""formats"":{""model_version"":""text"",""use_auth_token"":""text"",""task"":""text"",""batch_size"":""integer"",""devices"":""text"",""use_gpu"":""text"",""top_k"":""integer"",""progress_bar"":""text"",""classification_field"":""text"",""model_name_or_path"":""text"",""labels"":""list"",""tokenizer"":""text""},""classname"":""TransformersDocumentClassifier"",""name"":""TransformersDocumentClassifier"",""alias"":""TransformersDocumentClassifier"",""parentCategory"":""162"",""id"":74,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import TransformersDocumentClassifier""],""script"":""def TransformersDocumentClassifier_<id>(*compo_op, model_version_param=None, use_auth_token_param=None, task_param='text-classification', batch_size_param=16, devices_param=None, use_gpu_param=True, top_k_param=1, progress_bar_param=True, classification_field_param=None, model_name_or_path_param='bhadresh-savani/distilbert-base-uncased-emotion', tokenizer_param=None, labels_param=None):\\n    \\n    labels_list = list()\\n    if labels_param != None:\\n        for i in labels_param:\\n            labels_list.append(i['value'])\\n        \\n    doc_classifier = TransformersDocumentClassifier(\\n                                                    model_name_or_path = model_name_or_path_param,\\n                                                    model_version = None if model_version_param == 'None' else model_version_param,\\n                                                    tokenizer = None if tokenizer_param == 'None' else tokenizer_param,\\n                                                    use_gpu = use_gpu_param,\\n                                                    top_k = top_k_param,\\n                                                    task = task_param,\\n                                                    labels = None if labels_param == None else labels_list,\\n                                                    batch_size = batch_size_param,\\n                                                    classification_field = None if classification_field_param == 'None' else classification_field_param,\\n                                                    progress_bar = progress_bar_param,\\n                                                    use_auth_token = None if use_auth_token_param == 'None' else use_auth_token_param,\\n                                                    devices = None if devices_param == 'None' else devices_param\\n                                                )\\n    component_name = doc_classifier.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(doc_classifier, component_name, ['Query'])   \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(doc_classifier, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DocumentClassifier"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_version"":""None"",""use_auth_token"":""None"",""task"":""text-classification"",""batch_size"":16,""devices"":""None"",""use_gpu"":""True"",""top_k"":1,""progress_bar"":""True"",""classification_field"":""None"",""model_name_or_path"":""bhadresh-savani/distilbert-base-uncased-emotion"",""labels"":""None"",""tokenizer"":""None""}}","DragNDropLite-74"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""auto"",""full"",""None""],""sort_by_position"":""text"",""remove_numeric_tables"":""text"",""id_hash_keys"":""list"",""keep_physical_layout"":""text"",""ocr_language"":""text"",""multiprocessing"":""text"",""encoding"":""text"",""valid_languages"":""list"",""ocr"":""dropValues""},""classname"":""PDFToTextConverter"",""name"":""PDFToTextConverter"",""alias"":""PDFToTextConverter"",""parentCategory"":""159"",""id"":75,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import PDFToTextConverter""],""script"":""def PDFToTextConverter_<id>(*compo_op, sort_by_position_param=False, remove_numeric_tables_param=False, id_hash_keys_param=None, keep_physical_layout_param=None, ocr_language_param='eng', multiprocessing_param=True, encoding_param=None, valid_languages_param=None, ocr_param=None):\\n    \\n    valid_languages_list = list()\\n    if valid_languages_param != None:\\n        for i in valid_languages_param:\\n            valid_languages_list.append(i['value'])\\n\\n    id_hash_keys_list = list()\\n    if id_hash_keys_param != None:\\n        for i in id_hash_keys_param:\\n            id_hash_keys_list.append(i['value'])        \\n            \\n    pdf_converter = PDFToTextConverter(remove_numeric_tables = remove_numeric_tables_param,\\n                                        valid_languages = None if valid_languages_param == None else valid_languages_list,\\n                                        id_hash_keys = None if id_hash_keys_param == None else id_hash_keys_list,\\n                                        encoding = None if 'encoding' == 'None' else encoding_param,\\n                                        keep_physical_layout = keep_physical_layout_param,\\n                                        sort_by_position = sort_by_position_param,\\n                                        ocr = None if ocr_param == 'None' else ocr_param,\\n                                        ocr_language = ocr_language_param,\\n                                        multiprocessing = multiprocessing_param\\n                                      )\\n    component_name = pdf_converter.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(pdf_converter, component_name, ['File'])\\n            glob_dict['prev_component_name'].clear()\\n    elif compo_op[0] == 'FileTypeClassifier':\\n        index_pipe.add_node(pdf_converter, component_name, ['FileTypeClassifier.output_2'])\\n        if 'FileTypeClassifier' in glob_dict['prev_component_name']:\\n            glob_dict['prev_component_name'].remove('FileTypeClassifier')\\n    else:\\n        index_pipe.add_node(pdf_converter, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""sort_by_position"":""False"",""remove_numeric_tables"":""False"",""id_hash_keys"":""None"",""keep_physical_layout"":""None"",""ocr_language"":""eng"",""multiprocessing"":""True"",""encoding"":""None"",""valid_languages"":""None"",""ocr"":""None""}}","DragNDropLite-75"
"DragNDropLite","Core","{""formats"":{""remove_numeric_tables"":""text"",""id_hash_keys"":""list"",""progress_bar"":""text"",""valid_languages"":""list""},""classname"":""DocxToTextConverter"",""name"":""DocxToTextConverter"",""alias"":""DocxToTextConverter"",""parentCategory"":""159"",""id"":76,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import DocxToTextConverter""],""script"":""def DocxToTextConverter_<id>(*compo_op, remove_numeric_tables_param=False, id_hash_keys_param=None, progress_bar_param=True, valid_languages_param=None):\\n    \\n    valid_languages_list = list()\\n    if valid_languages_param != None:\\n        for i in valid_languages_param:\\n            valid_languages_list.append(i['value'])\\n\\n    id_hash_keys_list = list()\\n    if id_hash_keys_param != None:\\n        for i in id_hash_keys_param:\\n            id_hash_keys_list.append(i['value'])        \\n        \\n    docx_converter = DocxToTextConverter(remove_numeric_tables = remove_numeric_tables_param,\\n                                        valid_languages = None if valid_languages_param == None else valid_languages_list,\\n                                        id_hash_keys = None if id_hash_keys_param == None else id_hash_keys_list,\\n                                        progress_bar = progress_bar_param\\n                                            )\\n    component_name = docx_converter.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(docx_converter, component_name, ['File'])\\n            glob_dict['prev_component_name'].clear()\\n    elif compo_op[0] == 'FileTypeClassifier':\\n        index_pipe.add_node(docx_converter, component_name, ['FileTypeClassifier.output_4'])\\n        if 'FileTypeClassifier' in glob_dict['prev_component_name']:\\n            glob_dict['prev_component_name'].remove('FileTypeClassifier')\\n    else:\\n        index_pipe.add_node(docx_converter, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""remove_numeric_tables"":""False"",""id_hash_keys"":""None"",""progress_bar"":""True"",""valid_languages"":""None""}}","DragNDropLite-76"
"DragNDropLite","Core","{""formats"":{},""classname"":""FileTypeClassifier"",""name"":""FileTypeClassifier"",""alias"":""FileTypeClassifier"",""parentCategory"":""159"",""id"":77,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import FileTypeClassifier""],""script"":""def FileTypeClassifier_<id>(*compo_op, ):\\n    file_type_classifier = FileTypeClassifier()\\n    component_name = file_type_classifier.__class__.__name__\\n    if index_pipe.components == {}:\\n        index_pipe.add_node(file_type_classifier, component_name, ['File'])  \\n        glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(file_type_classifier, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name, component_name, component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""text_out"",""pdf_out"",""docx_out""],""attributes"":{}}","DragNDropLite-77"
"DragNDropLite","Core","{""formats"":{""document_store"":""text"",""top_k"":""integer"",""custom_query"":""text"",""scale_score"":""text"",""all_terms_must_match"":""text""},""classname"":""BM25Retriever"",""name"":""BM25Retriever"",""alias"":""BM25Retriever"",""parentCategory"":""163"",""id"":70,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import BM25Retriever""],""script"":""def BM25Retriever_<id>(*compo_op, document_store_param=None, top_k_param=10, custom_query_param=None, scale_score_param=True, all_terms_must_match_param=False):\\n    retriever = BM25Retriever(\\n                        document_store = glob_dict['document_store'],\\n                        top_k = top_k_param,\\n                        all_terms_must_match = all_terms_must_match_param,\\n                        custom_query = None if custom_query_param == 'None' else custom_query_param,\\n                        scale_score= scale_score_param,\\n                        )\\n\\n    component_name = retriever.__class__.__name__\\n    \\n    if index_pipe.components == {}:\\n            index_pipe.add_node(retriever, component_name, ['Query'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(retriever, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_SemanticSearch_Component"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""document_store"":""None"",""top_k"":10,""custom_query"":""None"",""scale_score"":""True"",""all_terms_must_match"":""False""}}","DragNDropLite-70"
"DragNDropLite","Core","{""formats"":{},""classname"":""PipelineTrigger"",""name"":""PipelineTrigger"",""alias"":""PipelineTrigger"",""parentCategory"":""164"",""id"":82,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def PipelineTrigger_<id>(dataset, ):\\n    files = None if glob_dict['input_files'] == [] else glob_dict['input_files']\\n    queries = None if glob_dict['input_queries'] == [] else glob_dict['input_queries'] \\n    documents = None if glob_dict['input_documents'] == [] else glob_dict['input_documents']\\n    out = index_pipe.run(query=queries, file_paths=files, documents=documents, debug=True)\\n    print(out)\\n    return out\\n\\n""},""category"":""HayStack_PipelineTrigger"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-82"
"DragNDropLite","Core","{""formats"":{""documents"":""list"",""files"":""list"",""queries"":""list""},""classname"":""PipelineInput"",""name"":""PipelineInput"",""alias"":""PipelineInput"",""parentCategory"":""164"",""id"":87,""codeGeneration"":{""requirements"":[""farm-haystack[all]"",""faiss-cpu==1.7.3""],""imports"":[""from haystack.pipelines import Pipeline"",""from haystack import Document""],""script"":""def PipelineInput_<id>(dataset, documents_param=None, files_param=None, queries_param=None):\\n    global index_pipe\\n    index_pipe = Pipeline()\\n    \\n    input_files_list = list()\\n    input_queries_list = list()\\n    input_documents_list = list()\\n    \\n    if files_param != None:\\n        for i in files_param:\\n            input_files_list.append(i['value'])\\n            \\n    if queries_param != None:\\n        for i in queries_param:\\n            input_queries_list.append(i['value'])\\n\\n    if documents_param != None:\\n        for i in documents_param:\\n            input_documents_list.append(Document(content=i['value']))\\n            \\n    global glob_dict\\n    glob_dict = {'input_queries': input_queries_list, 'input_files': input_files_list, 'input_documents':input_documents_list, 'prev_component_name': [None], 'document_store': None}\\n    return True\\n\\n""},""category"":""HayStack_PipelineInput"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""documents"":""None"",""files"":""None"",""queries"":""None""}}","DragNDropLite-87"
"DragNDropLite","Core","{""formats"":{},""classname"":""get_instance"",""name"":""get_instance"",""alias"":""get_instance"",""parentCategory"":""160"",""id"":88,""codeGeneration"":{""requirements"":[""seqeval"",""setuptools_scm"",""farm-haystack==1.15.1""],""imports"":[""import os"",""from haystack.document_stores import ElasticsearchDocumentStore""],""script"":""\\ndef get_Instance_<id>(input, ):\\n    ELASTICSTORE_HOST='10.68.191.135'\\n    ELASTICSTORE_PORT=9200\\n    ELASTICSTORE_USERNAME='isaimltestuser'\\n    ELASTICSTORE_PASSWORD= 'isaimlTEST_#21'\\n    ELASTIC_INDEX='isaiml_knowledgesearch_index-000001'\\n    document_store = ElasticsearchDocumentStore( host=ELASTICSTORE_HOST,port=ELASTICSTORE_PORT,username=ELASTICSTORE_USERNAME,password=ELASTICSTORE_PASSWORD,index=ELASTIC_INDEX,label_index=ELASTIC_INDEX)\\n    return document_store\\n\\n\\n""},""category"":""RFP_Elastic_API"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-88"
"DragNDropLite","Core","{""formats"":{""model_version"":""text"",""return_no_answer"":""text"",""batch_size"":""integer"",""use_confidence_scores"":""text"",""use_gpu"":""text"",""duplicate_filtering"":""integer"",""force_download"":""text"",""top_k_per_sample"":""integer"",""progress_bar"":""text"",""context_window_size"":""integer"",""use_auth_token"":""text"",""local_files_only"":""text"",""doc_stride"":""integer"",""top_k"":""integer"",""max_seq_len"":""integer"",""num_processes"":""integer"",""proxies"":""text"",""max_query_length"":""integer"",""top_k_per_candidate"":""integer"",""model_name_or_path"":""text""},""classname"":""FARMReader"",""name"":""FARMReader"",""alias"":""FARMReader"",""parentCategory"":""163"",""id"":92,""codeGeneration"":{""requirements"":[""farm-haystack[all]"",""faiss-cpu==1.7.3""],""imports"":[""from haystack.nodes import FARMReader""],""script"":""def FARMReader_<id>(*compo_op, model_version_param=None, return_no_answer_param=False, batch_size_param=50, use_confidence_scores_param=True, use_gpu_param=True, duplicate_filtering_param=0, force_download_param=False, top_k_per_sample_param=1, progress_bar_param=True, context_window_size_param=150, use_auth_token_param=None, local_files_only_param=False, doc_stride_param=128, top_k_param=10, max_seq_len_param=256, num_processes_param=None, proxies_param=None, max_query_length_param=64, top_k_per_candidate_param=3, model_name_or_path_param=None):\\n    reader =  FARMReader(\\n                            model_name_or_path = None if model_name_or_path_param == 'None' else model_name_or_path_param,\\n                            model_version = None if model_version_param == 'None' else model_version_param,\\n                            context_window_size = context_window_size_param,\\n                            batch_size = batch_size_param,\\n                            use_gpu = use_gpu_param,\\n                            devices = None if '<devices>' == 'None' else '<devices>',\\n                            return_no_answer = return_no_answer_param,\\n                            top_k = top_k_param,\\n                            top_k_per_candidate  = top_k_per_candidate_param,\\n                            top_k_per_sample = top_k_per_sample_param,\\n                            num_processes = None if num_processes_param == None else num_processes_param,\\n                            max_seq_len = max_seq_len_param,\\n                            doc_stride = doc_stride_param,\\n                            progress_bar = progress_bar_param,\\n                            duplicate_filtering = duplicate_filtering_param,\\n                            use_confidence_scores = use_confidence_scores_param,\\n                            local_files_only = local_files_only_param,\\n                            force_download = force_download_param,\\n                            use_auth_token = use_auth_token_param,\\n                            max_query_length  = max_query_length_param\\n                        )\\n\\n    \\n    component_name = reader.__class__.__name__\\n    \\n    if index_pipe.components == {}:\\n            index_pipe.add_node(reader, component_name, ['Query'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(reader, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_SemanticSearch_Component"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_version"":""None"",""return_no_answer"":""False"",""batch_size"":50,""use_confidence_scores"":""True"",""use_gpu"":""True"",""duplicate_filtering"":0,""force_download"":""False"",""top_k_per_sample"":1,""progress_bar"":""True"",""context_window_size"":150,""use_auth_token"":""None"",""local_files_only"":""False"",""doc_stride"":128,""top_k"":10,""max_seq_len"":256,""num_processes"":""None"",""proxies"":""None"",""max_query_length"":""64"",""top_k_per_candidate"":3,""model_name_or_path"":""None""}}","DragNDropLite-92"
"DragNDropLite","Core","{""id"": 0, ""name"": ""IngestFeatureOnline"", ""alias"": ""IngestFeatureOnline"", ""formats"": {}, ""category"": ""Feast"", ""classname"": ""IngestFeatureOnline"", ""attributes"": {}, ""codeGeneration"": {""script"": ""\\ndef IngestFeatureOnline_<id>(dataset):\\n    os.chdir('jobs/models/feature_repo/feature_repo')\\n    store= FeatureStore(repo_path='')\\n    result=store.materialize_incremental(end_date = datetime.now())\\n    return {'result': result}\\n"", ""imports"": [""from feast import FeatureStore"", ""from datetime import datetime""], ""requirements"": []}, ""inputEndpoints"": [""dataset1""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-0"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""/app/jobs/models/""],""File_name_tobe_store"":""text"",""select_location_tobe_store"":""dropValues"",""Folder_name_tobe_store"":""text"",""File_name_tobe_download"":""text""},""classname"":""File Extractor"",""name"":""File Extractor"",""alias"":""File Extractor"",""parentCategory"":""165"",""id"":66,""codeGeneration"":{""requirements"":[""boto3""],""imports"":[""import boto3"",""import os""],""script"":""def FileExtractor_<id>(dataset, file_name_tobe_store_param=None, select_location_tobe_store_param=[], folder_name_tobe_store_param=None, file_name_tobe_download_param=None):\\n    try:\\n        s3 = boto3.resource('s3',\\n             aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\n             aws_secret_access_key= '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')\\n\\n        bucket = s3.Bucket('aiplatdata1')\\n        \\n        if folder_name_tobe_store_param != 'None':\\n            path = select_location_tobe_store_param+'/folder_name_tobe_store_param'\\n        else:\\n            path = select_location_tobe_store_param\\n            \\n        for obj in bucket.objects.filter(Prefix = file_name_tobe_download_param): #made change            \\n            if not os.path.exists(path):\\n                os.makedirs(path)                    \\n            bucket.download_file(obj.key, path+'/file_name_tobe_store_param') #save to same path        \\n        print('Path===',path+'/file_name_tobe_store_param')\\n        return True\\n\\n    except Exception as e:\\n        print('There is an exception while downloading data as : ',e)\\n        return False\\n\\n""},""category"":""Haystack_DataExtractor"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""File_name_tobe_store"":""None"",""select_location_tobe_store"":[],""Folder_name_tobe_store"":""None"",""File_name_tobe_download"":""None""}}","DragNDropLite-66"
"DragNDropLite","Core","{""formats"":{""remove_numeric_tables"":""text"",""id_hash_keys"":""list"",""progress_bar"":""text"",""valid_languages"":""list""},""classname"":""TextConverter"",""name"":""TextConverter"",""alias"":""TextConverter"",""parentCategory"":""159"",""id"":67,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import TextConverter""],""script"":""def TextConverter_<id>(*compo_op, remove_numeric_tables_param=False, id_hash_keys_param=None, progress_bar_param=True, valid_languages_param=None):\\n    \\n    valid_languages_list = list()\\n    if valid_languages_param != None:\\n        for i in valid_languages_param:\\n            valid_languages_list.append(i['value'])\\n\\n    id_hash_keys_list = list()\\n    if id_hash_keys_param != None:\\n        for i in id_hash_keys_param:\\n            id_hash_keys_list.append(i['value'])        \\n        \\n    text_converter = TextConverter(remove_numeric_tables = remove_numeric_tables_param,\\n                                    valid_languages = None if valid_languages_param == None else valid_languages_list,\\n                                    id_hash_keys = None if id_hash_keys_param == None else id_hash_keys_list,\\n                                    progress_bar = progress_bar_param\\n                                  )\\n    component_name = text_converter.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(text_converter, component_name, ['File'])\\n            glob_dict['prev_component_name'].clear()\\n    elif compo_op[0] == 'FileTypeClassifier':\\n        index_pipe.add_node(text_converter, component_name, ['FileTypeClassifier.output_1'])\\n        if 'FileTypeClassifier' in glob_dict['prev_component_name']:\\n            glob_dict['prev_component_name'].remove('FileTypeClassifier')\\n    else:\\n        index_pipe.add_node(text_converter, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""remove_numeric_tables"":""False"",""id_hash_keys"":""None"",""progress_bar"":""True"",""valid_languages"":""None""}}","DragNDropLite-67"
"DragNDropLite","Core","{""formats"":{""dropValues"":[""word"",""sentence"",""passage""],""split_length"":""integer"",""id_hash_keys"":""list"",""clean_whitespace"":""text"",""tokenizer_model_folder"":""text"",""split_respect_sentence_boundary"":""text"",""language"":""text"",""progress_bar"":""text"",""clean_empty_lines"":""text"",""add_page_number"":""text"",""remove_substrings"":""list"",""split_by"":""dropValues"",""split_overlap"":""integer"",""clean_header_footer"":""text"",""max_chars_check"":""integer""},""classname"":""PreProcessor"",""name"":""PreProcessor"",""alias"":""PreProcessor"",""parentCategory"":""159"",""id"":68,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import PreProcessor""],""script"":""def PreProcessor_<id>(*compo_op, split_length_param=200, id_hash_keys_param=None, clean_whitespace_param=True, tokenizer_model_folder_param=None, split_respect_sentence_boundary_param=True, language_param='en', progress_bar_param=True, clean_empty_lines_param=True, add_page_number_param=False, remove_substrings_param=None, split_by_param='word', split_overlap_param=0, clean_header_footer_param=False, max_chars_check_param=10000):\\n    \\n    remove_substrings_list = list()\\n    if remove_substrings_param != None:\\n        for i in remove_substrings_param:\\n            remove_substrings_list.append(i['value'])\\n        \\n    id_hash_keys_list = list()\\n    if id_hash_keys_param != None:\\n        for i in id_hash_keys_param:\\n            id_hash_keys_list.append(i['value'])        \\n        \\n    preprocessor = PreProcessor(clean_whitespace = clean_whitespace_param,\\n                                clean_header_footer = clean_header_footer_param,\\n                                clean_empty_lines = clean_empty_lines_param,\\n                                remove_substrings = None if remove_substrings_param == None else remove_substrings_list,\\n                                split_by = split_by_param,\\n                                split_length = split_length_param,\\n                                split_overlap = split_overlap_param,\\n                                split_respect_sentence_boundary = split_respect_sentence_boundary_param,\\n                                tokenizer_model_folder = None if tokenizer_model_folder_param == 'None' else 'tokenizer_model_folder',\\n                                language = language_param,\\n                                id_hash_keys = None if id_hash_keys_param == None else id_hash_keys_list,\\n                                progress_bar = progress_bar_param,\\n                                add_page_number = add_page_number_param,\\n                                max_chars_check = max_chars_check_param\\n                               )    \\n    component_name = preprocessor.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(preprocessor, component_name, ['Query'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(preprocessor, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in1"",""in2"",""in3""],""outputEndpoints"":[""out""],""attributes"":{""dropValues"":"""",""split_length"":200,""id_hash_keys"":""None"",""clean_whitespace"":""True"",""tokenizer_model_folder"":""None"",""split_respect_sentence_boundary"":""True"",""language"":""en"",""progress_bar"":""True"",""clean_empty_lines"":""True"",""add_page_number"":""False"",""remove_substrings"":""None"",""split_by"":""word"",""split_overlap"":0,""clean_header_footer"":""False"",""max_chars_check"":10000}}","DragNDropLite-68"
"DragNDropLite","Core","{""formats"":{""add_prefix_space"":""text"",""dropValues"":[""None"",""simple"",""first"",""average"",""max""],""model_version"":""text"",""batch_size"":""integer"",""devices"":""text"",""use_gpu"":""text"",""ignore_labels"":""list"",""progress_bar"":""text"",""num_workers"":""integer"",""aggregation_strategy"":""dropValues"",""use_auth_token"":""text"",""max_seq_len"":""integer"",""pre_split_text"":""text"",""flatten_entities_in_meta_data"":""text"",""model_name_or_path"":""text""},""classname"":""EntityExtractor"",""name"":""EntityExtractor"",""alias"":""EntityExtractor"",""parentCategory"":""159"",""id"":69,""codeGeneration"":{""requirements"":[],""imports"":[""import boto3"",""import os"",""from haystack.nodes import EntityExtractor""],""script"":""def EntityExtractor_<id>(*compo_op, add_prefix_space_param=None, model_version_param=None, batch_size_param=16, devices_param=None, use_gpu_param=True, ignore_labels_param=None, progress_bar_param=True, aggregation_strategy_param='first', num_workers_param=0, use_auth_token_param=None, max_seq_len_param=None, pre_split_text_param=False, flatten_entities_in_meta_data_param=False, model_name_or_path_param=elastic/distilbert-base-cased-finetuned-conll03-english):\\n    \\n    ignore_labels_list = list()\\n    if ignore_labels_param != None:\\n        for i in ignore_labels_param:\\n            ignore_labels_list.append(i['value'])\\n             \\n    entity_extractor = EntityExtractor(model_name_or_path = model_name_or_path_param,\\n                                        model_version = None if model_version_param=='None' else model_version_param,\\n                                        use_gpu = use_gpu_param,\\n                                        batch_size = batch_size_param,\\n                                        progress_bar = progress_bar_param,\\n                                        use_auth_token = None if use_auth_token_param == 'None' else use_auth_token_param,\\n                                        aggregation_strategy = aggregation_strategy_param,\\n                                        add_prefix_space = add_prefix_space_param,\\n                                        num_workers = num_workers_param,\\n                                        flatten_entities_in_meta_data = flatten_entities_in_meta_data_param,\\n                                        max_seq_len = max_seq_len_param,\\n                                        pre_split_text = pre_split_text_param,\\n                                        ignore_labels = None if ignore_labels_param == None else ignore_labels_list\\n                               )    \\n    component_name = entity_extractor.__class__.__name__\\n    if index_pipe.components == {}:\\n            index_pipe.add_node(entity_extractor, component_name, ['Query'])    \\n            glob_dict['prev_component_name'].clear()\\n    else:\\n        index_pipe.add_node(entity_extractor, component_name, glob_dict['prev_component_name'])\\n        glob_dict['prev_component_name'].clear()\\n        \\n    glob_dict['prev_component_name'].append(component_name)\\n    \\n    return component_name\\n\\n""},""category"":""HayStack_DataHandling_Components"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""add_prefix_space"":""None"",""dropValues"":"""",""model_version"":""None"",""batch_size"":16,""devices"":""None"",""use_gpu"":""True"",""ignore_labels"":""None"",""progress_bar"":""True"",""num_workers"":0,""aggregation_strategy"":""first"",""use_auth_token"":""None"",""max_seq_len"":""None"",""pre_split_text"":""False"",""flatten_entities_in_meta_data"":""False"",""model_name_or_path"":""elastic/distilbert-base-cased-finetuned-conll03-english""}}","DragNDropLite-69"
"DragNDropLite","Core","{""formats"":{},""classname"":""print_documents"",""name"":""print_documents"",""alias"":""print_documents"",""parentCategory"":""160"",""id"":97,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""\\ndef print_documents_<id>(document, ):\\n    print('Printing the results')\\n    print(document)\\n    return 'success'\\n\\n\\n\\n\\n\\n\\n""},""category"":""RFP_Elastic_API"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-97"
"DragNDropLite","Core","{""id"": 98, ""name"": ""print_documents"", ""alias"": ""print_documents"", ""formats"": {}, ""category"": ""RFP_Elastic_API"", ""classname"": ""print_documents"", ""attributes"": {}, ""codeGeneration"": {""script"": ""\\ndef print_documents_<id>(document):\\n    print('Printing the results')\\n    print(document)\\n    return 'success'\\n"", ""imports"": [], ""requirements"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-98"
"DragNDropLite","Core","{""formats"":{""reference_value"":""integer"",""reference_feature"":""text""},""classname"":""Datasplit"",""name"":""Datasplit"",""alias"":""Datasplit"",""parentCategory"":""157"",""id"":99,""codeGeneration"":{""requirements"":[""whylogs""],""imports"":[""import pandas as pd"",""import whylogs as why""],""script"":""def Datasplit_<id>(data,reference_feature_param='',reference_value_param=0):\\n  wine = pd.DataFrame(data)\\n  cond_reference = (wine[reference_feature_param]<=int(reference_value_param))\\n  wine_reference = wine.loc[cond_reference]\\n  cond_target = (wine[reference_feature_param]>int(reference_value_param))\\n  wine_target = wine.loc[cond_target]\\n  return wine_reference,wine_target\\n\\n\\n""},""category"":""Data Profiling"",""inputEndpoints"":[""in""],""outputEndpoints"":[""model"",""out""],""attributes"":{""reference_value"":0,""reference_feature"":""""}}","DragNDropLite-99"
"DragNDropLite","Core","{""formats"":{""page_end_index"":""integer"",""page_start_index"":""integer"",""page_drop_indices"":""list""},""classname"":""pdf_to_text"",""name"":""pdf_to_text"",""alias"":""PDF to Text"",""parentCategory"":""166"",""id"":100,""codeGeneration"":{""requirements"":[""pymupdf""],""imports"":[""import fitz"",""import pandas as pd ""],""script"":""\\ndef pdf_to_text_<id>(dataset, page_start_index_param=0, page_end_index_param=-1, page_drop_indices_param=[]):\\n    page_drop_indices_param = [ss['name'] for ss in page_drop_indices_param]\\n    text = ''\\n    if dataset.split('.')[-1] == 'pdf':\\n        doc = fitz.open(data)\\n        start = max(0, page_start_index_param)\\n        end = min(page_end_index_param, len(doc))\\n        for page in range(start, end):\\n            if page not in page_drop_indices_param:\\n                page1 = doc[page]\\n                text += page1.get_text('text')\\n                text += '\\\\n'\\n    return text\\n\\n\\n""},""category"":""Convertors"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""page_end_index"":-1,""page_start_index"":0,""page_drop_indices"":[]}}","DragNDropLite-100"
"DragNDropLite","Core","{""formats"":{""slide_drop_indices"":""list"",""slide_end_index"":""integer"",""slide_start_index"":""integer""},""classname"":""ppt_to_text"",""name"":""ppt_to_text"",""alias"":""PPT to Text"",""parentCategory"":""166"",""id"":101,""codeGeneration"":{""requirements"":[""python-pptx""],""imports"":[""from pptx import Presentation"",""import glob""],""script"":""\\ndef ppt_to_text_<id>(dataset, slide_start_index_param=0, slide_end_index_param=-1, slide_drop_indices_param=[]):\\n    slide_drop_indices_param = [ss['name'] for ss in slide_drop_indices_param]\\n    text = ''\\n    prs = Presentation(dataset)\\n    slides = prs.slides\\n    start = max(0, slide_start_index_param)\\n    end = min(slide_end_index_param, len(doc)) \\n    for slide_ind in range(start, end):\\n        if slide_ind not in slide_drop_indices_param:\\n            slide = slides[slide_ind]\\n            for shape in slide.shapes:\\n                if hasattr(shape, 'text'):\\n                    text += shape.text\\n                    text += '\\\\n'\\n    return text\\n\\n\\n""},""category"":""Convertors"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""slide_drop_indices"":[],""slide_end_index"":-1,""slide_start_index"":0}}","DragNDropLite-101"
"DragNDropLite","Core","{""formats"":{},""classname"":""DataStatistics"",""name"":""DataStatistics"",""alias"":""DataStatistics"",""parentCategory"":""157"",""id"":102,""codeGeneration"":{""requirements"":[""whylogs""],""imports"":[""import pandas as pd"",""import whylogs as why""],""script"":""def DataStatistics_<id>(wine_reference,wine_target):\\n  result = why.log(pandas=wine_target)\\n  prof_view = result.view()\\n  target = prof_view.to_pandas()\\n  target = target.to_dict('records')\\n  result_ref = why.log(pandas=wine_reference)\\n  prof_view_ref = result_ref.view()\\n  reference = prof_view_ref.to_pandas()\\n  reference = reference.to_dict('records')\\n  return target,reference\\n\\n\\n""},""category"":""Data Profiling"",""inputEndpoints"":[""in"",""in""],""outputEndpoints"":[""model"",""out""],""attributes"":{}}","DragNDropLite-102"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{},""name"":""Profanity"",""alias"":""Profanity"",""parentCategory"":""167"",""id"":107,""codeGeneration"":{""requirements"":[""response"",""requests""],""imports"":[""import sys"",""import urllib3"",""import requests"",""import json""],""script"":""def Profanity(inputText):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/safety/profanity/censor'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {\\r\\n    'inputText': inputText\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-107"
"DragNDropLite","Core","{""id"": 103, ""name"": ""RegisterFeature"", ""alias"": ""RegisterFeature"", ""formats"": {""target_type_param"": ""text""}, ""category"": ""Feature Store"", ""classname"": ""RegisterFeature"", ""attributes"": {""target_type_param"": []}, ""codeGeneration"": {""script"": ""def RegisterFeature(price_feat, target_type_param=''):\\n    claimsFeatureStore = AIFFeatureStore(target_type_param)\\n    claimsFeatureStore.registerFeature(price_feat)\\n\\n    return claimsFeatureStore\\n"", ""imports"": [], ""requirements"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-103"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{},""name"":""Explainability"",""alias"":""Explainability"",""parentCategory"":""167"",""id"":108,""codeGeneration"":{""requirements"":[""requests"",""response""],""imports"":[""import sys"",""import requests"",""import response"",""import json""],""script"":""def Explainability(input):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    import sys\\r\\n    inputText_param, explainerID_param = input['inputText'], input['explainerID']\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/explainability/local/explain'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {\\r\\n       'inputText': inputText_param,\\r\\n       'explainerID': explainerID_param\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-108"
"DragNDropLite","Core","{""id"": 111, ""name"": ""AnalyzeImage"", ""alias"": ""AnalyzeImage"", ""formats"": {}, ""category"": ""RAI"", ""attributes"": {}, ""subCategory"": """", ""codeGeneration"": {""script"": ""def AnalyzeImage(image_payload):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/analyze'\\r\\n    headers = {'content-type': 'multipart/form-data'}\\r\\n    print(url)\\r\\n    with open(image_payload, 'rb') as file:\\r\\n        response = requests.post(url, files={'payload': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n"", ""imports"": [""import requests"", ""import json"", ""import urllib3"", ""import cv2"", ""import sys""], ""requirements"": [""opencv-python"", ""requests"", ""response""]}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-111"
"DragNDropLite","Core","{""id"": 112, ""name"": ""AnonymizeImage"", ""alias"": ""AnonymizeImage"", ""formats"": {}, ""category"": ""RAI"", ""attributes"": {}, ""subCategory"": """", ""codeGeneration"": {""script"": ""def AnonymizeImage(image_payload):\\r\\n    #print('ImagePayload:',image_payload)\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/anonymize'\\r\\n    headers = {'content-type': 'multipart/form-data'}\\r\\n    print(url)\\r\\n    with open(image_payload, 'rb') as file:\\r\\n        response = requests.post(url, files={'payload': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\n"", ""imports"": ["" import requests"", ""import json"", ""import urllib3"", ""import cv2"", ""import sys""], ""requirements"": [""opencv-python"", ""requests"", ""response""]}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","DragNDropLite-112"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{},""name"":""VerifyImage"",""alias"":""VerifyImage"",""parentCategory"":""167"",""id"":113,""codeGeneration"":{""requirements"":[""opencv-python"",""requests"",""response""],""imports"":[""import requests"",""import json"",""import urllib3"",""import cv2"",""import sys""],""script"":""def VerifyImage(image_payload):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/verify'\\r\\n    headers = {'content-type': 'multipart/form-data'}\\r\\n    print(url)\\r\\n    with open(image_payload, 'rb') as file:\\r\\n        response = requests.post(url, files={'payload': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-113"
"DragNDropLite","Core","{""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""classname"":""PythonScriptConfig"",""name"":""Python Script"",""alias"":""Python Script"",""parentCategory"":""146"",""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""id"":114,""category"":""BaseConfig"",""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""attributes"":{""FunctionName"":""PythonScript"",""requirements"":"""",""params"":[],""script"":""\\rdef PythonScript( dataset):    #python-script Data\\r\\r    return dataset""}}","DragNDropLite-114"
"DragNDropLite","Core","{""attributes"":{},""formats"":{}}","DragNDropLite-115"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""val_data"":""text"",""train_data"":""text""},""classname"":""Prepare_Data"",""name"":""Prepare_Data"",""alias"":""Prepare_Data"",""parentCategory"":""168"",""id"":117,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def Prepare_Data(data,bucket_param,train_data_param,val_data_param):\\r\\n    context_length = 2048\\r\\n    global pretrained_model\\r\\n    base_path = os.getenv('AICLD_INPUT_ARTIFACTS_PATH')\\r\\n    \\r\\n    train_dataset_file_path = Download_Data(bucket_param, train_data_param)\\r\\n    logger.info('train_dataset')\\r\\n    val_dataset_file_path = Download_Data(bucket_param, val_data_param)\\r\\n    logger.info('Reading data files')\\r\\n    #pretrained_model = Download_Data('aicloudprd','foundational_model/codegen/models/Salesforce/codegen-350M-multi')\\r\\n    #logger.info('pretrained_model')\\r\\n    pretrained_model = os.getenv('AI_CLD_PRETRAINED_MDL_PATH')\\r\\n    dir_content= os.listdir(pretrained_model)\\r\\n    logger.info(dir_content)\\r\\n    train_data = None\\r\\n    valid_data = None\\r\\n    if train_dataset_file_path.endswith('.json') & val_dataset_file_path.endswith('.json'):\\r\\n        train_data = [json.loads(line) for line in open(train_dataset_file_path, 'r',encoding='utf-8')]\\r\\n        valid_data = [json.loads(line) for line in open(val_dataset_file_path, 'r',encoding='utf-8')]\\r\\n    elif train_dataset_file_path.endswith('.csv') & val_dataset_file_path.endswith('.csv'):\\r\\n        train_data = pd.read_csv(train_dataset_file_path)\\r\\n        valid_data = pd.read_csv(val_dataset_file_path)\\r\\n        \\r\\n    #train_data = [json.loads(line) for line in open(train_dataset_file_path, 'r',encoding='utf-8')]\\r\\n    #valid_data = [json.loads(line) for line in open(val_dataset_file_path, 'r',encoding='utf-8')]\\r\\n    \\r\\n    ds_train = Dataset.from_pandas(pd.DataFrame(data=train_data))\\r\\n    ds_valid = Dataset.from_pandas(pd.DataFrame(data=valid_data))\\r\\n\\r\\n    logger.info('Creating raw_datasets .........')\\r\\n\\r\\n    raw_datasets = DatasetDict(\\r\\n        {\\r\\n            'train': ds_train.shuffle(),\\r\\n            'valid': ds_valid.shuffle()\\r\\n        }\\r\\n    )\\r\\n    datalist = []\\r\\n    datalist.append(raw_datasets)\\r\\n    #datalist.append(pretrained_model)\\r\\n    return datalist\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""LLM_Model"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""bucket"":"""",""val_data"":"""",""train_data"":""""}}","DragNDropLite-117"
"DragNDropLite","Core","{""formats"":{""weight_decay"":""int"",""epoch"":""int"",""eval_step"":""int""},""classname"":""Train_Model"",""name"":""Train_Model"",""alias"":""Train_Model"",""parentCategory"":""168"",""id"":118,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def Train_Model(train_data_list,epoch_param,weight_decay_param,eval_step_param):\\r\\n    logger.info('in training stage')\\r\\n    tokenized_datasets = train_data_list[0]\\r\\n    tokenizer = train_data_list[1]\\r\\n    #pretrained_model = train_data_list[2]\\r\\n    save_best_model_path = os.getenv('AICLD_MODEL_STORE_PATH')+'codegen-350M-java'\\r\\n    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\\r\\n    model = AutoModelForCausalLM.from_pretrained(pretrained_model)\\r\\n    logger.info('Training started')\\r\\n    resume_from_checkpoint=False\\r\\n    model_id='fft_2048'\\r\\n    args = TrainingArguments(output_dir='codegen_dir_2048', \\r\\n                             per_device_train_batch_size=1, \\r\\n                             per_device_eval_batch_size=1, \\r\\n                             evaluation_strategy='steps', \\r\\n                             eval_steps=eval_step_param, \\r\\n                             logging_steps=100, \\r\\n                             gradient_accumulation_steps=4, \\r\\n                             num_train_epochs=epoch_param, \\r\\n                             weight_decay=weight_decay_param, \\r\\n                             warmup_steps=100, \\r\\n                             lr_scheduler_type='linear', \\r\\n                             learning_rate=3e-5, \\r\\n                             save_strategy='steps', \\r\\n                             save_total_limit = 1, \\r\\n                             fp16 = True, \\r\\n                             load_best_model_at_end=True, \\r\\n                             save_steps=500)\\r\\n                             #deepspeed = 'ds_config.json') \\r\\n\\r\\n\\r\\n    trainer = Trainer(\\r\\n        model=model,\\r\\n        tokenizer=tokenizer,\\r\\n        args=args,\\r\\n        data_collator=data_collator,\\r\\n        train_dataset=tokenized_datasets['train'],\\r\\n        eval_dataset=tokenized_datasets['valid'],\\r\\n    )\\r\\n\\r\\n    if resume_from_checkpoint == 'True':\\r\\n        logger.info('Training started from checkpoint')\\r\\n        trainer.train(resume_from_checkpoint=True)\\r\\n        logger.info('Training completed')\\r\\n    else:\\r\\n        logger.info('Training started')\\r\\n        trainer.train()\\r\\n        logger.info('Training completed')\\r\\n\\r\\n    os.mkdir('./trainer_logs')\\r\\n    training_filename = './trainer_logs/train_logs.json'\\r\\n    output_file = open(training_filename, 'w', encoding='utf-8')\\r\\n    for dic in trainer.state.log_history:\\r\\n\\r\\n        json.dump(dic, output_file) \\r\\n        output_file.write('\\\\n') \\r\\n    logger.info('All logs saved at ',training_filename)\\r\\n\\r\\n    trainer.save_model(save_best_model_path)\\r\\n    logger.info('Finetuned model saved at ',save_best_model_path)\\r\\n    return save_best_model_path\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""LLM_Model"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""weight_decay"":0.1,""epoch"":2,""eval_step"":100}}","DragNDropLite-118"
"DragNDropLite","Core","{""formats"":{""model_version"":""int"",""model_name"":""text"",""user_id"":""text"",""image_uri"":""text"",""artifacts_uri"":""text"",""projectid"":""text"",""storagetype"":""text""},""classname"":""Register Model"",""name"":""Register Model"",""alias"":""Register Model"",""parentCategory"":""168"",""codeGeneration"":{""requirements"":[],""imports"":[""import logging"",""import requests"",""import json"",""from pathlib import Path""],""script"":""logging_dir = '/app/logs'\\r\\nlog_filename = 'create.log'\\r\\nPath(logging_dir).mkdir(parents=True, exist_ok=True)\\r\\nlog_filename = os.path.join(logging_dir, log_filename)\\r\\nlogging.basicConfig(filename=log_filename,\\r\\n                    format='%(asctime)s %(message)s',\\r\\n                    filemode='w')\\r\\nlogger = logging.getLogger()\\r\\nlogger.setLevel(logging.INFO)\\r\\n\\r\\ndef RegisterModel(user_id_param,model_name_param,model_version_param,projectid_param,image_uri_param,storagetype_param,artifacts_uri_param):\\r\\n\\r\\n    headers = {\\r\\n        'accept': 'application/json',\\r\\n        'Content-Type': 'application/json',\\r\\n    }\\r\\n    global model_version\\r\\n    model_version = model_version_param\\r\\n    headers['userId'] = user_id_param\\r\\n    json_data = {\\r\\n        'name': '',\\r\\n        'version': '',\\r\\n        'projectId': '',\\r\\n        'container': {\\r\\n            'imageUri': '',\\r\\n            'envVariables': [], \\r\\n            'ports': [{\\r\\n                'name': 'containerport',\\r\\n                'value': '8080'\\r\\n            }],\\r\\n        'labels': [{\\r\\n        'name': 'framework',\\r\\n        'value': 'flask'\\r\\n        }],\\r\\n        'healthProbeUri': '/'\\r\\n        },\\r\\n        'artifacts': {\\r\\n            'storageType': '',\\r\\n            'uri': '',\\r\\n        },    \\r\\n    }\\r\\n\\r\\n    json_data['name'] = model_name_param\\r\\n    json_data['version'] = model_version_param\\r\\n    json_data['projectId'] = projectid_param\\r\\n    json_data['container']['imageUri'] = image_uri_param\\r\\n    json_data['artifacts']['storageType'] = storagetype_param\\r\\n    json_data['artifacts']['uri'] = artifacts_uri_param\\r\\n    json_data = json.dumps(json_data)\\r\\n    logger.info('model register payload')\\r\\n    logger.info(json_data)\\r\\n    response = None\\r\\n    try:\\r\\n        response = requests.post('https://api-aicloud.ad.infosys.com/api/v1/models',headers=headers,data=json_data,verify=False)\\r\\n    except Exception as e:\\r\\n        response = e\\r\\n        logger.info(response)\\r\\n    model_register_details = json.loads(response.text)\\r\\n    logger.info('model register info')\\r\\n    logger.info(model_register_details)\\r\\n    model_id = model_register_details['data']['id']\\r\\n    logger.info('model_id')\\r\\n    logger.info(model_id)\\r\\n    return model_id\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""id"":119,""category"":""LLM_Model"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model_version"":"""",""model_name"":"""",""user_id"":""leapAccount01@infosys.com"",""image_uri"":"""",""artifacts_uri"":"""",""projectid"":""97dd455b57c64c6dba8bb5d0caa99dfd"",""storagetype"":""INFY_AICLD_NUTANIX""}}","DragNDropLite-119"
"DragNDropLite","Core","{""formats"":{""user_id"":""text"",""endpoint_name"":""text"",""contexturi"":""text"",""projectid"":""text""},""classname"":""Create Endpoint"",""name"":""Create Endpoint"",""alias"":""Create Endpoint"",""parentCategory"":""169"",""id"":120,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def CreateEndpoint(register_model_id,user_id_param,endpoint_name_param,contexturi_param,projectid_param):\\r\\n    headers = {\\r\\n        'accept': 'application/json',\\r\\n        'Content-Type': 'application/json',\\r\\n    }\\r\\n    headers['userId'] = user_id_param\\r\\n    body = {}\\r\\n    body['name'] = endpoint_name_param\\r\\n    body['contextUri'] = contexturi_param\\r\\n    body['projectId'] = projectid_param\\r\\n    body = json.dumps(body)\\r\\n    endpoint_model_ids = []\\r\\n    response = None\\r\\n    try:\\r\\n        response = requests.post('https://api-aicloud.ad.infosys.com/api/v1/endpoint', headers=headers,data=body,verify=False)\\r\\n    except Exception as e:\\r\\n        response = e\\r\\n        logger.info(response)\\r\\n    endpoint_details = json.loads(response.text)\\r\\n    endpoint_id = endpoint_details['data']['id']\\r\\n    logger.info('endpoint_id')\\r\\n    logger.info(endpoint_id)\\r\\n    endpoint_model_ids.append(register_model_id)\\r\\n    endpoint_model_ids.append(endpoint_id)\\r\\n    return endpoint_model_ids\\r\\n\\r\\n\\r\\n\\n""},""category"":""Endpoint"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""user_id"":""leapAccount01@infosys.com"",""endpoint_name"":"""",""contexturi"":"""",""projectid"":""97dd455b57c64c6dba8bb5d0caa99dfd""}}","DragNDropLite-120"
"DragNDropLite","Core","{""formats"":{""minreplicacount"":""int"",""maxreplicacount"":""int"",""compute_maxqty"":""int"",""container_volume_size"":""int"",""compute_minqty"":""int"",""compute_type"":""text"",""compute_memory"":""text"",""servingframework"":""text""},""classname"":""Deploy Endpoint"",""name"":""Deploy Endpoint"",""alias"":""Deploy Endpoint"",""parentCategory"":""169"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DeployEndpoint(endpoint_and_model_id,servingframework_param,minreplicacount_param,maxreplicacount_param,compute_type_param,compute_maxqty_param,compute_minqty_param,compute_memory_param,container_volume_size_param):\\r\\n\\r\\n    headers = {\\r\\n        'accept': 'application/json',\\r\\n        'Content-Type': 'application/json',\\r\\n    }\\r\\n\\r\\n    user_id_param='leapAccount01@infosys.com'\\r\\n    headers['userId'] = user_id_param\\r\\n    model_id = endpoint_and_model_id[0]\\r\\n    endpoint_id = endpoint_and_model_id[1]\\r\\n    json_data = {\\r\\n              'endpointId': '',\\r\\n              'modelId': '',\\r\\n              'version': '',\\r\\n              'inferenceConfig': {\\r\\n                'servingFramework': '',\\r\\n                'inferenceSpec': {\\r\\n                  'minReplicaCount': '',\\r\\n                  'maxReplicaCount': '',\\r\\n                  'containerResourceConfig': {\\r\\n                    'computes': [\\r\\n                      {\\r\\n                        'type': '',\\r\\n                        'maxQty': '',\\r\\n                        'memory': '',\\r\\n                        'minQty': ''\\r\\n                      }\\r\\n                    ],\\r\\n                    'volumeSizeinGB': ''\\r\\n                  },\\r\\n                  'modelSpec': [\\r\\n                              {\\r\\n                  'modelUris': {\\r\\n                    'prefixUri': '/codegenexecute',\\r\\n                    'predictUri': '/swagger.json'\\r\\n                  }\\r\\n                },\\r\\n                {\\r\\n                  'modelUris': {\\r\\n                    'prefixUri': '/codegenexecute',\\r\\n                    'predictUri': '/predictions/'\\r\\n                  }\\r\\n                }\\r\\n                  ]\\r\\n                }\\r\\n              }\\r\\n            }\\r\\n    json_data['endpointId'] = endpoint_id\\r\\n    json_data['modelId'] = model_id\\r\\n    json_data['version'] = model_version\\r\\n    json_data['inferenceConfig']['servingFramework'] = servingframework_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['minReplicaCount'] = minreplicacount_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['maxReplicaCount'] = maxreplicacount_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['containerResourceConfig']['computes'][0]['type'] = compute_type_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['containerResourceConfig']['computes'][0]['maxQty'] = compute_maxqty_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['containerResourceConfig']['computes'][0]['memory'] = compute_memory_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['containerResourceConfig']['computes'][0]['minQty'] = compute_minqty_param\\r\\n    json_data['inferenceConfig']['inferenceSpec']['containerResourceConfig']['volumeSizeinGB'] = container_volume_size_param\\r\\n    json_data = json.dumps(json_data)\\r\\n    response = None\\r\\n    try:\\r\\n        response = requests.post('https://api-aicloud.ad.infosys.com/api/v1/endpoint/deploy',headers=headers,data=json_data,verify=False)\\r\\n    except Exception as e:\\r\\n        response = e\\r\\n        logger.info(response)\\r\\n    response_data = json.loads(response.text)\\r\\n    logger.info('deployment_details')\\r\\n    logger.info(response_data)\\r\\n    return response_data\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""id"":121,""category"":""Endpoint"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""minreplicacount"":""1"",""maxreplicacount"":1,""compute_maxqty"":"""",""container_volume_size"":""2"",""compute_minqty"":"""",""compute_type"":""GPU"",""compute_memory"":""20GB"",""servingframework"":""Custom""}}","DragNDropLite-121"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{},""name"":""Analyze"",""alias"":""Analyze"",""parentCategory"":""167"",""id"":122,""codeGeneration"":{""requirements"":[""requests"",""json"",""urllib3""],""imports"":[""""],""script"":""def privacy_analyze(input):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    import sys\\r\\n    inputText = input['inputText']\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/analyze'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {'inputText': inputText}\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-122"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{},""name"":""Anonymize"",""alias"":""Anonymize"",""parentCategory"":""167"",""id"":123,""codeGeneration"":{""requirements"":[""requests"",""json"",""urllib3""],""imports"":[],""script"":""def privacy_anonymize(input):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    import sys\\r\\n    inputText_param, piiEntitiesToBeRedacted_param, redactionType_param = input['inputText'], input['piiEntitiesToBeRedacted'], input['redactionType']\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/anonymize'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {\\r\\n        'inputText': inputText_param,\\r\\n        'piiEntitiesToBeRedacted': [\\r\\n            piiEntitiesToBeRedacted_param\\r\\n        ],\\r\\n        'redactionType': redactionType_param\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response_anonymize:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-123"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{},""name"":""Profanity Analyze"",""alias"":""Profanity Analyze"",""parentCategory"":""167"",""id"":124,""codeGeneration"":{""requirements"":[""requests"",""json"",""urllib3""],""imports"":[],""script"":""\\r\\ndef profanity_analyze(inputText):\\r\\n    import requests\\r\\n    import json\\r\\n    import urllib3\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/safety/profanity/analyze'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {\\r\\n   'inputText': inputText\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-124"
"DragNDropLite","Core","{""subCategory"":"""",""formats"":{""stride"":""int"",""max_length"":""int""},""classname"":"" Prepare_Tokenized_Dataset"",""name"":""Prepare_Tokenized_Dataset"",""alias"":""Prepare_Tokenized_Dataset"",""parentCategory"":""168"",""id"":125,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""\\r\\ndef Prepare_Tokenized_Dataset(datalist,stride_param,max_length_param):\\r\\n    raw_datasets = datalist[0]\\r\\n    #pretrained_model= datalist[1]\\r\\n    def tokenize(element):\\r\\n        stride = stride_param\\r\\n        max_length=max_length_param\\r\\n        outputs = tokenizer(\\r\\n            element['code'],\\r\\n            truncation=True,\\r\\n            max_length=max_length,\\r\\n            return_overflowing_tokens=True,\\r\\n            return_length=True,\\r\\n            stride=stride,\\r\\n        )\\r\\n        input_batch = []\\r\\n        prev_chunk=[]\\r\\n        \\r\\n        not_first_chunk=False\\r\\n        for input_id in outputs.input_ids:\\r\\n            if len(input_id) == max_length:\\r\\n                input_batch.append(input_id)\\r\\n                prev_chunk=input_id\\r\\n                not_first_chunk=True\\r\\n            else:\\r\\n                if not_first_chunk:\\r\\n                    prev_chunk_last=prev_chunk[-(max_length-len(input_id)+stride):-(stride)]\\r\\n                    final_chunk=prev_chunk_last+input_id\\r\\n                    input_batch.append(final_chunk)\\r\\n                else:\\r\\n                    input_batch.append(input_id)\\r\\n                \\r\\n        return {'input_ids': input_batch}\\r\\n\\r\\n\\r\\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\\r\\n    tokenizer.pad_token = tokenizer.eos_token\\r\\n\\r\\n    logger.info('Creating tokenized_datasets .........')\\r\\n\\r\\n    tokenized_datasets = raw_datasets.map(\\r\\n        tokenize, batched=True, remove_columns=raw_datasets['train'].column_names\\r\\n    )\\r\\n    train_data_list = []\\r\\n    train_data_list.append(tokenized_datasets)\\r\\n    train_data_list.append(tokenizer)\\r\\n    #train_data_list.append(pretrained_model)\\r\\n    return train_data_list\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""LLM_Model"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""stride"":""256"",""max_length"":""2048""}}","DragNDropLite-125"
"DragNDropLite","Core","{""formats"":{""feature"":""text"",""group_name"":""text"",""featurevalue"":""text"",""type"":""text"",""feature_group"":""text""},""classname"":""GetOfflineFeature"",""name"":""GetOfflineFeature"",""alias"":""GetOfflineFeature"",""parentCategory"":""170"",""id"":126,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def GetOfflineFeature(claimsFeatureStore, featurevalue_param=[], feature_group_param='', feature_param='', group_name_param='', type_param=''):\\n    claimsFeatureStore.groupFeatures(feature_group_param,[feature_param])\\n    claimsFeatureStore.listFeature(group_name_param)\\n    claimsFeatureStore.listFeature(type_param)\\n    a, b, c = featurevalue_param.split(',')\\n    x=claimsFeatureStore.getOfflineFeatureValues(a, b, c)\\n    print(x)\\n    return x\\n\\n\\n\\n""},""category"":""Feature Store"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""feature"":[],""group_name"":[],""featurevalue"":[],""type"":[],""feature_group"":[]}}","DragNDropLite-126"
"DragNDropLite","Core","{""formats"":{""sourceplatform"":""text"",""feature"":""text"",""datatype"":""text"",""timestampfield"":""text"",""featuredescr"":""text"",""featurekey"":""text""},""classname"":""CreateFeature"",""name"":""CreateFeature"",""alias"":""CreateFeature"",""parentCategory"":""170"",""id"":127,""codeGeneration"":{""requirements"":[],""imports"":[""from UnifiedFeatureStore.AIFFeature import AIFFeature"",""from UnifiedFeatureStore.AIFFeatureStore import  AIFFeatureStore""],""script"":""def CreateFeature(feature_param='', datatype_param='', featurekey_param='', sourceplatform_param=[], featuredescr_param='', timestampfield_param=''):\\r\\n    price_feat = AIFFeature()\\r\\n    price_feat.setFeatureName(feature_param)\\r\\n    price_feat.setFeatureDatatype(datatype_param)\\r\\n    price_feat.setFeatureKey(featurekey_param)\\r\\n    database, source_entity, sourcePlatformCred = sourceplatform_param.split(',')\\r\\n    price_feat.setsourcePlatform(database, source_entity, sourcePlatformCred)\\r\\n    price_feat.setFeatureDescr(featuredescr_param)\\r\\n    price_feat.setFeatureTimestampField(timestampfield_param)\\r\\n    return price_feat\\r\\n\\r\\n\\n""},""category"":""Feature Store"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""sourceplatform"":[],""feature"":[],""datatype"":[],""timestampfield"":[],""featuredescr"":[],""featurekey"":[]}}","DragNDropLite-127"
"DragNDropLite","Core","{""formats"":{},""name"":""prompt"",""alias"":""prompt"",""parentCategory"":""171"",""id"":128,""codeGeneration"":{""requirements"":[""langchain[all]""],""imports"":[""from langchain.prompts import (     ChatPromptTemplate,      MessagesPlaceholder,      SystemMessagePromptTemplate,      HumanMessagePromptTemplate )"",""from langchain.chains import ConversationChain"",""from langchain.chat_models import ChatVertexAI"",""from langchain.memory import ConversationBufferMemory""],""script"":""def prompt():\\n    prompt = ChatPromptTemplate.from_messages([\\n    SystemMessagePromptTemplate.from_template('The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.'),\\n    MessagesPlaceholder(variable_name='history'),\\n    HumanMessagePromptTemplate.from_template('{input}')\\n    ])\\n    return prompt\\n\\n\\n""},""category"":""LangChain"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-128"
"DragNDropLite","Core","{""formats"":{""model"":""string""},""name"":""LLM"",""alias"":""LLM"",""parentCategory"":""171"",""id"":129,""codeGeneration"":{""requirements"":[""langchain[all]""],""imports"":[""from langchain.prompts import (     ChatPromptTemplate,      MessagesPlaceholder,      SystemMessagePromptTemplate,      HumanMessagePromptTemplate )"",""from langchain.chains import ConversationChain"",""from langchain.chat_models import ChatVertexAI"",""from langchain.memory import ConversationBufferMemory""],""script"":""def LLM(model_param='chat-bison'):\\n    LLM = ChatVertexAI(model=model_param)\\n    return LLM\\n\\n\\n""},""category"":""Langchain"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""model"":""chat-bison""}}","DragNDropLite-129"
"DragNDropLite","Core","{""formats"":{""return_messages"":""string""},""name"":""Memory"",""alias"":""Memory"",""parentCategory"":""171"",""id"":130,""codeGeneration"":{""requirements"":[""langchain[all]""],""imports"":[""from langchain.prompts import (     ChatPromptTemplate,      MessagesPlaceholder,      SystemMessagePromptTemplate,      HumanMessagePromptTemplate )"",""from langchain.chains import ConversationChain"",""from langchain.chat_models import ChatVertexAI"",""from langchain.memory import ConversationBufferMemory""],""script"":""def Memory(return_messages_param = 'True'):\\n    memory = ConversationBufferMemory(return_messages = return_messages_param)\\n    return memory\\n\\n\\n""},""category"":""LangChain"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""return_messages"":""True""}}","DragNDropLite-130"
"DragNDropLite","Core","{""formats"":{},""name"":""ImageAnalyzer"",""alias"":""ImageAnalyzer"",""parentCategory"":""167"",""id"":131,""codeGeneration"":{""requirements"":[""requests"",""json"",""urllib3""],""imports"":[""import requests"",""import json"",""import urllib3"",""import sys""],""script"":""def ImageAnalyzer(image_payload):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/analyze'\\r\\n    headers = {'content-type': 'multipart/form-data'}\\r\\n    print(url)\\r\\n    with open(image_payload, 'rb') as file:\\r\\n        response = requests.post(url, files={'payload': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-131"
"DragNDropLite","Core","{""formats"":{},""name"":""ImageAnonymizer"",""alias"":""ImageAnonymizer"",""parentCategory"":""167"",""id"":132,""codeGeneration"":{""requirements"":[""requests"",""json"",""urllib3""],""imports"":[""import requests"",""import json"",""import urllib3"",""import sys""],""script"":""def ImageAnonymizer(image_payload):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/privacy/pii/image/anonymize'\\r\\n    headers = {'content-type': 'multipart/form-data'}\\r\\n    print(url)\\r\\n    with open(image_payload, 'rb') as file:\\r\\n        response = requests.post(url, files={'payload': file}, verify=False)\\r\\n    print(response.status_code)\\r\\n    print(response.json())\\r\\n    return response.json()\\r\\n\\r\\n\\n""},""category"":""RAI"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","DragNDropLite-132"
"DragNDropLite","Core","{""id"":133,""name"":""AdapterMethod"",""alias"":""AdapterMethod"",""category"":"""",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{""adaptermethod"":""""},""formats"":{""adaptermethod"":""dropdown""}}","DragNDropLite-133"
"DragNDropLite","Core","{""formats"":{""bucket"":""text"",""key"":""text""},""classname"":""Download_Data"",""name"":""Download_Data"",""alias"":""Download_Data"",""parentCategory"":"""",""codeGeneration"":{""requirements"":[""boto3"",""re"",""botocore"",""pathlib"",""datasets"",""transformers"",""torch"",""tqdm"",""scikit-learn""],""imports"":[""import os"",""import pathlib"",""import boto3"",""import botocore"",""import json"",""import re"",""from datasets import Dataset, DatasetDict"",""from pathlib import Path"",""import logging"",""from collections import defaultdict"",""from tqdm import tqdm"",""from datasets import Dataset, DatasetDict"",""from transformers import AutoTokenizer, AutoModelForCausalLM"",""from transformers import DataCollatorForLanguageModeling"",""from transformers import Trainer, TrainingArguments"",""import torch"",""import random"",""from tqdm import tqdm"",""import argparse"",""import pandas as pd"",""from sklearn.model_selection import train_test_split"",""import gc"",""import pickle as pkl""],""script"":""logging_dir = '/app/logs'\\nlog_filename = 'create.log'\\nPath(logging_dir).mkdir(parents=True, exist_ok=True)\\nlog_filename = os.path.join(logging_dir, log_filename)\\nlogging.basicConfig(filename=log_filename,\\n                    format='%(asctime)s %(message)s',\\n                    filemode='w')\\nlogger = logging.getLogger()\\nlogger.setLevel(logging.INFO)\\n\\ndef Download_Data(bucket_param, key_param):\\n    bucket = bucket_param\\n    key = key_param\\n    WORKING_DIRECTORY = 'dataset_file'\\n    credentials = {'secret_key':'g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0','endpoint':'https://10.82.53.110','access_key':'GISeSU7xd6WBnXrU-QbffBee7WsCxaE2'}\\n    if not os.path.exists(WORKING_DIRECTORY):\\n        os.makedirs(WORKING_DIRECTORY)\\n    model_path = os.path.join(WORKING_DIRECTORY)\\n\\n    endpoint = credentials['endpoint']\\n    access_key = credentials['access_key']\\n    secret_key = credentials['secret_key']\\n    s3 = boto3.resource(service_name='s3', endpoint_url=endpoint, aws_access_key_id=access_key,\\n                        aws_secret_access_key=secret_key, verify=False)\\n    bucket_object = s3.Bucket(bucket)\\n    head, file_name = os.path.split(key)\\n    for my_bucket_object in bucket_object.objects.filter(Prefix=key):\\n        logger.info(my_bucket_object)\\n        object_save_path = (\\n            f'{model_path}/{pathlib.Path(my_bucket_object.key).name}'\\n        )\\n        bucket_object.download_file(my_bucket_object.key, object_save_path)\\n    \\n    dataset_file_path = os.path.join(WORKING_DIRECTORY,file_name)\\n    logger.info(dataset_file_path,'download completed')\\n    return dataset_file_path\\n\\n\\n""},""id"":134,""category"":"""",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""bucket"":"""",""key"":""""}}","DragNDropLite-134"
"DragNDropLite","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""DatasetExtractorConfig"",""name"":""MYSQL Extractor"",""alias"":""MYSQL Extractor"",""parentCategory"":""1"",""id"":135,""codeGeneration"":{""requirements"":[""mysql-connector-python""],""imports"":[""import mysql.connector"",""from urllib.parse import urlparse""],""script"":""def DatasetExtractorMYSQL(dataset_datasource_connectionDetails_userName='', dataset_datasource_connectionDetails_password='', dataset_datasource_salt='', dataset_datasource_connectionDetails_url='', dataset_attributes_Query=''):\\r\\n    def getConnection():\\r\\n        username = dataset_datasource_connectionDetails_userName\\r\\n        password = Security.decrypt(dataset_datasource_connectionDetails_password,dataset_datasource_salt)\\r\\n        url = dataset_datasource_connectionDetails_url\\r\\n        host = urlparse(url[5:]).hostname\\r\\n        port =urlparse(url[5:]).port\\r\\n        database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r\\n        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port = port)\\r\\n        return connection\\r\\n\\r\\n    connection = getConnection()\\r\\n    query = dataset_attributes_Query # self.mapQueryParams()\\r\\n    cursor = connection.cursor(dictionary=True)\\r\\n    cursor.execute(query)\\r\\n    results = cursor.fetchall()\\r\\n    return results\\n""},""category"":""ExtractorConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset"":""""}}","DragNDropLite-135"
"DragNDropLite","Core","{""formats"":{"""":"""",""dataset"":""dropdown""},""name"":""MYSQL Loader"",""alias"":""MYSQL Loader"",""parentCategory"":""2"",""id"":136,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DatasetLoader(dataset):\\r\\n    mode = '<dataset.attributes.writeMode>'\\r\\n    url='<dataset.datasource.connectionDetails.url>'\\r\\n    tablename = '<dataset.attributes.tableName>'\\r\\n    username = '<dataset.datasource.connectionDetails.userName>'\\r\\n    password = Security.decrypt('<dataset.datasource.connectionDetails.password>','<dataset.datasource.salt>')\\r\\n    host = urlparse(url[5:]).hostname\\r\\n    port = urlparse(url[5:]).port\\r\\n    database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r\\n    \\r\\n\\r\\n    cnx = mysql.connect(user=username, password=password, host=host, port=port, database=database)\\r\\n    mycursor = cnx.cursor()\\r\\n    if dataset != None and len(dataset) > 0:\\r\\n        columnList = list(dataset[0].keys())\\r\\n    if mode in 'overwrite':\\r\\n        mycursor.execute('Drop table IF EXISTS {0}'.format(tablename))\\r\\n\\r\\n    # create table if not exists\\r\\n    column_definition = ', '.join(['`{0}` TEXT'.format(c) for c in columnList])\\r\\n    createQuery = ' CREATE TABLE IF NOT EXISTS {0} ({1})'.format(tablename, column_definition)\\r\\n    mycursor.execute(createQuery)\\r\\n    data = []\\r\\n    for row in dataset:\\r\\n        try:\\r\\n            paramsDict = {}\\r\\n            values = []\\r\\n            for i in range(0, len(columnList)):\\r\\n                paramsDict[columnList[i]] = row[columnList[i]]\\r\\n                values.append(row[columnList[i]])\\r\\n\\r\\n            columns = ', '.join('`{0}`'.format(k) for k in paramsDict)\\r\\n            duplicates = ', '.join('{0}=VALUES({0})'.format(k) for k in paramsDict)\\r\\n            place_holders = ', '.join('%s'.format(k) for k in paramsDict)\\r\\n\\r\\n            query = 'INSERT INTO {0} ({1}) VALUES ({2})'.format(tablename, columns, place_holders)\\r\\n            if mode in ('update'):\\r\\n                query = '{0} ON DUPLICATE KEY UPDATE {1}'.format(query, duplicates)\\r\\n            data.append(values)\\r\\n        \\r\\n        except Exception as e:\\r\\n            logging.error('{0}:{1}'.format(e,row))\\r\\n    if(len(data) > 0):\\r\\n        mycursor.executemany(query, data)\\r\\n        cnx.commit()\\r\\n\\r\\n    mycursor.close()\\r\\n    cnx.close()\\r\\n\\r\\n\\n""},""category"":""DatasetLoaderConfig"",""inputEndpoints"":[""in""],""outputEndpoints"":[],""classname"":""DatasetLoaderConfig"",""attributes"":{"""":"""",""dataset"":""""}}","DragNDropLite-136"
"DragNDropLite","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""DatasetSourceConfig"",""name"":""DataSourcedict"",""alias"":""DataSourcedict"",""parentCategory"":""1"",""id"":137,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DataSourcedictREST(dataset_datasource_Url='', dataset_datasource_AuthDetails_username='', dataset_datasource_salt='', dataset_datasource_AuthDetails_password=''):\\r\\n    DSdict = {\\r\\n        'Url': dataset_datasource_Url,\\r\\n        'salt': dataset_datasource_salt,\\r\\n        'AuthDetails': {\\r\\n            'username': dataset_datasource_AuthDetails_username,\\r\\n            'password': dataset_datasource_AuthDetails_password\\r\\n        }\\r\\n    }\\r\\n    return DSdict\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""DatasetSourceConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out1""],""attributes"":{""dataset"":""""}}","DragNDropLite-137"
"DragNDropLite","Core","{""formats"":{""dataset"":""dropdown""},""classname"":""DatasetExtractorConfig"",""name"":""MINIO Extractor"",""alias"":""MINIO Extractor"",""parentCategory"":""1"",""codeGeneration"":{""imports"":[""from urllib.parse import urlparse"",""from minio import Minio""],""requirments"":[""minio""],""script"":""\\n\\nd\\ne\\nf\\n \\nD\\na\\nt\\na\\ns\\ne\\nt\\nE\\nx\\nt\\nr\\na\\nc\\nt\\no\\nr\\nM\\nI\\nN\\nI\\nO\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\nu\\nr\\nl\\n=\\n'\\n'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\na\\nc\\nc\\ne\\ns\\ns\\nK\\ne\\ny\\n=\\n'\\n'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\ns\\ne\\nc\\nr\\ne\\nt\\nK\\ne\\ny\\n=\\n'\\n'\\n,\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n=\\n'\\n'\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n=\\n'\\n'\\n)\\n:\\n\\n\\n \\n \\n \\n \\nU\\nR\\nL\\n \\n=\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\nu\\nr\\nl\\n\\n\\n \\n \\n \\n \\ns\\ne\\nc\\nu\\nr\\ne\\n \\n=\\n \\nT\\nr\\nu\\ne\\n \\ni\\nf\\n \\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\ns\\nc\\nh\\ne\\nm\\ne\\n \\n=\\n=\\n \\n'\\nh\\nt\\nt\\np\\ns\\n'\\n \\ne\\nl\\ns\\ne\\n \\nF\\na\\nl\\ns\\ne\\n\\n\\n \\n \\n \\n \\nc\\nl\\ni\\ne\\nn\\nt\\n \\n=\\nM\\ni\\nn\\ni\\no\\n(\\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\nh\\no\\ns\\nt\\nn\\na\\nm\\ne\\n+\\n'\\n:\\n'\\n+\\ns\\nt\\nr\\n(\\nu\\nr\\nl\\np\\na\\nr\\ns\\ne\\n(\\nU\\nR\\nL\\n)\\n.\\np\\no\\nr\\nt\\n)\\n,\\na\\nc\\nc\\ne\\ns\\ns\\n_\\nk\\ne\\ny\\n=\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\na\\nc\\nc\\ne\\ns\\ns\\nK\\ne\\ny\\n,\\ns\\ne\\nc\\nr\\ne\\nt\\n_\\nk\\ne\\ny\\n=\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\nd\\na\\nt\\na\\ns\\no\\nu\\nr\\nc\\ne\\n_\\nc\\no\\nn\\nn\\ne\\nc\\nt\\ni\\no\\nn\\nD\\ne\\nt\\na\\ni\\nl\\ns\\n_\\ns\\ne\\nc\\nr\\ne\\nt\\nK\\ne\\ny\\n,\\ns\\ne\\nc\\nu\\nr\\ne\\n=\\ns\\ne\\nc\\nu\\nr\\ne\\n)\\n\\n\\n \\n \\n \\n \\ni\\nf\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n.\\ns\\np\\nl\\ni\\nt\\n(\\n'\\n.\\n'\\n)\\n[\\n-\\n1\\n]\\n \\n=\\n=\\n \\n'\\nc\\ns\\nv\\n'\\n:\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\no\\nb\\nj\\n \\n=\\n \\nc\\nl\\ni\\ne\\nn\\nt\\n.\\ng\\ne\\nt\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n \\n=\\n \\np\\nd\\n.\\nr\\ne\\na\\nd\\n_\\nc\\ns\\nv\\n(\\no\\nb\\nj\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\nt\\nu\\nr\\nn\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n\\n\\n \\n \\n \\n \\ne\\nl\\ns\\ne\\n:\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n \\n=\\n \\n'\\n.\\n/\\n'\\n \\n+\\n \\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\ns\\nu\\nl\\nt\\n \\n=\\n \\nc\\nl\\ni\\ne\\nn\\nt\\n.\\nf\\ng\\ne\\nt\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n(\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\nb\\nu\\nc\\nk\\ne\\nt\\n,\\nd\\na\\nt\\na\\ns\\ne\\nt\\n_\\na\\nt\\nt\\nr\\ni\\nb\\nu\\nt\\ne\\ns\\n_\\no\\nb\\nj\\ne\\nc\\nt\\n,\\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n)\\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\nr\\ne\\nt\\nu\\nr\\nn\\n \\nf\\ni\\nl\\ne\\n_\\np\\na\\nt\\nh\\n""},""id"":138,""category"":""ExtractorConfig"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset"":""""}}","DragNDropLite-138"
"DragNDropLite","Core","{""formats"":{""inputbody"":""text""},""name"":""inference"",""alias"":""inference"",""parentCategory"":""172"",""id"":140,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def inference(url,inputbody_param):\\r\\n    \\r\\n    body=json.loads(inputbody_param)\\r\\n    headers= {'Content-Type':'application/json','Accept':'application/json'}\\r\\n    \\r\\n    response = requests.post(url, headers=headers,json=body,verify=False)\\r\\n    response_data = None\\r\\n    if response.status_code == 200:\\r\\n        response_data = json.loads(response.text)\\r\\n    else:\\r\\n        print(response.text)\\r\\n    logger.info('Inference Result : ')\\r\\n    logger.info(response_data)\\r\\n    return response_data\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""inference"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""inputbody"":""{}""}}","DragNDropLite-140"
"DragNDropLite","Core","{""id"":141,""name"":""LoaderConfig"",""alias"":""LoaderConfig"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-141"
"DragNDropLite","Core","{""formats"":{},""name"":"" TransformerConfig"",""alias"":"" TransformerConfig"",""parentCategory"":""143"",""id"":143,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{}}","DragNDropLite-143"
"DragNDropLite","Core","{""id"":145,""name"":""TransformerConfig"",""alias"":""TransformerConfig"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-145"
"DragNDropLite","Core","{""id"":146,""name"":""BaseConfig"",""alias"":""BaseConfig"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-146"
"DragNDropLite","Core","{""id"":147,""name"":""ModelBaseConfig"",""alias"":""ModelBaseConfig"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-147"
"DragNDropLite","Core","{""id"":148,""name"":""Regression"",""alias"":""Regression"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-148"
"DragNDropLite","Core","{""id"":149,""name"":"" DocumentComprehension"",""alias"":"" DocumentComprehension"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-149"
"DragNDropLite","Core","{""id"":150,""name"":""Classification"",""alias"":""Classification"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-150"
"DragNDropLite","Core","{""id"":151,""name"":""Haystack"",""alias"":""Haystack"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-151"
"DragNDropLite","Core","{""id"":152,""name"":""Ludwig"",""alias"":""Ludwig"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-152"
"DragNDropLite","Core","{""id"":153,""name"":""Legacy_AI"",""alias"":""Legacy_AI"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-153"
"DragNDropLite","Core","{""id"":154,""name"":""Cheque_detection"",""alias"":""Cheque_detection"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-154"
"DragNDropLite","Core","{""id"":155,""name"":"" Semantic Similarity"",""alias"":"" Semantic Similarity"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-155"
"DragNDropLite","Core","{""id"":156,""name"":""Sentiment analysis"",""alias"":""Sentiment analysis"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-156"
"DragNDropLite","Core","{""id"":157,""name"":""Data Profiling"",""alias"":""Data Profiling"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-157"
"DragNDropLite","Core","{""id"":158,""name"":""Feast"",""alias"":""Feast"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-158"
"DragNDropLite","Core","{""id"":159,""name"":"" HayStack_DataHandling_Components"",""alias"":"" HayStack_DataHandling_Components"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-159"
"DragNDropLite","Core","{""id"":160,""name"":""RFP_Elastic_API"",""alias"":""RFP_Elastic_API"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-160"
"DragNDropLite","Core","{""id"":161,""name"":""ExtractorConfig"",""alias"":""ExtractorConfig"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-161"
"DragNDropLite","Core","{""id"":162,""name"":""HayStack_DocumentStore"",""alias"":""HayStack_DocumentStore"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-162"
"DragNDropLite","Core","{""id"":163,""name"":""HayStack_SemanticSearch_Component"",""alias"":""HayStack_SemanticSearch_Component"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-163"
"DragNDropLite","Core","{""id"":164,""name"":""HayStack_PipelineTrigger"",""alias"":""HayStack_PipelineTrigger"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-164"
"DragNDropLite","Core","{""id"":165,""name"":""Haystack_DataExtractor"",""alias"":""Haystack_DataExtractor"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-165"
"DragNDropLite","Core","{""id"":166,""name"":""Convertors"",""alias"":""Convertors"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-166"
"DragNDropLite","Core","{""id"":167,""name"":""RAI"",""alias"":""RAI"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-167"
"DragNDropLite","Core","{""id"":168,""name"":""LLM_Model"",""alias"":""LLM_Model"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-168"
"DragNDropLite","Core","{""id"":169,""name"":""Endpoint"",""alias"":""Endpoint"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-169"
"DragNDropLite","Core","{""id"":170,""name"":""Feature Store"",""alias"":""Feature Store"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-170"
"DragNDropLite","Core","{""id"":171,""name"":""LangChain"",""alias"":""LangChain"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-171"
"DragNDropLite","Core","{""id"":172,""name"":""inference"",""alias"":""inference"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-172"
"DragNDropLite","Core","{""id"":173,""name"":""VG Nodes"",""alias"":""VG Nodes"",""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""parentCategory"":"""",""attributes"":{},""formats"":{}}","DragNDropLite-173"
"DragNDropLite","Core","{""formats"":{""path"":""text"",""file_path"":""text""},""classname"":""zipfileextraction"",""name"":""zipfileextraction"",""alias"":""zipfileextraction"",""parentCategory"":""173"",""id"":174,""codeGeneration"":{""requirements"":[""os"",""shutil"",""zipfile""],""imports"":[""import os"",""import shutil"",""from zipfile import ZipFile""],""script"":""def zipfileextraction_<id>(model_path,path_param='',file_path_param=''):\\n        #make directory\\n    try:         \\n        os.makedirs(path_param)\\n    except FileExistsError:         \\n        print('file is there')\\n        \\n    #clear directory\\n    for root, dirs, files in os.walk(path_param):\\n        for f in files:\\n            os.unlink(os.path.join(root, f))\\n        for d in dirs:\\n            shutil.rmtree(os.path.join(root, d))\\n    print('directory cleared')\\n    \\n    #zip extraction\\n    with ZipFile(model_path, 'r') as zObject:\\n        zObject.extractall(path=path_param)\\n    print('file is extracted')\\n    \\n    #files in path \\n    files = os.listdir(path_param)\\n    print('files in directory',files)\\n    \\n    finalpath = f'{path_param}/{file_path_param}'\\n    print(finalpath)\\n    return finalpath\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""category"":"""",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""path"":"""",""file_path"":""""}}","DragNDropLite-174"
"DragNDropLite","Core","{""formats"":{},""classname"":""datasetProcess"",""name"":""datasetProcess"",""alias"":""datasetProcess"",""parentCategory"":""173"",""id"":175,""codeGeneration"":{""requirements"":[""pandas"",""scikit-learn""],""imports"":[""import json"",""import os   "",""import pandas as pd"",""import requests"",""import ast"",""from sklearn.preprocessing import LabelEncoder""],""script"":""def datasetProcess_<id>(dataset1,dataset2):\\r    #Post-process Data\\r    with open(dataset1, 'r') as json_file:\\r        data = json.load(json_file)\\r    def img_to_text(path1):\\r        url = 'http://10.177.28.36:8024/PdfOcrExtraction/tocr'\\r        payload = {'userKey': 'user',\\r        'isMetaData': 'false'}\\r        files=[\\r          ('file',('image.jpg',open(path1,'rb'),'image/jpeg'))\\r        ]\\r        headers = {}\\r        response = requests.request('POST', url, headers=headers, data=payload, files=files)\\r        text = response.text\\r        import ast\\r        result = ast.literal_eval(text)\\r        result = result['TEXT']\\r        return result\\r    Context_text = []\\r    json_label =[]\\r    for i in range(len(data['labelsArray'])):\\r        path = data['labelsArray'][i]['filePath']\\r        label = data['labelsArray'][i]['label']\\r        final_path = f'{dataset2}/{path}'\\r        result = img_to_text(final_path)\\r        #result = final_path\\r        #text = result[0].content\\r        Context_text.append(result)\\r        json_label.append(label)\\r    data = {'CONTENT': Context_text,\\r            'LABEL': json_label}\\r    df = pd.DataFrame(data)\\r    le = LabelEncoder() # Fit the encoder to the data \\r    df['LABEL'] = le.fit_transform(df['LABEL'])\\r    new_df = df[['CONTENT', 'LABEL']]\\r    return new_df\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[""out1""],""attributes"":{}}","DragNDropLite-175"
"DragNDropLite","Core","{""formats"":{""zip_file_path"":""text"",""epochs"":""integer"",""model_path"":""text"",""max_len"":""integer""},""classname"":""trainingModel"",""name"":""trainingModel"",""alias"":""trainingModel"",""parentCategory"":""173"",""id"":176,""codeGeneration"":{""requirements"":[""pandas"",""numpy"",""shutil"",""sklearn"",""seaborn"",""transformers"",""logging"",""tqdm"",""torch""],""imports"":[""import os"",""import numpy as np"",""import pandas as pd"",""import numpy as np"",""from sklearn.model_selection import train_test_split"",""import torch"",""import seaborn as sns"",""import transformers"",""import json"",""from tqdm import tqdm"",""from torch.utils.data import Dataset, DataLoader"",""from transformers import RobertaModel, RobertaTokenizer"",""import logging"",""from torch import cuda"",""from torch.utils.data import Dataset, DataLoader"",""from transformers import RobertaModel"",""import zipfile""],""script"":""def trainingModel_<id>(new_df,save_path,epochs_param='',model_path_param='',zip_file_path_param='',max_len_param=''):\\r    print(new_df)\\r    logging.basicConfig(level=logging.ERROR)\\r    device = 'cuda' if cuda.is_available() else 'cpu'\\r    #MAX_LEN = 256\\r    TRAIN_BATCH_SIZE = 8\\r    VALID_BATCH_SIZE = 4\\r    learning_rate=1e-05\\r    # EPOCHS = 1\\r    print(save_path)\\r    tokenizer = RobertaTokenizer.from_pretrained(save_path)\\r    train_size = 0.8\\r    train_data=new_df.sample(frac=train_size,random_state=200)\\r    test_data=new_df.drop(train_data.index).reset_index(drop=True)\\r    train_data = train_data.reset_index(drop=True)\\r    print('FULL Dataset: {}'.format(new_df.shape))\\r    print('TRAIN Dataset: {}'.format(train_data.shape))\\r    print('TEST Dataset: {}'.format(test_data.shape))\\r    training_set = Encoder(train_data, tokenizer, max_len_param)\\r    testing_set = Encoder(test_data, tokenizer, max_len_param)\\r    train_params = {'batch_size': TRAIN_BATCH_SIZE,\\r                    'shuffle': True,\\r                    'num_workers': 0\\r                    }\\r    test_params = {'batch_size': VALID_BATCH_SIZE,\\r                    'shuffle': True,\\r                    'num_workers': 0\\r                    }\\r    training_loader = DataLoader(training_set, **train_params)\\r    testing_loader = DataLoader(testing_set, **test_params)\\r    model = RobertaClass(save_path)\\r    model.to(device)\\r    for epoch in range(epochs_param):\\r        train(epoch,training_loader,testing_loader,model,learning_rate)\\r    save_model(model,tokenizer,model_path_param)\\r    #model_path_param = '/app/jobs/models/roberta'\\r    #zip_file_path ='/app/jobs/models/roberta.zip'\\r    zip_folder(model_path_param, zip_file_path_param)\\r    print(zip_file_path_param)\\r    return zip_file_path_param   \\rclass Encoder(Dataset):\\r    def __init__(self, dataframe, tokenizer, max_len):\\r        self.tokenizer = tokenizer\\r        self.data = dataframe\\r        self.text = dataframe.CONTENT\\r        self.targets = self.data.LABEL\\r        self.max_len = max_len\\r    def __len__(self):\\r        return len(self.text)\\r    def __getitem__(self, index):\\r        text = str(self.text[index])\\r        text = ' '.join(text.split())\\r        inputs = self.tokenizer.encode_plus(\\r            text,\\r            None,\\r            add_special_tokens=True,\\r            max_length=self.max_len,\\r            pad_to_max_length=True,\\r            return_token_type_ids=True\\r        )\\r        ids = inputs['input_ids']\\r        mask = inputs['attention_mask']\\r        token_type_ids = inputs['token_type_ids']\\r        return {\\r            'ids': torch.tensor(ids, dtype=torch.long),\\r            'mask': torch.tensor(mask, dtype=torch.long),\\r            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\\r            'targets': torch.tensor(self.targets[index], dtype=torch.float)\\r        }\\rclass RobertaClass(torch.nn.Module):\\r    def __init__(self,save_path):\\r        super(RobertaClass, self).__init__()\\r        self.l1 = RobertaModel.from_pretrained(save_path)\\r        self.pre_classifier = torch.nn.Linear(768, 768)\\r        self.dropout = torch.nn.Dropout(0.3)\\r        self.classifier = torch.nn.Linear(768, 5)\\r    def forward(self, input_ids, attention_mask, token_type_ids):\\r        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\\r        hidden_state = output_1[0]\\r        pooler = hidden_state[:, 0]\\r        pooler = self.pre_classifier(pooler)\\r        pooler = torch.nn.ReLU()(pooler)\\r        pooler = self.dropout(pooler)\\r        output = self.classifier(pooler)\\r        return output\\rdef calcuate_accuracy(preds, targets):\\r    n_correct = (preds==targets).sum().item()\\r    return n_correct\\rdef train(epoch,training_loader,testing_loader,model,learning_rate_param):\\r    from torch import cuda\\r    device = 'cuda' if cuda.is_available() else 'cpu'\\r    from tqdm import tqdm\\r    #LEARNING_RATE = 1e-05\\r    loss_function = torch.nn.CrossEntropyLoss()\\r    optimizer = torch.optim.Adam(params =  model.parameters(), lr=learning_rate_param)\\r    tr_loss = 0\\r    n_correct = 0\\r    nb_tr_steps = 0\\r    nb_tr_examples = 0\\r    model.train()\\r    for _,data in tqdm(enumerate(training_loader, 0)):\\r        ids = data['ids'].to(device, dtype = torch.long)\\r        mask = data['mask'].to(device, dtype = torch.long)\\r        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\\r        targets = data['targets'].to(device, dtype = torch.long)\\r        outputs = model(ids, mask, token_type_ids)\\r        loss = loss_function(outputs, targets)\\r        tr_loss += loss.item()\\r        big_val, big_idx = torch.max(outputs.data, dim=1)\\r        n_correct += calcuate_accuracy(big_idx, targets)\\r        nb_tr_steps += 1\\r        nb_tr_examples+=targets.size(0)\\r        if _%5000==0:\\r            loss_step = tr_loss/nb_tr_steps\\r            accu_step = (n_correct*100)/nb_tr_examples \\r            print(f'Training Loss per 5000 steps: {loss_step}')\\r            print(f'Training Accuracy per 5000 steps: {accu_step}')\\r        optimizer.zero_grad()\\r        loss.backward()\\r        # # When using GPU\\r        optimizer.step()\\r    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\\r    epoch_loss = tr_loss/nb_tr_steps\\r    epoch_accu = (n_correct*100)/nb_tr_examples\\r    print(f'Training Loss Epoch: {epoch_loss}')\\r    print(f'Training Accuracy Epoch: {epoch_accu}')\\r    return model\\rdef save_model(model,tokenizer,model_path_param):\\r\\r    path=model_path_param\\r    #clear directory\\r    for root, dirs, files in os.walk(path):\\r        for f in files:\\r            os.unlink(os.path.join(root, f))\\r        for d in dirs:\\r            shutil.rmtree(os.path.join(root, d))\\r    print('directory cleared')\\r    try:         \\r        os.makedirs(path)\\r    except FileExistsError:         \\r        print('file is there')\\r    arr = os.listdir(path)\\r    print(arr)\\r    #with open('/app/jobs/models/roberta/robertamodel.pkl','wb') as f:\\r    #    pickle.dump(model,f)\\r    #output_model_file = '/app/jobs/models/roberta/pytorch_roberta_sentiment.pt'\\r    output_model_file =f'{path}'+'/pytorch_roberta_sentiment.pt'\\r    torch.save(model, output_model_file)\\r    #torch.save(model.state_dict(), output_model_file)\\r    tokenizer.save_pretrained(path) \\r    return path  \\rdef zip_folder(folder_path, zip_path):\\r    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:\\r        for root, _, files in os.walk(folder_path):\\r            for file in files:\\r                file_path = os.path.join(root, file)\\r                arcname = os.path.relpath(file_path, folder_path)\\r                zip_file.write(file_path, arcname)\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[""out""],""attributes"":{""zip_file_path"":""/app/jobs/models/roberta.zip"",""epochs"":""4"",""model_path"":""/app/jobs/models/roberta"",""max_len"":""256""}}","DragNDropLite-176"
"DragNDropLite","Core","{""formats"":{""minio_path"":""text"",""bucket_name"":""text""},""classname"":""upload_to_minio"",""name"":""upload_to_minio"",""alias"":""upload_to_minio"",""parentCategory"":""173"",""id"":177,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def upload_to_minio_<id>(local_path,client,bucket_name_param='', minio_path_param=''):\\r    obj_name= minio_path_param\\r    result = client.fput_object(bucket_name_param , obj_name, local_path)\\r    return result,local_path\\r\\r\\r\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[],""attributes"":{""minio_path"":"""",""bucket_name"":""datasets""}}","DragNDropLite-177"
"DragNDropLite","Core","{""formats"":{""secret_key"":""text"",""access_key"":""text"",""url"":""text""},""classname"":""Client_cred"",""name"":""Client_cred"",""alias"":""Client_cred"",""parentCategory"":""173"",""id"":178,""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def Client_cred_<id>(url_param='', access_key_param='', secret_key_param=''):\\r    secure = True if urlparse(url_param).scheme == 'https' else False\\r    client =Minio(urlparse(url_param).hostname+':'+str(urlparse(url_param).port),access_key=access_key_param,secret_key=secret_key_param,secure=secure)\\r    return client\\r\\r\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""secret_key"":""miniosecret"",""access_key"":""minio"",""url"":""http://10.85.12.143:31262/""}}","DragNDropLite-178"
"DragNDropLite","Core","{""formats"":{""dataset_datasource_connectiondetails_secretkey"":""text"",""dataset_datasource_connectiondetails_accesskey"":""text"",""dataset_attributes_object"":""text"",""dataset_datasource_connectiondetails_url"":""text"",""dataset_attributes_bucket"":""text""},""classname"":""Extractor"",""name"":""Extractor"",""alias"":""Extractor"",""parentCategory"":""173"",""id"":179,""codeGeneration"":{""requirements"":[],""imports"":[""import logging as logger"",""from urllib.parse import urlparse"",""from minio import Minio "",""import os""],""script"":""def Extractor_<id>(dataset_datasource_connectiondetails_url_param='', dataset_datasource_connectiondetails_accesskey_param='', dataset_datasource_connectiondetails_secretkey_param='', dataset_attributes_object_param='',dataset_attributes_bucket_param=''):\\r    URL = dataset_datasource_connectiondetails_url_param\\r    secure = True if urlparse(URL).scheme == 'https' else False\\r    client =Minio(urlparse(URL).hostname+':'+str(urlparse(URL).port),access_key=dataset_datasource_connectiondetails_accesskey_param,secret_key=dataset_datasource_connectiondetails_secretkey_param,secure=secure)\\r    if dataset_attributes_object_param.split('.')[-1] == 'csv':\\r        obj = client.get_object(dataset_attributes_bucket_param,dataset_attributes_object_param)\\r        dataset = pd.read_csv(obj)\\r        return dataset\\r    else:\\r        file_path = './' + dataset_attributes_object_param\\r        print('file_path====', file_path)\\r        \\r        current_path = os.getcwd()\\r        print(current_path)\\r        result = client.fget_object(dataset_attributes_bucket_param,dataset_attributes_object_param, file_path)\\r        print('result++++', result)\\r        \\r        return file_path\\r\\r\\r\\r\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[""out""],""attributes"":{""dataset_datasource_connectiondetails_secretkey"":""miniosecret"",""dataset_datasource_connectiondetails_accesskey"":""minio"",""dataset_attributes_object"":"""",""dataset_datasource_connectiondetails_url"":""http://10.85.12.143:31262"",""dataset_attributes_bucket"":""""}}","DragNDropLite-179"
"Azure","Core","{""id"": ""1"", ""name"": ""Dataset Extractor"", ""alias"": ""Dataset Extractor"", ""formats"": {""dataset"": ""dropdown""}, ""category"": ""ExtractorConfig"", ""classname"": ""ExtractorConfig"", ""attributes"": {""azuredataset"": """"}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Azure-1"
"Azure","Core","{""id"": ""2"", ""name"": ""Python Script"", ""alias"": ""Python Script"", ""formats"": {""inputs"": ""textarea"", ""script"": ""textarea"", ""arguments"": ""list"", ""compute_target"": ""textarea"", ""pip_dependencies"": ""textarea""}, ""category"": ""TransformerConfig"", ""classname"": ""TransformerConfig"", ""attributes"": {""inputs"": """", ""script"": [], ""arguments"": [], ""compute_target"": """", ""pip_dependencies"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Azure-2"
"Azure","Core","{""id"": ""3"", ""name"": ""Auto ML"", ""alias"": ""Auto ML"", ""formats"": {""task"": ""taskdropdown"", ""outputs"": ""list"", ""compute_target"": ""textarea"", ""label_column_name"": ""textarea""}, ""category"": ""AutoMLConfig"", ""classname"": ""AutoMLConfig"", ""attributes"": {""task"": """", ""outputs"": [], ""compute_target"": """", ""label_column_name"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Azure-3"
"Azure","Core","{""id"": ""4"", ""name"": ""Batch Prediction"", ""alias"": ""Batch Prediction"", ""formats"": {""type"": ""Azure"", ""model"": ""dropdown"", ""score_script"": ""textarea"", ""compute_target"": ""textarea"", ""ouput_dataset_name"": ""textarea""}, ""category"": ""BatchConfig"", ""classname"": ""BatchConfig"", ""attributes"": {""model"": """", ""script"": [], ""compute_target"": """", ""ouput_dataset_name"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Azure-4"
"Vertex","Core","{""id"": ""1"", ""name"": ""Dataset Extractor"", ""alias"": ""Dataset Extractor"", ""formats"": {""dataset"": ""textarea"", ""dropValues"": [""image"", ""tabular"", ""text""], ""gcs_source"": ""textarea"", ""dataset_type"": ""dropValues""}, ""category"": ""ExtractorConfig"", ""classname"": ""ExtractorConfig"", ""attributes"": {""gcs_source"": """", ""dataset_name"": """", ""dataset_type"": """"}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Vertex-1"
"Vertex","Core","{""id"": ""2"", ""name"": ""Auto ML"", ""alias"": ""Auto ML"", ""formats"": {""task"": ""dropValues1"", ""dropValues1"": [""image-classification"", ""image-object-detection"", ""tabular-classification"", ""tabular-regression"", ""text-classification"", ""text-extraction"", ""text-sentiment"", ""video-classification"", ""video-action-recognition"", ""video-object-tracking""], ""dropValues2"": [""maximize-au-roc"", ""minimize-log-loss"", ""maximize-au-prc"", ""maximize-precision-at-recall"", ""maximize-recall-at-precision"", ""minimize-log-loss"", ""minimize-rmse"", ""minimize-mae"", ""minimize-rmsle""], ""target_column"": ""textarea"", ""column_transformations"": ""textarea"", ""optimization_objective"": ""dropValues2""}, ""category"": ""AutoMLConfig"", ""classname"": ""AutoMLConfig"", ""attributes"": {""task"": """", ""target_column"": """", ""column_transformations"": """", ""optimization_objective"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Vertex-2"
"Vertex","Core","{""id"": ""3"", ""name"": ""Endpoint"", ""alias"": ""Endpoint"", ""formats"": {""endpoint_name"": ""textarea""}, ""category"": ""CreateEndpoint"", ""classname"": ""CreateEndpoint"", ""attributes"": {""endpoint_name"": """"}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Vertex-3"
"Vertex","Core","{""id"": ""4"", ""name"": ""Model Deployment"", ""alias"": ""Model Deployment"", ""formats"": {""dropValues1"": [""n1-standard-2"", ""n1-standard-4"", ""n1-standard-8"", ""n1-standard-16"", ""n1-standard-32"", ""n1-highmem-2"", ""n1-highmem-4"", ""n1-highmem-8"", ""n1-highmem-16"", ""n1-highmem-32"", ""n1-highcpu-4"", ""n1-highcpu-8"", ""n1-highcpu-16"", ""n1-highcpu-32""], ""dedicated_resources_machine_type"": ""dropValues1"", ""dedicated_resources_max_replica_count"": ""textarea"", ""dedicated_resources_min_replica_count"": ""textarea""}, ""category"": ""CreateEndpoint"", ""classname"": ""ModelDeployment"", ""attributes"": {""dedicated_resources_machine_type"": """", ""dedicated_resources_max_replica_count"": """", ""dedicated_resources_min_replica_count"": """"}, ""inputEndpoints"": [""in1"", ""in2""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Vertex-4"
"Vertex","Core","{""id"": ""5"", ""name"": ""Batch Prediction"", ""alias"": ""Batch Prediction"", ""formats"": {""type"": ""vertex-ai"", ""model"": ""dropdown"", ""dropValues"": [""jsonl"", ""csv""], ""predictions_format"": ""dropValues"", ""gcs_destination_prefix"": ""textarea""}, ""category"": ""BatchConfig"", ""classname"": ""BatchConfig"", ""attributes"": {""model"": """", ""predictions_format"": """", ""gcs_destination_prefix"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Vertex-5"
"ICMM","Core","{""id"": ""1"", ""name"": ""Dataset Extractor"", ""alias"": ""Dataset Extractor"", ""formats"": {""api"": ""http://victlptst-19:8000/dataset/fetchalldataset/"", ""payload"": {""userId"": ""demo_mlflow@infosys.com""}, ""response"": ""displayName"", ""mlflowdataset"": ""api""}, ""category"": ""ExtractorConfig"", ""classname"": ""ExtractorConfig"", ""attributes"": {""mlflowdataset"": """"}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-1"
"ICMM","Core","{""id"": ""2"", ""name"": ""Paragraph Classifier"", ""alias"": ""Paragraph Classifier"", ""formats"": {""api"": ""http://10.177.28.36:9090/model/list_models/"", ""api1"": ""http://10.217.10.232:8102/configuration/getalltrainingsets/"", ""label"": ""api1"", ""payload"": {""userId"": ""demo_mlflow@infosys.com""}, ""payload1"": {""validateUser"": {""role"": ""Admin"", ""appID"": ""5f8d657cfb9d17629a972af2"", ""orgID"": ""5f8d6567fb9d17629a972af1"", ""emailId"": ""admin@user"", ""password"": ""123456@a""}}, ""response1"": ""trainSetName"", ""classifierconfig"": ""api""}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""label"": """", ""classifierconfig"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-2"
"ICMM","Core","{""id"": ""3"", ""name"": ""Para Classifier"", ""alias"": ""Para Classifier"", ""formats"": {""api"": ""http://10.217.10.232:8102/configuration/getalltrainingsets/"", ""api1"": ""http://10.217.10.232:8102/configuration/getlabelsforclassifier/"", ""class"": ""api1"", ""payload"": {""validateUser"": {""role"": ""Admin"", ""appID"": ""5f8d657cfb9d17629a972af2"", ""orgID"": ""5f8d6567fb9d17629a972af1"", ""emailId"": ""admin@user"", ""password"": ""123456@a""}}, ""payload1"": {""validateUser"": {""role"": ""Admin"", ""appID"": ""5f7f4f4e2b06ff4bd1f31cb7"", ""orgID"": ""5f7f4f2e2b06ff4bd1f31cb6"", ""emailId"": ""admin@user"", ""password"": ""123456@a"", ""classifierID"": ""_id""}}, ""response"": ""trainSetName"", ""apiresult"": ""userapi"", ""classifierconfig"": ""api""}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""class"": """", ""classifierconfig"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-3"
"ICMM","Core","{""id"": ""4"", ""name"": ""Document Classifier"", ""alias"": ""Document Classifier"", ""formats"": {""api"": ""http://10.217.10.232:8102/configuration/getalltrainingsets/"", ""api1"": ""http://10.217.10.232:8102/configuration/getlabelsforclassifier/"", ""class"": ""api1"", ""payload"": {""validateUser"": {""role"": ""Admin"", ""appID"": ""5f8d657cfb9d17629a972af2"", ""orgID"": ""5f8d6567fb9d17629a972af1"", ""emailId"": ""admin@user"", ""password"": ""123456@a""}}, ""payload1"": {""validateUser"": {""role"": ""Admin"", ""appID"": ""5f7f4f4e2b06ff4bd1f31cb7"", ""orgID"": ""5f7f4f2e2b06ff4bd1f31cb6"", ""emailId"": ""admin@user"", ""password"": ""123456@a"", ""classifierID"": ""_id""}}, ""response"": ""trainSetName"", ""apiresult"": ""userapi"", ""classifierconfig"": ""api""}, ""category"": ""AnalyzerConfig"", ""classname"": ""AnalyzerConfig"", ""attributes"": {""class"": """", ""classifierconfig"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-4"
"ICMM","Core","{""id"": ""5"", ""name"": ""Label Value Classifier"", ""alias"": ""Label Value Classifier"", ""formats"": {""api"": ""http://10.217.10.232:8102/configuration/getalltrainingsets/"", ""payload"": {""validateUser"": {""role"": ""Admin"", ""appID"": ""5f8d657cfb9d17629a972af2"", ""orgID"": ""5f8d6567fb9d17629a972af1"", ""emailId"": ""admin@user"", ""password"": ""123456@a""}}, ""response"": ""trainSetName"", ""classifierconfig"": ""api""}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""classifierconfig"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-5"
"ICMM","Core","{""id"": ""6"", ""name"": ""Document Analysis"", ""alias"": ""Document Analysis"", ""formats"": {""tag"": ""dropValues"", ""dropValues"": [""Header"", ""Footer"", ""Table of Contents"", ""All Tables"", ""All Sections"", ""Page Wise Data"", ""Section"", ""Document Table"", ""Table Row Column""]}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""tag"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-6"
"ICMM","Core","{""id"": ""7"", ""name"": ""Date Extraction"", ""alias"": ""Date Extraction"", ""formats"": {""DateFormat"": ""textarea""}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""DateFormat"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-7"
"ICMM","Core","{""id"": ""8"", ""name"": ""String Match"", ""alias"": ""String Match"", ""formats"": {""string"": ""textarea"", ""caseSensitive"": """"}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""string"": """", ""caseSensitive"": ""False""}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-8"
"ICMM","Core","{""id"": ""9"", ""name"": ""Block Text"", ""alias"": ""Block Text"", ""formats"": {""endString"": ""textarea"", ""startString"": ""textarea"", ""caseSensitive"": """"}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""endString"": """", ""startString"": """", ""caseSensitive"": ""False""}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-9"
"ICMM","Core","{""id"": ""10"", ""name"": ""RegEx Match"", ""alias"": ""RegEx Match"", ""formats"": {""regex"": ""textarea"", ""caseSensitive"": """"}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""regexp"": """", ""caseSensitive"": ""False""}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-10"
"ICMM","Core","{""id"": ""11"", ""name"": ""NER Match"", ""alias"": ""NER Match"", ""formats"": {""applyOn"": ""dropValues2"", ""matchWith"": ""dropValues1"", ""dropValues1"": [""Stanford"", ""NLTK""], ""dropValues2"": [""All"", ""Organization"", ""Person"", ""Location""]}, ""category"": ""BaseConfig"", ""classname"": ""BaseConfig"", ""attributes"": {""applyOn"": [], ""matchWith"": []}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-11"
"ICMM","Core","{""id"": ""12"", ""name"": ""Auto ML"", ""alias"": ""Auto ML"", ""formats"": {""task"": ""taskdropdown"", ""outputs"": ""list"", ""compute_target"": ""textarea"", ""label_column_name"": ""textarea""}, ""category"": ""AutoMLConfig"", ""classname"": ""AutoMLConfig"", ""attributes"": {""task"": """", ""outputs"": [], ""compute_target"": """", ""label_column_name"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","ICMM-12"
"Mlflow","Core","{""id"": ""1"", ""name"": ""Dataset Extractor"", ""alias"": ""Dataset Extractor"", ""formats"": {""task"": ""dropValues1"", ""type"": ""Mlflow"", ""dataset"": ""dropdown"", ""dropValues"": [""tabular"", ""text"", ""image""], ""dropValues1"": """", ""dataset_type"": ""dropValues""}, ""category"": ""ExtractorConfig"", ""classname"": ""ExtractorConfig"", ""attributes"": {""task"": ""single_label_classification"", ""dataset"": """", ""dataset_type"": ""tabular""}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Mlflow-1"
"Mlflow","Core","{""id"": ""2"", ""name"": ""Auto ML"", ""alias"": ""Auto ML"", ""formats"": {""api"": ""http://victlptst-19:8000/training/automl_training/"", ""type"": ""MlflowAutoml"", ""domain"": ""domainDropValues"", ""dataset"": """", ""payload"": {""label"": """", ""domain"": """", ""userId"": ""admin@infosys.com"", ""platform"": ""mlflow"", ""model_name"": """", ""time_limit"": """", ""val_dataset"": """", ""train_dataset"": """", ""experiment_name"": """"}, ""model_name"": ""textarea"", ""time_limit"": ""textarea"", ""OutputColumn"": ""datasetdropdown"", ""mlflowdataset"": ""api"", ""experiment_name"": ""textarea"", ""domainDropValues"": [""image"", ""tabular"", ""text""]}, ""category"": ""AutoMLConfig"", ""classname"": ""AutoMLConfig"", ""attributes"": {""domain"": """", ""model_name"": """", ""time_limit"": ""60"", ""OutputColumn"": """", ""experiment_name"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Mlflow-2"
"Mlflow","Core","{""id"": ""3"", ""name"": ""Endpoint"", ""alias"": ""Endpoint"", ""formats"": {""api"": ""http://victlptst-19:8000/endpoints/create_endpoint/"", ""type"": ""MlflowEndpoint"", ""payload"": {""name"": """", ""userId"": ""admin@infosys.com"", ""version"": """", ""platform"": ""mlflow"", ""model_name"": """"}, ""version"": ""textarea"", ""model_name"": ""textarea"", ""endpoint_name"": ""textarea"", ""mlflowdataset"": ""api""}, ""category"": ""CreateEndpoint"", ""classname"": ""CreateEndpoint"", ""attributes"": {""version"": """", ""model_name"": """", ""endpoint_name"": """"}, ""inputEndpoints"": [], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Mlflow-3"
"Mlflow","Core","{""id"": ""4"", ""name"": ""Batch Prediction"", ""alias"": ""Batch Prediction"", ""formats"": {""api"": ""http://victlptst-19:8000/endpoints/batchpredict/"", ""api1"": ""http://victlptst-19:8000/endpoints/list_endpoints/"", ""type"": ""BatchPrediction"", ""dataset"": ""dropdown"", ""payload"": {""userId"": ""admin@infosys.com"", ""platform"": ""mlflow""}, ""response1"": ""displayName"", ""endpoint_name"": ""api1"", ""mlflowdataset"": ""api""}, ""category"": ""BatchConfig"", ""classname"": ""BatchConfig"", ""attributes"": {""dataset"": """", ""endpoint_name"": """"}, ""inputEndpoints"": [""in""], ""parentCategory"": """", ""outputEndpoints"": [""out""]}","Mlflow-4"
"Langchain","Core","{\"formats\":{\"aws_access_key_id\":\"text\",\"aws_secret_access_key\":\"text\",\"glue_databucket_name\":\"text\",\"region_name\":\"text\",\"glue_db_name\":\"text\"},\"classname\":\"S3_Athena\",\"name\":\"S3_Athena\",\"alias\":\"S3_Athena\",\"parentCategory\":\"18\",\"id\":27,\"codeGeneration\":{\"requirements\":[],\"imports\":[\"import boto3\",\"from sqlalchemy import create_engine\",\"from langchain import PromptTemplate,SQLDatabase, SQLDatabaseChain, LLMChain\",\"from langchain.prompts.prompt import PromptTemplate\",\"from langchain.chat_models import AzureChatOpenAI\",\"from langchain.callbacks import StreamlitCallbackHandler\",\"from langchain.chains import APIChain\",\"from langchain.chains.api import open_meteo_docs\",\"import streamlit as st\",\"import certifi\",\"import ssl\"],\"script\":\"def S3_Athena(glue_databucket_name_param=\'\',glue_db_name_param=\'\', aws_access_key_id_param=\'\', aws_secret_access_key_param=\'\', region_name_param=\'\'):\\r\\n    # Connect to S3 using Athena\\r\\n    # Athena variables\\r\\n    global dbathena\\r\\n    client = boto3.client(\'glue\',\\r\\n                          aws_access_key_id=aws_access_key_id_param, \\r\\n                          aws_secret_access_key=aws_secret_access_key_param, \\r\\n                          region_name=region_name_param)\\r\\n    region=client.meta.region_name\\r\\n    connathena = f\'athena.{region}.amazonaws.com\' \\r\\n    portathena = \'443\' #Update, if port is different\\r\\n    schemaathena = glue_db_name_param #from user defined params\\r\\n    s3stagingathena = f\'s3://{glue_databucket_name_param}/athenaresults/\'#from cfn params\\r\\n    wkgrpathena = \'primary\'#Update, if workgroup is different\\r\\n    # tablesathena=[\'dataset\']#[<tabe name>]\\r\\n\\r\\n    # Create the athena connection string\\r\\n    connection_string = f\'awsathena+rest://{aws_access_key_id_param}:{aws_secret_access_key_param}@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}/&work_group={wkgrpathena}\'\\r\\n\\r\\n    # Create the athena SQLAlchemy engine\\r\\n    engine_athena = create_engine(connection_string)\\r\\n    \\r\\n    # dbathena = SQLDatabase.from_uri(connection_string)\\r\\n    dbathena = SQLDatabase(engine_athena)\\r\\n\\r\\n    gdc = [schemaathena] \\r\\n    return gdc, client\\r\\n    \\r\\n\\r\\n\\r\\n\\r\\n\\n\"},\"category\":\"S3_Athena\",\"inputEndpoints\":[],\"outputEndpoints\":[\"out1\",\"out2\"],\"attributes\":{\"aws_access_key_id\":\"AKIAWSEIAMU6H5SKF2E2\",\"aws_secret_access_key\":\"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\",\"glue_databucket_name\":\"sagemaker-studio-741094476554-9zkt2s8krvd\",\"region_name\":\"us-east-1\",\"glue_db_name\":\"mda-llm-langchain\"}}","Langchain-27"
"SemanticSearch","Core","{""id"":1,""name"":""Semantic Search"",""category"":""SemanticSearch"",""parentCategory"":"""",""classname"":""SemanticSearch"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Semantic Search""}","SemanticSearchCore-1"
"SemanticSearch","Core","{""id"":2,""name"":""Ingestion"",""category"":""Ingestion"",""parentCategory"":""1"",""classname"":""Ingest"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Ingestion""}","SemanticSearchCore-2"
"SemanticSearch","Core","{""id"":3,""name"":""Dataset Extractor"",""category"":""DatasetExtractor"",""parentCategory"":""2"",""classname"":""DatasetExtractor"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import boto3\\r\\nfrom typing import List, Optional\\r\\nimport logging as logger \\r\\nimport pathlib\\r\\nimport os\\r\\nimport json\\r\\nimport shutil\\r\\nimport requests\\r\\nimport boto3\\r\\nimport json\\r\\nfrom typing import List, Optional, Union, Dict, Sequence, Any\\r\\nfrom urllib.parse import quote_plus\\r\\nfrom leaputils import Security\\r\\nimport sqlite3\\r\\n# !pip install unstructured\\r\\n# !pip install \\""unstructured[pdf]\\""\\r\\nos.environ['DECRYPTION_KEY']='leapAppInfosys12'\\r\\nclass DatasetExtractorConfig:\\r\\n    \\""\\""\\""\\r\\n    DatasetExtractorConfig class to get dataset config.\\r\\n    Args:\\r\\n        config (dict): Dataset config\\r\\n    \\""\\""\\""\\r\\n    def __init__(self, config: Optional[Any]={}) -> None:   \\r\\n        self.local_path: str = config.get('local_path','RAG_data')            \\r\\n        self.query: str = config.get('query',None)   \\r\\n        self.index_search: str = config.get('index_search', 'Default_Index')              \\r\\nclass DatasetExtractor:                            \\r\\n    \\""\\""\\""\\r\\n    Extract dataset. Currently We support S3 datastore.        \\r\\n    Args:\\r\\n        dataset_id (str): Dataset id to get dataset config  \\r\\n        local_path (str): Local file path to store file downloaded from data store,   \\r\\n        index_search (str): To store embedding at particular index          \\r\\n    Returns:\\r\\n        if dataset_type is S3: file_path (str): Local file path\\r\\n        if dataset_type is MYSQL, POSTGRESQL, MSSQL: metadata (dict): metadata\\r\\n    \\""\\""\\""\\r\\n    def __init__(self, dataset_id:str, organization:str) -> None:\\r\\n        self.organization = organization\\r\\n        self.dataset_id = dataset_id        \\r\\n    def get_data(self, config) -> str:         \\r\\n        try:\\r\\n            # get dataset config dict\\r\\n            datasetcofig = self.getdatasetconfig(dataset_id=self.dataset_id, organization=self.organization)              \\r\\n            view = datasetcofig['views']      \\r\\n            dataset_type = datasetcofig['datasource']['type']        \\r\\n            metadata = {\\r\\n                'dataset_id': self.dataset_id,\\r\\n                'organization': self.organization,\\r\\n                'dataset_type': dataset_type,                   \\r\\n            }\\r\\n            connection_details = dict()\\r\\n            if dataset_type == 'S3':  \\r\\n                if view == 'Folder View':\\r\\n                    connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r\\n                    s3_access_key = connection_dict['accessKey']\\r\\n                    s3_secret_key = connection_dict['secretKey']\\r\\n                    s3_end_point_url = connection_dict['url']\\r\\n                    attribute = json.loads(datasetcofig['attributes'])\\r\\n                    bucket = attribute['bucket']    \\r\\n                    path = attribute['path'] \\r\\n                    local_path = '/'+config.local_path+'_'+self.dataset_id\\r\\n                    count = 0\\r\\n                    flag = False\\r\\n                    for i in os.listdir(\\""/\\""):\\r\\n                        if i.find(local_path[1:])==0:\\r\\n                            flag = True\\r\\n                            count += 1\\r\\n                    if flag:\\r\\n                        local_path = local_path+(count*'_')\\r\\n                    file_path = self.s3_download_data(end_point_url = s3_end_point_url, access_key = s3_access_key, secret_key=s3_secret_key, bucket = bucket, obj_key = path, local_path = local_path)\\r\\n                    metadata['source'] = file_path \\r\\n                    return metadata\\r\\n                else:                \\r\\n                    connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])\\r\\n                    s3_access_key = connection_dict['accessKey']\\r\\n                    s3_secret_key = connection_dict['secretKey']\\r\\n                    s3_end_point_url = connection_dict['url'] \\r\\n                    attribute = json.loads(datasetcofig['attributes'])\\r\\n                    bucket = attribute['bucket']               \\r\\n                    path = attribute['path']   \\r\\n                    obj_key = attribute['object']  \\r\\n                    key = f'{path}/{obj_key}'                \\r\\n                    local_path = '/'+config.local_path+'_'+self.dataset_id                           \\r\\n                    count = 0\\r\\n                    flag = False\\r\\n                    for i in os.listdir(\\""/\\""):\\r\\n                        if i.find(local_path[1:])==0:\\r\\n                            flag = True\\r\\n                            count += 1\\r\\n                    if flag:\\r\\n                        local_path = local_path+(count*'_')                                 \\r\\n                    file_path = self.s3_download_data(end_point_url = s3_end_point_url, access_key = s3_access_key, secret_key=s3_secret_key, bucket = bucket, obj_key = key, local_path = local_path)\\r\\n                    metadata['source'] = file_path                \\r\\n                    return metadata\\r\\n            elif dataset_type == 'MYSQL':   \\r\\n                connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r\\n                attributes = json.loads(datasetcofig['attributes'])            \\r\\n                db_user_name = connection_dict['userName']           \\r\\n                db_url = connection_dict['url']\\r\\n                db_password = connection_dict['password']   \\r\\n                salt = datasetcofig['datasource']['salt']\\r\\n                sql_query = attributes['Query'] \\r\\n                connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:mysql://')            \\r\\n                self.store_connection_details(connection_details, connection_string, config.index_search, metadata)\\r\\n                return metadata            \\r\\n            elif dataset_type == 'POSTGRESQL':\\r\\n                connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r\\n                attributes = json.loads(datasetcofig['attributes'])            \\r\\n                db_user_name = connection_dict['userName']           \\r\\n                db_url = connection_dict['url']\\r\\n                db_password = connection_dict['password']   \\r\\n                salt = datasetcofig['datasource']['salt']\\r\\n                sql_query = attributes['Query'] \\r\\n                connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query, prefix='jdbc:postgresql://')            \\r\\n                self.store_connection_details(connection_details, connection_string, config.index_search)\\r\\n                return metadata         \\r\\n            elif dataset_type == 'MSSQL':\\r\\n                connection_dict = json.loads(datasetcofig['datasource']['connectionDetails'])            \\r\\n                attributes = json.loads(datasetcofig['attributes'])            \\r\\n                db_user_name = connection_dict['userName']           \\r\\n                db_url = connection_dict['url']\\r\\n                db_password = connection_dict['password']   \\r\\n                salt = datasetcofig['datasource']['salt']\\r\\n                sql_query = attributes['Query'] \\r\\n                connection_string, connection_details = self.get_connection_details(db_user_name, db_password, db_url, salt, sql_query,prefix='jdbc:sqlserver://')            \\r\\n                self.store_connection_details(connection_details, connection_string, config.index_search)\\r\\n                return metadata \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in DatasetExtractor as: ',e)\\r\\n            return e\\r\\n    #helper functions\\r\\n    def getdatasetconfig(self, dataset_id:str, organization:str):\\r\\n        '''\\r\\n        call ai-plat api to get dataset config\\r\\n        return {dataset_config}\\r\\n        '''  \\r\\n        try:                  \\r\\n                      \\r\\n            api_referer = os.environ.get(\\""API_URL\\"")\\r\\n            url = f'{api_referer}/api/services/fetchDatasetDetails/{dataset_id}/{organization}'\\r\\n            logger.info(url)\\r\\n            headers = {\\r\\n            'access-token': os.environ.get('app_access_token'),\\r\\n            'Cookie': 'JSESSIONID=1F892FB2E1384C5D0DF451AAD2AC311B',\\r\\n            'Project':'2'\\r\\n            }\\r\\n            response = requests.request(\\""GET\\"", url, headers=headers, verify=False)\\r\\n            dataset_config = json.loads(response.text)\\r\\n            return dataset_config\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in getdatasetconfig as: ', e)\\r\\n            raise e\\r\\n    def s3_download_data(self, end_point_url:str = '', access_key:str = '', secret_key:str = '', bucket:str = '', obj_key:str = '', local_path:str='/data'):\\r\\n        '''\\r\\n        download data from s3         \\r\\n        return local file path       \\r\\n        '''     \\r\\n        try:\\r\\n            s3_client = boto3.resource(service_name='s3',\\r\\n                        endpoint_url=end_point_url,\\r\\n                        aws_access_key_id=access_key,\\r\\n                        aws_secret_access_key=secret_key,\\r\\n                        verify=False)\\r\\n            bucket_object = s3_client.Bucket(bucket)\\r\\n            object_save_path = ''\\r\\n            logger.info(bucket_object.objects.filter(Prefix=obj_key))        \\r\\n            if os.path.exists(local_path):\\r\\n                shutil.rmtree(local_path)\\r\\n            os.makedirs(local_path)\\r\\n            model_path = os.path.join(local_path)\\r\\n            for my_bucket_object in bucket_object.objects.filter(Prefix=obj_key):   \\r\\n                if my_bucket_object.key.endswith('/'):\\r\\n                    pass\\r\\n                else:                 \\r\\n                    object_save_path = (f\\""{model_path}/{pathlib.Path(my_bucket_object.key).name}\\"")\\r\\n                    bucket_object.download_file(my_bucket_object.key, object_save_path)\\r\\n            return local_path \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in s3_download_data as: ', e)\\r\\n            raise e\\r\\n    def get_connection_details(self, db_user_name:str, db_password:str, db_url:str, salt:str, sql_query:str, prefix:str):\\r\\n        '''\\r\\n        Get connection string for the database.\\r\\n        Args:\\r\\n            db_user_name (str): Database user name.\\r\\n            db_password (str): Database password.\\r\\n            db_url (str): Database URL.\\r\\n            salt (str): Salt used to encrypt the password.\\r\\n            prefix (str): Protocol prefix.\\r\\n        Returns:\\r\\n            connection_details (dict): Dictionary containing connection string, user name, password, host, port, and database.           \\r\\n        '''\\r\\n        try:\\r\\n            connection_details = {}\\r\\n            # decrypt password\\r\\n            db_password = Security.decrypt(db_password,salt)\\r\\n            # Remove protocol prefix and leading slash\\r\\n            stripped_url = db_url[len(prefix):]\\r\\n            # Split by components, assuming username and password are not present\\r\\n            host, port_database = stripped_url.split(\\""/\\"", 1)\\r\\n            # Extract host and port\\r\\n            host, port = host.split(\\"":\\"")\\r\\n            # Extract database\\r\\n            database = port_database.strip(\\""/\\"")\\r\\n            # Form connection string\\r\\n            connection_string = f'mysql://{db_user_name}:{quote_plus(db_password)}@{host}:{port}/{database}'\\r\\n            connection_details['db_user_name'] = db_user_name\\r\\n            connection_details['db_password'] = db_password\\r\\n            connection_details['db_host'] = host\\r\\n            connection_details['db_port'] = port\\r\\n            connection_details['database'] = database\\r\\n            connection_details['query'] = sql_query\\r\\n            return connection_string, connection_details\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_connection_details as: ', e)\\r\\n            raise e\\r\\n    def store_connection_details(self, connection_details:dict, connection_string:str, index_search:str, metadata:dict):         \\r\\n        try:\\r\\n            # Connect to the database or create it if it doesn't exist\\r\\n            root_dir = \\""/RAG_DB\\""\\r\\n            os.makedirs(root_dir, exist_ok=True)  # Create only if needed\\r\\n            root_path = os.path.abspath(root_dir)\\r\\n            path = os.path.join(root_path,'rag_database.db')\\r\\n            # path = os.path.join('../database','rag_database.db')\\r\\n            conn = sqlite3.connect(path) \\r\\n            # Create a cursor object to execute SQL commands\\r\\n            cursor = conn.cursor()\\r\\n            # Create a table (replace with your desired table name and columns)\\r\\n            cursor.execute('''\\r\\n            CREATE TABLE IF NOT EXISTS dataset_details (\\r\\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\\r\\n                dataset_id TEXT NOT NULL,\\r\\n                organization TEXT NOT NULL,\\r\\n                connection_string TEXT NOT NULL,\\r\\n                connection_details JSON NOT NULL,\\r\\n                metadata JSON,\\r\\n                index_search TEXT NOT NULL\\r\\n            )\\r\\n            ''')\\r\\n            connection_details_json = json.dumps(connection_details)\\r\\n            metadata_json = json.dumps(metadata)\\r\\n            # Prepare SQL query with placeholders for values\\r\\n            sql = \\""INSERT INTO dataset_details (dataset_id, ORGANIZATION, connection_string, connection_details, metadata, index_search) VALUES (?, ?, ?, ?, ?, ?)\\""\\r\\n            # Data to be inserted (replace with your actual values)\\r\\n            values = (self.dataset_id, self.organization, connection_string, connection_details_json, metadata_json, index_search)\\r\\n            # Execute the query with prepared values\\r\\n            cursor.execute(sql, values)\\r\\n            # Commit the changes to the database\\r\\n            conn.commit()\\r\\n            logger.info(\\""data inserted successfully!....\\"")                   \\r\\n        except sqlite3.connector.Error as e:\\r\\n            logger.info('Thre is some error whiling storing data as:',e)\\r\\n            raise e\\r\\n        finally:\\r\\n            # Close database connection properly\\r\\n            cursor.close()\\r\\n            conn.close()\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Dataset Extractor"",""attributes"":{}}","SemanticSearchCore-3"
"SemanticSearch","Core","{""id"":4,""name"":""Data Chunker"",""category"":""DataChunker"",""parentCategory"":""2"",""classname"":""DataChunker"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nfrom langchain_core.documents import Document\\r\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\r\\nimport base64\\r\\nimport requests\\r\\nimport moviepy.editor as mp  \\r\\nfrom pydub import AudioSegment\\r\\nimport json\\r\\nfrom typing import List, Optional, Union, Dict, Sequence, Any, Type\\r\\nfrom langchain_community.document_loaders.text import TextLoader\\r\\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\\r\\nfrom langchain_community.document_loaders.unstructured import UnstructuredFileLoader\\r\\nfrom langchain_community.document_loaders.html_bs import BSHTMLLoader\\r\\nfrom pypdf import PdfReader\\r\\nimport base64\\r\\nimport boto3\\r\\nfrom langchain_community.chat_models.bedrock import BedrockChat\\r\\nimport json\\r\\nfrom langchain.schema.document import Document\\r\\nimport uuid\\r\\nimport logging \\r\\nlogger = logging.getLogger(__name__)\\r\\nlogger.setLevel(logging.DEBUG)\\r\\nhandler = logging.StreamHandler()\\r\\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\\r\\nhandler.setFormatter(formatter)\\r\\nlogger.addHandler(handler)\\r\\nFILE_LOADER_TYPE = Union[Type[UnstructuredFileLoader], Type[TextLoader], Type[CSVLoader],Type[BSHTMLLoader]\\r\\n                            ]\\r\\nclass DatasetChunkerConfig:\\r\\n    def __init__(self, config: Optional[Any]={}) -> None:        \\r\\n        self.encoding: Optional[str] = config.get('encoding','utf8')\\r\\n        self.autodetect_encoding: bool = config.get('autodetect_encoding',False)\\r\\n        self.password: Optional[Union[str, bytes]] = config.get('password',None)\\r\\n        self.headers: Optional[Dict] = config.get('headers',None)\\r\\n        self.extract_images: bool = config.get('extract_images',False)\\r\\n        self.source_column: str = config.get('source_column',None)\\r\\n        self.metadata_columns: Sequence[str] = config.get('metadata_columns',())\\r\\n        self.csv_args: Dict | None = config.get('csv_args',None)\\r\\n        self.mode: str = config.get('mode','single')\\r\\n        self.strategy: str = config.get('strategy','fast')\\r\\n        self.chunk_size: int = config.get('chunk_size',500)\\r\\n        self.chunk_overlap: int = config.get('chunk_overlap',50)\\r\\n        self.separators: List[str] = config.get('separators',None)\\r\\n        self.keep_separator: bool = config.get('keep_separator',True)\\r\\n        self.audio_chunk_size: int = config.get('audio_chunk_size', 60)\\r\\nclass DatasetChunker:   \\r\\n    \\""\\""\\""Chunk and split the data.    \\r\\n    Args:\\r\\n        file_path (str): Path of the file to load, \\r\\n        textloader:              \\r\\n            encoding (Optional,str): File encoding to use. If `None`, the file will be loaded with the default system encoding \\r\\n            autodetect_encoding (Optional,bool): Whether to try to autodetect the file encoding if the specified encoding fails\\r\\n        pdfloader:\\r\\n            password (Optional,[Union[str, bytes]]): Password for PyPDFLoader\\r\\n            headers (Optional,[Dict]): Header for PyPDFLoader\\r\\n            extract_images (Optional,bool): Extract image from pdf\\r\\n        csvloader:\\r\\n            source_column (Optional, str): The name of the column in the CSV file to use as the source\\r\\n            metadata_columns  (Optional,Sequence[str]): A sequence of column names to use as metadata\\r\\n            csv_args (Optonal, Dict): A dictionary of arguments to pass to the csv.DictReader              \\r\\n            encoding (Optional,str): File encoding to use. If `None`, the file will be loaded with the default system encoding \\r\\n            autodetect_encoding (Optional,bool): Whether to try to autodetect the file encoding if the specified encoding fails\\r\\n        markdown:\\r\\n            mode (Optional,str):  You can run the loader in one of two modes: \\""single\\"" and \\""elements\\"". If you use \\""single\\"" mode, the document will be returned as a single langchain Document object. If you use \\""elements\\"" mode, the unstructured library will split the document into elements such as Title and NarrativeText\\r\\n            strategy (Optional,str): strategy to chunk, default is\\""fast\\""        \\r\\n        splitdocs:\\r\\n            chunk_size (Optiona,int): Define chunk size for RecursiveCharacterTextSplitter\\r\\n            chunk_overlap (Optiona,int): Define chunk overlape for RecursiveCharacterTextSplitter\\r\\n            separators (Optional,List[str]): Define separators for RecursiveCharacterTextSplitter,\\r\\n            keep_separator (Optional,bool): Define keep_separator for RecursiveCharacterTextSplitter\\r\\n    Returns:        \\r\\n        List of splitted documents to be stored in vector dbs \\r\\n        documents: List[Document]\\r\\n    \\""\\""\\""\\r\\n    def __init__(self, metadata: dict) -> None:\\r\\n        self.file_path = metadata['source']\\r\\n        self.metadata = metadata\\r\\n    # Main function    \\r\\n    def chunk_data(self, config) -> List[Document]:\\r\\n        import os\\r\\n        '''\\r\\n            this function will chunk the data based on the config provided\\r\\n            return: List[Document]\\r\\n        '''\\r\\n        try:\\r\\n            allsplit_doc_list = list()\\r\\n            if config.extract_images:\\r\\n                loader = self.directory_loader(path=self.file_path,silent_errors=True,show_progress=True)\\r\\n                documents = loader.load()\\r\\n                del self.metadata['source']\\r\\n                final_document = self.document_with_metadata(documents, self.metadata)\\r\\n                allsplit_doc_list.extend(final_document)\\r\\n                for root, dirs, files in os.walk(self.file_path):\\r\\n                    for file in files:\\r\\n                        file_type = os.path.splitext(file)[1].lower()\\r\\n                        filename = os.path.join(root, file)\\r\\n                        if file_type == '.mp4'.strip().lower() :\\r\\n                            audio_file = self.audio_extractor(filename)\\r\\n                            chunker_folder = self.audio_chunker(audio_file, chunk_size_param=config.audio_chunk_size)\\r\\n                            overall_transcribe = self.audio_transcriber(chunker_folder)\\r\\n                            df = self.GetTranscribeWithMetaData(overall_transcribe,file_type)\\r\\n                            loader = self.dataframe_loader(df, page_content_column=\\""Transcribe\\"")\\r\\n                            documents = loader.load()  \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document)\\r\\n                        elif file_type.strip().lower() == '.mp3'.strip().lower():\\r\\n                            audio_file = self.mp3_converter(filename)\\r\\n                            chunker_folder = self.audio_chunker(audio_file, chunk_size_param=config.audio_chunk_size)\\r\\n                            overall_transcribe = self.audio_transcriber(chunker_folder)\\r\\n                            df = self.GetTranscribeWithMetaData(overall_transcribe, file_type)\\r\\n                            loader = self.dataframe_loader(df, page_content_column=\\""Transcribe\\"")\\r\\n                            documents = loader.load()  \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document)\\r\\n                        elif file_type.strip().lower() == '.wav'.strip().lower():                     \\r\\n                            chunker_folder = self.audio_chunker(filename, chunk_size_param=config.audio_chunk_size)\\r\\n                            overall_transcribe = self.audio_transcriber(chunker_folder)\\r\\n                            df = self.GetTranscribeWithMetaData(overall_transcribe, file_type)\\r\\n                            loader = self.dataframe_loader(df, page_content_column=\\""Transcribe\\"")\\r\\n                            documents = loader.load()\\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document)\\r\\n            else:\\r\\n\\r\\n                for subdir, dirs, files in os.walk(self.file_path):\\r\\n                    for file in files:\\r\\n                        filename = os.path.join(subdir, file)                \\r\\n                        file_type = self.checkfiletype(file_path=filename)  \\r\\n                        if file_type.strip().lower() == '.txt'.strip().lower():                                        \\r\\n                            loader = self.text_loader(file_path=filename, encoding=config.encoding, autodetect_encoding=config.autodetect_encoding)                \\r\\n                            documents = loader.load()\\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/')                     \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata)\\r\\n                            allsplit_doc_list.extend(final_document) \\r\\n                        elif file_type.strip().lower()  == '.pdf'.strip().lower() or file_type.strip().lower()  == '.ppt'.strip().lower()  or file_type.strip().lower() =='.pptx'.strip().lower()  or file_type.strip().lower() =='docx'.strip().lower() :\\r\\n                            if file_type=='.ppt'or file_type=='.pptx':\\r\\n                                import comtypes.client \\r\\n                                import pythoncom\\r\\n                                pythoncom.CoInitialize()\\r\\n                                powerpoint = comtypes.client.CreateObject(\\""Powerpoint.Application\\"")\\r\\n                                deck = powerpoint.Presentations.Open(filename)\\r\\n                                output_pdf_file = os.path.splitext(filename)[0] + \\"".pdf\\""\\r\\n                                deck.SaveAs(output_pdf_file, 32)\\r\\n                                deck.Close()\\r\\n                                powerpoint.Quit()\\r\\n                                os.remove(filename)\\r\\n                                base_name, _ = os.path.splitext(filename)\\r\\n                                filename=base_name +'.pdf'\\r\\n                            if file_type=='docx':\\r\\n                                from docx2pdf import convert\\r\\n                                import os\\r\\n                                convert(filename)\\r\\n                                os.remove(filename)\\r\\n                                base_name, _ = os.path.splitext(filename)\\r\\n                                filename=base_name +'.pdf'\\r\\n                            loader = self.pdf_loader(file_path=filename, password=config.password, headers=config.headers, extract_images=config.extract_images)\\r\\n                            documents = loader.load()  \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata)\\r\\n                            if config.extract_images:\\r\\n                                image_summary_document= self.get_images_summary(filename)\\r\\n                                final_document.extend(image_summary_document) \\r\\n                            allsplit_doc_list.extend(final_document)    \\r\\n                        elif file_type.strip().lower()  == '.csv'.strip().lower() :\\r\\n                            loader = self.csv_loader(file_path=filename, source_column=config.source_column, metadata_columns=config.metadata_columns, csv_args=config.csv_args, encoding=config.encoding, autodetect_encoding=config.autodetect_encoding)\\r\\n                            documents = loader.load() \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/')              \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata)               \\r\\n                            allsplit_doc_list.extend(final_document) \\r\\n                        elif file_type.strip().lower()  == '.md'.strip().lower() :\\r\\n                            loader = self.markdown_loader(file_path=filename, mode=config.mode, strategy=config.strategy)\\r\\n                            documents = loader.load()\\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/')               \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document) \\r\\n                        elif file_type.strip().lower() == '.mp4'.strip().lower():                \\r\\n                            audio_file = self.audio_extractor(filename)\\r\\n                            chunker_folder = self.audio_chunker(audio_file, chunk_size_param=config.audio_chunk_size)\\r\\n                            overall_transcribe = self.audio_transcriber(chunker_folder)\\r\\n                            df = self.GetTranscribeWithMetaData(overall_transcribe,file_type)\\r\\n                            loader = self.dataframe_loader(df, page_content_column=\\""Transcribe\\"")\\r\\n                            documents = loader.load()  \\r\\n                            \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document)\\r\\n                        elif file_type.strip().lower() == '.mp3'.strip().lower():\\r\\n                            audio_file = self.mp3_converter(filename)\\r\\n                            chunker_folder = self.audio_chunker(audio_file, chunk_size_param=config.audio_chunk_size)\\r\\n                            overall_transcribe = self.audio_transcriber(chunker_folder)\\r\\n                            df = self.GetTranscribeWithMetaData(overall_transcribe, file_type)\\r\\n                            loader = self.dataframe_loader(df, page_content_column=\\""Transcribe\\"")\\r\\n                            documents = loader.load()  \\r\\n                            \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document)  \\r\\n                        elif file_type.strip().lower() == '.wav'.strip().lower():                     \\r\\n                            chunker_folder = self.audio_chunker(filename, chunk_size_param=config.audio_chunk_size)\\r\\n                            overall_transcribe = self.audio_transcriber(chunker_folder)\\r\\n                            df = self.GetTranscribeWithMetaData(overall_transcribe, file_type)\\r\\n                            loader = self.dataframe_loader(df, page_content_column=\\""Transcribe\\"")\\r\\n                            documents = loader.load()  \\r\\n                            \\r\\n                            self.metadata['source'] = filename.replace('\\\\\\\\', '/') \\r\\n                            final_document = self.document_with_metadata(documents, self.metadata) \\r\\n                            allsplit_doc_list.extend(final_document)\\r\\n                            \\r\\n                        else:\\r\\n                            raise f'Currently {file_type} file type is not supported'\\r\\n            if allsplit_doc_list != None:\\r\\n                final_splitted_document = self.splitdocs(allsplit_doc_list, chunk_size=config.chunk_size, chunk_overlap=config.chunk_overlap, separators=config.separators, keep_separator=config.keep_separator)\\r\\n                return final_splitted_document        \\r\\n            # if final_document != None:\\r\\n            #     final_splitted_document = self.splitdocs(final_document, chunk_size=config.chunk_size, chunk_overlap=config.chunk_overlap, separators=config.separators, keep_separator=config.keep_separator)\\r\\n            #     return final_splitted_document        \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in chunk_data as',e)\\r\\n            raise e\\r\\n    # Helper functions\\r\\n    def get_imageSummary(self,base64_image):\\r\\n        from langchain_community.chat_models.bedrock import BedrockChat\\r\\n        ACCESS_ID=os.environ.get('app_aws_secret_access_id')\\r\\n        ACCESS_KEY=os.environ.get('app_aws_secret_access_key')\\r\\n        bedrock = boto3.client(service_name='bedrock-runtime',aws_access_key_id=ACCESS_ID,aws_secret_access_key=ACCESS_KEY,region_name='us-east-1',endpoint_url='https://bedrock-runtime.us-east-1.amazonaws.com')\\r\\n        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\\r\\n        chat = BedrockChat(model_id=modelId,\\r\\n                        client=bedrock,\\r\\n                        model_kwargs={\\r\\n                            'temperature': 0.1,\\r\\n                            'top_p': 0.1\\r\\n                        }\\r\\n                        )\\r\\n        prompt = \\""\\""\\""\\r\\n            Summarize all the details in the image, focus on the statistics provided in the image such as bar charts and graphs. \\r\\n            Make sure to include the time period in the summary, if available. For example, the images shows line graph showing data from 1993 to 2022.\\r\\n            \\""\\""\\""\\r\\n        content = [\\r\\n            {\\""type\\"": \\""text\\"", \\""text\\"": prompt},\\r\\n            {\\""type\\"": \\""image\\"", \\""source\\"": {\\""type\\"": \\""base64\\"", \\""media_type\\"": \\""image/png\\"", \\""data\\"": base64_image}}\\r\\n        ]\\r\\n        message_list = [\\r\\n            {\\r\\n                \\""role\\"": 'user',\\r\\n                \\""content\\"": content\\r\\n            }\\r\\n        ]\\r\\n        try:\\r\\n            msg = chat.invoke(message_list)\\r\\n            return msg.content  \\r\\n        except Exception as e:\\r\\n            logger.info(\\""got error in generating image summary from bedrock model\\"")\\r\\n            return 'Empty'\\r\\n    def get_images_summary(self,pdf_path):\\r\\n        from pypdf import PdfReader\\r\\n        import uuid\\r\\n        images_summary={}\\r\\n        image_summary_list=[]\\r\\n        reader = PdfReader(pdf_path)\\r\\n        for page in reader.pages:\\r\\n            for image in page.images:\\r\\n                base64_image= base64.b64encode(image.data).decode('utf-8')\\r\\n                image_summary= self.get_imageSummary(base64_image)\\r\\n                images_summary[image.name]=image_summary\\r\\n                image_summary_list.append(image_summary)      \\r\\n        img_ids = [str(uuid.uuid4()) for _ in image_summary_list]        \\r\\n        summary_img = [\\r\\n                Document(page_content=s, metadata={'doc_id': img_ids[i]})\\r\\n                for i, s in enumerate(image_summary_list)\\r\\n            ]  \\r\\n        return summary_img\\r\\n    def audio_transcriber(self, chunker_folder:str): \\r\\n        import whisper\\r\\n        import os\\r\\n        from whisper.utils import get_writer \\r\\n        flag = True\\r\\n        try:\\r\\n            input_directory = chunker_folder            \\r\\n            model = whisper.load_model('base')\\r\\n            for dirs, subdirs, files in os.walk(input_directory):  \\r\\n                output_directory = ''\\r\\n                for i, file_ in enumerate(files):\\r\\n                    if file_.endswith(('.wav', '.WAV')):\\r\\n                        file = os.path.join(dirs, file_)\\r\\n                        inputfile = file\\r\\n                        subdirs = dirs.split('\\\\\\\\')[-1]\\r\\n                        output_directory = os.path.join(input_directory,subdirs,'output')                    \\r\\n                        if not os.path.isdir(output_directory):\\r\\n                            os.makedirs(output_directory)\\r\\n                        filename = inputfile                        \\r\\n                        result = model.transcribe(filename)\\r\\n                        # Set some initial options values\\r\\n                        options = {\\r\\n                            'max_line_width': None,\\r\\n                            'max_line_count': None,\\r\\n                            'highlight_words': False\\r\\n                        }                    \\r\\n                        file_ = os.path.splitext(file_)[0]\\r\\n                        output_filename = os.path.join(output_directory, file_)                                    \\r\\n                        # Save as a JSON file\\r\\n                        json_writer = get_writer('json', output_directory)\\r\\n                        json_writer(result, output_filename, options)\\r\\n                return output_directory\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Transcriber as: ',e)\\r\\n            raise e\\r\\n    def mp3_converter(self, root_audio_file_path: str):\\r\\n        '''\\r\\n        This function will extract the audio from the video file\\r\\n        Args:\\r\\n            root_video_file_path: str\\r\\n        return: str\\r\\n        '''        \\r\\n        try:                \\r\\n            video_folder = os.path.dirname(root_audio_file_path)\\r\\n            audio_folder = video_folder+'/data/audios'\\r\\n            if not os.path.isdir(audio_folder):\\r\\n                os.makedirs(audio_folder)\\r\\n            # Load the video \\r\\n            sound = AudioSegment.from_mp3(root_audio_file_path)\\r\\n            filename = os.path.basename(root_audio_file_path)          \\r\\n            audio_file_name = audio_folder+'/'+os.path.splitext(filename)[0]+'.wav'   \\r\\n            sound.export(audio_file_name, format=\\""wav\\"")            \\r\\n            logger.info('Converted .mp3 Audio to .wav Audio : ',filename) \\r\\n            logger.info('mp3_converter executed successfully!')\\r\\n            return audio_file_name\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in mp3_converter as: ',e)\\r\\n            raise e\\r\\n    def GetTranscribeWithMetaData(self, output_directory, extension):\\r\\n        import pandas as pd        \\r\\n        videotranscripts_df = pd.DataFrame(columns=['Transcribe', 'start_time','end_time'])     \\r\\n        video_audio_name = output_directory.split(\\""\\\\\\\\\\"")[0].split('/')[-1]+extension\\r\\n        for dirs, subdirs, files in os.walk(output_directory):  \\r\\n            for file in files:\\r\\n                file_key = file\\r\\n                logger.info(\\""file_key===\\"",file_key)\\r\\n                if file_key.endswith('.json'):                     \\r\\n                    file_name = file_key\\r\\n                    with open(dirs+'/'+file_key) as f:\\r\\n                        file_content = f.read()                        \\r\\n                        json_content = json.loads(file_content)\\r\\n                    Transcribe = json_content['text'] \\r\\n                    if videotranscripts_df.empty:                                \\r\\n                        start_time = json_content['segments'][0]['start']\\r\\n                        end_time = json_content['segments'][-1]['end']                \\r\\n                        pre_end_time = end_time\\r\\n                    else:\\r\\n                        start_time = pre_end_time        \\r\\n                        end_time = pre_end_time + json_content['segments'][-1]['end']\\r\\n                        pre_end_time =  end_time               \\r\\n                    if video_audio_name.endswith(('.mp4','.MP4')):\\r\\n                        tempdf = pd.DataFrame({'Transcribe':Transcribe, 'start_time':start_time, 'end_time':end_time, 'file_name': file_name, 'video_name': video_audio_name}, index=[0])                        \\r\\n                    else:\\r\\n                        tempdf = pd.DataFrame({'Transcribe':Transcribe, 'start_time':start_time, 'end_time':end_time, 'file_name': file_name, 'audio_name': video_audio_name}, index=[0])    \\r\\n                    videotranscripts_df = pd.concat([videotranscripts_df, tempdf], ignore_index=True)\\r\\n        logger.info(\\""videotranscripts_df==\\"",videotranscripts_df)              \\r\\n        return videotranscripts_df\\r\\n    def checkfiletype(self, file_path: str) -> str:\\r\\n        '''\\r\\n        This function will check the file type and return the file type\\r\\n        Args:\\r\\n            file_path: str\\r\\n        return: str        \\r\\n        '''    \\r\\n        try:\\r\\n            extension = os.path.splitext(file_path)[1].lower()\\r\\n            self.type = extension\\r\\n            return self.type\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in checkfiletype as', e)\\r\\n            raise e\\r\\n    def document_with_metadata(self, documents: List[Document], metadata) -> List[Document]:\\r\\n        '''\\r\\n        This function will add the metadata to the document\\r\\n        Args:\\r\\n            documents: List[Document]\\r\\n            metadata: dict\\r\\n        return: List[Document]\\r\\n        '''   \\r\\n        try:     \\r\\n            for index, doc in enumerate(documents):\\r\\n                doc.metadata.update(metadata)  # Inject metadata directly into the document object\\r\\n            return documents\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in document_with_metadata as', e)\\r\\n            raise e\\r\\n    def text_loader(self, file_path : str ='', encoding : str = None, autodetect_encoding : bool = False) -> object:    \\r\\n        \\""\\""\\""\\r\\n        Args:\\r\\n            file_path: Path of the file to load.\\r\\n            encoding: File encoding to use. If `None`, the file will be loaded\\r\\n            with the default system encoding.\\r\\n            autodetect_encoding: Whether to try to autodetect the file encoding\\r\\n                if the specified encoding fails.\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain.document_loaders import TextLoader\\r\\n            loader = TextLoader(file_path=file_path, encoding=encoding, autodetect_encoding=autodetect_encoding)\\r\\n            return loader \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in text_loader as', e)\\r\\n            raise e\\r\\n    def pdf_loader(self, file_path : str ='', password: Optional[Union[str, bytes]] = None, headers: Optional[Dict] = None, extract_images: bool = False) -> object:\\r\\n        \\""\\""\\""\\r\\n        Load PDF using pypdf into list of documents.\\r\\n        Loader chunks by page and stores page numbers in metadata.\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.document_loaders import PyPDFLoader       \\r\\n            loader = PyPDFLoader(file_path=file_path, password=password, headers=headers, extract_images=extract_images)        \\r\\n            return loader        \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in pdf_loader as', e)\\r\\n            raise e\\r\\n    def csv_loader(self, file_path: str, source_column: Optional[str] = None, metadata_columns: Sequence[str] = (), csv_args: Optional[Dict] = None, encoding: Optional[str] = None, autodetect_encoding: bool = False):\\r\\n        \\""\\""\\""\\r\\n        Load a `CSV` file into a list of Documents.\\r\\n        Each document represents one row of the CSV file. Every row is converted into a\\r\\n        key/value pair and outputted to a new line in the document's page_content.\\r\\n        The source for each document loaded from csv is set to the value of the\\r\\n        `file_path` argument for all documents by default.\\r\\n        You can override this by setting the `source_column` argument to the\\r\\n        name of a column in the CSV file.\\r\\n        The source of each document will then be set to the value of the column\\r\\n        with the name specified in `source_column`.\\r\\n        Output Example:\\r\\n            .. code-block:: txt\\r\\n                column1: value1\\r\\n                column2: value2\\r\\n                column3: value3\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.document_loaders.csv_loader import CSVLoader\\r\\n            loader = CSVLoader(file_path=file_path, source_column=source_column, metadata_columns=metadata_columns, csv_args=csv_args,encoding=encoding, autodetect_encoding=autodetect_encoding)\\r\\n            return loader\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in csv_loader as', e)\\r\\n            raise e\\r\\n    def dataframe_loader(self, df, page_content_column: str = ''):\\r\\n        \\""\\""\\""Initialize with dataframe object.\\r\\n        Args:\\r\\n            data_frame: Pandas DataFrame object.\\r\\n            page_content_column: Name of the column containing the page content.\\r\\n              Defaults to \\""text\\"".\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.document_loaders.dataframe import DataFrameLoader\\r\\n            loader = DataFrameLoader(df, page_content_column=page_content_column)\\r\\n            return loader\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in dataframe_loader as', e)\\r\\n            raise e\\r\\n    def markdown_loader(self, file_path: str = '', mode: str = \\""single\\"", strategy: str=\\""fast\\""):\\r\\n        \\""\\""\\""Load `Markdown` files using `Unstructured`.\\r\\n        You can run the loader in one of two modes: \\""single\\"" and \\""elements\\"".\\r\\n        If you use \\""single\\"" mode, the document will be returned as a single\\r\\n        langchain Document object. If you use \\""elements\\"" mode, the unstructured\\r\\n        library will split the document into elements such as Title and NarrativeText.\\r\\n        You can pass in additional unstructured kwargs after mode to apply\\r\\n        different unstructured settings.\\r\\n        Examples\\r\\n        --------\\r\\n        from langchain_community.document_loaders import UnstructuredMarkdownLoader\\r\\n        loader = UnstructuredMarkdownLoader(\\r\\n            \\""example.md\\"", mode=\\""elements\\"", strategy=\\""fast\\"",\\r\\n        )\\r\\n        docs = loader.load()\\r\\n        References\\r\\n        ----------\\r\\n        https://unstructured-io.github.io/unstructured/core/partition.html#partition-md\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.document_loaders import UnstructuredMarkdownLoader\\r\\n            loader = UnstructuredMarkdownLoader(file_path=file_path, mode=mode, strategy=strategy)\\r\\n            return loader\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in markdown_loader as', e)\\r\\n            raise e\\r\\n    def string_loader(self, text_content: str, metadata: dict = {}):\\r\\n        '''\\r\\n        This function will convert the string into list of document object\\r\\n        Args:\\r\\n            text_content: str\\r\\n            metadata: dict\\r\\n        return: List[Document]         \\r\\n        '''\\r\\n        try:\\r\\n            from langchain.schema.document import Document\\r\\n            docs = [Document(page_content=text_content,metadata=metadata)]                \\r\\n            return docs\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in string_loader as', e)\\r\\n            raise e\\r\\n    def directory_loader(self, \\r\\n                         path: str,\\r\\n                         glob: str = \\""**/[!.]*\\"",\\r\\n                         silent_errors: bool = False,\\r\\n                         load_hidden: bool = False,\\r\\n                         loader_cls: FILE_LOADER_TYPE = UnstructuredFileLoader,\\r\\n                         loader_kwargs: Union[dict, None] = None,\\r\\n                         recursive: bool = False,\\r\\n                         show_progress: bool = False,\\r\\n                         use_multithreading: bool = False,\\r\\n                         max_concurrency: int = 4,                         \\r\\n                         sample_size: int = 0,\\r\\n                         randomize_sample: bool = False,\\r\\n                         sample_seed: Union[int, None] = None):\\r\\n        '''\\r\\n        This function will load the directory and return the list of document object\\r\\n        Args:\\r\\n            path: Path to directory.\\r\\n            glob: Glob pattern to use to find files. Defaults to \\""**/[!.]*\\""\\r\\n               (all files except hidden).\\r\\n            silent_errors: Whether to silently ignore errors. Defaults to False.\\r\\n            load_hidden: Whether to load hidden files. Defaults to False.\\r\\n            loader_cls: Loader class to use for loading files.\\r\\n              Defaults to UnstructuredFileLoader.\\r\\n            loader_kwargs: Keyword arguments to pass to loader_cls. Defaults to None.\\r\\n            recursive: Whether to recursively search for files. Defaults to False.\\r\\n            show_progress: Whether to show a progress bar. Defaults to False.\\r\\n            use_multithreading: Whether to use multithreading. Defaults to False.\\r\\n            max_concurrency: The maximum number of threads to use. Defaults to 4.\\r\\n            sample_size: The maximum number of files you would like to load from the\\r\\n                directory.\\r\\n            randomize_sample: Shuffle the files to get a random sample.\\r\\n            sample_seed: set the seed of the random shuffle for reproducibility.                \\r\\n        '''\\r\\n        try:\\r\\n            from langchain_community.document_loaders import DirectoryLoader\\r\\n            loader = DirectoryLoader(path = path,\\r\\n                                        glob = glob,\\r\\n                                        silent_errors = silent_errors,\\r\\n                                        load_hidden = load_hidden,\\r\\n                                        loader_cls = loader_cls,\\r\\n                                        loader_kwargs = loader_kwargs,\\r\\n                                        recursive = recursive,\\r\\n                                        show_progress = show_progress,\\r\\n                                        use_multithreading = use_multithreading,\\r\\n                                        max_concurrency = max_concurrency,                                    \\r\\n                                        sample_size = sample_size,\\r\\n                                        randomize_sample = randomize_sample,\\r\\n                                        sample_seed = sample_seed)        \\r\\n            return loader\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in directory_loader as', e)\\r\\n            raise e\\r\\n    def splitdocs(self, docs, chunk_size:int=50, chunk_overlap:int=10, separators:List[str]=None, keep_separator:bool=True):\\r\\n        '''\\r\\n        This function will split the document in to chunk size with overlap\\r\\n        Args:\\r\\n            docs: List[Document]\\r\\n            chunk_size: int\\r\\n            chunk_overlap: int\\r\\n            separators: List[str]\\r\\n            keep_separator: bool\\r\\n        return: List[Document]\\r\\n        '''\\r\\n        try:\\r\\n            text_splitter  = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,separators = separators,keep_separator=keep_separator)\\r\\n            splitted_doc = text_splitter.split_documents(docs)\\r\\n            return splitted_doc\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in splitdocs as', e)\\r\\n            raise e\\r\\n    def audio_extractor(self, root_video_file_path: str):\\r\\n        '''\\r\\n        This function will extract the audio from the video file\\r\\n        Args:\\r\\n            root_video_file_path: str\\r\\n        return: str\\r\\n        '''        \\r\\n        try:                \\r\\n            video_folder = os.path.dirname(root_video_file_path)\\r\\n            audio_folder = video_folder+'/data/audios'\\r\\n            if not os.path.isdir(audio_folder):\\r\\n                os.makedirs(audio_folder)\\r\\n            if root_video_file_path.endswith(('.mp4', '.MP4')):            \\r\\n                # Load the video \\r\\n                video = mp.VideoFileClip(root_video_file_path) \\r\\n                # Extract the audio from the video \\r\\n                audio_file = video.audio \\r\\n                filename = os.path.basename(root_video_file_path)          \\r\\n                audio_file_name = audio_folder+'/'+os.path.splitext(filename)[0]+'.wav'                  \\r\\n                audio_file.write_audiofile(audio_file_name) \\r\\n                logger.info('Extracted Audio from video : ',filename) \\r\\n            logger.info('AudioExtractor executed successfully!')\\r\\n            return audio_file_name\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in AudioExtractor as: ',e)\\r\\n            raise e\\r\\n    def audio_chunker(self, audio_file_name: str, chunk_size_param: int = 300):  \\r\\n        '''\\r\\n        This function will divide the audio into chunks of chunk_size_param\\r\\n        Args:\\r\\n            audio_file_name: str\\r\\n            chunk_size_param: int\\r\\n        return: str        \\r\\n        '''\\r\\n        try:\\r\\n            chunk_size = chunk_size_param      \\r\\n            '''Divide the audio file into chunk_size samples'''\\r\\n            f = AudioSegment.from_wav(audio_file_name)\\r\\n            j = 0\\r\\n            chunker_folder = os.path.splitext(audio_file_name)[0].replace('audios','chunks')\\r\\n            if not os.path.exists(chunker_folder):\\r\\n                os.makedirs(chunker_folder)\\r\\n            os.chdir(chunker_folder)\\r\\n            chunked_audio_file_name = os.path.basename(audio_file_name)\\r\\n            while len(f[:]) >= chunk_size * 1000:\\r\\n                chunk = f[:chunk_size * 1000]\\r\\n                chunk.export(chunked_audio_file_name[:-4] + '_{:04d}.wav'.format(j), format='wav')\\r\\n                logger.info('Padded file stored as ' + chunked_audio_file_name[:-4] + '_{:04d}.wav'.format(j))\\r\\n                f = f[chunk_size * 1000:]\\r\\n                j += 1\\r\\n            if 0 < len(f[:]) and len(f[:]) < chunk_size * 1000:\\r\\n                silent = AudioSegment.silent(duration=chunk_size * 1000)\\r\\n                paddedData = silent.overlay(f, position=0, times=1)\\r\\n                paddedData.export(chunked_audio_file_name[:-4] + '_{:04d}.wav'.format(j), format='wav')\\r\\n                logger.info('Padded file stored as ' + chunked_audio_file_name[:-4] + '_{:04d}.wav'.format(j))\\r\\n            os.chdir('../../')\\r\\n            logger.info('Chunker executed successfully!')\\r\\n            return chunker_folder\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Chunker as: ',e)\\r\\n            raise e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Data Chunker"",""attributes"":{}}","SemanticSearchCore-4"
"SemanticSearch","Core","{""id"":15,""name"":""Define LLM"",""category"":""Definellm"",""parentCategory"":""14"",""classname"":""Definellm"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""from typing import Optional,Any\\r\\nfrom langchain.chat_models import AzureChatOpenAI\\r\\nimport os\\r\\nimport logging as logger\\r\\n\\r\\nclass LLMDefineConfig:\\r\\n    '''\\r\\n    define llm model. currently we support Azureopenai\\r\\n    Args:\\r\\n        deployment_name (str): Name of the Azure OpenAI deployment to use for text generation\\r\\n        model_name (str): Name of the Azure OpenAI model to use for text generation\\r\\n        openai_api_key (str): API key for authentication if using an API-based text generation engine\\r\\n        openai_api_version (str): API version, e.g. 'v1'\\r\\n        openai_api_base (str): Base URL endpoint if using an API-based text generation engine\\r\\n        openai_api_type (str): The type of API if using an API-based text generation engine, e.g. 'OpenAI'\\r\\n        temperature (float): Float value controlling randomness/creativity of text generation\\r\\n    Returns:\\r\\n        llm: llm model's object.\\r\\n    '''    \\r\\n    def __init__(self,config_json:Optional[Any]={}):\\r\\n        self.deployment_name:Optional[str]=config_json.get('deployment_name','')\\r\\n        self.model_name:Optional[str]=config_json.get('model_name','')\\r\\n        self.openai_api_key:Optional[str]=config_json.get('openai_api_key','') \\r\\n        self.openai_api_version:Optional[str]= config_json.get('openai_api_version','') \\r\\n        self.openai_api_base:Optional[str]=config_json.get('openai_api_base','') \\r\\n        self.openai_api_type:Optional[str]=config_json.get('openai_api_type','azure')\\r\\n        self.temperature:Optional[float]=config_json.get('temperature',0)\\r\\n        self.LLM_Type : Optional[str] = config_json.get('LLM_Type','AzureOpenai')\\r\\n           \\r\\n\\r\\nclass LLMDefine:    \\r\\n    def getmodel(self,config): \\r\\n        '''\\r\\n        define llm model. currently we support Azureopenai\\r\\n        Args:\\r\\n            config (LLMDefineConfig): define llm model. currently we support Azureopenai\\r\\n        Returns:\\r\\n            llm: llm model's object.\\r\\n        '''        \\r\\n        try:                 \\r\\n            if config.LLM_Type.lower() == 'azureopenai':\\r\\n                '''\\r\\n                AzureOpenai\\r\\n                deployment_name: # Name of the Azure OpenAI deployment to use for text generation\\r\\n                model_name: # Name of the Azure OpenAI model to use for text generation\\r\\n                openai_api_key: # API key for authentication if using an API-based text generation engine\\r\\n                openai_api_version: # API version, e.g. 'v1'\\r\\n                openai_api_base: # Base URL endpoint if using an API-based text generation engine\\r\\n                openai_api_type: # The type of API if using an API-based text generation engine, e.g. 'OpenAI'\\r\\n                temperature: # Float value controlling randomness/creativity of text generation\\r\\n                '''           \\r\\n                llm = self.AzureOpenai(deployment_name=config.deployment_name, \\r\\n                                    model_name=config.model_name, \\r\\n                                    openai_api_key=config.openai_api_key, \\r\\n                                    openai_api_version=config.openai_api_version,\\r\\n                                    openai_api_base=config.openai_api_base,\\r\\n                                    openai_api_type=config.openai_api_type,\\r\\n                                    temperature=config.temperature\\r\\n                                    )\\r\\n                return llm\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in getmodel as',e)\\r\\n            raise e    \\r\\n    \\r\\n    def AzureOpenai(self,\\r\\n                    deployment_name:str='', \\r\\n                    model_name:str='', \\r\\n                    openai_api_key:str='', \\r\\n                    openai_api_version:str='',\\r\\n                    openai_api_base:str='',\\r\\n                    openai_api_type:str='',\\r\\n                    temperature:float=0        \\r\\n                    ):  \\r\\n        '''\\r\\n        define llm model. currently we support Azureopenai\\r\\n        Args:\\r\\n            deployment_name (str): Name of the Azure OpenAI deployment to use for text generation\\r\\n            model_name (str): Name of the Azure OpenAI model to use for text generation\\r\\n            openai_api_key (str): API key for authentication if using an API-based text generation engine\\r\\n            openai_api_version (str): API version, e.g. 'v1'\\r\\n            openai_api_base (str): Base URL endpoint if using an API-based text generation engine\\r\\n            openai_api_type (str): The type of API if using an API-based text generation engine, e.g. 'OpenAI'\\r\\n            temperature (float): Float value controlling randomness/creativity of text generation\\r\\n        Returns:\\r\\n            llm: llm model's object.                \\r\\n        '''\\r\\n        try:              \\r\\n            # openai.proxy = {'http' : 'http://blrproxy.ad.infosys.com:80','https' : 'http://blrproxy.ad.infosys.com:80'}\\r\\n            llm = AzureChatOpenAI(\\r\\n                                    deployment_name=deployment_name,\\r\\n                                    model_name=model_name,\\r\\n                                    openai_api_key=openai_api_key,\\r\\n                                    openai_api_version=openai_api_version,\\r\\n                                    openai_api_base=openai_api_base,\\r\\n                                    openai_api_type=openai_api_type, \\r\\n                                    temperature=temperature                               \\r\\n                                )\\r\\n            return llm\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in AzureOpenai as', e)\\r\\n            raise e\\r\\n\\n""},""formats"":{},""alias"":""Define LLM"",""attributes"":{}}","SemanticSearchCore-15"
"SemanticSearch","Core","{""id"":5,""name"":""Embedding Models"",""category"":""EmbeddingModels"",""parentCategory"":""2"",""classname"":""EmbeddingModels"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""from typing import Optional,Any\\r\\nimport os\\r\\nimport logging as logger\\r\\nclass EmbeddingConfig:\\r\\n    def __init__(self,config_json:Optional[Any] = {}):\\r\\n        self.embedding_type:Optional[str]=config_json.get('embedding_type',\\""HuggingFace\\"")\\r\\n        self.model_name:Optional[str]=config_json.get('model_name',\\""sentence-transformers/all-MiniLM-L6-v2\\"")\\r\\n        self.azure_openai_api_key:Optional[str]=config_json.get('azure_openai_api_key',os.environ.get('app_openai_api_key'))\\r\\n        self.azure_openai_endpoint:Optional[str]=config_json.get('azure_openai_endpoint','https://azureft.openai.azure.com/')\\r\\n        self.azure_api_version:Optional[str]=config_json.get('azure_api_version','2023-03-15-preview')\\r\\n        self.azure_deployment:Optional[str]=config_json.get('azure_deployment','openaiada2')\\r\\n        self.openai_model:Optional[str]=config_json.get('openai_model','text-embedding-ada-002')\\r\\n        self.openai_api_base:Optional[str]=config_json.get('openai_api_base','https://azureft.openai.azure.com/')\\r\\n        self.openai_api_type:Optional[str]=config_json.get('openai_api_type','azure')\\r\\n        self.bedrock_access_id:Optional[str]=config_json.get('bedrock_access_id',os.environ.get('app_aws_secret_access_id'))\\r\\n        self.bedrock_access_key:Optional[str]=config_json.get('bedrock_access_key',os.environ.get('app_aws_secret_access_key'))\\r\\n        self.bedrock_region_name:Optional[str]=config_json.get('bedrock_region_name','us-east-1')\\r\\n        self.bedrock_endpoint_url:Optional[str]=config_json.get('bedrock_endpoint_url','https://bedrock-runtime.us-east-1.amazonaws.com')\\r\\n        self.bedrock_model_id:Optional[str]=config_json.get('bedrock_model_id','amazon.titan-embed-g1-text-02')\\r\\n        self.elastic_search_model_id:Optional[str]=config_json.get('elastic_search_model_id',None)\\r\\n        self.elastic_search_cloud_id:Optional[str]=config_json.get('elastic_search_cloud_id',None)\\r\\n        self.elastic_search_user_id:Optional[str]=config_json.get('elastic_search_user_id',None)\\r\\n        self.elastic_search_password:Optional[str]=config_json.get('elastic_search_password',None)\\r\\nclass EmbeddingModels:\\r\\n    def __init__(self) -> None:\\r\\n        pass\\r\\n    def get_embedding_function(self,config):\\r\\n        \\""\\""\\""Embedding Model Defination for vector db. We support FaissDB,chromadb,ElasticsearchDB, it's not required if you prepared your own vector db.\\r\\n        Args:\\r\\n            embedding_type(str): Type of Embedding to be used, We support HuggingFace, AzureOpenAI, Bedrock, ElasticSearch, Default is HuggingFace.\\r\\n            model_name(str): Model Name to generate the embedding according to your embedding type, Default is \\""sentence-transformers/all-MiniLM-L6-v2\\"".\\r\\n            AzureOpenAI_Params(str):azure_openai_api_key,\\r\\n                                    azure_openai_endpoint,\\r\\n                                    azure_api_version,\\r\\n                                    azure_deployment,\\r\\n                                    model_name= Open AI Embedding Model Name ,\\r\\n                                    openai_api_base,\\r\\n                                    openai_api_type\\r\\n            Bedrock_Params(str):bedrock_access_id,\\r\\n                                bedrock_access_key,\\r\\n                                bedrock_region_name,\\r\\n                                bedrock_endpoint_url,\\r\\n                                bedrock_model_id\\r\\n            ElasticSearch_Params(str):  elastic_search_model_id,\\r\\n                                        elastic_search_cloud_id,\\r\\n                                        elastic_search_user,\\r\\n                                        elastic_search_password\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            logger.info(config.azure_openai_api_key)\\r\\n            if config.embedding_type.strip().lower() == \\""huggingface\\"":\\r\\n                return self.HuggingFace(model_name=config.model_name)\\r\\n            elif config.embedding_type.strip().lower() == \\""azureopenai\\"":\\r\\n                return self.AzureOpenAI(azure_openai_api_key_param=config.azure_openai_api_key,azure_api_version_param=config.azure_api_version,azure_deployment_param=config.azure_deployment,model_param=config.model_name,openai_api_base_param=config.openai_api_base,openai_api_type_param=config.openai_api_type)\\r\\n            elif config.embedding_type.strip().lower() == \\""bedrock\\"":\\r\\n                return self.Bedrock(access_id_param=config.bedrock_access_id,access_key_param=config.bedrock_access_key,region_name_param=config.bedrock_region_name,endpoint_url_param=config.bedrock_endpoint_url,model_id_param=config.bedrock_model_id)\\r\\n            elif config.embedding_type.strip().lower() == \\""elasticsearch\\"":\\r\\n                return self.ElasticSearch(elastic_model_id_param=config.elastic_search_model_id,es_cloud_id_param=config.elastic_search_cloud_id,es_user_param=config.elastic_search_user,es_password_param=config.elastic_search_password)\\r\\n            else:\\r\\n                raise Exception(\\""Unknown embedding type, we support only ElasticSearch, Bedrock, AzureOpenAI, and HuggingFace\\"")\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_embedding_function',e)\\r\\n            raise e\\r\\n    def HuggingFace(self,model_name:str=''):\\r\\n        try:\\r\\n            from langchain.embeddings import HuggingFaceEmbeddings\\r\\n            embeddings = HuggingFaceEmbeddings(\\r\\n                    model_name=model_name\\r\\n            )\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in HuggingFace', e)\\r\\n            raise e\\r\\n    def AzureOpenAI(self,azure_openai_api_key_param:str='',azure_api_version_param:str='',azure_deployment_param:str='',model_param:str='',openai_api_base_param:str='',openai_api_type_param:str=''):\\r\\n        try:\\r\\n            from langchain.embeddings import AzureOpenAIEmbeddings\\r\\n            embeddings = AzureOpenAIEmbeddings(api_key=azure_openai_api_key_param,api_version=azure_api_version_param,azure_deployment=azure_deployment_param,model=model_param,openai_api_type=openai_api_type_param,base_url=openai_api_base_param)\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in AzureOpenAI', e)\\r\\n            raise e\\r\\n    def Bedrock(self,access_id_param:str='',access_key_param:str='',region_name_param:str='',endpoint_url_param:str='',model_id_param:str=''):\\r\\n        try:\\r\\n            import boto3\\r\\n            from langchain.embeddings import BedrockEmbeddings\\r\\n            bedrock = boto3.client('bedrock-runtime',aws_access_key_id=access_id_param,aws_secret_access_key=access_key_param,region_name=region_name_param,endpoint_url=endpoint_url_param)\\r\\n            embeddings = BedrockEmbeddings(\\r\\n                model_id=model_id_param, \\r\\n                client=bedrock\\r\\n            )\\r\\n            embeddings\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Bedrock', e)\\r\\n            raise e\\r\\n    def Elastic_search(self,model_id_param:str='',es_cloud_id_param:str='',es_user_param:str='',es_password_param:str=''):\\r\\n        try:\\r\\n            from langchain.embeddings.elasticsearch import ElasticsearchEmbeddings\\r\\n            embeddings = ElasticsearchEmbeddings.from_credentials(\\r\\n                model_id=model_id_param,\\r\\n                es_cloud_id=es_cloud_id_param,\\r\\n                es_user=es_user_param,\\r\\n                es_password=es_password_param,\\r\\n            )\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Elastic_search', e)\\r\\n            raise e\\n""},""formats"":{},""alias"":""Embedding Models"",""attributes"":{}}","SemanticSearchCore-5"
"SemanticSearch","Core","{""id"":7,""name"":""Inference"",""category"":""Inference"",""parentCategory"":""1"",""classname"":""Infer"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Inference""}","SemanticSearchCore-7"
"SemanticSearch","Core","{""id"":11,""name"":""Utils"",""category"":""Utils"",""parentCategory"":"""",""classname"":""util"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Utils""}","SemanticSearchCore-11"
"SemanticSearch","Core","{""id"":12,""name"":""Python Class"",""category"":""PythonClass"",""parentCategory"":""11"",""classname"":""PythonClass"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{""script"":""""},""formats"":{""script"":""textArea""},""alias"":""Python Class""}","SemanticSearchCore-12"
"SemanticSearch","Core","{""id"":14,""name"":""NLP2SQL"",""category"":""NLP2SQL"",""parentCategory"":"""",""classname"":""nlptwosql"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""NLP2SQL""}","SemanticSearchCore-14"
"SemanticSearch","Core","{""id"":16,""name"":""SQL Engine"",""category"":""SQLEngine"",""parentCategory"":""14"",""classname"":""SQLEngine"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nimport json\\r\\nimport requests\\r\\nimport json\\r\\nfrom urllib.parse import quote_plus\\r\\nfrom leaputils import Security\\r\\nfrom langchain.sql_database import SQLDatabase\\r\\nfrom sql_metadata import Parser\\r\\nimport sqlite3\\r\\n\\r\\n#!pip install sql-metadata\\r\\n#!pip install leaputils-3.0.0-py3-none-any.whl\\r\\n           \\r\\nclass SQLEngine:                            \\r\\n    '''\\r\\n    Extract dataset. Currently We support S3 datastore.        \\r\\n \\r\\n    Args:\\r\\n        dataset_id (str): Dataset id to get dataset config  \\r\\n        local_path (str): Local file path to store file downloaded from data store,     \\r\\n        s3:\\r\\n            s3_end_point_url (str): s3 end point url\\r\\n            s3_access_key (str): s3 access key\\r\\n            s3_secret_key (str): s3 secret key \\r\\n            bucket (str): s3 bucket name\\r\\n            obj_key (str): s3 object(file, folder) name\\r\\n        mysql db:\\r\\n            db_host:str = None, \\r\\n            db_port:str = None, \\r\\n            db_user_name:str = None, \\r\\n            db_password:str = None, \\r\\n            query:str = None\\r\\n    Returns:\\r\\n        file_path (str): Local file path            \\r\\n    '''\\r\\n    def __init__(self, dataset_id:str, organization:str) -> None:\\r\\n        self.organization = organization\\r\\n        self.dataset_id = dataset_id        \\r\\n\\r\\n    def get_db(self) -> str: \\r\\n        '''\\r\\n        Get database connection and metadata from database.\\r\\n        Returns:\\r\\n            db (SQLDatabase): Database connection object\\r\\n            metadata (dict): Dataset metadata\\r\\n            connection_details (dict): Connection details for database connection        \\r\\n        '''\\r\\n        try: \\r\\n            # Connect to the database\\r\\n            root_dir = '/RAG_DB'\\r\\n            os.makedirs(root_dir, exist_ok=True)  # Create only if needed\\r\\n            root_path = os.path.abspath(root_dir)\\r\\n            path = os.path.join(root_path,'rag_database.db')\\r\\n            # path = os.path.join('../database','rag_database.db')\\r\\n            conn = sqlite3.connect(path)\\r\\n\\r\\n            # Create a cursor object\\r\\n            cursor = conn.cursor()\\r\\n\\r\\n            sql = 'SELECT * FROM dataset_details WHERE dataset_id=? AND organization=?'\\r\\n            values = (self.dataset_id, self.organization)\\r\\n            cursor.execute(sql, values)\\r\\n\\r\\n            data = cursor.fetchone()\\r\\n\\r\\n            connection_string = data[3]   \\r\\n            connection_details = json.loads(data[4]) \\r\\n            sql_query = connection_details['query']\\r\\n            metadata = json.loads(data[5])\\r\\n\\r\\n            tablesToInclude = Parser(sql_query).tables\\r\\n            db = SQLDatabase.from_uri(connection_string,include_tables=tablesToInclude)  \\r\\n            \\r\\n        except sqlite3.connector.Error as err:\\r\\n            print('Thre is some error whiling storing data as:',err)\\r\\n            raise err\\r\\n        finally:\\r\\n            # Close database connection properly\\r\\n            cursor.close()\\r\\n            conn.close()  \\r\\n\\r\\n        return db, metadata, connection_details  \\r\\n        \\n""},""attributes"":{},""formats"":{},""alias"":""SQL Engine""}","SemanticSearchCore-16"
"SemanticSearch","Core","{""id"":17,""name"":""Query Generator"",""category"":""QueryGenerator"",""parentCategory"":""14"",""classname"":""QueryGenerator"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import boto3\\r\\nfrom typing import List, Optional\\r\\nimport pathlib\\r\\nimport os\\r\\nimport json\\r\\nimport shutil\\r\\nimport requests\\r\\nimport boto3\\r\\nimport json\\r\\nfrom typing import List, Optional, Union, Dict, Sequence, Any\\r\\nfrom langchain.chains import create_sql_query_chain\\r\\nimport logging as logger\\r\\nclass QueryGeneratorConfig:\\r\\n    '''\\r\\n    Config class for QueryGenerator.\\r\\n    Args:\\r\\n        user_query (str): The user's query.\\r\\n    '''\\r\\n    def __init__(self, config_json: Optional[Any]={}) -> None:\\r\\n        self.user_query: Optional[str] = config_json.get('user_query',None)        \\r\\n\\r\\nclass QueryGenerator:\\r\\n    '''\\r\\n    QueryGenerator class.\\r\\n    Args:\\r\\n        db (str): The database to query.\\r\\n        llm (str): The language model to use.\\r\\n    Returns:\\r\\n        sql_query (str): The generated SQL query.\\r\\n    '''                                \\r\\n    def __init__(self, db, llm) -> None:        \\r\\n        self.db = db\\r\\n        self.llm = llm\\r\\n\\r\\n    def get_sql_query(self, config) -> str: \\r\\n        '''\\r\\n        Get the SQL query.\\r\\n        Args:\\r\\n            config (QueryGeneratorConfig): The configuration for the query generator.\\r\\n        Returns:\\r\\n            sql_query (str): The generated SQL query.\\r\\n        '''    \\r\\n        try:    \\r\\n            # define chain                      \\r\\n            chain = create_sql_query_chain(self.llm, self.db)        \\r\\n            sql_query = chain.invoke({\\""question\\"": config.user_query})        \\r\\n            return sql_query\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in QueryGenerator as: ', e)\\r\\n            return e\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Query Generator"",""attributes"":{}}","SemanticSearchCore-17"
"SemanticSearch","Core","{""id"":9,""name"":""Query VectorDB"",""category"":""QueryVectorDB"",""parentCategory"":""7"",""classname"":""QueryVectorDB"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nfrom typing import List,Optional,Dict,Union,Sequence,Any\\r\\nimport logging as logger\\r\\nclass VectordbConfig:\\r\\n    def __init__(self,config_json:Optional[Any]={}) :\\r\\n        self.DB_Type : Optional[str] = config_json.get('DB_Type',\\""Faiss\\"")\\r\\n        self.elastic_search_url : str = config_json.get('elastic_search_url',os.environ.get('app_elastic_search_url'))\\r\\n        self.index_name : Optional[str] = config_json.get('index_name',\\""Vector_Store\\"")\\r\\n        self.qdrant_url : Optional[str] = config_json.get('qdrant_url',os.environ.get('app_qdrant_url'))\\r\\n        self.force_recreate : Optional[bool] = config_json.get('force_recreate',False)\\r\\n        self.query : Optional[str] = config_json.get('query',\\""Hi, How are you?\\"")\\r\\n        self.k : Optional[int] = config_json.get('k',5)\\r\\n        \\r\\nclass QueryVectorDB:\\r\\n    def __init__(self,embedding) -> None:\\r\\n        self.embeddings = embedding\\r\\n    \\r\\n    def get_similar_docs(self,config):\\r\\n        \\""\\""\\""`Meta Faiss` vector store.\\r\\n        To use, you must have the ``faiss`` python package installed.\\r\\n        Example:\\r\\n            .. code-block:: python\\r\\n                from langchain_community.embeddings.openai import OpenAIEmbeddings\\r\\n                from langchain_community.vectorstores import FAISS\\r\\n                embeddings = OpenAIEmbeddings()\\r\\n                texts = [\\""FAISS is an important library\\"", \\""LangChain supports FAISS\\""]\\r\\n                faiss = FAISS.from_texts(texts, embeddings)\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            if config.DB_Type.lower() == 'faiss':\\r\\n                return self.get_similar_docs_faiss_db(embeddings=self.embeddings,persist_directory='/'+config.index_name,query=config.query,k=config.k)                \\r\\n            elif config.DB_Type.lower() == 'chroma':  \\r\\n                return self.get_similar_docs_chroma_db(embeddings=self.embeddings,persist_directory='/'+config.index_name,query=config.query,k=config.k)        \\r\\n            elif config.DB_Type.lower() == 'elasticsearch':                   \\r\\n                return self.get_similar_docs_elastic_db(embeddings=self.embeddings,docs=self.docs,es_url=config.elastic_search_url,index_name=config.index_name,query=config.query,k=config.k)\\t\\t    \\r\\n            elif config.DB_Type.lower() == 'qdrant':      \\r\\n                return self.get_similar_docs_qdrant_db(embeddings=self.embeddings,qdrant_url=config.qdrant_url,collection_name=config.index_name,query=config.query,k=config.k)\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs', e)\\r\\n            raise e\\r\\n        \\r\\n    def get_similar_docs_chroma_db(self,embeddings:Any=None,persist_directory:str=\\""\\"",query:str='',k:int=None):\\r\\n        \\""\\""\\""`ChromaDB` vector store.\\r\\n        To use, you should have the ``chromadb`` python package installed.\\r\\n        Example:\\r\\n        .. code-block:: python\\r\\n        from langchain_community.vectorstores import Chroma\\r\\n        from langchain_community.embeddings.openai import OpenAIEmbeddings\\r\\n        embeddings = OpenAIEmbeddings()\\r\\n        vectorstore = Chroma(\\""langchain_store\\"", embeddings)\\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.vectorstores import Chroma\\r\\n            db3 = Chroma(persist_directory=persist_directory, embeddings=embeddings)\\r\\n            docs = db3.similarity_search(query,k=k)\\r\\n            \\r\\n            return docs\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs_chroma_db', e)\\r\\n            raise e\\r\\n    def get_similar_docs_faiss_db(self,embeddings:Any=None,persist_directory:str=\\""\\"",query:str='',k:int=None):   \\r\\n        try:     \\r\\n            from langchain.vectorstores import FAISS\\r\\n            db = FAISS.load_local(persist_directory, embeddings)\\r\\n            docs = db.similarity_search(query,k=k)\\r\\n            \\r\\n            return docs\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs_faiss_db', e)\\r\\n            raise e\\r\\n    def get_similar_docs_qdrant_db(self,qdrant_url:str='',embeddings:Any='',query:str='',collection_name:str='',k:int=None):\\r\\n        \\""\\""\\""\\r\\n        QdrantvectorDb\\r\\n        Args:[]\\r\\n            embeddings=self.embeddings, # Vector embeddings to use for similarity search\\r\\n            qdrant_url=config.qdrant_url, # URL of Qdrant search server \\r\\n            collection_name=config.index_name, # Name of indexed document collection in Qdrant \\r\\n            query=config.query # Search query text to find similar documents for\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from qdrant_client import QdrantClient\\r\\n            from sentence_transformers import SentenceTransformer\\r\\n            import openai\\r\\n            client = QdrantClient(url=qdrant_url)\\r\\n            if str(type(embeddings))==\\""<class 'langchain_community.embeddings.huggingface.HuggingFaceEmbeddings'>\\"":\\r\\n                model=embeddings.model_name\\r\\n                name=model.split('/')\\r\\n                encoder = SentenceTransformer(name[1])\\r\\n                query_vector=encoder.encode(query).tolist()\\r\\n            elif str(type(embeddings))==\\""<class 'langchain_community.embeddings.azure_openai.AzureOpenAIEmbeddings'>\\"":\\r\\n                model=embeddings.deployment\\r\\n                openai.api_key = embeddings.openai_api_key\\r\\n                openai.api_version = embeddings.openai_api_version\\r\\n                openai.api_type = embeddings.openai_api_type\\r\\n                openai.api_base = embeddings.openai_api_base\\r\\n                query_vector = openai.Embedding.create(\\r\\n                engine = model,\\r\\n                input=query,\\r\\n                model=embeddings.model,\\r\\n        )['data'][0]['embedding']\\r\\n            elif str(type(embeddings))==\\""<class 'langchain_community.embeddings.bedrock.BedrockEmbeddings'>\\"":\\r\\n                import json\\r\\n                model_id = embeddings.model_id\\r\\n                bedrock = embeddings.client\\r\\n                input_text = query\\r\\n                body = json.dumps({\\r\\n                    \\""inputText\\"": input_text\\r\\n                    })\\r\\n                accept = \\""application/json\\""\\r\\n                content_type = \\""application/json\\""\\r\\n                query_vector = bedrock.invoke_model(\\r\\n                    body=body, modelId=model_id, accept=accept, contentType=content_type\\r\\n                )\\r\\n                query_vector = json.loads(query_vector.get('body').read())['embedding']\\r\\n            hits = client.search(collection_name=collection_name,query_vector=query_vector,limit=k)\\r\\n            l=[]\\r\\n            for hit in hits:\\r\\n                l.append(hit.payload)        \\r\\n            \\r\\n            return l\\r\\n        except Exception as e:  \\r\\n            logger.info('Exception in get_similar_docs_qdrant_db', e)\\r\\n            raise e\\r\\n        \\r\\n    def get_similar_docs_elastic_db(self,embeddings:Any=None,docs:Any=None,query:str='',es_url=\\""\\"",index_name=\\""\\"",k:int=None):\\r\\n        \\""\\""\\""`Elasticsearch` vector store.\\r\\n        Args:\\r\\n            index_name: Name of the Elasticsearch index to create.\\r\\n            es_url: URL of the Elasticsearch instance to connect to.\\r\\n            cloud_id: Cloud ID of the Elasticsearch instance to connect to.\\r\\n            es_user: Username to use when connecting to Elasticsearch.\\r\\n            es_password: Password to use when connecting to Elasticsearch.\\r\\n            es_api_key: API key to use when connecting to Elasticsearch.\\r\\n            es_connection: Optional pre-existing Elasticsearch connection.\\r\\n            vector_query_field: Optional. Name of the field to store\\r\\n                                the embedding vectors in.\\r\\n            query_field: Optional. Name of the field to store the texts in.\\r\\n            strategy: Optional. Retrieval strategy to use when searching the index.\\r\\n                        Defaults to ApproxRetrievalStrategy. Can be one of\\r\\n                        ExactRetrievalStrategy, ApproxRetrievalStrategy,\\r\\n                        or SparseRetrievalStrategy.\\r\\n            distance_strategy: Optional. Distance strategy to use when\\r\\n                                searching the index.\\r\\n                                Defaults to COSINE. Can be one of COSINE,\\r\\n                                    EUCLIDEAN_DISTANCE, or DOT_PRODUCT. \\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            from langchain_community.vectorstores import ElasticsearchStore\\r\\n            db = ElasticsearchStore.from_documents(\\r\\n            docs,\\r\\n            embeddings,\\r\\n            es_url=es_url,\\r\\n            index_name=index_name,\\r\\n            )\\r\\n            db.client.indices.refresh(index=index_name)\\r\\n            results = db.similarity_search(query,k=k)            \\r\\n            return results\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_similar_docs_elastic_db', e)\\r\\n            raise e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Query VectorDB"",""attributes"":{}}","SemanticSearchCore-9"
"SemanticSearch","Core","{""id"":13,""name"":""Python Script"",""category"":""PythonScript"",""parentCategory"":""11"",""classname"":""PythonScriptConfig"",""inputEndpoints"":[""in1"",""in2"",""in3""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""\\n\\n""},""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""alias"":""Python Script"",""attributes"":{""FunctionName"":""PythonScript"",""requirements"":"""",""params"":"""",""script"":""def PythonScript( dataset):    return dataset""}}","SemanticSearchCore-13"
"SemanticSearch","Core","{""id"":10,""name"":""LLM Infer"",""category"":""LLMInfer"",""parentCategory"":""7"",""classname"":""LLM Infer"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nfrom typing import List,Optional,Dict,Union,Sequence,Any\\r\\nimport json\\r\\nimport requests\\r\\nimport logging as logger\\r\\nclass LLMInferenceConfig:\\r\\n    def __init__(self,config_json:Optional[Any]={}):\\r\\n        self.LLM_Type : Optional[str] = config_json.get('LLM_Type',\\""OnPrem\\"")\\r\\n        self.prompt_template : Optional[str] = config_json.get('prompt_template','Answer the query asked by user based on the given Context below.')\\r\\n        self.model_url : Optional[str] = config_json.get('url',\\""https://itgateway.infosys.com/ai-platform/mxtral-aic/v1/language/generate\\"")\\r\\n        self.falcon_url : Optional[str] = config_json.get('falcon_url',os.environ.get('app_falcon_url'))\\r\\n        self.max_tokens : Optional[int] = config_json.get('max_tokens',2000)\\r\\n        self.top_k :  Optional[int] = config_json.get('top_k',5)\\r\\n        self.top_p :  Optional[float] = config_json.get('top_p',0.25)\\r\\n        self.typical_p : Optional[float] = config_json.get('typical_p',0.25)\\r\\n        self.temperature : Optional[float] = config_json.get('temperature',0)\\r\\n        self.repetition_penalty : Optional[float] = config_json.get('repetition_penalty',None)\\r\\n        self.api_type : Optional[str] = config_json.get('api_type',\\""azure\\"")\\r\\n        self.api_base : Optional[str] = config_json.get('api_base',\\""https://azureft.openai.azure.com/\\"") \\r\\n        self.api_version : Optional[str] = config_json.get('api_version',\\""2023-03-15-preview\\"")\\r\\n        self.api_key : Optional[str] = config_json.get('api_key',os.environ.get('app_openai_api_key'))\\r\\n        self.stop : Optional[str] = config_json.get('stop',\\""\\"")\\r\\n        self.service_name : Optional[str] = config_json.get('service_name','bedrock-runtime')\\r\\n        self.aws_access_key_id : Optional[str] = config_json.get('aws_access_key_id',os.environ.get('app_aws_secret_access_id'))\\r\\n        self.aws_secret_access_key : Optional[str] = config_json.get('aws_secret_access_key',os.environ.get('app_aws_secret_access_key'))\\r\\n        self.region_name : Optional[str] = config_json.get('region_name','us-east-1')\\r\\n        self.endpoint_url : Optional[str] = config_json.get('endpoint_url','https://bedrock-runtime.us-east-1.amazonaws.com')\\r\\n        self.model_id : Optional[str] = config_json.get('model_id','anthropic.claude-v2')\\r\\n        self.query : Optional[str] = config_json.get('query',\\""Hi\\"")\\r\\n        self.engine : Optional[str] = config_json.get('engine','gtp35turbo')\\r\\n        self.client_id : Optional[str] = config_json.get('client_id',os.environ.get('app_client_id'))\\r\\n        self.client_secret : Optional[str] = config_json.get('client_secret',os.environ.get('app_client_secret'))\\r\\n        \\r\\n\\r\\nclass LLMInference:\\r\\n    def __init__(self, context):\\r\\n        self.context = context\\r\\n    \\r\\n    def Query_LLM(self,config):\\r\\n        ''' \\r\\n        Falcon40b model\\r\\n        args[]\\r\\n            prompt_template:  # Prompt template string to prepend before the input context, question and parameters\\r\\n            falcon_40b_model_url:  # URL of the Falcon 4.0b model to use\\r\\n            context:  # Context string to provide background for answering the question\\r\\n            question:  # Question string to get an answer for\\r\\n            max_tokens:  # Maximum number of tokens to generate in the answer\\r\\n            top_k:  # Sample from the top k tokens, overrides typical_p if set\\r\\n            top_p:  # Sample from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p, overrides top_k if set\\r\\n            typical_p:  # Typical sampling probability, overrides top_p if set\\r\\n            temperature:  # Randomness of the sampling, higher values means more random, overrides typical_p if set\\r\\n            repetition_penalty:  # Penalty for repetitive phrases, higher values mean more diversity \\r\\n        '''\\r\\n        try:\\r\\n            if config.LLM_Type.strip().lower() == 'falcon':\\r\\n                return self.OnPrem(context=self.context,prompt_template=config.prompt_template,falcon_40b_model_url = config.falcon_url,question=config.query,max_tokens=config.max_tokens,top_k=config.top_k,top_p=config.top_p,typical_p=config.typical_p,temperature=config.temperature,repetition_penalty=config.repetition_penalty)\\r\\n            elif config.LLM_Type.strip().lower() == 'onprem':\\r\\n                return self.Mixtral(prompt_template=config.prompt_template,context=self.context,Mixtral_model_url = config.model_url,question=config.query,max_tokens=config.max_tokens,top_k=config.top_k,top_p=config.top_p,typical_p=config.typical_p,temperature=config.temperature,repetition_penalty=config.repetition_penalty,client_id=config.client_id,client_secret=config.client_secret)\\r\\n\\r\\n            elif config.LLM_Type.strip().lower() == 'azureopenai':\\r\\n                '''\\r\\n                Openaimodel\\r\\n                args[]\\r\\n                engine: # Specify the AI engine/model to use for text generation\\r\\n                prompt_template:  # Prompt template string to prepend before the input context and question\\r\\n                context:  # Context string providing background information to inform the answer\\r\\n                question:  # Question string specifying the actual question to get an answer for\\r\\n                api_type:  # The type of API if using an API-based text generation engine, e.g. 'OpenAI'  \\r\\n                api_base:  # Base URL endpoint if using an API-based text generation engine \\r\\n                api_version:   # API version, e.g. 'v1'\\r\\n                api_key:  # API key for authentication if using an API-based text generation engine\\r\\n                temperature:  # Float value controlling randomness/creativity of text generation\\r\\n                max_tokens:  # Maximum number of tokens to generate in the answer\\r\\n                stop: s # String sequence to stop further text generation\\r\\n                '''        \\r\\n                return self.AzureOpenai(engine=config.engine,prompt_template=config.prompt_template,context=self.context,question=config.query,api_type=config.api_type,api_base=config.api_base,api_version=config.api_version,api_key=config.api_key,temperature=config.temperature, max_tokens=config.max_tokens, stop=config.stop, top_p=config.top_p)\\r\\n            elif config.LLM_Type.lower() == 'bedrock':\\r\\n                return self.Bedrock(service_name=config.service_name,prompt_template=config.prompt_template,question=config.query,context=self.context,aws_access_key_id=config.aws_access_key_id,aws_secret_access_key=config.aws_secret_access_key,region_name=config.region_name,endpoint_url=config.endpoint_url,temperature=config.temperature, max_tokens=config.max_tokens, top_p=config.top_p,modelId=config.model_id)\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Query_LLM as',e)\\r\\n            raise e\\r\\n    def Mixtral(self, prompt_template:str='',Mixtral_model_url:str='',context:str='', question:str='',max_tokens:float=None, top_k:float=None, top_p:float=None, typical_p:float=None, temperature:float=None, repetition_penalty: float=None,client_id:str='',client_secret:str=''):\\r\\n        def generate_token():\\r\\n            url='https://login.microsoftonline.com/63ce7d59-2f3e-42cd-a8cc-be764cff5eb6/oauth2/token'\\r\\n            headers={\\r\\n                \\""Content-Type\\"": \\""application/x-www-form-urlencoded\\"",\\r\\n                \\""Cookie\\"":\\""fpc=Akan455tw-pPuiXfSA1sz_fwGWPkAQAAAO7xkNwOAAAA; stsservicecookie=estsfd; x-ms-gateway-slice=estsfd; fpc=Akan455tw-pPuiXfSA1sz_eqfEQzAQAAAF04ktwOAAAAEWQ-MAEAAADuOJLcDgAAAExTSTMBAAAADzmS3A4AAAA; x-ms-gateway-slice=estsfd; fpc=AjRjfSpYATNGtUVf3IhQhQ71dL7bAQAAANrZid0OAAAA\\""\\r\\n            }\\r\\n            data={\\r\\n                \\""client_id\\"":client_id,\\r\\n                \\""client_secret\\"":client_secret,\\r\\n                \\""grant_type\\"":\\""client_credentials\\"",\\r\\n                \\""scope\\"":\\""b3490b10-6bd3-4f66-908d-fa1950e46598/.default\\""\\r\\n            }\\r\\n            response = requests.request('POST', url, headers=headers, data=data, verify=False)\\r\\n            \\r\\n            return json.loads(response.text)[\\""access_token\\""]\\r\\n        token=generate_token()\\r\\n        auth=f'Bearer {token}'\\r\\n        generated_answer = ''\\r\\n        input_prompt = f'''{prompt_template} \\\\\\\\n\\\\\\\\nContext: '{context}'. \\\\\\\\n\\\\\\\\nQuestion: {question}. \\\\\\\\n\\\\\\\\nAnswer:'''\\r\\n        payload = json.dumps({\\r\\n        'inputs': [input_prompt],\\r\\n        'parameters': {\\r\\n                'max_new_tokens': max_tokens\\r\\n            }\\r\\n        })\\r\\n        \\r\\n        headers = {\\r\\n            'Content-Type':'application/json', \\r\\n            'Authorization' : auth\\r\\n\\r\\n        }\\r\\n        \\r\\n        response = requests.request('POST', Mixtral_model_url, headers=headers, data=payload, verify=False)\\r\\n        \\r\\n        generated_answer = json.loads(response.text)\\r\\n        generated_answer=generated_answer['generated_text']\\r\\n        return generated_answer   \\r\\n    def OnPrem(self, prompt_template:str='',falcon_40b_model_url:str='',context:str='', question:str='',max_tokens:float=None, top_k:float=None, top_p:float=None, typical_p:float=None, temperature:float=None, repetition_penalty: float=None):\\r\\n        try:\\r\\n            generated_answer = ''\\r\\n            input_prompt = f'''{prompt_template} \\\\\\\\n\\\\\\\\nContext: '{context}'. \\\\\\\\n\\\\\\\\nQuestion: {question}. \\\\\\\\n\\\\\\\\nAnswer:'''\\r\\n            payload = json.dumps({\\r\\n            'inputs': input_prompt,\\r\\n            'parameters': {\\r\\n                    'max_new_tokens': max_tokens,\\r\\n                    'top_k': top_k,\\r\\n                    'top_p': top_p,\\r\\n                    'typical_p': typical_p,\\r\\n                    'temperature': temperature,\\r\\n                    'repetition_penalty': repetition_penalty\\r\\n                }\\r\\n            })\\r\\n            \\r\\n            headers = {\\r\\n                'Content-Type': 'application/json' \\r\\n            }\\r\\n            \\r\\n            response = requests.request('POST', falcon_40b_model_url, headers=headers, data=payload, verify=False)\\r\\n            \\r\\n            generated_answer = json.loads(response.text)[0]['generated_text']\\r\\n\\r\\n            return generated_answer\\r\\n        \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in OnPrem as', e)\\r\\n            raise e\\r\\n    \\r\\n    def AzureOpenai(self,engine:str='',prompt_template:str='',context:str='',question:str='',api_type:str='',api_base:str='',api_version:str='',api_key:str='',temperature:float='', max_tokens:float='', stop:str='', top_p:float=''):\\r\\n        try:\\r\\n            import openai\\r\\n            openai.proxy = {'http' : 'http://blrproxy.ad.infosys.com:80','https' : 'http://blrproxy.ad.infosys.com:80'}\\r\\n            openai.api_type = api_type\\r\\n            openai.api_base = api_base\\r\\n            openai.api_version = api_version\\r\\n            openai.api_key = api_key\\r\\n            prompt = f'''{prompt_template} \\\\n\\\\nContext: '{context}'. \\\\n\\\\nQuestion: {question}. \\\\n\\\\nAnswer:'''\\r\\n            response = openai.ChatCompletion.create(engine=engine,\\r\\n                    messages = [{\\""role\\"":\\""user\\"",\\""content\\"":prompt}],\\r\\n                    temperature=temperature,\\r\\n                    max_tokens=max_tokens,\\r\\n                    top_p=top_p,\\r\\n                    stop=stop)\\r\\n            out = response[\\""choices\\""][0][\\""message\\""][\\""content\\""]\\r\\n            return out\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in AzureOpenai as', e)\\r\\n            raise e\\r\\n                \\r\\n    def Bedrock(self,service_name:str='',prompt_template:str='',question:str='',context:str='',aws_access_key_id:str='',aws_secret_access_key:str='',region_name:str='',endpoint_url:str='',max_tokens:float=None,temperature:float=None,top_p:float=None,modelId:str=''):\\r\\n        try:            \\r\\n            import boto3\\r\\n            bedrock = boto3.client(service_name=service_name,aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,region_name=region_name,endpoint_url=endpoint_url)\\r\\n    \\r\\n            prompt_data = f\\""\\""\\""Human: {prompt_template} \\\\n\\\\nContext: '{context}'. \\\\n\\\\nQuestion: {question}\\r\\n            Assistant:\\""\\""\\""\\r\\n            body = json.dumps({\\r\\n                \\""prompt\\"": prompt_data,\\r\\n                \\""max_tokens_to_sample\\"":max_tokens,\\r\\n                \\""temperature\\"":temperature,\\r\\n                \\""top_p\\"":top_p\\r\\n            })\\r\\n            modelId = modelId\\r\\n            logger.info(body)\\r\\n            response = bedrock.invoke_model(body=body, modelId=modelId)\\r\\n            response_body = json.loads(response.get('body').read())\\r\\n            outputText = response_body.get('completion')\\r\\n            return outputText\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Bedrock as', e)\\r\\n            raise e \\n""},""formats"":{},""alias"":""LLM Infer"",""attributes"":{}}","SemanticSearchCore-10"
"SemanticSearch","Core","{""id"":18,""name"":""Query Executor"",""category"":""QueryExecutor"",""parentCategory"":""14"",""classname"":""QueryExecutor"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import boto3\\r\\nfrom typing import List, Optional\\r\\nimport boto3\\r\\nimport json\\r\\nfrom typing import List, Optional, Union, Dict, Sequence, Any\\r\\nfrom langchain.chains import create_sql_query_chain\\r\\nfrom langchain.agents import create_sql_agent\\r\\nfrom langchain.agents.agent_types import AgentType\\r\\nfrom langchain.sql_database import SQLDatabase\\r\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\\r\\nfrom langchain_experimental.sql import SQLDatabaseChain\\r\\nimport mysql.connector\\r\\nimport pandas as pd\\r\\nimport logging as logger\\r\\n\\r\\nclass QueryExecutorConfig:\\r\\n    '''\\r\\n    Config class for QueryExecutor.\\r\\n    Args:\\r\\n        user_query (str): The user's query.\\r\\n    '''\\r\\n    def __init__(self, config_json: Optional[Any]={}) -> None:\\r\\n        self.user_query: Optional[str] = config_json.get('user_query',None)   \\r\\n\\r\\nclass QueryExecutor:  \\r\\n    '''\\r\\n    Execute sql query, chain and returns dataframe and summarize answer.\\r\\n    Args:\\r\\n        connection_details (dict): The connection details.\\r\\n        llm (str): The language model to use.\\r\\n        db (str): The database to query.\\r\\n        metadata (dict): The metadata.\\r\\n    Returns:\\r\\n        df (DataFrame): The dataframe.\\r\\n        ans (str): The summarize answer.                        \\r\\n    '''                              \\r\\n    def __init__(self, connection_details:dict, llm, db, sql_query, metadata) -> None:   \\r\\n        self.connection_string = connection_details.get('connection_string', None)        \\r\\n        self.db_user_name = connection_details.get('db_user_name', None)\\r\\n        self.db_password = connection_details.get('db_password', None)\\r\\n        self.db_host = connection_details.get('db_host', None)\\r\\n        self.db_port = connection_details.get('db_port', None)\\r\\n        self.database = connection_details.get('database', None)  \\r\\n        self.llm = llm\\r\\n        self.db = db\\r\\n        self.metadata = metadata  \\r\\n        self.sql_query = sql_query\\r\\n\\r\\n    def execute_query(self, config) -> str:\\r\\n        try:\\r\\n            '''\\r\\n            Execute query, chain and returns dataframe and summarize answer.\\r\\n            Args:\\r\\n                config (QueryExecutorConfig): The config.\\r\\n            Returns:\\r\\n                df (DataFrame): The dataframe.\\r\\n                ans (str): The summarize answer.\\r\\n            '''           \\r\\n            df = self.get_df(host=self.db_host, user=self.db_user_name, password=self.db_password, database=self.database, sql_query=self.sql_query)       \\r\\n            df.to_dict('records') \\r\\n            ans = self.get_ans_summary(self.llm, self.db, config.user_query)               \\r\\n            return df, ans\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in QueryExecutor as: ',e)\\r\\n            return e\\r\\n                \\r\\n    def get_df(self, host:str='', user:str='', password:str='', database:str='', sql_query:str=''):\\r\\n        '''\\r\\n        Get dataframe.\\r\\n        Args:\\r\\n            host (str): The host.\\r\\n            user (str): The user.\\r\\n            password (str): The password.\\r\\n            database (str): The database.\\r\\n            sql_query (str): The SQL query.\\r\\n        Returns:\\r\\n            data_df (DataFrame): The dataframe.\\r\\n        '''\\r\\n        try:\\r\\n            mydb = mysql.connector.connect(host=host,user=user,password=password,database=database)        \\r\\n            mycursor = mydb.cursor(dictionary=True)\\r\\n            mycursor.execute(sql_query)\\r\\n            data = mycursor.fetchall()\\r\\n            column_names = [desc[0] for desc in mycursor.description]\\r\\n            data_df = pd.DataFrame(data, columns=column_names)\\r\\n            return data_df    \\r\\n        except Exception as e:\\r\\n            logger.info('Exception in QueryExecutor as: ', e)\\r\\n            return e        \\r\\n\\r\\n    def get_ans_summary(self, llm, db, user_query:str=''):\\r\\n        '''\\r\\n        Get summarize answer.\\r\\n        Args:\\r\\n            llm (str): The language model.\\r\\n            db (str): The database.\\r\\n            user_query (str): The user query.\\r\\n        Returns:\\r\\n            response (str): The summarize answer.\\r\\n        '''   \\r\\n        try:    \\r\\n            db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=False, metadata=self.metadata)\\r\\n            response = db_chain.run(user_query)      \\r\\n            return response\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in QueryExecutor as: ', e)\\r\\n            return e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""formats"":{},""alias"":""Query Executor"",""attributes"":{}}","SemanticSearchCore-18"
"SemanticSearch","Core","{""id"":20,""name"":""FlaskAPP"",""category"":""FlaskAPP"",""parentCategory"":"""",""classname"":""FlaskAPP"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[""from flask import Flask"",""from flask import request"",""from flask import jsonify""],""requirements"":[""flask""],""script"":""def FlaskAPP(port_param=5000,script_param=''):\\n    app.run(debug=False, host='0.0.0.0', port = port_param)\\n\\n\\n""},""formats"":{""port"":""integer"",""script"":""textarea""},""alias"":""FlaskAPP"",""attributes"":{""port"":"""",""script"":""""}}","SemanticSearchCore-20"
"SemanticSearch","Core","{""id"":22,""name"":""Evaluate"",""category"":""LLM_Evaluation"",""parentCategory"":""1"",""classname"":""LLM_Evaluation"",""inputEndpoints"":[],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""""},""attributes"":{},""formats"":{},""alias"":""Evaluate""}","SemanticSearchCore-22"
"SemanticSearch","Core","{""id"":23,""name"":""RAGAS Evaluate"",""category"":""RAGAS_Evaluate"",""parentCategory"":""22"",""classname"":""RAGAS_Evaluate"",""inputEndpoints"":[""in""],""outputEndpoints"":[],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""def RAGAS_Evaluate(dataset,first_model,second_model):\\n    df=pd.read_csv(dataset)\\n    \\n        \\n    from langchain.embeddings import HuggingFaceEmbeddings\\n    \\n    \\n    \\n    import streamlit as st\\n    \\n    \\n    \\n    import requests\\n    \\n    \\n    \\n    import math\\n    \\n    \\n    \\n    import os\\n    \\n    import pandas as pd\\n    \\n    import boto3\\n    \\n    import requests\\n    \\n    \\n    \\n    from datasets import load_dataset\\n    \\n    import boto3\\n    \\n    import json\\n    from langchain_community.chat_models import BedrockChat\\n    \\n    from langchain_community.embeddings import BedrockEmbeddings\\n    \\n    from langchain_community.chat_models import AzureChatOpenAI\\n    \\n    from langchain_openai import AzureOpenAIEmbeddings\\n    \\n    from langchain.llms.bedrock import Bedrock\\n    \\n    from ragas import evaluate\\n    \\n    import nest_asyncio  # CHECK NOTES\\n    \\n    \\n    \\n    # NOTES: Only used when running on a jupyter notebook, otherwise comment or remove this function.\\n    \\n    \\n    \\n    from ragas import evaluate\\n    \\n    from ragas.metrics import (\\n    \\n        context_precision,\\n    \\n        faithfulness,\\n    \\n        context_recall,answer_relevancy\\n    \\n        )\\n    \\n    from datasets import Dataset, DatasetDict\\n    \\n    import pandas as pd\\n    \\n    nest_asyncio.apply()\\n    \\n    \\n    \\n    def rag_eval(df,first_model,second_model):\\n        \\n       \\n    \\n        access_id='AKIAWSEIAMU6H5SKF2E2'\\n    \\n        access_key='7jHVztJmePTs5Em33uEsxrlNg7vUmgeMSZyrmyUD'\\n    \\n        service_name='bedrock-runtime'\\n    \\n        region_name='us-east-1'\\n    \\n        endpoint_url='https://bedrock-runtime.us-east-1.amazonaws.com'\\n        \\n\\n        metrics = [\\n    \\n        faithfulness,\\n    \\n        context_recall,\\n    \\n        context_precision,answer_relevancy\\n    \\n        ]\\n    \\n        \\n    \\n        df['contexts'] = df['contexts'].apply(lambda x: eval(x) if isinstance(x, str) else [])\\n    \\n        ds_dict = {'eval' : Dataset.from_pandas(df)}\\n    \\n        ds = DatasetDict(ds_dict) \\n        \\n        bedrock = boto3.client(service_name=service_name,aws_access_key_id=access_id,aws_secret_access_key=access_key,region_name=region_name,\\n    \\n                                 endpoint_url=endpoint_url)\\n        azure_embeddings = AzureOpenAIEmbeddings(api_key='85b968a4b5c84d849c99661788c2c1ed',\\n                    openai_api_version='2023-03-15-preview',\\n                    azure_endpoint='https://azureft.openai.azure.com/',\\n                    model='gpt-35-turbo',\\n                )\\n        \\n        if first_model=='anthropic.claude-v2' or 'anthropic.claude-v2:1' :\\n            \\n    \\n            llm_1 = BedrockChat(model_id=first_model, client=bedrock, model_kwargs={'max_tokens_to_sample':3000})    \\n            bedrock_embeddings = BedrockEmbeddings(client=bedrock,region_name='us-east-1',)\\n            result_1= evaluate(ds['eval'],\\n    \\n                            metrics=metrics,\\n    \\n                            llm=llm_1,\\n    \\n                            embeddings=bedrock_embeddings,raise_exceptions=False,)\\n        elif first_model=='gpt-35-turbo':\\n            llm_1 = AzureChatOpenAI(api_key='85b968a4b5c84d849c99661788c2c1ed',\\n                        openai_api_version='2023-03-15-preview',\\n                        azure_endpoint='https://azureft.openai.azure.com/',\\n                        azure_deployment='gtp35turbo',\\n                        model=first_model,\\n                        validate_base_url=False,\\n                    )\\n            result_1= evaluate(ds['eval'],\\n    \\n                            metrics=metrics,\\n    \\n                            llm=llm_2,\\n    \\n                            embeddings=azure_embeddings,raise_exceptions=False,)\\n        if second_model=='anthropic.claude-v2' or 'anthropic.claude-v2:1' :\\n            llm_2 = BedrockChat(model_id=second_model, client=bedrock, model_kwargs={'max_tokens_to_sample':3000})\\n            bedrock_embeddings = BedrockEmbeddings(client=bedrock,region_name='us-east-1',)\\n            result_2= evaluate(ds['eval'],\\n    \\n                            metrics=metrics,\\n    \\n                            llm=llm_2,\\n    \\n                            embeddings=bedrock_embeddings,raise_exceptions=False,)\\n               \\n        elif second_model=='gpt-35-turbo':\\n            llm_2 = AzureChatOpenAI(api_key='85b968a4b5c84d849c99661788c2c1ed',\\n                        openai_api_version='2023-03-15-preview',\\n                        azure_endpoint='https://azureft.openai.azure.com/',\\n                        azure_deployment='gtp35turbo',\\n                        model=second_model,\\n                        validate_base_url=False,\\n                    )\\n            result_2= evaluate(ds['eval'],\\n    \\n                            metrics=metrics,\\n    \\n                            llm=llm_2,\\n    \\n                            embeddings=azure_embeddings,raise_exceptions=False,)\\n        \\n \\n    \\n        \\n    \\n        \\n        \\n        return dict(result_1),dict(result_2)\\n    \\n    print(rag_eval(df,first_model,second_model))\\n   \\n\\n\\n\\n\\n\\n\\n""},""formats"":{""first_model"":""string"",""second_model"":""string""},""alias"":""RAGAS Evaluate"",""attributes"":{""first_model"":"""",""second_model"":""""}}","SemanticSearchCore-23"
"SemanticSearch","Core","{""id"":21,""name"":""Evaluation Datauploader"",""category"":""Extractor"",""parentCategory"":""22"",""classname"":""DatasetExtractor"",""inputEndpoints"":[],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n""},""formats"":{""dataset"":""dropdown""},""alias"":""Evaluation Datauploader"",""attributes"":{""dataset"":""""}}","SemanticSearchCore-21"
"SemanticSearch","Core","{""id"":6,""name"":""Vector Store"",""category"":""VectorStore"",""parentCategory"":""2"",""classname"":""VectorStore"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""import os\\r\\nfrom typing import List,Optional,Any\\r\\nimport logging as logger\\r\\nclass VectorConfig:\\r\\n    def __init__(self,config_json:Optional[Any]={}) :\\r\\n        self.DB_Type : Optional[str] = config_json.get('DB_Type',\\""Faiss\\"")\\r\\n        self.elastic_search_url : str = config_json.get('elastic_search_url',os.environ.get('app_elastic_search_url'))\\r\\n        self.index_name : Optional[str] = config_json.get('index_name',\\""Vector_Store\\"")\\r\\n        self.qdrant_url : Optional[str] = config_json.get('qdrant_url',os.environ.get('app_qdrant_url'))\\r\\n        self.force_recreate : Optional[bool] = config_json.get('force_recreate',False)\\r\\nclass VectorStore:      \\r\\n    def __init__(self,embedding,docs) -> None:\\r\\n        self.embedding_function = embedding\\r\\n        self.docs = docs\\r\\n    def SaveEmbedding(self,config:dict):\\r\\n        \\""\\""\\""Store Embeddings In vector db. We support FaissDB,chromadb,ElasticsearchDB, it's not required if you prepared your own vector db.\\r\\n        Args:\\r\\n            DB_Type (str): The type of vector store to be used available options are Elasticsearch,Faiss,Chroma .\\r\\n            embedding_function (Callable): the embedding function to use.\\r\\n            docs(list[Document]): List of splitted text that has to be stored in vector DB.        \\r\\n            elastic_search_url(str): Elastic Search hosted URL \\r\\n            index_name(str): The Index Name Default is \\""Vector_Store\\"".\\r\\n            qdrant_url(str): qdrant URL \\r\\n            force_recreate(bool): Create new index, If it exists it will delete and create new index while it is True, If False, It will append the existing Index and if index is not present will create new Index.\\r\\n                                    Default is False.\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            # logger.info(config.persist_directory)\\r\\n            if config.DB_Type.strip().lower() == 'faiss':\\r\\n                return self.FaissDB(embedding_function=self.embedding_function,docs=self.docs,persist_directory=\\""/\\""+config.index_name)\\r\\n            elif config.DB_Type.strip().lower() == 'chroma':\\r\\n                return self.ChromaDB(embedding_function=self.embedding_function,docs=self.docs,persist_directory=\\""/\\""+config.index_name)\\r\\n            elif config.DB_Type.strip().lower() == 'elasticsearch':\\r\\n                return self.ElasticsearchDB(embedding=self.embedding_function, docs=self.docs, es_url=config.elastic_search_url, index_name=config.index_name)\\r\\n            elif config.DB_Type.strip().lower() == 'qdrant':\\r\\n                return self.Qdrant(embeddings=self.embedding_function,texts=self.docs,url=config.qdrant_url,collection_name=config.index_name,force_recreate=config.force_recreate)\\r\\n            else:\\r\\n                logger.info('Unsupported VectorStore type)')\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in SaveEmbedding',e)\\r\\n            raise e\\r\\n        \\r\\n    def FaissDB(self,embedding_function:Any=None,docs:Any=None,persist_directory:str=''):\\r\\n        try:\\r\\n            from langchain.vectorstores import FAISS\\r\\n            faiss_db = FAISS.from_documents(docs, embedding_function)                    \\r\\n            if os.path.exists(persist_directory):\\r\\n                local_db = FAISS.load_local(persist_directory, embedding_function)\\r\\n                local_db.merge_from(faiss_db)\\r\\n                local_db.save_local(persist_directory)\\r\\n            else:\\r\\n                faiss_db.save_local(persist_directory)\\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in FaissDB', e)\\r\\n            raise e\\r\\n        \\r\\n    def ChromaDB(self,embedding_function:Any=None,docs:Any=None, persist_directory:str=None):\\r\\n        try:\\r\\n            from langchain.vectorstores import Chroma\\r\\n            db = Chroma.from_documents(documents=docs, embedding=embedding_function, persist_directory=persist_directory)\\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in ChromaDB', e)\\r\\n            raise e\\r\\n        \\r\\n    def ElasticsearchDB(self, embedding:Any=None, docs:Any=None, es_url:str='', index_name:str=\\""Vector_Store\\""):   \\r\\n        try:     \\r\\n            from langchain.vectorstores import ElasticVectorSearch       \\r\\n            db = ElasticVectorSearch(elasticsearch_url=es_url, index_name=index_name, embedding=embedding)        \\r\\n            db.add_documents(docs)        \\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in ElasticsearchDB', e)\\r\\n            raise e\\r\\n        \\r\\n    def Qdrant(self,embeddings:Any=None,texts:str='',url:str='',collection_name:str='',force_recreate:bool=False):\\r\\n        try:\\r\\n            from langchain.vectorstores import Qdrant\\r\\n            qdrant = Qdrant.from_documents(\\r\\n            texts,\\r\\n            embeddings,\\r\\n            url= url,\\r\\n            collection_name=collection_name,\\r\\n            force_recreate=force_recreate,\\r\\n            )\\r\\n            return \\""Completed\\""\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Qdrant', e)\\r\\n            raise e \\n""},""formats"":{},""alias"":""Vector Store"",""attributes"":{}}","SemanticSearchCore-6"
"SemanticSearch","Core","{""id"":8,""name"":""Embedding Functions"",""category"":""EmbeddingFunctions"",""parentCategory"":""7"",""classname"":""EmbeddingFunction"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""codeGeneration"":{""imports"":[],""requirements"":[],""script"":""from typing import Optional,Any\\r\\nimport os\\r\\nimport logging as logger\\r\\nclass EmbeddingConfig:\\r\\n    def __init__(self,config_json:Optional[Any] = {}):\\r\\n        self.embedding_type:Optional[str]=config_json.get('embedding_type',\\""HuggingFace\\"")\\r\\n        self.model_name:Optional[str]=config_json.get('model_name',\\""sentence-transformers/all-MiniLM-L6-v2\\"")\\r\\n        self.azure_openai_api_key:Optional[str]=config_json.get('azure_openai_api_key',os.environ.get('app_openai_api_key'))\\r\\n        self.azure_openai_endpoint:Optional[str]=config_json.get('azure_openai_endpoint','https://azureft.openai.azure.com/')\\r\\n        self.azure_api_version:Optional[str]=config_json.get('azure_api_version','2023-03-15-preview')\\r\\n        self.azure_deployment:Optional[str]=config_json.get('azure_deployment','openaiada2')\\r\\n        self.openai_model:Optional[str]=config_json.get('openai_model','text-embedding-ada-002')\\r\\n        self.openai_api_base:Optional[str]=config_json.get('openai_api_base','https://azureft.openai.azure.com/')\\r\\n        self.openai_api_type:Optional[str]=config_json.get('openai_api_type','azure')\\r\\n        self.bedrock_access_id:Optional[str]=config_json.get('bedrock_access_id',os.environ.get('app_aws_secret_access_id'))\\r\\n        self.bedrock_access_key:Optional[str]=config_json.get('bedrock_access_key',os.environ.get('app_aws_secret_access_key'))\\r\\n        self.bedrock_region_name:Optional[str]=config_json.get('bedrock_region_name','us-east-1')\\r\\n        self.bedrock_endpoint_url:Optional[str]=config_json.get('bedrock_endpoint_url','https://bedrock-runtime.us-east-1.amazonaws.com')\\r\\n        self.bedrock_model_id:Optional[str]=config_json.get('bedrock_model_id','amazon.titan-embed-g1-text-02')\\r\\n        self.elastic_search_model_id:Optional[str]=config_json.get('elastic_search_model_id',None)\\r\\n        self.elastic_search_cloud_id:Optional[str]=config_json.get('elastic_search_cloud_id',None)\\r\\n        self.elastic_search_user_id:Optional[str]=config_json.get('elastic_search_user_id',None)\\r\\n        self.elastic_search_password:Optional[str]=config_json.get('elastic_search_password',None)\\r\\nclass EmbeddingModels:\\r\\n    def __init__(self) -> None:\\r\\n        pass\\r\\n    def get_embedding_function(self,config):\\r\\n        \\""\\""\\""Embedding Model Defination for vector db. We support FaissDB,chromadb,ElasticsearchDB, it's not required if you prepared your own vector db.\\r\\n        Args:\\r\\n            embedding_type(str): Type of Embedding to be used, We support HuggingFace, AzureOpenAI, Bedrock, ElasticSearch, Default is HuggingFace.\\r\\n            model_name(str): Model Name to generate the embedding according to your embedding type, Default is \\""sentence-transformers/all-MiniLM-L6-v2\\"".\\r\\n            AzureOpenAI_Params(str):azure_openai_api_key,\\r\\n                                    azure_openai_endpoint,\\r\\n                                    azure_api_version,\\r\\n                                    azure_deployment,\\r\\n                                    model_name= Open AI Embedding Model Name ,\\r\\n                                    openai_api_base,\\r\\n                                    openai_api_type\\r\\n            Bedrock_Params(str):bedrock_access_id,\\r\\n                                bedrock_access_key,\\r\\n                                bedrock_region_name,\\r\\n                                bedrock_endpoint_url,\\r\\n                                bedrock_model_id\\r\\n            ElasticSearch_Params(str):  elastic_search_model_id,\\r\\n                                        elastic_search_cloud_id,\\r\\n                                        elastic_search_user,\\r\\n                                        elastic_search_password\\r\\n        \\""\\""\\""\\r\\n        try:\\r\\n            logger.info(config.azure_openai_api_key)\\r\\n            if config.embedding_type.strip().lower() == \\""huggingface\\"":\\r\\n                return self.HuggingFace(model_name=config.model_name)\\r\\n            elif config.embedding_type.strip().lower() == \\""azureopenai\\"":\\r\\n                return self.AzureOpenAI(azure_openai_api_key_param=config.azure_openai_api_key,azure_api_version_param=config.azure_api_version,azure_deployment_param=config.azure_deployment,model_param=config.model_name,openai_api_base_param=config.openai_api_base,openai_api_type_param=config.openai_api_type)\\r\\n            elif config.embedding_type.strip().lower() == \\""bedrock\\"":\\r\\n                return self.Bedrock(access_id_param=config.bedrock_access_id,access_key_param=config.bedrock_access_key,region_name_param=config.bedrock_region_name,endpoint_url_param=config.bedrock_endpoint_url,model_id_param=config.bedrock_model_id)\\r\\n            elif config.embedding_type.strip().lower() == \\""elasticsearch\\"":\\r\\n                return self.ElasticSearch(elastic_model_id_param=config.elastic_search_model_id,es_cloud_id_param=config.elastic_search_cloud_id,es_user_param=config.elastic_search_user,es_password_param=config.elastic_search_password)\\r\\n            else:\\r\\n                raise Exception(\\""Unknown embedding type, we support only ElasticSearch, Bedrock, AzureOpenAI, and HuggingFace\\"")\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in get_embedding_function',e)\\r\\n            raise e\\r\\n        \\r\\n    def HuggingFace(self,model_name:str=''):\\r\\n        try:\\r\\n            from langchain.embeddings import HuggingFaceEmbeddings\\r\\n            embeddings = HuggingFaceEmbeddings(\\r\\n                    model_name=model_name\\r\\n            )\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in HuggingFace', e)\\r\\n            raise e\\r\\n        \\r\\n    def AzureOpenAI(self,azure_openai_api_key_param:str='',azure_api_version_param:str='',azure_deployment_param:str='',model_param:str='',openai_api_base_param:str='',openai_api_type_param:str=''):\\r\\n        try:\\r\\n            from langchain.embeddings import AzureOpenAIEmbeddings\\r\\n            embeddings = AzureOpenAIEmbeddings(api_key=azure_openai_api_key_param,api_version=azure_api_version_param,azure_deployment=azure_deployment_param,model=model_param,openai_api_type=openai_api_type_param,base_url=openai_api_base_param)\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in AzureOpenAI', e)\\r\\n            raise e\\r\\n        \\r\\n    def Bedrock(self,access_id_param:str='',access_key_param:str='',region_name_param:str='',endpoint_url_param:str='',model_id_param:str=''):\\r\\n        try:\\r\\n            import boto3\\r\\n            from langchain.embeddings import BedrockEmbeddings\\r\\n            bedrock = boto3.client('bedrock-runtime',aws_access_key_id=access_id_param,aws_secret_access_key=access_key_param,region_name=region_name_param,endpoint_url=endpoint_url_param)\\r\\n            embeddings = BedrockEmbeddings(\\r\\n                model_id=model_id_param, \\r\\n                client=bedrock\\r\\n            )\\r\\n            embeddings\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Bedrock', e)\\r\\n            raise e\\r\\n        \\r\\n    def Elastic_search(self,model_id_param:str='',es_cloud_id_param:str='',es_user_param:str='',es_password_param:str=''):\\r\\n        try:\\r\\n            from langchain.embeddings.elasticsearch import ElasticsearchEmbeddings\\r\\n            embeddings = ElasticsearchEmbeddings.from_credentials(\\r\\n                model_id=model_id_param,\\r\\n                es_cloud_id=es_cloud_id_param,\\r\\n                es_user=es_user_param,\\r\\n                es_password=es_password_param,\\r\\n            )\\r\\n            return embeddings\\r\\n        except Exception as e:\\r\\n            logger.info('Exception in Elastic_search', e)\\r\\n            raise e\\n""},""formats"":{},""alias"":""Embedding Functions"",""attributes"":{}}","SemanticSearchCore-8"
"Chatbot","Core","{""formats"":{""port"":""text"",""script"":""textarea""},""classname"":""FlaskApp"",""name"":""FlaskApp"",""parentCategory"":""LEOCMNND34003"",""alias"":""FlaskApp"",""id"":""LEOFLSKP57879"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def FlaskApp(port_param=5000,script_param=''):\\n    app.run(debug=False, host='0.0.0.0', port = port_param)\\n""},""category"":""FlaskApp"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""port"":"""",""script"":""""}}","LEOFLSKP57879"
"Chatbot","Core","{""formats"":{},""classname"":""Tools"",""name"":""Tools"",""parentCategory"":"""",""alias"":""Tools"",""attributes"":{},""id"":""LEOTLSBM21587"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOTLSBM21587"
"Chatbot","Core","{""formats"":{},""classname"":""Common Nodes"",""name"":""Common Nodes"",""parentCategory"":"""",""alias"":""Common Nodes"",""attributes"":{},""id"":""LEOCMNND34003"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Common Nodes"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOCMNND34003"
"Chatbot","Core","{""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""classname"":""PythonScriptConfig"",""name"":""Python Script"",""parentCategory"":""LEOCMNND34003"",""alias"":""Python Script"",""attributes"":{""FunctionName"":""PythonScript"",""requirements"":"""",""params"":"""",""script"":""def PythonScript():     return \\""Done\\""""},""id"":""LEOPYTHN77386"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""BaseConfig"",""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""]}","LEOPYTHN77386"
"Chatbot","Core","{""formats"":{""script"":""textarea""},""classname"":""PythonClass"",""name"":""PythonClass"",""parentCategory"":""LEOCMNND34003"",""alias"":""PythonClass"",""attributes"":{""script"":""""},""id"":""LEOPYTHN23924"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""PythonClass"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""]}","LEOPYTHN23924"
"Chatbot","Core","{""formats"":{""id"":""text"",""query"":""text""},""classname"":""youtube_scrapper_with_id"",""name"":""youtube_scrapper_with_id"",""parentCategory"":""LEOSRCHY97728"",""alias"":""youtube_scrapper_with_id"",""id"":""LEOYTB_S92340"",""codeGeneration"":{""requirements"":[""youtube-transcript-api"",""langchain-openai""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""from langchain_community.document_loaders import YoutubeLoader"",""from langchain.text_splitter import CharacterTextSplitter"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""llm = AzureChatOpenAI(    \\r\\n        deployment_name='gpt-4',\\r\\n        model_name='gpt-4',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version = '2023-03-15-preview',\\r\\n        azure_endpoint='https://azureft.openai.azure.com/',\\r\\n        openai_api_type='azure',        \\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n\\r\\nclass YoutubeScrapperWithIdTool(BaseModel):\\r\\n    id: str = Field(..., description='ID of the youtube video to scrape.')\\r\\n    query: str = Field(..., description='The query to ask the model.')\\r\\n\\r\\n@tool('youtube_scrapper_with_id', args_schema=YoutubeScrapperWithIdTool, return_direct=True)\\r\\ndef youtube_scrapper_with_id(id: str, query: str):\\r\\n    \\""\\""\\""\\r\\n    This tool will be used to scrape transcripts from all the videos related to the given id and then summarize the content.\\r\\n    parameters:\\r\\n        - id: The id of the video to be scraped.\\r\\n        - query: The query to ask the model.\\r\\n    returns:\\r\\n        - A summary based on the topic.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        splitter = CharacterTextSplitter(\\r\\n                    chunk_size=2000, \\r\\n                    chunk_overlap=50,\\r\\n                    separator=\\"" \\""\\r\\n                    )\\r\\n        yt_loader = YoutubeLoader(id)\\r\\n        yt_data = yt_loader.load()\\r\\n        yt_data_split = splitter.split_documents(yt_data)\\r\\n        chain = load_qa_chain(llm=llm, chain_type='stuff')\\r\\n        response = chain.invoke(input={'input_documents':yt_data_split, 'question':query})\\r\\n        return response['output_text']\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""@@@@@@@@@@@ Exception in youtube scrapper with id method: {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""id"":""None"",""query"":""None""}}","LEOYTB_S92340"
"Chatbot","Core","{""formats"":{""git_url"":""text"",""query"":""text""},""classname"":""git_scrapper"",""name"":""GIT Scrapper"",""parentCategory"":""LEOSRCHY97728"",""alias"":""GIT Scrapper"",""id"":""LEOGT_SC22937"",""codeGeneration"":{""requirements"":[""GitPython"",""langchain_openai""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""from langchain_community.document_loaders import GitLoader"",""from langchain.text_splitter import CharacterTextSplitter"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""llm = AzureChatOpenAI(    \\r\\n        deployment_name='gpt-4',\\r\\n        model_name='gpt-4',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version = '2023-03-15-preview',\\r\\n        azure_endpoint='https://azureft.openai.azure.com/',\\r\\n        openai_api_type='azure',        \\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n\\r\\nclass GitHubScrapperTool(BaseModel):\\r\\n    git_url: str = Field(..., description='Local Path of the git repo.')\\r\\n    query: str = Field(..., description='The query to ask the model.')\\r\\n\\r\\n@tool('git_scrapper', args_schema=GitHubScrapperTool, return_direct=True)\\r\\ndef git_scrapper(git_url: str, query: str):\\r\\n    \\""\\""\\""Summarizes the content of a GitHub repository.\\r\\n    Parameters:\\r\\n    - url (str): The URL of the GitHub repository to summarize.\\r\\n    Returns:\\r\\n    - A summary of the repository content.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        splitter = CharacterTextSplitter(\\r\\n                    chunk_size=2000, \\r\\n                    chunk_overlap=50,\\r\\n                    separator=' '\\r\\n                    )\\r\\n        path = git_url.split(\\""/\\"")[-1]\\r\\n        repo_path = f'./repos/{path}'\\r\\n        if os.path.exists(repo_path):\\r\\n            os.remove('./repos')\\r\\n            loader = GitLoader(clone_url= git_url, repo_path= repo_path, branch='main', \\r\\n                           file_filter = lambda file_path: file_path.endswith('.md')\\r\\n                           )\\r\\n            git_data = loader.load()\\r\\n            git_data_split = splitter.split_documents(git_data)\\r\\n            chain = load_qa_chain(llm=llm, chain_type=\\""stuff\\"")\\r\\n            response = chain.invoke(input={'input_documents':git_data_split, 'question':query})\\r\\n            return response['output_text']\\r\\n        else:\\r\\n            loader = GitLoader(clone_url= git_url, repo_path= repo_path, branch='main', \\r\\n                           file_filter = lambda file_path: file_path.endswith('.md')\\r\\n                           )\\r\\n            git_data = loader.load()\\r\\n            git_data_split = splitter.split_documents(git_data)\\r\\n            chain = load_qa_chain(llm=llm, chain_type='stuff')\\r\\n            response = chain.invoke(input={'input_documents':git_data_split, 'question':query})\\r\\n            return response['output_text']\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""@@@@@@@@@@@ Exception in userAssignment method: {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""git_url"":""None"",""query"":""None""}}","LEOGT_SC22937"
"Chatbot","Core","{""formats"":{""url"":""text"",""topic"":""text"",""query"":""text""},""classname"":""youtube_scrapper"",""name"":""YouTube Scrapper"",""parentCategory"":""LEOSRCHY97728"",""alias"":""YouTube Scrapper"",""id"":""LEOYTB_S41993"",""codeGeneration"":{""requirements"":[""youtube-transcript-api"",""langchain-openai""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""from langchain_community.document_loaders import YoutubeLoader"",""from langchain.text_splitter import CharacterTextSplitter"",""import requests"",""from langchain.tools import tool"",""from pydantic import BaseModel, Field""],""script"":""llm = AzureChatOpenAI(    \\r\\n        deployment_name='gpt-4',\\r\\n        model_name='gpt-4',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version = '2023-03-15-preview',\\r\\n        azure_endpoint='https://azureft.openai.azure.com/',\\r\\n        openai_api_type='azure',        \\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n        )\\r\\n\\r\\nclass YoutubeScrapperTool(BaseModel):\\r\\n    topic: str = Field(..., description='The Topic of the websites.')\\r\\n    query: str = Field(..., description='The query to ask the model')\\r\\n\\r\\n@tool('youtube_scrapper', args_schema=YoutubeScrapperTool, return_direct=True)\\r\\ndef youtube_scrapper(topic: str, query: str):\\r\\n    \\""\\""\\""\\r\\n    This tool will be used to scrape transcripts from all the videos related to the topic and then summarize the content.\\r\\n    parameters:\\r\\n        - Topic: The topic of the videos to be scraped.\\r\\n        - query: The query to ask the model.\\r\\n    returns:\\r\\n        - A summary based on the topic.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        url = chatbot_params[\\""youtube_scrapper_url\\""]\\r\\n        youtube_api_key = os.environ.get('youtube_api_key')\\r\\n        params = {\\r\\n            'part': 'snippet',\\r\\n            'q': topic,\\r\\n            'maxResults': 3,\\r\\n            'type': 'video',\\r\\n            'key': youtube_api_key\\r\\n        }\\r\\n        splitter = CharacterTextSplitter(\\r\\n                    chunk_size=2000, \\r\\n                    chunk_overlap=50,\\r\\n                    separator=' '\\r\\n                    )\\r\\n        response = requests.get(url, params=params, verify=False)\\r\\n        if response.status_code == 200:\\r\\n            video_urls = [f\\""https://www.youtube.com/watch?v={item['id']['videoId']}\\"" for item in response.json()['items']]\\r\\n            yt_data = []\\r\\n            for url in video_urls:\\r\\n                yt_loader = YoutubeLoader.from_youtube_url(youtube_url=url)\\r\\n                yt_data.append(yt_loader.load())\\r\\n            flat_list = [document for sublist in yt_data for document in sublist]\\r\\n            yt_data_split = splitter.split_documents(flat_list)\\r\\n            chain = load_qa_chain(llm=llm, chain_type='stuff')\\r\\n            response = chain.invoke(input={'input_documents':yt_data_split, 'question':query})\\r\\n            return response['output_text']\\r\\n        else:\\r\\n            print\\r\\n            \\r\\n    except Exception as e:\\r\\n        logger.info(f\\""@@@@@@@@@@@ Exception in youtube scrapper method: {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""url"":""https://www.googleapis.com/youtube/v3/search"",""topic"":""None"",""query"":""None""}}","LEOYTB_S41993"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""create_ticket"",""name"":""Create Ticket"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Create Ticket"",""id"":""LEOCRT_T78045"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests,json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""class CreateTicket(BaseModel):\\r\\n    short_description: str =Field(None,description=\\""Short description provided by the user\\"")\\r\\n\\r\\n@tool(\\""create_ticket\\"",args_schema=CreateTicket, return_direct=True)\\r\\ndef create_ticket(short_description: str):\\r\\n    '''\\r\\n    use to create ticket with given short description\\r\\n    Parameters: Short description provided by the user\\r\\n    Returns: response includes whether the ticket was successfully created or not\\r\\n    '''\\r\\n    try:\\r\\n        logger.info(f\\""createTicket method called with short description: {short_description}\\"") \\r\\n        url =  chatbot_params[\\""create_ticket_url\\""]\\r\\n        json_data = {\\r\\n                        \\""environment\\"": [\\r\\n                            {\\r\\n                                \\""key\\"": \\""incidentPayload\\"",\\r\\n                                \\""value\\"": f\\""{{\\\\\\""short_description\\\\\\"":\\\\\\""{short_description}\\\\\\"",\\\\\\""state\\\\\\"":null,\\\\\\""pritory\\\\\\"":null}}\\""\\r\\n                            }\\r\\n                        ]\\r\\n                    }\\r\\n        response = requests.post(url=url,json=json_data,verify=False)\\r\\n        logger.info(f\\""response status  {response.status_code}\\"")\\r\\n        if response.status_code == 200:\\r\\n            response = {'chat_system_response':\\""Ticket created succesfully\\"",\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[],\\r\\n                        'navigate': {\\""TicketList\\"": 'aip/tickets/alerts'}\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:        \\r\\n            response = {'chat_system_response':\\""Ticket not created, Please contact admin\\"",\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[],\\r\\n                        'navigate': {\\""TicketList\\"": 'aip/tickets/alerts'}\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in createTicket method: {e}\\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://victlpth5-04:8095/api/event/trigger/createIncident?org=leo1311&corelid=&datasourceName=LEALCLCL12132""}}","LEOCRT_T78045"
"Chatbot","Core","{""formats"":{},""classname"":""semantic_search"",""name"":""Semantic Search"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Semantic Search"",""id"":""LEOSMNTC84402"",""codeGeneration"":{""requirements"":[""langchain==0.1.16"",""pandas"",""pydantic==1.10.10"",""openai"",""boto3"",""requests"",""sentence-transformers"",""faiss-cpu""],""imports"":[""import openai"",""import requests"",""import boto3"",""import json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from langchain.chat_models import AzureChatOpenAI"",""from langchain.embeddings import HuggingFaceEmbeddings"",""from langchain.vectorstores import FAISS"",""from langchain_core.prompts import ChatPromptTemplate"",""from langchain_core.runnables import RunnablePassthrough"",""from langchain_core.output_parsers import StrOutputParser""],""script"":""flag= False\\r\\nreferences = list()\\r\\nclass Semantic_search(BaseModel):\\r\\n    query: str =Field(None,description=\\""Query passed by the user\\"")\\r\\n\\r\\n@tool(\\""query_about_policy\\"",args_schema=Semantic_search, return_direct=True)\\r\\ndef query_about_policy(query: str)->str:\\r\\n    \\""\\""\\""\\r\\n    this tool is used to answer all the queries related to policies of adoption, leaves and employee related queries. Use this to answer any resolution related and process invoice types of queries as well.\\r\\n\\r\\n    Parameters:\\r\\n        query (str): A natural language question or statement from the user regarding employee policies, adoption procedures, or leave-related matters.\\r\\n\\r\\n    Returns:\\r\\n        str: A contextual response addressing the user's query, fetched from the appropriate Knowledge base.\\r\\n    \\""\\""\\""\\r\\n        \\r\\n    global flag   \\r\\n    flag = True   \\r\\n    model_type=os.environ.get(\\""model_type\\"")\\r\\n    retriever = getconext(query)\\r\\n    context = retriever.get_relevant_documents(query)\\r\\n    lst2 = context\\r\\n    response_list= list()\\r\\n    if len(lst2)>0:\\r\\n        for source in lst2:\\r\\n            temp_dict = dict()   \\r\\n            temp_dict['dataset_id'] = source.metadata['dataset_id']\\r\\n            temp_dict['source'] = source.metadata['source']\\r\\n            temp_dict['context'] = source.page_content\\r\\n            response_list.append(temp_dict)\\r\\n    \\r\\n    global references\\r\\n    references = response_list\\r\\n    prompt_template = 'Answer the query asked by user based on the given Context below.'\\r\\n    model_url = os.environ.get(\\""app_model_url\\"")\\r\\n    max_tokens = 2000\\r\\n    top_k =  5\\r\\n    top_p =  0.95\\r\\n    typical_p = 0.95\\r\\n    temperature = 0.1\\r\\n    repetition_penalty = 1\\r\\n    generated_answer = ''\\r\\n    if model_type == 'onprem':\\r\\n        try:\\r\\n            input_prompt = f'''{prompt_template} Context: '{context}'. Question: {query}. Answer:''' \\r\\n            payload = json.dumps({\\r\\n            'inputs': input_prompt,\\r\\n            'parameters': {\\r\\n                    'max_new_tokens': max_tokens,\\r\\n                    'top_k': top_k,\\r\\n                    'top_p': top_p,\\r\\n                    'typical_p': typical_p,\\r\\n                    'temperature': temperature,\\r\\n                    'repetition_penalty': repetition_penalty\\r\\n                }\\r\\n            })\\r\\n    \\r\\n            headers = {\\r\\n                'Content-Type': 'application/json' \\r\\n            }\\r\\n            response = requests.request('POST', model_url, headers=headers, data=payload, verify=False)\\r\\n            generated_answer = json.loads(response.text)[0]['generated_text']\\r\\n            return generated_answer\\r\\n        except Exception as ex:\\r\\n            return ex\\r\\n\\r\\n    elif model_type == 'azure':\\r\\n        try:\\r\\n            api_type = 'azure'\\r\\n            api_base = os.environ.get('app_openai_api_base')\\r\\n            api_version = '2023-03-15-preview'\\r\\n            api_key = os.environ.get('app_openai_api_key')\\r\\n            stop = 'stop'\\r\\n            prompt_template = 'Answer the query asked by user based on the given Context below.'\\r\\n            engine='gtp35turbo'\\r\\n            openai.api_type = api_type\\r\\n            openai.api_base = api_base\\r\\n            openai.api_version = api_version\\r\\n            openai.api_key = api_key\\r\\n            \\r\\n            llm = AzureChatOpenAI(    \\r\\n                        deployment_name='gtp35turbo', \\r\\n                        model_name='gpt-35-turbo', \\r\\n                        openai_api_key=api_key,\\r\\n                        openai_api_version = api_version, \\r\\n                        azure_endpoint= api_base,\\r\\n                        openai_api_type=api_type,\\r\\n                        streaming=True,\\r\\n                        verbose=True,\\r\\n                        temperature=0\\r\\n                    )\\r\\n            \\r\\n            template = \\""\\""\\""Understand the question and Answer the question based only on the following context, find what is the best suitable from the context to generate the answer for the question, If you don't know the answer, just say that you don't know, don't try to make up an answer.\\r\\n                        {context}\\r\\n                        Query: {query}\\r\\n                        \\""\\""\\""               \\r\\n            prompt = ChatPromptTemplate.from_template(template)   \\r\\n            chain = (\\r\\n                {\\""context\\"": retriever, \\""query\\"": RunnablePassthrough()}\\r\\n                | prompt\\r\\n                | llm\\r\\n                | StrOutputParser()\\r\\n            )\\r\\n            \\r\\n            answer = chain.invoke(query)\\r\\n            return answer\\r\\n        except Exception as ex:\\r\\n            return ex\\r\\n\\r\\n    elif model_type == 'bedrock':\\r\\n        service_name = 'bedrock-runtime'\\r\\n        aws_access_key_id = os.environ.get('app_aws_access_key_id')\\r\\n        aws_secret_access_key = os.environ.get('app_aws_secret_access_key')\\r\\n        region_name = 'us-east-1'\\r\\n        endpoint_url = os.environ.get(\\""app_bedrock_endpoint_url\\"")\\r\\n        model_id = 'anthropic.claude-v2'\\r\\n        bedrock = boto3.client(service_name=service_name,aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,region_name=region_name,endpoint_url=endpoint_url)    \\r\\n        prompt_data = f\\""\\""\\""Human: {prompt_template} \\r\\nContext: '{context}'. \\r\\nQuestion: {query}\\r\\n        Assistant:\\""\\""\\""\\r\\n        body = json.dumps({\\r\\n            \\""prompt\\"": prompt_data,\\r\\n            \\""max_tokens_to_sample\\"":max_tokens,\\r\\n            \\""temperature\\"":temperature,\\r\\n            \\""top_p\\"":top_p\\r\\n        })\\r\\n        modelId = model_id\\r\\n        response = bedrock.invoke_model(body=body, modelId=modelId)\\r\\n        response_body = json.loads(response.get('body').read())\\r\\n        generated_answer = response_body.get('completion')\\r\\n        return generated_answer\\r\\n\\r\\n\\r\\ndef getconext(query):\\r\\n    try:\\r\\n        model_name = \\""sentence-transformers/all-MiniLM-L6-v2\\""\\r\\n        persist_directory = '/default'#f'{os.getcwd()}/Vector_Store'\\r\\n    \\r\\n        embeddings = HuggingFaceEmbeddings(\\r\\n        model_name=model_name\\r\\n        )    \\r\\n        db = FAISS.load_local(persist_directory, embeddings,allow_dangerous_deserialization=True)                    \\r\\n        retriever = db.as_retriever(search_type=\\""mmr\\"")        \\r\\n        return retriever\\r\\n    except Exception as e:\\r\\n        return e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","LEOSMNTC84402"
"Chatbot","Core","{""formats"":{},""classname"":""Ticket_Summarization"",""name"":""Ticket Summarization"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Ticket Summarization"",""id"":""LEOTCKT_32789"",""codeGeneration"":{""requirements"":[""langchain"",""pydantic==1.10.10"",""openai"",""mysql-connector-python""],""imports"":[""import mysql.connector"",""import os"",""import json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from langchain.output_parsers import ResponseSchema, StructuredOutputParser"",""from langchain.chains import LLMChain"",""from langchain.chat_models import AzureChatOpenAI"",""from langchain_core.prompts import PromptTemplate""],""script"":""class Ticket_Summarization(BaseModel):\\r\\n    query: str =Field(None,description=\\""Ticket number passed by the user\\"")\\r\\nos.environ['app_aws_access_key_id']=\\""AKIAWSEIAMU6H5SKF2E2\\""\\r\\nos.environ['app_aws_secret_access_key']= \\""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\\""\\r\\nos.environ['app_openai_api_key']= \\""85b968a4b5c84d849c99661788c2c1ed\\""\\r\\nos.environ['app_azure_endpoint']= \\""https://azureft.openai.azure.com/\\""\\r\\nos.environ['app_mysql_user']= \\""leapadm\\""\\r\\nos.environ['app_mysql_password']=\\""hPwpmwTz\\""\\r\\nos.environ[\\""OPENAI_API_KEY\\""] = '85b968a4b5c84d849c99661788c2c1ed'\\r\\nos.environ[\\""azure_endpoint\\""] = 'https://azureft.openai.azure.com/'\\r\\nos.environ[\\""OPENAI_API_BASE\\""] = 'https://azureft.openai.azure.com/'\\r\\n@tool(\\""summary_about_ticket\\"",args_schema=Ticket_Summarization, return_direct=True)\\r\\ndef summary_about_ticket(query:str)->str:\\r\\n    \\""\\""\\""\\r\\n    this tool retrieves the complete details of the ticket based on the ticket number provided, if no ticket number is provided return no ticket found, Return the exact response.\\r\\n    \\r\\n    Parameters:\\r\\n        query (str): The unique identifier or ticket number.\\r\\n    \\r\\n    Returns:\\r\\n        str: A summary or brief description of the requested ticket, or \\""No ticket found\\"" if the ticket number is not provided. \\r\\n    \\""\\""\\""\\r\\n    # mydb = mysql.connector.connect(\\r\\n    #     host=os.environ.get('app_host_name'),\\r\\n    #     user=os.environ.get('app_mysql_user'),\\r\\n    #     password=os.environ.get('app_mysql_password'),\\r\\n    #     database=os.environ.get('app_database_name')\\r\\n    # )\\r\\n    mydb = mysql.connector.connect(\\r\\n        host=\\""10.82.123.211\\"",\\r\\n        user=\\""leapadm\\"",\\r\\n        password=\\""hPwpmwTz\\"",\\r\\n        database=\\""300_leapmaster_ref_data\\""\\r\\n    )\\r\\n    mycursor = mydb.cursor(dictionary=True)\\r\\n    query = \\""SELECT * FROM {0} WHERE number in ('{1}')\\"".format(\\""leo1311_tickets\\"", query)\\r\\n    mycursor.execute(query)\\r\\n    inc_dict = mycursor.fetchall()\\r\\n\\r\\n    llm = AzureChatOpenAI(\\r\\n        deployment_name='gtp35turbo',\\r\\n        model_name='gpt-35-turbo',\\r\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\r\\n        openai_api_version='2023-03-15-preview',\\r\\n        openai_api_base=os.environ.get('azure_endpoint'),\\r\\n        openai_api_type='azure',\\r\\n        streaming=True,\\r\\n        verbose=True\\r\\n    )\\r\\n        \\r\\n\\r\\n    def get_completion(query):    \\r\\n        summary_template = \\""\\""\\""\\r\\n                    given the information {information} about a ticket from I want you to generate :\\r\\n                    1. Summary basing on the whole information given.\\r\\n                    2. A short summary on Title \\r\\n                    3. A short summary on Short description\\r\\n                    4. A short summary on Resolution and worknotes\\r\\n                    5. A short summary on Priority and urgency\\r\\n                    6. A short summary on created_by, assignmentgroup and assignedto\\r\\n                    7. A short summary on Current Status\\r\\n                    8. A short summary on SLA Timeline \\r\\n                    {format_instructions}\\r\\n                \\""\\""\\""\\r\\n        response_schemas = [\\r\\n            ResponseSchema(name=\\""Summary\\"", description=\\""Summary basing on the whole information given.\\""),\\r\\n            ResponseSchema(name=\\""Title\\"", description=\\""A short summary on the title\\""),\\r\\n            ResponseSchema(name=\\""Short description\\"", description=\\""A short summary on Short Discription\\""),\\r\\n            ResponseSchema(name=\\""ResolutionNotes\\"", description=\\""A short summary on Resolution and worknotes\\""),\\r\\n            ResponseSchema(name=\\""Priority and Urgency\\"", description=\\""A short summary on Priority and urgency\\""),\\r\\n            ResponseSchema(name=\\""Owner\\"", description=\\""A short summary on created_by, assignmentgroup and assignedto\\""),\\r\\n            ResponseSchema(name=\\""current_status\\"", description=\\""A short summary on Current Status\\""),\\r\\n            ResponseSchema(name=\\""SLA Timeline\\"", description=\\""A short summary on SLA Timeline\\""),\\r\\n        ]\\r\\n        try:\\r\\n            output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\\r\\n            format_instructions = output_parser.get_format_instructions()\\r\\n            summary_prompt_template = PromptTemplate(input_variables=[\\""information\\""], template=summary_template,\\r\\n                                                        partial_variables={\\""format_instructions\\"": format_instructions})\\r\\n            response = LLMChain(llm=llm, prompt=summary_prompt_template)\\r\\n            result = response.run(information=inc_dict)\\r\\n            title_data = f'''JSON :\\r\\n\\r\\n                            {inc_dict}\\r\\n\\r\\n                        '''  \\r\\n            result = result[7:]\\r\\n            result = result[:-3]\\r\\n            result = json.loads(result)\\r\\n            \\r\\n            summary_result = f'''Summary of Ticket {query}:  {result['Summary']} \\\\\\r\\n                                Below are the point wise details of the incident: \\\\\\r\\n                                1.Title: {result[\\""Title\\""]}\\r\\n                                2.Short Description: {result['Short description']}\\r\\n                                3.Owner: {result['Owner']}\\r\\n                                4.Priority and Urgency: {result['Priority and Urgency']}\\r\\n                                5.ResolutionNotes: {result['ResolutionNotes']}\\r\\n                                6.Current Status: {result['current_status']}\\r\\n                                7.SLA Timeline:  {result[\\""SLA Timeline\\""]} \\r\\n                            '''\\r\\n                \\r\\n            return summary_result\\r\\n        except Exception as e:\\r\\n            return e\\r\\n    output = get_completion(query)\\r\\n    return output\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","LEOTCKT_32789"
"Chatbot","Core","{""formats"":{""topic_name"":""text"",""no_of_post"":""text""},""classname"":""scrape_medium"",""name"":""Scrape Medium"",""parentCategory"":""LEOSRCHY97728"",""alias"":""Scrape Medium"",""id"":""LEOSCRP_78502"",""codeGeneration"":{""requirements"":[""selenium"",""bs4""],""imports"":[""from selenium import webdriver"",""from selenium.webdriver.chrome.options import Options"",""from bs4 import BeautifulSoup"",""import logging, warnings, time, re, requests"",""from datetime import datetime"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""warnings.filterwarnings('ignore')\\r\\n\\r\\ndef find_links(driver, no_of_post):\\r\\n    scroll = 2000\\r\\n    link_dict = {}\\r\\n    while True:\\r\\n        logging.info(\\""Inside while loop\\"")\\r\\n        driver.execute_script(f'window.scrollBy(0, {scroll})')\\r\\n        # wait for page to load\\r\\n        time.sleep(15)\\r\\n        # check if at bottom of page\\r\\n        soup = BeautifulSoup(driver.page_source, 'html.parser')\\r\\n        # print(soup.prettify())\\r\\n        link = soup.find_all('div', class_=\\""l er lm\\"")\\r\\n        logging.info(f'No of link found: {len(link)}')\\r\\n        link_dict.update({add_prefix(s.find('a').get('href')): s.find('a').find('h2').text for s in link if not s.find('div', class_=\\""kt ab\\"")})\\r\\n        if len(link_dict) >= no_of_post:\\r\\n            logging.info(f'No of link found: {len(link_dict)}')\\r\\n            break\\r\\n        if driver.execute_script('return window.innerHeight + window.pageYOffset >= document.body.offsetHeight'):\\r\\n            break\\r\\n        scroll += 500\\r\\n    return link_dict\\r\\n\\r\\n\\r\\ndef add_prefix(h: str, prefix='https://medium.com'):\\r\\n    \\""\\""\\""\\r\\n    This function adds the prefix to the hrefs\\r\\n    :param h:\\r\\n    :param prefix:\\r\\n    :return:\\r\\n    \\""\\""\\""\\r\\n    if prefix not in h:\\r\\n        m = prefix + h\\r\\n        return m\\r\\n    else:\\r\\n        return h\\r\\n\\r\\n\\r\\ndef single_page_scraper(page_url):\\r\\n    response = requests.get(url=page_url, verify=False)\\r\\n    soup = BeautifulSoup(response.content, \\""html.parser\\"")\\r\\n    return soup\\r\\n\\r\\n# Define a Pydantic model for the tool's input parameters\\r\\nclass scrape_medium(BaseModel):\\r\\n    topic_name: str = Field(...,description=\\""should be a search query\\"")\\r\\n    no_of_post: int = Field(...,description=\\""should be a search query\\"")\\r\\n\\r\\n@tool(\\""scrape_medium\\"", args_schema=scrape_medium, return_direct=True)\\r\\ndef scrape_medium(topic_name, no_of_post):\\r\\n    \\""\\""\\""\\r\\n    Searches for information on the internet using Tavily's search API and returns the results.\\r\\n    Args:\\r\\n        query (str): The search query\\r\\n    return: The search results\\r\\n    \\""\\""\\""\\r\\n    years = list(range(datetime.now().year, 2000, -1))\\r\\n    # setting up the chrome options\\r\\n    option = Options()\\r\\n    # option.add_argument(\\""headless\\"")\\r\\n    option.add_argument(\\""InPrivate\\"")\\r\\n    option.binary_location = r\\""C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\\""\\r\\n    link_dict = {}\\r\\n    for year in years:\\r\\n        logging.info(f'Looking in records of: {year}')\\r\\n        base_url = 'https://medium.com/tag/{0}/archive/{1}'.format(topic_name, year)\\r\\n        driver = webdriver.Chrome(options=option)\\r\\n        driver.get(base_url)\\r\\n        link_dict = find_links(driver, no_of_post)\\r\\n        # soup = BeautifulSoup(driver.page_source, 'html.parser')\\r\\n        # link = soup.find_all('div', class_=\\""l er lh\\"")\\r\\n        # link_dict = {add_prefix(s.find('a').get('href')): s.find('a').find('h2').text for s in link if not s.find('div', class_=\\""kt ab\\"")}\\r\\n        if len(link_dict) >= no_of_post:\\r\\n            logging.info(f'No of link found: {len(link_dict)}')\\r\\n            driver.quit()\\r\\n            break\\r\\n    files_saved = 0\\r\\n    for count, (link, name) in enumerate(link_dict.items(), 1):\\r\\n        try:\\r\\n            soup = single_page_scraper(link)\\r\\n        except:\\r\\n            pass\\r\\n        # step1 = soup.find('div', class_=\\""fr gh gi gj gk gl\\"")\\r\\n        step2 = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'ol'])\\r\\n        step3 = [s.text for s in step2]\\r\\n        article = \\""\\\\n\\"".join(\\"" \\"".join(step3).strip().splitlines())\\r\\n        article = re.sub('\\\\s+', ' ', article)\\r\\n        try:\\r\\n            with open(r'D:\\\\gautamsumanyu.t\\\\OneDrive - Infosys Limited\\\\Project Tasks\\\\March_2024\\\\test_files\\\\{0}.txt'.format(\\r\\n                    re.sub('[^A-Za-z0-9]+', '', name)), 'w+',\\r\\n                      encoding='utf-8') as f:\\r\\n                f.write(article)\\r\\n                f.close()\\r\\n            logging.info(f'Article {count} saved successfully')\\r\\n            files_saved += 1\\r\\n            if files_saved == no_of_post:\\r\\n                break\\r\\n        except:\\r\\n            pass\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""topic_name"":""None"",""no_of_post"":""None""}}","LEOSCRP_78502"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""create_case"",""name"":""Create Case"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Create Case"",""id"":""LEOCRT_C45541"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests,json,re"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from typing import Optional""],""script"":""class CreateCase(BaseModel):\\r\\n    ticket_number: str= Field(\\""Ticket number expressed as a string\\"")\\r\\n    customer_id: str=Field(\\""Customer id expressed as a string\\"")\\r\\n    invoice_id: str=Field(\\""Invoice id expressed as a string\\"")\\r\\n    requested_by: Optional[str]=None\\r\\n\\r\\n@tool(\\""create_case\\"",args_schema=CreateCase, return_direct=True)\\r\\ndef create_case(ticket_number: str, customer_id: str, invoice_id: str, requested_by: str )->str:\\r\\n\\r\\n    \\""\\""\\""\\r\\n    This tool is use to create case with when all three parameters customer id, ticket number and invoice id are given.\\r\\n    Parameters:\\r\\n        ticket_id(str): string representing ticket number\\r\\n        invoice_id (str):string representing the invoice id\\r\\n        customer_id (str): string representing the customer id\\r\\n        requested_id (str): string representing the requested id, which is optional\\r\\n    Returns:\\r\\n        Returns the case id.\\r\\n    \\""\\""\\""\\r\\n    \\r\\n    try: \\r\\n        logger.info(f\\"" CASE method called with params: TICKET NUMBER: {ticket_number}, CUSTOMER ID: {customer_id}, INVOICE ID: {invoice_id}, REQUESTED BY: {requested_by}\\"")        \\r\\n        token = generateBearerToken() \\r\\n\\r\\n        headers = {\\r\\n            \\""Authorization\\"": f\\""Bearer {token}\\"",\\r\\n            'accept': 'application/json',\\r\\n            'Content-Type': 'application/json',\\r\\n        }\\r\\n        url = chatbot_params[\\""create_case_url\\""]\\r\\n        json_data = {\\r\\n            \\""ticket_id\\"":ticket_number,\\r\\n            \\""customer_id\\"":customer_id,\\r\\n            \\""invoice_id\\"":invoice_id,\\r\\n            \\""requested_by\\"":\\""thippesha.thippesha@infosys.com\\""}\\r\\n        response = requests.post(url=url,headers=headers, json=json_data,verify=False)\\r\\n        logger.info(f\\""Response Status: {response.status_code}\\"")\\r\\n        if response.status_code == 200 or response.status_code== 201:\\r\\n            status_message = response.text\\r\\n            match = re.search(r'inv:\\\\d+', response.text)\\r\\n            if match:\\r\\n                case_id = match.group()\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[f'get case status of {case_id}'],\\r\\n                        'navigate': {}\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            status_message = \\""Error in Response\\""\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':['get case status'],\\r\\n                        'navigate': {}\\r\\n                        }\\r\\n            return json.dumps(response) \\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception Inside createCase method  {e}\\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://livebpm/api/inbox/startProcess/jci/jci_invoice_processing/Manual?ocrIdList=""}}","LEOCRT_C45541"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""ITSM"",""parentCategory"":""LEOTLSBM21587"",""alias"":""ITSM"",""attributes"":{},""id"":""LEOITSMQ55029"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOITSMQ55029"
"Chatbot","Core","{""formats"":{},""classname"":"""",""name"":""Search"",""parentCategory"":""LEOTLSBM21587"",""alias"":""Search"",""attributes"":{},""id"":""LEOSRCHY97728"",""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[]}","LEOSRCHY97728"
"Chatbot","Core","{""formats"":{""query"":""text""},""classname"":""tavily_tool"",""name"":""Trivaly"",""parentCategory"":""LEOSRCHY97728"",""alias"":""Trivaly"",""id"":""LEOTVLY_16784"",""codeGeneration"":{""requirements"":[""langchain==0.1.16""],""imports"":[""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""import requests, json, os""],""script"":""# Define a Pydantic model for the tool's input parameters\\r\\nclass SearchInput(BaseModel):\\r\\n    query: str = Field(description='should be a search query')\\r\\n\\r\\n@tool(\\""tavily_tool\\"", args_schema=SearchInput, return_direct=True)\\r\\ndef tavily_tool(query: str) -> str:\\r\\n    '''\\r\\n    Searches for information on the internet using Tavily's search API and returns the results.\\r\\n    Args:\\r\\n        query (str): The search query\\r\\n    return: The search results\\r\\n    '''\\r\\n    try:\\r\\n        logger.info(f'tavily_tool method called with query = {query}')\\r\\n        base_url = 'https://api.tavily.com/search'\\r\\n        api_key = os.environ.get('app_tavily_api_key')\\r\\n        headers = {'Content-Type': 'application/json'}\\r\\n        data = {\\r\\n                'query': query,\\r\\n                'search_depth': 'advanced',\\r\\n                'api_key': api_key,\\r\\n            }\\r\\n        response = requests.post(base_url, data=json.dumps(data), headers=headers,verify=False)\\r\\n        if response.status_code == 200:\\r\\n            status_message = response.text\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[]\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            status_message = \\""Some error occured while fetching the data\\""\\r\\n            response = {'chat_system_response':status_message,\\r\\n                        'type': 'Text',\\r\\n                        'chat_suggestions':[]\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        #return response.text\\r\\n    except Exception as ex:\\r\\n        logger.info(f'Exception occured in Tavily Tool = {ex}')\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""""}}","LEOTVLY_16784"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""userAssignment"",""name"":""User Assignment"",""parentCategory"":""LEOITSMQ55029"",""alias"":""User Assignment"",""id"":""LEOUSRSG67802"",""codeGeneration"":{""requirements"":[],""imports"":[""import logging as logger"",""import ast"",""import requests,json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""from typing import Optional""],""script"":""class User_Assignment(BaseModel):\\r\\n    ticket_id: str =Field(None,description=\\""ticket ID passed by the user\\"")\\r\\n    invoice_id: str=Field(None,description=\\""Invoice ID passed by the user\\"")\\r\\n    customer_id: str=Field(None,description=\\""Customer ID passed by the user\\"")\\r\\n    requested_by: Optional[str]=None\\r\\n\\r\\n@tool(\\""userAssignment\\"",args_schema=User_Assignment, return_direct=True)\\r\\ndef userAssignment(ticket_id: str = None, invoice_id: str = None, customer_id: str = None, requested_by:str = None)-> str:\\r\\n    '''\\r\\n    User assignment, returns the form id for the user assignment. ticket_id, invoice_id, customer_id are not mandatory parameters.\\r\\n    parameters: \\r\\n    ticket_id(str): string representing ticket number\\r\\n    invoice_id (str):string representing the invoice id\\r\\n    customer_id (str): string representing the customer id\\r\\n    requested_id (str): string representing the requested id, which is optional\\r\\n    returns:\\r\\n     valid json type response contaning all the details\\r\\n    '''\\r\\n    try:\\r\\n        logger.info(f\\""userAssignment assignment method called with params: Ticket ID: {ticket_id}, Invoice ID: {invoice_id}, Customer ID: {customer_id}, REQUESTED BY: {requested_by}\\"")\\r\\n        cookies = {\\r\\n            'JSESSIONID': '205A48EA5D6FF43FFB3AC2C7962D55C3',\\r\\n            'XSRF-TOKEN': 'a2432b90-ae00-4717-8621-d4a4e4141f1f',\\r\\n        }\\r\\n        token = generateBearerToken()\\r\\n        logger.info(f\\""Token received: {token}\\"")\\r\\n        headers = {\\r\\n            \\""Authorization\\"": f\\""Bearer {token}\\"",\\r\\n            'Cookie': \\""JSESSIONID = 205A48EA5D6FF43FFB3AC2C7962D55C3 ; XSRF-TOKEN = a2432b90-ae00-4717-8621-d4a4e4141f1f\\""\\r\\n        }\\r\\n        params = {\\r\\n            'pKey': 'jci_invoice_processing',\\r\\n            'org': 'jci',\\r\\n            'constant': 'forms',\\r\\n        }\\r\\n        url = chatbot_params[\\""userAssignment_url\\""]\\r\\n        response = requests.get(url, params=params, headers=headers, verify=False)\\r\\n        logger.info(f\\""Response status for get form ID: {response.status_code}\\"")\\r\\n        if response.status_code == 200:\\r\\n            response = ast.literal_eval(response.text)\\r\\n            form_id = response.get('completed')\\r\\n            response = {\\r\\n                        'chat_system_response':form_id,\\r\\n                        'type': 'Form',\\r\\n                        'chat_formName': form_id,\\r\\n                        'form_fields':{\\r\\n                                    'ticket_id':ticket_id,\\r\\n                                    'invoice_id':invoice_id,\\r\\n                                    'customer_id':customer_id,\\r\\n                                    'requested_by':requested_by\\r\\n                                  },\\r\\n                        'chat_suggestions':None,\\r\\n                        'navigate': {}\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            issue = \\""User Assignment unsuccessful\\""\\r\\n            form_id = response.get('completed')\\r\\n            response = {\\r\\n                        'chat_system_response': issue,\\r\\n                        'type': 'Form',\\r\\n                        'chat_formName': form_id,\\r\\n                        'form_fields':{\\r\\n                                    'ticket_id':ticket_id,\\r\\n                                    'invoice_id':invoice_id,\\r\\n                                    'customer_id':customer_id,\\r\\n                                    'requested_by':requested_by\\r\\n                                  },\\r\\n                        'chat_suggestions':None,\\r\\n                        'navigate': {}\\r\\n                        }\\r\\n            return json.dumps(response)\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in userAssignment method: {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://livebpm/api/inbox/constant/""}}","LEOUSRSG67802"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""ticket_Assignment"",""name"":""Ticket Assignment"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Ticket Assignment"",""id"":""LEOTCKT_54196"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests,json"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""\\r\\n@tool(\\""ticket_Assignment\\"",return_direct=True)\\r\\ndef ticket_Assignment()->str:\\r\\n    \\""\\""\\""\\r\\n    Use this tool when the user query is something related to Assign the ticket or ticket assignment, this tool doesn't require any input parameters.\\r\\n\\r\\n    Returns:\\r\\n        Tickets Assigned succesfully or not \\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        url = chatbot_params[\\""ticket_Assignment_url\\""]\\r\\n        json_data = {}\\r\\n        response = requests.post(url=url,json=json_data,verify=False)\\r\\n        print(f\\""response status, {response.status_code}, {response.text}\\"")\\r\\n        if response.status_code == 200:\\r\\n            response = {\\r\\n                'chat_system_response':\\""Tickets Assigned succesfully\\"",\\r\\n                'type': 'Text',\\r\\n                'chat_suggestions':[],\\r\\n                'navigate': {}\\r\\n            }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            return \\""Error occured while assigning tickets\\""\\r\\n            \\r\\n    except Exception as e:\\r\\n        return(f\\""Exception Inside ticketAssignment method  {e}\\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://victlpth5-04:9876/api/event/trigger/AssignTicket?org=leo1311&corelid=&datasourceName=LEALCLCL12132""}}","LEOTCKT_54196"
"Chatbot","Core","{""formats"":{},""classname"":""fetch_details_from_database"",""name"":""NLP to SQL"",""parentCategory"":""LEOTLSBM21587"",""alias"":""NLP to SQL"",""id"":""LEONLP_T49952"",""codeGeneration"":{""requirements"":[""langchain"",""pydantic==1.10.10"",""mysql-connector-python"",""openai"",""pandas""],""imports"":[""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""import pandas as pd"",""import mysql.connector"",""from langchain.chat_models import AzureChatOpenAI"",""from langchain.utilities import SQLDatabase"",""from langchain.chains import create_sql_query_chain"",""from langchain_core.prompts import PromptTemplate""],""script"":""class NLP_to_SQL(BaseModel):\\n    query: str =Field(None,description=\\""Query passed by the user\\"")\\n\\n# NLP to SQL - to fetch details from db\\n@tool(\\""fetch_details_from_database\\"",args_schema=NLP_to_SQL, return_direct=True)\\ndef fetch_details_from_database(query:str)->str:\\n    \\""\\""\\""\\n    this tool is used to answer all the queries related to database, or queries askinng to fetch something from database only.\\n    Parameters:\\n        query (str): A question in natural language form.\\n    Returns:\\n        A  dictionary containing the relevant details fetched from the database based on the SQL query.\\n    \\""\\""\\""\\n\\n    # Connecting to the database\\n    mydb = mysql.connector.connect(\\n        host=os.environ.get('app_host_name'),\\n        user=os.environ.get('app_mysql_user'),\\n        password=os.environ.get('app_mysql_password'),\\n        database=os.environ.get('database_name')\\n    )\\n    mycursor = mydb.cursor(dictionary=True)\\n    # Initializing the LLM\\n    llm = AzureChatOpenAI(\\n        deployment_name='gtp35turbo',\\n        model_name='gpt-35-turbo',\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\n        openai_api_version='2023-03-15-preview',\\n        openai_api_base=os.environ.get('azure_endpoint'),\\n        openai_api_type='azure',\\n        streaming=True,\\n        verbose=True\\n    ) \\n    def LLMresponse(query):   \\n        try:\\n            connection_string = os.environ.get('app_sql_connection')\\n            db = SQLDatabase.from_uri(connection_string)\\n            template = '''\\n                Given an input question, first create a syntactically correct query to run, then look at the results of the query and return the answer and it should be {top_k} and the sql query should include \\n                title, shortdescription, priority, assignedto, number, createdDate, createdby and state columns for all the query unless and untill the query wants to have some specific columns then have those only.\\n                Use the following format:\\n                Question: \\""Question here\\"" SQLQuery: \\""SQL Query to run\\"" SQLResult: \\""Result of the SQLQuery\\"" Answer: \\""Final answer here\\""\\n                Only use the following tables: {table_info}.\\n                Question: {input}\\n                '''  \\n            prompt = PromptTemplate.from_template(template)\\n            chain = create_sql_query_chain(llm, db, prompt=prompt)\\n            response = chain.invoke({\\""question\\"": query})\\n            return response\\n        except Exception as e:\\n            return e\\n\\n    response = LLMresponse(query)\\n    mycursor.execute(response)\\n    data = mycursor.fetchall()\\n    column_names = [desc[0] for desc in mycursor.description]\\n    data_df = pd.DataFrame(data, columns=column_names)\\n    return data_df.to_dict('records')\\n\\n\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{}}","LEONLP_T49952"
"Chatbot","Core","{""formats"":{},""classname"":""Chat Utility"",""name"":""Chat Utility"",""parentCategory"":"""",""alias"":""Chat Utility"",""id"":""HRCHTLT54029"",""codeGeneration"":{""requirements"":[],""imports"":[""from flask_cors import CORS"",""from langchain_community.chat_models import AzureChatOpenAI"",""from langchain.memory import ConversationBufferMemory"",""import json,requests,ast"",""from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder"",""from langchain.agents import AgentExecutor, create_structured_chat_agent"",""import sqlite3""],""script"":""'''\\r\\rSetup sqlite db table to store chat \\r\\r'''\\r\\rsql_db_path = 'Chat1.db'\\r\\rsql_table_name = 'Chat'\\r\\rconnection_obj = sqlite3.connect(sql_db_path,check_same_thread=False)\\r\\rcursor_obj = connection_obj.cursor()\\r\\rtable = \\""\\""\\"" CREATE TABLE IF NOT EXISTS Chat (\\r\\r            chat_user_id VARCHAR(25) NOT NULL,\\r\\r            chat_id CHAR(25) NOT NULL,\\r\\r            chat_user_query VARCHAR(1000),\\r\\r            chat_system_response VARCHAR(1000),\\r\\r            rating REAL,\\r\\r            feedback VARCHAR(1000)\\r\\r        ); \\""\\""\\""\\r\\rcursor_obj.execute(table)\\r\\rconnection_obj.close()\\r    \\r\\r\\""\\""\\""\\rProvides context for individual chat interactions between FE and BE services\\r\\r\\""\\""\\""\\r\\rclass ChatContext:\\r\\r    #: Unique chat session id\\r    chat_id: str = None\\r\\r    #: Unique chat user id, to be derived from jwt implicitly once auth is implemented\\r    chat_user_id: str = None\\r\\r    #: Context storage for the session\\r    chat_context: list[str] = None\\r\\r    # User query\\r    chat_user_query: str = None\\r\\r    #: AI chatbot response\\r    chat_system_response: str = None   \\r\\r    #: Send the user uploaded file in blob format\\r    chat_user_upload_file: object = None\\r\\r    #: User uploaded file type\\r    chat_user_upload_file_type: str = None \\r\\r    #: Response time for the chat in miliseconds\\r    chat_response_time: float = None\\r\\r    #: RAI metric value\\r    chat_rai_metric: float = None\\r\\r    #: Confidence score/ accuracy metric for the chat query response\\r    chat_confidence_score: float = None\\r\\r    #: If chat response contain a file, populate it with the extention such as png, jpg, mp4, mp3\\r    chat_response_file_type: list[str] = None\\r\\r    #: If chat response contain a file, populate it with the extention such as png, jpg, mp4, mp3\\r    chat_response_file_data: list[object] = None\\r\\r\\r\\rdef create_chat_context_object(chat_id, chat_user_id, chat_user_query,chat_system_response,chat_response_time):\\r\\r    logger.info(f\\""Create chat context object called with query: {chat_user_query}\\"")\\r    chat_context_obj = ChatContext()\\r    chat_context_obj.chat_id = chat_id\\r    chat_context_obj.chat_user_id = chat_user_id\\r    chat_context_obj.chat_user_query= chat_user_query\\r    chat_context_obj.chat_system_response = chat_system_response\\r    chat_context_obj.chat_response_time = chat_response_time \\r    return chat_context_obj\\r    \\rdef intialise_memory(previous_chat_history):\\r    memory = ConversationBufferMemory(memory_key=\\""chat_history\\"", return_messages=True,input_key=\\""input\\"", output_key=\\""output\\"")\\r    if len(previous_chat_history) > 0:\\r        for context_obj in previous_chat_history:\\r            memory.chat_memory.add_user_message(context_obj.chat_user_query)\\r            memory.chat_memory.add_ai_message(context_obj.chat_system_response)\\r    return memory\\r\\r\\rdef getFormatedResponse(data,user_prompt):\\r    llm2 = AzureChatOpenAI(openai_api_key=os.environ.get(\\""app_openai_api_key\\""),openai_api_base=os.environ.get(\\""app_openai_api_base\\""),deployment_name='gtp35turbo', temperature=0, openai_api_version='2023-07-01-preview')\\r    prompt = ChatPromptTemplate.from_template(user_prompt)\\r    chain = prompt | llm2\\r    response = chain.invoke({\\""data\\"": data})\\r    formatted_response = response.content\\r    return formatted_response\\r\\r\\rsystem = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\r\\r{tools}\\rUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\r\\rValid \\""action\\"" values: \\""Final Answer\\"" or {tool_names}\\r\\rProvide only ONE action per $JSON_BLOB, as shown:\\r\\r```\\r{{\\r\\r\\""action\\"": $TOOL_NAME,\\r\\r\\""action_input\\"": $INPUT\\r\\r}}\\r\\r```\\rFollow this format:\\r\\rQuestion: input question to answer\\rThought: consider previous responses and subsequent steps\\rAction:\\r\\r```\\r\\r$JSON_BLOB\\r\\r```\\rObservation: action result\\r... (repeat Thought/Action/Observation N times)\\r\\rThought: I know what to respond\\r\\rAction:\\r\\r```\\r\\r{{\\r\\r\\""action\\"": \\""Final Answer\\"",\\r\\""action_input\\"": \\""Final response to human\\""\\r\\r}}\\r\\rBegin! Reminder to ALWAYS respond with a valid json blob of a single action. You should always try to use tools for the user's query. Do not ask for default parameters of tools. If user provides important information such as role ID, system ID, case ID, save this information in memory and use this information for subsequent user queries. Respond to user provided information with Thank you for providing details, how may i help you. Respond directly only when none of the tools can help in answering. Format is Action:```$JSON_BLOB```then Observation'''\\r\\rhuman = '''\\r\\r{input}\\r\\r{agent_scratchpad}\\r\\r(reminder to respond in a JSON blob no matter what)'''\\r\\rprompt = ChatPromptTemplate.from_messages(\\r            [\\r                (\\""system\\"", system),\\r                MessagesPlaceholder(\\""chat_history\\"", optional=True),\\r                (\\""human\\"", human),\\r            ]\\r        )\\r\\r\\rdef refreshDB():\\r    logger.info(f\\""Refresh DB method called\\"")\\r    url = \\""https://victlpth5-04:8095/api/event/trigger/refreshTickets?org=leo1311&corelid=&param=%257B%257D&datasourceName=LEALCLCL12132\\""\\r    response = requests.get(url=url,verify=False)\\r    logger.info(f\\""Response Status:  {response.status_code}\\"")\\r    if response.status_code == 200:\\r        return True\\r    return False\\r    \\r\\rdef generateBearerToken():\\r    logger.info(f\\""Generate Bearer Token method called\\"")\\r    url = \\""https://login.microsoftonline.com/63ce7d59-2f3e-42cd-a8cc-be764cff5eb6/oauth2/token\\""\\r    payload = 'client_id=9090f1c5-d381-4ef6-b845-4bac98d02fbe&client_secret=CNl8Q~IA-EUpiyA5Kkh97-4uH3ajo2PqQOkpHbp~&grant_type=client_credentials&scope=b3490b10-6bd3-4f66-908d-fa1950e46598%2F.default'\\r    headers = {\\r    'Content-Type': 'application/x-www-form-urlencoded'\\r    }\\r    response = requests.request(\\""POST\\"", url, headers=headers, data=payload)\\r    logger.info(f\\""Response Status:  {response.status_code}\\"")\\r    return json.loads(response.text)['access_token']\\r    \\rdef get_chain_response(chat_user_query,memory):\\r\\r    try:\\r        logger.info(f\\""Get Chain Response method called with query: {chat_user_query}\\"")\\r        openai_api_key = os.environ.get(\\""app_openai_api_key\\"")\\r        openai_api_base = os.environ.get(\\""app_openai_api_base\\"")\\r        tools = [nlp2Sql_adapter,ticket_summarization_adapter,create_ticket,semantic_search_adapter,get_case_status,ticket_Assignment,create_case,get_related_tickets,userAssignment]\\r        llm = AzureChatOpenAI(openai_api_key= openai_api_key, openai_api_base = openai_api_base, deployment_name='gpt-4', temperature=0, openai_api_version='2023-05-15')\\r        logger.info(\\""init llm\\"")\\r        agent = create_structured_chat_agent(llm, tools,prompt)\\r        agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True,return_intermediate_steps=True, memory=memory,handle_parsing_errors=True)\\r        logger.info(\\""init agent\\"")\\r        chat_system_response = agent_executor.invoke({\\""input\\"": chat_user_query})\\r\\r        try:\\r            chat_system_response = json.loads(chat_system_response[\\""output\\""])\\r\\r        except Exception as e:\\r            system_response = chat_system_response[\\""output\\""]\\r            chat_system_response = dict()\\r            chat_system_response = {\\""chat_system_response\\"":  system_response}\\r            \\r        chat_chain_of_thoughts = list()\\r        chat_system_response['chat_chain_of_thoughts'] = chat_chain_of_thoughts\\r        return chat_system_response\\r\\r    except Exception as e:\\r        logger.info(f\\""Exception in get_chain_response method: {e}\\"")\\r\\r\\r\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{}}","HRCHTLT54029"
"Chatbot","Core","{""formats"":{""Tools"":""list""},""classname"":""Agent"",""name"":""Agent"",""parentCategory"":"""",""alias"":""Agent"",""id"":""HRAGNTC20083"",""codeGeneration"":{""requirements"":[],""imports"":[""from langchain_community.chat_models import AzureChatOpenAI"",""from langchain.agents import AgentExecutor, create_structured_chat_agent"",""import json,ast""],""script"":""\\ndef get_chain_response(chat_user_query,memory):\\n\\n    try:\\n        \\n        logger.info(f\\""Get chain response method called with query: {chat_user_query}\\"")\\n        openai_api_key = os.environ.get(\\""app_openai_api_key\\"")\\n        openai_api_base = os.environ.get(\\""app_openai_api_base\\"")\\n        tools = chatbot_params[\\""Agent_Tools\\""]\\n        logger.info(f\\""@@@@@@@@@@@ TOOLS RECEIVED: {tools} @@@@@@@@@@@@@@\\"")\\n        # tools = [nlp2Sql_adapter,ticket_summarization_adapter,create_ticket,semantic_search_adapter,get_case_status,ticket_Assignment,create_case,get_related_tickets,userAssignment]\\n        llm = AzureChatOpenAI(openai_api_key= openai_api_key, openai_api_base = openai_api_base, deployment_name='gpt-4', temperature=0, openai_api_version='2023-05-15')\\n        logger.info(\\""Got LLM\\"")\\n        agent = create_structured_chat_agent(llm, tools,prompt)\\n        agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True,return_intermediate_steps=True, memory=memory,handle_parsing_errors=True)\\n        logger.info(\\""Init Agent\\"")\\n        chat_system_response = agent_executor.invoke({\\""input\\"": chat_user_query})\\n        try:\\n            \\n            chat_system_response = json.loads(chat_system_response[\\""output\\""])\\n            \\n        except Exception as e:\\n            \\n            system_response = chat_system_response[\\""output\\""]\\n            chat_system_response = dict()\\n            chat_system_response = {\\""chat_system_response\\"":  system_response}\\n        chat_chain_of_thoughts = list()\\n        chat_system_response['chat_chain_of_thoughts'] = chat_chain_of_thoughts\\n        \\n        return chat_system_response\\n        \\n    except Exception as e:\\n        logger.info(f\\""Exception in get_chain_response method: {e}\\"")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""Tools"":""nlp2Sql_adapter,ticket_summarization_adapter,create_ticket,semantic_search_adapter,get_case_status,ticket_Assignment,create_case,get_related_tickets,userAssignment""}}","HRAGNTC20083"
"Chatbot","Core","{""formats"":{""query"":""text""},""classname"":""web_scrapper"",""name"":""WebScrapper"",""parentCategory"":""LEOSRCHY97728"",""alias"":""WebScrapper"",""id"":""LEOWB_SC54827"",""codeGeneration"":{""requirements"":[""bs4"",""playwright""],""imports"":[""from langchain_openai import AzureChatOpenAI"",""from langchain.chains.question_answering import load_qa_chain"",""import requests"",""from bs4 import BeautifulSoup"",""from langchain_community.document_loaders.chromium import AsyncChromiumLoader"",""from langchain_community.document_transformers import BeautifulSoupTransformer"",""from langchain_text_splitters import RecursiveCharacterTextSplitter"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool"",""import subprocess""],""script"":""llm = AzureChatOpenAI(    \\n        deployment_name='gpt-4',\\n        model_name='gpt-4',\\n        openai_api_key=os.environ.get('OPENAI_API_KEY'),\\n        openai_api_version = '2023-03-15-preview',\\n        azure_endpoint='https://azureft.openai.azure.com/',\\n        openai_api_type='azure',        \\n        streaming=True,\\n        verbose=True\\n        )\\n\\nclass WebScrapperTool(BaseModel):\\n    query: str = Field(..., description='The query to ask the model from website content.')\\n\\n@tool('web_scrapper', args_schema=WebScrapperTool, return_direct=True)\\ndef web_scrapper(query: str):\\n    \\""\\""\\""\\n    This tool will be used to scrape the content from the top 3 websites and then summarize the content.\\n    parameters:\\n        - Topic: The topic of the websites to be scraped.\\n        - query: The query to ask the model.\\n    returns:\\n        - A summary based on the topic.\\n    \\""\\""\\""\\n    try:\\n        cmd = 'playwright install-deps'\\n        subprocess.run(cmd, shell=True)\\n        encoded_topic = \\""+\\"".join(query.split())\\n        url = f'https://www.google.com/search?q={encoded_topic}'\\n        response = requests.get(url, verify=False)\\n        if response.status_code == 200:\\n            soup = BeautifulSoup(response.text, 'html.parser')\\n            search_results = soup.find_all('a')\\n            top_urls = []\\n            for results in search_results:\\n                href = results.get('href')\\n                if href.startswith('/url?q='):\\n                    url = href.split('/url?q=')[-1].split('&')[0]\\n                    top_urls.append(url)\\n                    if len(top_urls) == 5:\\n                        break\\n            top_urls = top_urls[2:]\\n            loader = AsyncChromiumLoader(top_urls)\\n            docs = loader.load()\\n            bs_transformer = BeautifulSoupTransformer()\\n            docs_transformed = bs_transformer.transform_documents(\\n                    docs, tags_to_extract=['div','span','h2','a'],\\n                )\\n            splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=4000, chunk_overlap=0)\\n            splits = splitter.split_documents(docs_transformed)\\n            chain = load_qa_chain(llm=llm, chain_type='stuff')\\n            response = chain.invoke(input={'input_documents':splits, 'question':query})\\n            return response['output_text']\\n        else:\\n            print('Invalid response...')\\n    except Exception as e:\\n        logger.info(f\\""@@@@@@@@@@@ Exception in webscrapper method: {e} \\"")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n""},""category"":""Tools"",""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""attributes"":{""query"":""None""}}","LEOWB_SC54827"
"Chatbot","Core","{""formats"":{},""classname"":""get_case_status"",""name"":""Case Status"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Case Status"",""id"":""LEOGTCSS91908"",""codeGeneration"":{""requirements"":[],""imports"":[""import requests,json"",""from langchain.tools import tool"",""from pydantic import BaseModel, Field"",""import requests,json""],""script"":""class Get_Case_Status(BaseModel):\\r\\n    case_id: str=Field(None,description=\\""case_id provided by the user\\"")\\r\\n@tool(\\""get_case_status\\"",args_schema=Get_Case_Status, return_direct=True)\\r\\ndef get_case_status(case_id: str):\\r\\n    \\r\\n    '''\\r\\n    use to get status of the case based on case ID provided, case ID is of the format inv:00000008.\\r\\n \\r\\n    '''\\r\\n    logger.info(f\\""getCaseStatus method called with caseID: {case_id}\\"")\\r\\n    token = generateBearerToken()\\r\\n    headers = {\\r\\n        \\""Authorization\\"": f\\""Bearer {token}\\"",\\r\\n        'accept': 'application/json',\\r\\n        'Content-Type': 'application/json',\\r\\n    }\\r\\n    url = f\\""https://livebpm/api/datasets/searchData/JCIINVCP12565/jci?page=0&size=10&sortEvent=BID&sortOrder=-1&searchParams=%7B'and':%5B%7B'or':%7B'property':'business_key_','equality':'=','value':{case_id}%7D%7D%5D%7D\\""\\r\\n    logger.info(f\\""final url :{url}\\"")\\r\\n    response = requests.get(url=url,headers=headers,verify=False)\\r\\n    logger.info(f\\""Response status code for GET CASE STATUS method: {response.status_code}\\"")\\r\\n    if response.status_code == 200:\\r\\n        response = json.loads(response.text)\\r\\n        status_message = response[0]\\r\\n        response_dict={\\r\\n            'Ticket Number':status_message['ticket_id'],\\r\\n            'Customer ID': status_message['customer_id'],\\r\\n            'Invoice ID':status_message['invoice_id'],\\r\\n            'Requested by': status_message['requested_by'],\\r\\n            'Created On': status_message['created_date_'],\\r\\n            'Status': status_message['status_'],\\r\\n            'State':status_message['state_'],\\r\\n            'Last Updated': status_message['last_updated_date_']\\r\\n            }\\r\\n        user_prompt = '''\\r\\n            Summarize the given data\\r\\n            {data}\\r\\n            '''\\r\\n        status_message = getFormatedResponse(response_dict,user_prompt)\\r\\n        response = {'chat_system_response':status_message,\\r\\n                    'type': 'Text',\\r\\n                    'chat_suggestions':None,\\r\\n                    'navigate': {}\\r\\n                    }\\r\\n        return json.dumps(response)\\r\\n    else:\\r\\n        status_message = \\""Error in Response\\""\\r\\n        response = {'chat_system_response':status_message,\\r\\n                    'type': 'Text',\\r\\n                    'chat_suggestions':None,\\r\\n                    'navigate': {}\\r\\n                    }\\r\\n \\r\\n        return json.dumps(response)\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{}}","LEOGTCSS91908"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""sop_recommender_tool"",""name"":""SOPRecommender"",""parentCategory"":""LEOITSMQ55029"",""alias"":""SOPRecommender"",""id"":""LEOSP_RC17321"",""codeGeneration"":{""requirements"":[""langchain"",""pydantic==1.10.10"",""requests""],""imports"":[""import ast"",""import json"",""import requests"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""class SOP_Recommender(BaseModel):\\n    query: str =Field(None,description=\\""query passed by the user\\"")\\n@tool(\\""sop_recommender_tool\\"",args_schema=SOP_Recommender, return_direct=True)\\ndef sop_recommender_tool(query:str)->str:\\n    \\""\\""\\""\\n    This tool is use to get the SOP recommendation of ticket.\\n    Parameters:\\n        query (str): The query for SOP recommendation.\\n    Returns:\\n        SOP for resolving the ticket.\\n    \\""\\""\\""\\n    try:\\n        url = chatbot_params['SOPRecommender_url']\\n        headers = {\\n            'access-token': os.environ.get('access_token'),\\n            'accept': 'application/json',\\n            'Content-Type': 'application/json',\\n            }\\n        json_data = {\\n                        \\""config\\"": {\\n                            \\""VectorStoreConfig\\"": {\\n                                \\""DB_Type\\"": \\""Faiss\\"",\\n                                \\""query\\"": query,\\n                                \\""index_name\\"": \\""default\\"",\\n                                \\""k\\"": 10\\n                            },\\n                            \\""LLMConfig\\"": {\\n                                \\""LLM_Type\\"": \\""bedrock\\"",\\n                                \\""query\\"": query,\\n                                \\""aws_access_key_id\\"": os.environ.get('aws_access_key_id'),\\n                                \\""aws_secret_access_key\\"":os.environ.get('aws_secret_access_key'),\\n                                \\""max_tokens\\"": 4000\\n                            },\\n                            \\""EmbeddingConfig\\"": {}\\n                        }\\n                    }\\n        response = requests.post(url=url,headers=headers,json=json_data,verify=False)\\n        answer = ast.literal_eval(response.text)[0].get('Answer')\\n        response = {'chat_system_response':answer,\\n                'type': 'Text',\\n                'chat_suggestions':None,\\n                'navigate': {}\\n                }\\n        return json.dumps(response)\\n    except Exception as e:\\n        return(f\\""Exception Inside semantic_search method  {e}\\"")\\n\\n\\n\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""https://localhost:5000""}}","LEOSP_RC17321"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""semantic_search_adapter"",""name"":""Semantic Search A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Semantic Search A"",""id"":""LEOSMNTC35379"",""codeGeneration"":{""requirements"":[],""imports"":[""from langchain.tools import tool"",""import json, requests""],""script"":""class SemanticSearch(BaseModel):\\r    query: str =Field(None,description=\\""query passed by the user\\"")\\r\\r@tool(\\""semantic_search_adapter\\"",args_schema=SemanticSearch,return_direct=True) \\rdef semantic_search_adapter(query: str):\\r\\r    '''\\r\\r    use to provide resolution, SOP for given ticket description\\r\\r    '''\\r    try:\\r        \\r        logger.info(f\\""Semantic Search method called with query: {query}\\"")\\r\\r        url = chatbot_params[\\""semantic_search_adapter_url\\""]\\r    \\r        headers = {\\r    \\r            'access-token': os.environ.get(\\""ACCESS_TOKEN\\""),\\r        \\r            'accept': 'application/json',\\r\\r            'Content-Type': 'application/json',\\r            \\r            }\\r\\r        json_data = {\\r                        \\""config\\"": {\\r                            \\""EmbeddingConfig\\"": {\\r                                \\""embedding_type\\"": \\""azureopenai\\""\\r                            },\\r                            \\""VectorStoreConfig\\"": {\\r                                \\""DB_Type\\"": \\""Faiss\\"",\\r                                \\""query\\"": query,\\r                                \\""index_name\\"": \\""SOP-index\\""\\r                            },\\r                            \\""LLMConfig\\"": {\\r                                \\""LLM_Type\\"": \\""azureopenai\\"",\\r                                \\""temperature\\"": 0.1,\\r                                \\""query\\"": query,\\r                                \\""prompt_template\\"": \\""Human: Use the following pieces of context to provide a concise answer to the question at the end. If you dont know the answer, just say that you dont know, dont try to make up an answer{context}.For the query, if it requires drawing a table give results as a table markdown format.Question: {question}Assistant: \\"",\\r                                \\""max_tokens\\"": 2000\\r                            }\\r                        }\\r                    }\\r        response = requests.post(url=url,headers=headers,json=json_data,verify=False)\\r    \\r        logger.info(f\\""Response Status:  {response.status_code}\\"")\\r\\r        answer = ast.literal_eval(response.text)[0].get('Answer')\\r    \\r        response = {'chat_system_response':answer,\\r    \\r                'type': 'Text',\\r    \\r                'chat_suggestions':None,\\r                \\r                'navigate': {}\\r    \\r                }\\r    \\r        return json.dumps(response)\\r    \\r    except Exception as e:\\r        logger.info(f\\""Exception in semantic_search method  {e}\\"" )\\r\\r\\r\\r\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victlptst-57.ad.infosys.com:9080/Infer""}}","LEOSMNTC35379"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""get_related_tickets"",""name"":""Related Tickets"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Related Tickets"",""id"":""LEOGT_RL62937"",""codeGeneration"":{""requirements"":[],""imports"":[""import json, requests"",""from pydantic import BaseModel, Field"",""from langchain.tools import tool""],""script"":""class RelatedTickets(BaseModel):\\r\\n    short_description: str =Field(None,description=\\""short description passed by the user\\"")\\r\\n\\r\\n@tool(\\""get_related_Tickets\\"",args_schema=RelatedTickets, return_direct=True)\\r\\ndef get_related_tickets(short_description:str)->str:\\r\\n    \\""\\""\\""\\r\\n    This tool is use to fetch tickets with similar short description.\\r\\n    Parameters:\\r\\n        short_description (str): The short description of a ticket.\\r\\n    Returns:\\r\\n        A list of tickets with short descriptions similar to the input short description.\\r\\n    \\""\\""\\""\\r\\n    try:\\r\\n        url = chatbot_params[\\""get_related_tickets_url\\""]\\r\\n        headers = {\\r\\n                    'access-token': os.environ.get('ACCESS_TOKEN'),\\r\\n                    'accept': 'application/json',\\r\\n                    'Content-Type': 'application/json',\\r\\n                }\\r\\n        json_data = {\\r\\n        \\""config\\"": {\\r\\n            \\""VectordbConfig\\"": {\\r\\n                \\""DB_Type\\"": \\""Qdrant\\"",\\r\\n                \\""query\\"": short_description,\\r\\n                \\""index_name\\"": os.environ.get('INDEX_NAME'),\\r\\n                \\""qdrant_url\\"": os.environ.get('QDRANT_URL'),\\r\\n                \\""limit\\"": 2\\r\\n            },\\r\\n            \\""EmbeddingConfig\\"": {\\r\\n                \\""embedding_type\\"": \\""AzureOpenAI\\"",\\r\\n                \\""azure_openai_api_key\\"": os.environ.get('app_openai_api_key'),\\r\\n                \\""azure_api_version\\"": \\""2023-03-15-preview\\"",\\r\\n                \\""azure_openai_endpoint\\"": os.environ.get('app_openai_api_base'),\\r\\n                \\""azure_deployment\\"": \\""openaiada2\\"",\\r\\n                \\""model_name\\"": \\""text-embedding-ada-002\\"",\\r\\n                \\""openai_api_base\\"": os.environ.get('app_openai_api_base'),\\r\\n                \\""openai_api_type\\"": \\""azure\\""\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n        response = requests.post(url=url,json=json_data,headers=headers,verify=False)\\r\\n        \\r\\n        answer = json.loads(response.text)\\r\\n        answer_data = answer[0][\\""Answer\\""]\\r\\n        print(\\""Answer: \\"",answer_data)\\r\\n        answer_list = []\\r\\n        for key, value in answer_data.items():\\r\\n            response_dict = {\\r\\n                'Ticket Number':value.get('number',None),\\r\\n                'Description':value.get(\\""shortdescription\\"", None),\\r\\n                'Assigned To':value.get(\\""assignedto\\"", None),\\r\\n                'Assignment Group': value.get(\\""assignmentgroup\\"",None),\\r\\n                'Category': value.get(\\""category\\"",None),\\r\\n                'Created by': value.get(\\""createdby\\"",None),\\r\\n                'Created Date': value.get(\\""createdDate\\"",None),\\r\\n                'Priority': value.get(\\""priority\\"",None),\\r\\n                'State': value.get(\\""state\\"",None)                 \\r\\n            }\\r\\n            answer_list.append(response_dict)\\r\\n            \\r\\n        user_prompt = \\""\\""\\""\\r\\n            Generate summary for each of the tickets in the given data with ticket number as the heading then the summary of that ticket. \\r\\n            {data} \\r\\n            \\""\\""\\""\\r\\n        Answer = getFormatedResponse(answer_list,user_prompt)\\r\\n        global ticket_number\\r\\n        response = {\\r\\n            'chat_system_response':Answer,\\r\\n            'type': 'Text',\\r\\n            'chat_suggestions':None,\\r\\n            'navigate': { \\""RelatedTickets\\"" : f'aip/tickets/create-ticket/{ticket_number}?tab=RelatedTickets'},\\r\\n                }\\r\\n        return json.dumps(response)\\r\\n    except Exception as e:\\r\\n        return(f\\""Exception in Related_Tickets Method: {e}\\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victadpst-38:6190/ticketsrecommendation""}}","LEOGT_RL62937"
"Chatbot","Core","{""formats"":{""deployment_name"":""text"",""openai_api_version"":""text""},""classname"":""get_formatted_response"",""name"":""Response Formatting"",""parentCategory"":"""",""alias"":""Response Formatting"",""id"":""HRRSPNS39317"",""codeGeneration"":{""requirements"":[],""imports"":[""from langchain_community.chat_models import AzureChatOpenAI"",""from langchain_core.prompts.chat import ChatPromptTemplate""],""script"":""def getFormatedResponse(data,user_prompt):\\r\\n    logger.info(f\\""Formatted Response method called\\"")\\r\\n    llm2 = AzureChatOpenAI(openai_api_key=os.environ.get(\\""app_openai_api_key\\""),openai_api_base=os.environ.get(\\""app_openai_api_base\\""),deployment_name=chatbot_params[\\""get_formatted_response_deployment_name\\""], temperature=0, openai_api_version=chatbot_params['get_formatted_response_openai_api_version'])\\r\\n    prompt = ChatPromptTemplate.from_template(user_prompt)\\r\\n    chain = prompt | llm2\\r\\n    response = chain.invoke({\\""data\\"": data})\\r\\n    formatted_response = response.content\\r\\n    return formatted_response\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":"""",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""deployment_name"":""gtp35turbo"",""openai_api_version"":""2023-07-01-preview""}}","HRRSPNS39317"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""nlp2Sql_adapter"",""name"":""NLP To SQL A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""NLP To SQL A"",""id"":""LEONLP2S39547"",""codeGeneration"":{""requirements"":[""pydantic==1.10.10""],""imports"":[""import logging as logger"",""import requests,json"",""from langchain.tools import tool"",""from pydantic import BaseModel, Field""],""script"":""class NLP_to_SQL_Adapter(BaseModel):\\r\\n    query: str =Field(None,description=\\""Query passed by the user\\"")\\r\\n\\r\\n# NLP to SQL - to fetch details from db\\r\\n@tool(\\""nlp2Sql_adapter\\"",args_schema=NLP_to_SQL_Adapter,return_direct=True)\\r\\ndef nlp2Sql_adapter(query:str)->str:\\r\\n    '''\\r\\n    use to fetch data from database\\r\\n    '''\\r\\n    try:    \\r\\n        logger.info(f\\"" NLP2SQL Method called with query : {query} \\"")\\r\\n        value = True\\r\\n        if value == True:\\r\\n            access_token = os.environ.get('ACCESS_TOKEN')    \\r\\n            headers = {   \\r\\n            'access-token': access_token,   \\r\\n            'accept': 'application/json',   \\r\\n            'Content-Type': 'application/json',    \\r\\n            }    \\r\\n            json_data = {\\""query\\"":query}\\r\\n            url = chatbot_params[\\""nlp2Sql_adapter_url\\""]\\r\\n            response = requests.post(url, headers=headers, json=json_data,verify=False)\\r\\n            logger.info(f\\""API request status {response.status_code}\\"")\\r\\n            user_prompt = '''\\r\\n            \\r\\n            Generate summary for each of the tickets in the given data with ticket number as the heading after that there should summary of each fields of that ticket like Short Description,Created By,Created Date,Assigned To,State.\\r\\n            {data}\\r\\n            \\r\\n            Generated summary for a ticket should look like: \\r\\n            Ticket Number: INC424244\\r\\n            Ticket Details: \\r\\n            This ticket has short description as Process_Invoice_Id:12345, the ticket was created by admin, on Thu, 01 Oct 2020 17:11:11 GMT with high priority and it's been assigned to DAS group and the ticket is currently in open state.\\r\\n            \\r\\n            '''\\r\\n            answer = getFormatedResponse(response.text,user_prompt=user_prompt)\\r\\n            response = {'chat_system_response':answer,\\r\\n                'type': 'Text',\\r\\n                'chat_suggestions':None,\\r\\n                'navigate': {'TicketList': 'aip/tickets/alerts'}\\r\\n                }\\r\\n            return json.dumps(response)\\r\\n        else:\\r\\n            return \\""Unable to refresh DB!!\\""\\r\\n    except Exception as e:\\r\\n        logger.info(f\\""Exception in fetch_details_from_database method : {e} \\"")\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""category"":""tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victcpast-24:2443/infer""}}","LEONLP2S39547"
"Chatbot","Core","{""formats"":{""url"":""text""},""classname"":""ticket_summarization_adapter"",""name"":""Summarization A"",""parentCategory"":""LEOITSMQ55029"",""alias"":""Summarization A"",""id"":""LEOTCKT_75963"",""codeGeneration"":{""requirements"":[],""imports"":[""import json, requests"",""from langchain.tools import tool""],""script"":""class TicketSummarization(BaseModel):\\r    ticketNumber: str =Field(None,description=\\""Ticket number provided by the user\\"")\\r\\r\\r@tool(\\""ticket_summarization_adapter\\"",args_schema=TicketSummarization,return_direct=True)\\rdef ticket_summarization_adapter(ticketNumber:str):\\r    '''\\r    use to get summary about a ticket, takes ticket number as parameter and returns the summary of the ticket\\r\\r    '''\\r    try:\\r\\r        logger.info(f\\""Summary About Ticket method called with ticket no {ticketNumber}\\"")\\r        \\r        global ticket_number\\r        \\r        ticket_number = ticketNumber\\r\\r        access_token = os.environ.get(\\""ACCESS_TOKEN\\"")\\r\\r        headers = {\\r\\r        'access-token': access_token,\\r\\r        'accept': 'application/json',\\r\\r        'Content-Type': 'application/json',\\r\\r        }\\r\\r        json_data = {\\""number\\"":ticketNumber}\\r\\r\\r        url = chatbot_params['ticket_summarization_adapter_url']\\r\\r        response = requests.post(url=url, headers=headers, json=json_data,verify=False)\\r\\r        logger.info(f\\""Response status code {response.status_code}\\"")\\r        \\r        user_prompt = '''\\r        Using the given data, create a summary of the ticket using all it's details and in the final response, the Ticket number should be the heading and then it's summary.\\r        {data}\\r        '''\\r        answer = getFormatedResponse(response.text,user_prompt=user_prompt)\\r\\r        response = {'chat_system_response':answer,\\r\\r            'type': 'Text',\\r\\r            'chat_suggestions':None,\\r            \\r            'navigate': {\\""Summary\\"": f'aip/tickets/create-ticket/{ticketNumber}?tab=Summary'}\\r\\r            }\\r\\r        return json.dumps(response)\\r\\r    except Exception as e:\\r\\r        logger.info(f\\""Exception in ticket_summarization_adapter method : {e} \\"")  \\r\\r\\r\\r\\r\\n""},""category"":""Tools"",""inputEndpoints"":[],""outputEndpoints"":[],""attributes"":{""url"":""http://victlpth5-07.ad.infosys.com:2430/infer""}}","LEOTCKT_75963"
