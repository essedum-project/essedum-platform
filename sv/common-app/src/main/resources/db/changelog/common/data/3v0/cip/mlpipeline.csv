"created_by","created_date","deleted","description","job_id","json_content","lastmodifiedby","alias","lastmodifieddate","name","organization","type","version","tags","interfacetype","pipeline_metadata"
"admin","2022-09-06T06:11:39.738","false","","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"DEMCDGNR69853_Demo.py\"],\"arguments\":[{\"name\":\"scriptPath\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"},{\"name\":\"pipelineName\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"2\"}],\"dataset\":[]}}]}","admin","CodeGeneration_DragandDrop","2022-10-11T05:44:19","DEMCDGNR69853","leo1311","NativeScript","NULL","NULL","NULL","NULL"
"poornasai.nagendra@ad.infosys.com","2023-07-26 07:28:32.796000","\0","","NULL","{""elements"":[{""id"":""XAlzh"",""alias"":""DatasetExtractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-03 12:46:51"",""alias"":""cleantext"",""id"":2774,""name"":""LEACLNTX34353"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\""\\\\\\"" AND   clean_text IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""247"",""position_y"":""52"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""ZJsok"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""ZJsok"",""alias"":""Extract Words"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""Extract_words"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rnltk.download('punkt')\\rnltk.download('stopwords')\\rdef Extract_words( dataset): \\r    dataset = pd.DataFrame(dataset)\\r    def clean_text(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        return alphanumeric  \\r    def tokenizer(text):       \\r        token = nltk.word_tokenize(text)        \\r        tokens = [t for t in token if t.isalpha()]        \\r        tokens = ''.join(tokens)        \\r        return tokens \\r    def remove_stopwords(text):                \\r        words = [t for t in text if t.lower() not in stopwords.words('english')]        \\r        text = ''.join(words)       \\r        return text \\r    def lemmatize_text(text):       \\r        words = text.split() \\r        wordnet_lemmatizer = WordNetLemmatizer()\\r        words = [wordnet_lemmatizer.lemmatize(word, pos = 'v') for word in words]        \\r        return ''.join(words)\\r    def remove_numbers(tokens):        \\r        filter_tokens = []        \\r        for t in tokens:            \\r            if not t.isdigit() and len(t)>2:               \\r                filter_tokens.append(t)        \\r        return filter_tokens     \\r    \\r    dataset['cleanText'] = [clean_text(desc) for desc in dataset['shortdescription']]\\r    dataset['tokens'] = dataset['cleanText'].apply(tokenizer)\\r    dataset['cleanWords'] = dataset['tokens'].apply(remove_stopwords)\\r    dataset['cleanText'] = dataset['cleanWords'].apply(lemmatize_text) \\r    dataset['filteredTokens'] = dataset['cleanText'].apply(remove_numbers)\\r    print(dataset)\\r    dataset = dataset[['filteredTokens','NUMBER']]   \\r    dataset['words'] = dataset.explode('filteredTokens')    \\r    dataset = dataset.drop_duplicates()    \\r    dataset = dataset.groupby(['words']).agg(numberList = pd.NameAgg(column = 'number',aggfunc=list))   \\r    dataset['frequency'] = dataset['numberList'].apply(lambda x: len(x))    \\r    dataset['numberList'] = dataset['numberList'].apply(lambda x: ','.join(x)) \\r    return dataset\\r""]},""position_x"":""509"",""position_y"":""49"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""XAlzh"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""oEWkx"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-03 12:46:51"",""alias"":""cleantext"",""id"":2774,""name"":""LEACLNTX34353"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\""\\\\\\"" AND   clean_text IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""oEWkx"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-16 12:32:45"",""alias"":""InvertedIndex"",""id"":2806,""name"":""LEAINVRT90611"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_invertedindex order by frequency\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_invertedindex\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""704"",""position_y"":""53"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""ZJsok"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""Extract_words"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rnltk.download('punkt')\\rnltk.download('stopwords')\\rdef Extract_words( dataset): \\r    dataset = pd.DataFrame(dataset)\\r    def clean_text(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        return alphanumeric  \\r    def tokenizer(text):       \\r        token = nltk.word_tokenize(text)        \\r        tokens = [t for t in token if t.isalpha()]        \\r        tokens = ''.join(tokens)        \\r        return tokens \\r    def remove_stopwords(text):                \\r        words = [t for t in text if t.lower() not in stopwords.words('english')]        \\r        text = ''.join(words)       \\r        return text \\r    def lemmatize_text(text):       \\r        words = text.split() \\r        wordnet_lemmatizer = WordNetLemmatizer()\\r        words = [wordnet_lemmatizer.lemmatize(word, pos = 'v') for word in words]        \\r        return ''.join(words)\\r    def remove_numbers(tokens):        \\r        filter_tokens = []        \\r        for t in tokens:            \\r            if not t.isdigit() and len(t)>2:               \\r                filter_tokens.append(t)        \\r        return filter_tokens     \\r    \\r    dataset['cleanText'] = [clean_text(desc) for desc in dataset['shortdescription']]\\r    dataset['tokens'] = dataset['cleanText'].apply(tokenizer)\\r    dataset['cleanWords'] = dataset['tokens'].apply(remove_stopwords)\\r    dataset['cleanText'] = dataset['cleanWords'].apply(lemmatize_text) \\r    dataset['filteredTokens'] = dataset['cleanText'].apply(remove_numbers)\\r    print(dataset)\\r    dataset = dataset[['filteredTokens','NUMBER']]   \\r    dataset['words'] = dataset.explode('filteredTokens')    \\r    dataset = dataset.drop_duplicates()    \\r    dataset = dataset.groupby(['words']).agg(numberList = pd.NameAgg(column = 'number',aggfunc=list))   \\r    dataset['frequency'] = dataset['numberList'].apply(lambda x: len(x))    \\r    dataset['numberList'] = dataset['numberList'].apply(lambda x: ','.join(x)) \\r    return dataset\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-03 12:46:51"",""alias"":""cleantext"",""id"":2774,""name"":""LEACLNTX34353"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\""\\\\\\"" AND   clean_text IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","ExtractWords","2023-08-24 04:32:28","LEACLNTC74662","leo1311","DragNDropLite","229","NULL","NULL","{""229"":{""taskId"":""be0f1f95-ba0f-450d-b207-39532aa00ba9""}}"
"poornasai.nagendra@ad.infosys.com","2023-07-26 07:45:46.889000","\0","","NULL","{""elements"":[{""attributes"":{""filetype"":""Jython"",""files"":[""LEACDGNR30505_Leap.py""],""arguments"":[{""name"":""scriptPath"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""},{""name"":""pipelineName"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""2""}],""dataset"":[]}}]}","admin","CodeGeneration_DragNDropLite","2023-08-28 06:45:32","LEACDGNR30505","leo1311","NativeScript","97","NULL","NULL","NULL"
"admin","2023-07-26 10:15:02.259000","\0","NGram","NULL","{""elements"":[{""id"":""ifzHf"",""alias"":""PythonScriptTransformer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PythonScriptTransformer"",""requirements"":"""",""params"":[],""script"":[""\\r"",""import logging\\r"",""from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r"",""import pandas as pd\\r"",""import numpy as np\\r"",""from datetime import datetime\\r"",""\\r"",""\\r"",""import logging\\r"",""from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r"",""import pandas as pd\\r"",""import numpy as np\\r"",""from datetime import datetime\\r"",""\\r"",""def PythonScriptTransformer(dataset):\\r"",""    dataset=pd.DataFrame(dataset)\\r"",""    txt1 =[]\\r"",""    for index, row in dataset.iterrows():\\r"",""        txt1.append(row['clean_text'])\\r"",""    logging.info('Generating NGrams')\\r"",""    # Getting trigrams \\r"",""    vectorizer = CountVectorizer(ngram_range = (3,3))\\r"",""    X1 = vectorizer.fit_transform(txt1) \\r"",""    features = (vectorizer.get_feature_names_out())\\r"",""    # Applying TFIDF\\r"",""    vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r"",""    X2 = vectorizer.fit_transform(txt1)\\r"",""    scores = (X2.toarray())\\r"",""    # Getting top ranking features\\r"",""    logging.info('Getting Top 50 Grams')\\r"",""    sums = X2.sum(axis = 0)\\r"",""    data1 = []\\r"",""    for col, term in enumerate(features):\\r"",""        data1.append( (term, sums[0,col] ))\\r"",""    ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r"",""    words = (ranking.sort_values('rank', ascending = False))\\r"",""    top50df = words.nlargest(50,'rank')\\r"",""    top50 = words['term'].tolist()\\r"",""    \\r"",""    distinctText = list(set(txt1))\\r"",""    count=0\\r"",""    totalRecords = len(distinctText)\\r"",""    logging.info('Mapping Text to Gram')\\r"",""    logging.info('Total Unique Values: {0}'.format(totalRecords))\\r"",""    ngramDict = {}\\r"",""    for item in distinctText:\\r"",""        matchingGrams = [gram for gram in top50 if gram in item]\\r"",""        if len(matchingGrams) >0:\\r"",""            ngramDict[item] = matchingGrams[0]\\r"",""        count = count+1\\r"",""        if count%1000 ==0:\\r"",""            logging.info('{0} rows mapped'.format(count))\\r"",""\\r"",""    logging.info('Mapping Tickets')\\r"",""    \\r"",""    dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r"",""    \\r"",""    return dataset\\r"",""\\r"",""\\r"",""\\r"",""\\r""]},""position_x"":""432"",""position_y"":""160"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""mmoHr"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 13:47:21"",""alias"":""Input_NGram"",""id"":2771,""name"":""LEAINPT_46654"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""Select number, CAST(clean_text as CHAR) as clean_text from leo1311_tickets_enriched where clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""mmoHr"",""alias"":""DatasetExtractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 13:47:21"",""alias"":""Input_NGram"",""id"":2771,""name"":""LEAINPT_46654"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""Select number, CAST(clean_text as CHAR) as clean_text from leo1311_tickets_enriched where clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""185"",""position_y"":""158"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""ifzHf"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","NGram","2023-08-23 10:22:48","LEANGRMY96622","leo1311","DragNDropLite","55","""""","NULL","{""55"":{""taskId"":""70b3e569-380d-4f9e-982e-416a7c408d99""}}"
"admin","2023-07-26 12:15:26.768000","\0","Predict Assignee Group and Assignee for the ticket","NULL","{""elements"":[{""id"":""DckAM"",""alias"":""PreProcessingScript"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PreProcessingScript"",""requirements"":"" "",""params"":"""",""script"":[""\\r"",""import logging\\r"",""import pandas \\r"",""\\r"",""def PreProcessingScript(dataset1,dataset2,incidentid_param=''):\\r"",""    incidentId=incidentid_param\\r"",""    print('Running for incident '+ incidentId)\\r"",""    dataset1_df = pd.DataFrame(dataset1)\\r"",""    dataset2_df = pd.DataFrame(dataset2)\\r"",""\\r"",""    if incidentId !='':\\r"",""        dataset1_df = dataset1_df[(dataset1_df['number'] == incidentId)]\\r"",""    dataset=pd.merge(dataset1_df, dataset2_df, on='configurationItem')\\r"",""    print(dataset)\\r"",""    return dataset\\r"",""\\r"",""\\r"",""\\r""]},""position_x"":""400"",""position_y"":""183"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""FMhJf"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""Wrumo"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""OFJxV"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-26 14:46:31"",""alias"":""Assignment Config"",""id"":2717,""name"":""LEAASGNM37501"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_assignment_config\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-31 12:34:55"",""alias"":""unassigned tickets"",""id"":2718,""name"":""LEAUNSGN76884"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, configurationItem FROM leo1311_tickets\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""Wrumo"",""alias"":""Assignment config"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-26 14:46:31"",""alias"":""Assignment Config"",""id"":2717,""name"":""LEAASGNM37501"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_assignment_config\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""143"",""position_y"":""132"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""DckAM"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""FMhJf"",""alias"":""PythonScriptTransformer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":"""",""requirements"":"""",""params"":"""",""script"":[""import logging\\r"",""import pandas\\r"",""\\r"",""def PythonScriptTransformer(dataset):\\r"",""    dataset = dataset[['number','assignmentgroup']].rename(columns={'assignmentgroup':'predicted_assignment_group'})\\r"",""    return dataset\\r"",""\\r""]},""position_x"":""648"",""position_y"":""183"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""DckAM"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""FunctionName"":""PreProcessingScript"",""requirements"":"" "",""params"":"""",""script"":[""\\r"",""import logging\\r"",""import pandas \\r"",""\\r"",""def PreProcessingScript(dataset1,dataset2,incidentid_param=''):\\r"",""    incidentId=incidentid_param\\r"",""    print('Running for incident '+ incidentId)\\r"",""    dataset1_df = pd.DataFrame(dataset1)\\r"",""    dataset2_df = pd.DataFrame(dataset2)\\r"",""\\r"",""    if incidentId !='':\\r"",""        dataset1_df = dataset1_df[(dataset1_df['number'] == incidentId)]\\r"",""    dataset=pd.merge(dataset1_df, dataset2_df, on='configurationItem')\\r"",""    print(dataset)\\r"",""    return dataset\\r"",""\\r"",""\\r"",""\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-26 14:46:31"",""alias"":""Assignment Config"",""id"":2717,""name"":""LEAASGNM37501"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_assignment_config\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-31 12:34:55"",""alias"":""unassigned tickets"",""id"":2718,""name"":""LEAUNSGN76884"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, configurationItem FROM leo1311_tickets\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""OFJxV"",""alias"":""unassigned tickets"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-31 12:34:55"",""alias"":""unassigned tickets"",""id"":2718,""name"":""LEAUNSGN76884"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, configurationItem FROM leo1311_tickets\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""227"",""position_y"":""327"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""DckAM"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","Auto Assignment","2023-07-31 13:13:57","LEAATSGN97511","leo1311","DragNDropLite","55","""""","NULL","{""55"":{""taskId"":""978e7bd8-1623-4b24-b0ec-856c16bdc48d""}}"
"admin","2023-07-26 12:44:28.214000","\0","Map Key Phrases","NULL","{""elements"":[{""id"":""yFLrc"",""alias"":""Extracted Phrases"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""Phrases"",""name"":""X3WC6MZTYS"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT DISTINCT TRIM(extracted_phrase) as phrase from leo1311_phrase_extraction where extracted_phrase is not null and extracted_phrase <> '' "",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_phrase_extraction"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""234"",""position_y"":""30"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""YtDzC"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""CaOML"",""alias"":""Keywords"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""EASE Keywords"",""name"":""X6S9LXRB33"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":null,""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT DISTINCT Key_Word from leo1311_EASEMapping where Key_Word is not null and Key_Word <> ''"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_EASEMapping"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":"""",""organization"":""leo1311""}},""position_x"":""239"",""position_y"":""129"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""YtDzC"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""YtDzC"",""alias"":""ScriptTransformer"",""name"":""ScriptTransformer"",""classname"":""ScriptTransformerConfig"",""category"":""TransformerConfig"",""attributes"":{""requirements"":"""",""script"":[""import pandas as pd \\r"",""def getSimilar(phrases, keywords):\\r"",""    keywordstartIndex = len(phrases)\\r"",""    tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r"",""    pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r"",""    arr = pairwise_similarity.toarray()\\r"",""    np.fill_diagonal(arr, np.nan)\\r"",""    results = {}\\r"",""    for s in phrases:\\r"",""        input_idx = phrases.index(s)\\r"",""        result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r"",""        match = arr[input_idx][keywordstartIndex + result_idx]\\r"",""        r = keywords[result_idx]\\r"",""        if match > 0:\\r"",""            results[s] = r + ':' + str(match)\\r"",""        else:\\r"",""            results[s] = 'NO MATCH:0'\\r"",""            \\r"",""    dataset = []\\r"",""    for extracted_phrase in results.keys():\\r"",""        mapped_phrase = results[extracted_phrase].split(':')[0]\\r"",""        mapped_phrase_confidennce = results[extracted_phrase].split(':')[-1]\\r"",""        if mapped_phrase != 'NO MATCH':\\r"",""            dataset.append((mapped_phrase, mapped_phrase_confidennce, extracted_phrase))\\r"",""        #print(dataset)\\r"",""    resultdf=pd.DataFrame(dataset,column = [mapped_phrase,mapped_phrase_confidennce,extracted_phrase])\\r"",""    rows=[]\\r"",""    rows.append({\\r"",""                'mapped_phrase': resultdf['term'],\\r"",""                'extracted_phrase': resultdf['extracted_phrase'],\\r"",""                'extracted_phrase':resultdf['extracted_phrase']\\r"",""                })\\r"",""    return rows\\r""]},""position_x"":""492"",""position_y"":""127"",""connectors"":[{""type"":""target"",""endpoint"":""in2"",""position"":""TopCenter"",""elementId"":""yFLrc"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""in1"",""position"":""LeftMiddle"",""elementId"":""CaOML"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""dAsHV"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""requirements"":""text"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""""},""context"":[{""dataset"":{""alias"":""Phrases"",""name"":""X3WC6MZTYS"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT DISTINCT TRIM(extracted_phrase) as phrase from leo1311_phrase_extraction where extracted_phrase is not null and extracted_phrase <> '' "",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_phrase_extraction"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},{""dataset"":{""alias"":""EASE Keywords"",""name"":""X6S9LXRB33"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":null,""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT DISTINCT Key_Word from leo1311_EASEMapping where Key_Word is not null and Key_Word <> ''"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_EASEMapping"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":"""",""organization"":""leo1311""}}]},{""id"":""dAsHV"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""alias"":""Mappings"",""name"":""S5V8F2IES1"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select * from MappingsDLite"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""overwrite"",""params"":""{}"",""tableName"":""MappingsDLite"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""715"",""position_y"":""125"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""YtDzC"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""requirements"":"""",""script"":[""import pandas as pd \\r"",""def getSimilar(phrases, keywords):\\r"",""    keywordstartIndex = len(phrases)\\r"",""    tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r"",""    pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r"",""    arr = pairwise_similarity.toarray()\\r"",""    np.fill_diagonal(arr, np.nan)\\r"",""    results = {}\\r"",""    for s in phrases:\\r"",""        input_idx = phrases.index(s)\\r"",""        result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r"",""        match = arr[input_idx][keywordstartIndex + result_idx]\\r"",""        r = keywords[result_idx]\\r"",""        if match > 0:\\r"",""            results[s] = r + ':' + str(match)\\r"",""        else:\\r"",""            results[s] = 'NO MATCH:0'\\r"",""            \\r"",""    dataset = []\\r"",""    for extracted_phrase in results.keys():\\r"",""        mapped_phrase = results[extracted_phrase].split(':')[0]\\r"",""        mapped_phrase_confidennce = results[extracted_phrase].split(':')[-1]\\r"",""        if mapped_phrase != 'NO MATCH':\\r"",""            dataset.append((mapped_phrase, mapped_phrase_confidennce, extracted_phrase))\\r"",""        #print(dataset)\\r"",""    resultdf=pd.DataFrame(dataset,column = [mapped_phrase,mapped_phrase_confidennce,extracted_phrase])\\r"",""    rows=[]\\r"",""    rows.append({\\r"",""                'mapped_phrase': resultdf['term'],\\r"",""                'extracted_phrase': resultdf['extracted_phrase'],\\r"",""                'extracted_phrase':resultdf['extracted_phrase']\\r"",""                })\\r"",""    return rows\\r""]},{""dataset"":{""alias"":""Phrases"",""name"":""X3WC6MZTYS"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT DISTINCT TRIM(extracted_phrase) as phrase from leo1311_phrase_extraction where extracted_phrase is not null and extracted_phrase <> '' "",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_phrase_extraction"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},{""dataset"":{""alias"":""EASE Keywords"",""name"":""X6S9LXRB33"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":null,""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT DISTINCT Key_Word from leo1311_EASEMapping where Key_Word is not null and Key_Word <> ''"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_EASEMapping"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":"""",""organization"":""leo1311""}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","KeyPhraseMapping","2023-08-08 14:41:00","LEAKYPHR58901","leo1311","DragNDropLite","14","""""","NULL","{""14"":{""taskId"":""d4f076d1-7d9c-46af-a808-d69e16294c0e""}}"
"admin","2023-07-26 15:21:22.828000","\0","Pick Final Cluster based on Prioritization","NULL","{""elements"":[{""id"":""OQZdi"",""alias"":""Dataset  Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-27 04:19:52"",""alias"":""LDA Clusters"",""id"":2721,""name"":""LEALDCLS65715"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT alias AS Cluster , COUNT(*) AS COUNT FROM leo1311_LDA Clusters  te JOIN @projectname_clustering_topics t ON te.lda_cluster = t.words WHERE lda_cluster IS NOT NULL AND lda_cluster <> ' ' GROUP BY lda_cluster ORDER BY COUNT(*) DESC limit 10\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""184"",""position_y"":""85"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""ZMuXg"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""CetfF"",""alias"":""Python  Script  Transformer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PythonScriptTransformer"",""requirements"":"""",""params"":"""",""script"":[""import pandas\\r"",""def PythonScriptTransformer(dataset):\\r"",""      def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r"",""            cluster = ''\\r"",""            if tags is not None and tags != '':\\r"",""                cluster = tags\\r"",""            elif ngram is not None and ngram != '':\\r"",""                cluster = ngram\\r"",""            elif soundex_cluster is not None and soundex_cluster != '':\\r"",""                cluster = soundex_cluster\\r"",""            elif mapped_phrase is not None and mapped_phrase != '':\\r"",""                cluster = mapped_phrase\\r"",""            elif lda_cluster is not None and lda_cluster != '':\\r"",""                cluster = lda_cluster\\r"",""            return cluster\\r"",""    \\r"",""            dataset = dataset.assign(post_ranking_cluster = lambda row: getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase']))\\r"",""            dataset['last_updated'] = pd.to_datetime(datetime.now())\\r"",""            dataset = dataset[['number','ngram','soundex_cluster','lda_cluster','mapped_phrase','last_updated','post_ranking_cluster']]\\r"",""            return dataset\\r"",""\\r"",""\\r""]},""position_x"":""814"",""position_y"":""235"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""ZMuXg"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""requirements"":"""",""script"":[""\\rimport logging as logger\\rimport pandas as pd\\r\\rdef PreProcessingScript(dataset1, dataset2, dataset3):\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    dataset3 = pd.DataFrame(dataset3)\\r    dataset  = pd.merge(dataset1,dataset2, on='number',how='left')\\r    dataset  = pd.merge(dataset,dataset3, on='number',how='left')\\r    return dataset\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-27 04:19:52"",""alias"":""LDA Clusters"",""id"":2721,""name"":""LEALDCLS65715"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT alias AS Cluster , COUNT(*) AS COUNT FROM leo1311_LDA Clusters  te JOIN @projectname_clustering_topics t ON te.lda_cluster = t.words WHERE lda_cluster IS NOT NULL AND lda_cluster <> ' ' GROUP BY lda_cluster ORDER BY COUNT(*) DESC limit 10\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-31 17:02:49"",""alias"":""Clusters"",""id"":2731,""name"":""LEACLSTR66680"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT cast(t.number as char) AS number, tags AS tags, ng.ngram, sound.soundex_cluster FROM leo1311_tickets t LEFT JOIN leo1311_ngram ng ON t.number=cast(ng.number as char) LEFT JOIN leo1311_soundex sound ON t.number=cast(sound.number as char)\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-01 04:57:45"",""alias"":""Tickets_Gen"",""id"":2768,""name"":""LEATCKTS19697"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\""CLOSED\\\\\\"", \\\\\\""RESOLVED\\\\\\"") \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""ZMuXg"",""alias"":""PreProcessingScript"",""name"":""Pre Processing Script"",""classname"":""PreProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""requirements"":"""",""script"":[""\\rimport logging as logger\\rimport pandas as pd\\r\\rdef PreProcessingScript(dataset1, dataset2, dataset3):\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    dataset3 = pd.DataFrame(dataset3)\\r    dataset  = pd.merge(dataset1,dataset2, on='number',how='left')\\r    dataset  = pd.merge(dataset,dataset3, on='number',how='left')\\r    return dataset\\r""]},""position_x"":""498"",""position_y"":""235"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""OQZdi"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""CetfF"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""XpXaX"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset3"",""position"":""BottomCenter"",""elementId"":""ETokA"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""requirements"":""textarea"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-27 04:19:52"",""alias"":""LDA Clusters"",""id"":2721,""name"":""LEALDCLS65715"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT alias AS Cluster , COUNT(*) AS COUNT FROM leo1311_LDA Clusters  te JOIN @projectname_clustering_topics t ON te.lda_cluster = t.words WHERE lda_cluster IS NOT NULL AND lda_cluster <> ' ' GROUP BY lda_cluster ORDER BY COUNT(*) DESC limit 10\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-31 17:02:49"",""alias"":""Clusters"",""id"":2731,""name"":""LEACLSTR66680"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT cast(t.number as char) AS number, tags AS tags, ng.ngram, sound.soundex_cluster FROM leo1311_tickets t LEFT JOIN leo1311_ngram ng ON t.number=cast(ng.number as char) LEFT JOIN leo1311_soundex sound ON t.number=cast(sound.number as char)\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-01 04:57:45"",""alias"":""Tickets_Gen"",""id"":2768,""name"":""LEATCKTS19697"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\""CLOSED\\\\\\"", \\\\\\""RESOLVED\\\\\\"") \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""XpXaX"",""alias"":""DatasetExtractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-07-31 17:02:49"",""alias"":""Clusters"",""id"":2731,""name"":""LEACLSTR66680"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT cast(t.number as char) AS number, tags AS tags, ng.ngram, sound.soundex_cluster FROM leo1311_tickets t LEFT JOIN leo1311_ngram ng ON t.number=cast(ng.number as char) LEFT JOIN leo1311_soundex sound ON t.number=cast(sound.number as char)\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""187"",""position_y"":""236"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""ZMuXg"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""ETokA"",""alias"":""DatasetExtractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-01 04:57:45"",""alias"":""Tickets_Gen"",""id"":2768,""name"":""LEATCKTS19697"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\""CLOSED\\\\\\"", \\\\\\""RESOLVED\\\\\\"") \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""190"",""position_y"":""367"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""ZMuXg"",""elementPosition"":""BottomCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","Cluster Prioritization","2023-08-17 10:47:07","LEACLSTR81610","leo1311","DragNDropLite","66","""""","NULL","{""66"":{""taskId"":""91e81bed-a7ed-4d6a-a97b-a51ac02a629d""}}"
"admin","2023-07-27 04:30:03.927000","\0","Takes incident Workflow name and executes corresponding resolution on Script","NULL","{""elements"":[{""id"":""apVuI"",""alias"":""Pre  Processing  Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PreProcessingScript"",""requirements"":"""",""params"":[{""name"":""incidentid"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""},{""name"":""workflowname"",""value"":""FacebookInsights"",""type"":""Text"",""alias"":""FacebookInsights"",""index"":""2""}],""script"":[""\\r"",""import pandas\\r"",""\\r"",""def PreProcessingScript(dataset,incidentid_param='',workflowname_param=''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    incidentid=incidentid_param\\r"",""    workflow=workflowname_param\\r"",""    print('Running for incident '+ incidentid)\\r"",""    print('workflow' + workflow)\\r"",""    if incidentid !='':\\r"",""        dataset = dataset[dataset['number'] == incidentid]\\r"",""        dataset['workflow_name'] = workflow\\r"",""        return dataset\\r"",""\\r""]},""position_x"":""504"",""position_y"":""197"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""PdtoP"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""OTFBf"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-01 04:57:45"",""alias"":""Tickets_Gen"",""id"":2768,""name"":""LEATCKTS19697"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\""CLOSED\\\\\\"", \\\\\\""RESOLVED\\\\\\"") \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""PdtoP"",""alias"":""Execute workflow"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""Executeworkflow"",""requirements"":"""",""params"":"""",""script"":[""from leaputils import Security\\r"",""import requests\\r"",""import json\\r"",""import os\\r"",""import pandas\\r"",""from datetime import datetime\\r"",""\\r"",""def Executeworkflow (dataset):\\r"",""    def parentresolve(ticket,workflow,sysid):\\r"",""        out = []\\r"",""        ticket,workflow,sysid = ticket.tolist(),workflow.tolist(),sysid.tolist()\\r"",""        for i in range(len(ticket)):\\r"",""            value = resolve(ticket[i],workflow[i],sysid[i])\\r"",""            if value is None:\\r"",""                value = ''\\r"",""            out.append(value)\\r"",""        return out\\r"",""    def resolve(ticket,workflow,sysid):\\r"",""        if workflow == 'FacebookInsights':\\r"",""            try:\\r"",""                ####----Congigure rundeck api  URL/params/headers corresponding to workflow and return response from api\\r"",""                url= 'http://myzul02u:4440/api/28/job/1a0ccbeb-887d-4c6c-9a7e-7bc3193bf545/executions?authtoken=MQqGyiIIRyUynjavzHWVgovg96qi3C3K' \\r"",""                data={'argString': '-IncidentSystemID '+sysid }\\r"",""                #Headers required by rundeck Api\\r"",""                headers={'Accept':'application/json'}\\r"",""                response=requests.post(url,data=data,headers=headers,verify=False)\\r"",""                #return response.text.encode('utf8')\\r"",""                return 'Workflow Executed for IncidentId:' + ticket + ' ' +str(response.text.encode('utf8'))\\r"",""            except:\\r"",""                return 'workflow execution error'+ ticket\\r"",""        elif workflow == 'CancelShipment':\\r"",""            try:\\r"",""                url= 'http://victadpst-10:6032/api/workflow/saveworkflowinputdata'\\r"",""                InputJson = [{'IncidentId': ticket}]\\r"",""                data={\\r"",""                    'ScheduleID': 1,\\r"",""                        'WorkFlowID': '2',\\r"",""                        'WorkFlowName': 'Shipment Cancel',\\r"",""                        'ServerName': 'victadpst-10',\\r"",""                        'AgentList': [5],\\r"",""                        'InputJson': json.dumps(InputJson)\\r"",""                    }\\r"",""\\r"",""            #Headers required by rundeck Api\\r"",""                headers={'Content-Type':'application/json' }\\r"",""                response=requests.post(url,auth=('admin','ZTLiPW70rwTT/6VoFmw0zg=='), data=json.dumps(data),headers=headers)\\r"",""                return 'Workflow Executed for IncidentId:' + ticket + ' ' +str(response.text.encode('utf8'))\\r"",""            except:\\r"",""                return 'Workflow execution error'\\r"",""        elif workflow == 'ProcessInvoice':\\r"",""            try:\\r"",""                url= 'https://infosysq3dev1.service-now.com/api/now/table/incident/'+sysid \\r"",""                proxyDict = {\\r"",""                'http'  : os.environ['HTTP_PROXY'],\\r"",""                'https' : os.environ['HTTPS_PROXY']            }\\r"",""                data=json.dumps({'state':'7','close_code':'Closed by LEAP','caller_id':'6816f79cc0a8016401c5a33be04be441'})\\r"",""                #Headers required by rundeck Api\\r"",""                headers={'Content-Type':'application/json'}\\r"",""                pwd=Security.decrypt('enc36lBo7boFl9A7QUQ86Tq/YWPxM4/+6Oj', 'ay0lj4Po/1FsrtKnrODl+Pk3yfkG7gLDzeCOtJRwpXRnIbnLtrHoDE7ktm3mmgq3MjGSJZ+ojMxtynt4BqHJwg==') \\r"",""                pwd='qwer1234'\\r"",""                response=requests.patch(url,auth=('ICSP_icap_user', pwd),proxies=proxyDict,data=data,headers=headers,verify=False)\\r"",""                #return response.text.encode('utf8')\\r"",""                return 'Workflow Executed for IncidentId:' + ticket + ' ' +str(response.text.encode('utf8'))\\r"",""            except:\\r"",""                return 'Workflow execution error'\\r"",""        else:\\r"",""            return 'No API configured for workflow'\\r"",""        \\r"",""    \\r"",""    # dfresolve = dataset.apply(resolve).astype(str)\\r"",""    # dataset['apiResponse']=dataset.apply(lambda row:dfresolve(row['number'],row['workflow_name'],row['sysId']),axis=1)\\r"",""    # dataset['apiResponse']=dataset.apply(lambda row: resolve(row['number'],row['workflow_name'],row['sysId']))\\r"",""    #dataset['apiResponse']=dataset.apply(lambda row: resolve(row['number'],row['workflow_name'],row['sysId']))\\r"",""    dataset = dataset.assign(apiResponse = lambda row: parentresolve(row['number'],row['workflow_name'],row['sysId']))\\r"",""    #dataset = dataset.assign(apiResponse = lambda row: resolve(row['number'],row['workflow_name'],row['sysId']))\\r"",""    dataset['run_timestamp'] = pd.to_datetime(datetime.now())\\r"",""    dataset.head()\\r"",""    \\r"",""    return dataset\\r"",""\\r"",""\\r""]},""position_x"":""819"",""position_y"":""198"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""apVuI"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""PreProcessingScript"",""requirements"":"""",""params"":[{""name"":""incidentid"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""},{""name"":""workflowname"",""value"":""FacebookInsights"",""type"":""Text"",""alias"":""FacebookInsights"",""index"":""2""}],""script"":[""\\r"",""import pandas\\r"",""\\r"",""def PreProcessingScript(dataset,incidentid_param='',workflowname_param=''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    incidentid=incidentid_param\\r"",""    workflow=workflowname_param\\r"",""    print('Running for incident '+ incidentid)\\r"",""    print('workflow' + workflow)\\r"",""    if incidentid !='':\\r"",""        dataset = dataset[dataset['number'] == incidentid]\\r"",""        dataset['workflow_name'] = workflow\\r"",""        return dataset\\r"",""\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-01 04:57:45"",""alias"":""Tickets_Gen"",""id"":2768,""name"":""LEATCKTS19697"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\""CLOSED\\\\\\"", \\\\\\""RESOLVED\\\\\\"") \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""OTFBf"",""alias"":""DatasetExtractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-01 04:57:45"",""alias"":""Tickets_Gen"",""id"":2768,""name"":""LEATCKTS19697"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\""CLOSED\\\\\\"", \\\\\\""RESOLVED\\\\\\"") \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""224"",""position_y"":""196"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""apVuI"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","GenericWorkflowResolver","2023-08-23 10:28:45","LEAGNRCW43683","leo1311","DragNDropLite","125","""""","NULL","{""125"":{""taskId"":""09d08e96-181a-4a0c-aefd-45ec1e6fc913""}}"
"admin","2023-07-27 05:16:24.387000","\0","","NULL","{""elements"":[{""id"":""yMfUR"",""alias"":""dataset extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""cleanText"",""name"":""BB4Z1MPQOM"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-26 07:22:02"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enczgutPsHhVctC8gmUqm/dg2jROmKw8L56\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""ya4r3lodgB7JsURuLGwvZUOdqSG6fiD7rSjzh/pOL0G8X5u1kVJ+QK15WYgeVCrvrvsga8TktILVx98XOVPChQ=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-26 07:22:01"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""r"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select * from @projectname_tickets"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident  }"",""tableName"":""@projectname_tickets"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""227"",""position_y"":""36"",""connectors"":[],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}}}]}","admin","SNOW_Update_api","2023-07-27 05:50:42","LEASNW_P24496","leo1311","DragNDropLite","0","""""","legacy","NULL"
"poornasai.nagendra@ad.infosys.com","2023-07-27 06:07:48.383000","\0","","NULL","{""elements"":[{""id"":""sYFjt"",""alias"":""LeapCreateTicket"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""leap_create_ticket"",""requirements"":"""",""params"":[{""name"":""leapdatasource"",""value"":""leo1311"",""type"":""Text"",""alias"":""leo1311"",""index"":""1""},{""name"":""datatable"",""value"":""leo1311_tickets"",""type"":""Text"",""alias"":""leo1311_tickets"",""index"":""2""},{""name"":""tickettype"",""value"":""Incident"",""type"":""Text"",""alias"":""Incident"",""index"":""3""},{""name"":""incidentpayload"",""value"":""{\\""shortdescription\\"":\\""test local create\\"",\\""priority\\"":{\\""displayValue\\"":\\""5\\"",\\""systemId\\"":\\""5\\""},\\""state\\"":{\\""displayValue\\"":\\""New\\"",\\""systemId\\"":\\""1\\""}, \\""category\\"":{\\""displayValue\\"":\\""Inquiry / Help\\"",\\""systemId\\"":\\""inquiry\\""},\\""impact\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""}, \\""urgency\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""},\\""configurationitem\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""description\\"":null,\\""assignmentgroup\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""assignedto\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""},\\""sop\\"":\\""Facebook Error\\"", \\""resolutionStepsClusterName\\"":\\""FacebookInsights\\""}"",""type"":""Text"",""alias"":""{\\""shortdescription\\"":\\""test local create\\"",\\""priority\\"":{\\""displayValue\\"":\\""5\\"",\\""systemId\\"":\\""5\\""},\\""state\\"":{\\""displayValue\\"":\\""New\\"",\\""systemId\\"":\\""1\\""}, \\""category\\"":{\\""displayValue\\"":\\""Inquiry / Help\\"",\\""systemId\\"":\\""inquiry\\""},\\""impact\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""}, \\""urgency\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""},\\""configurationitem\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""description\\"":null,\\""assignmentgroup\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""assignedto\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""},\\""sop\\"":\\""Facebook Error\\"", \\""resolutionStepsClusterName\\"":\\""FacebookInsights\\""}"",""index"":""4""}],""script"":[""import sys\\rimport os\\rimport mysql.connector\\rimport json\\rfrom datetime import datetime\\rfrom leaputils import Security\\rimport logging as logger\\rfrom datetime import timezone\\rfrom urllib.parse import quote_plus, urlparse\\r\\rlogger.basicConfig(level=logger.INFO,format='%(asctime)s INFO %(message)s', datefmt='%y/%m/%d %H:%M:%S')\\r\\rdef leap_create_ticket(leapdatasource, incidentpayload_param='', tickettype_param='', datatable_param='', leapdatasource_param=''):\\r    argdict={'incidentPayload': incidentpayload_param,\\r    'ticketType':tickettype_param,\\r    'dataTable':datatable_param,\\r    'LEAPDataSource':leapdatasource\\r    }\\r    icmPayload = json.loads(argdict['incidentPayload'])\\r\\r    icmPayloadMapping = {'number':'number','shortdescription':'shortdescription','priority.displayValue':'priority','state.displayValue':'state',\\r                            'description':'description','category.displayValue':'category','impact.displayValue':'impact',\\r                            'assignmentgroup.displayValue':'assignmentgroup','configurationitem.displayValue':'configurationItem','assignedto.displayValue':'assignedto',\\r                            'sop':'sop','resolutionStepsClusterName':'workflow', 'caller.displayValue':'caller'}\\r    #mapColumns\\r    foundColumns = []\\r    row = {}\\r    for key in icmPayloadMapping.keys():\\r        jv = icmPayload\\r        icmColumn = key.split('.')\\r        for item in icmColumn:\\r            try:\\r                jv = jv[item]\\r                row[icmPayloadMapping[key]] = jv\\r            except:\\r                a='No mapping'\\r\\r    row['number'] = 'INC{0}'.format(int(datetime.now().replace(tzinfo=timezone.utc).timestamp()))\\r    row['source'] = 'LEAP'\\r    row['type'] = argdict['ticketType']\\r    row['lastUpdated']=datetime.now()\\r    row['openedDate']=datetime.now()\\r    row['updatedDate']=datetime.now()\\r    row['createdDate']=datetime.now()\\r    row['createdby'] = 'admin'\\r    print('Incident number is: ', row['number'])\\r\\r    return [row]\\r\\r    \\r\\r\\r\\r""]},""position_x"":""497"",""position_y"":""139"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""SDEcI"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""wbrpI"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""wbrpI"",""alias"":""DataSourcedict"",""name"":""DataSourcedict"",""classname"":""DatasetSourceConfig"",""category"":""DatasetSourceConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""195"",""position_y"":""141"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""sYFjt"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out1""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DataSourcedictREST(dataset_datasource_Url='', dataset_datasource_AuthDetails_username='', dataset_datasource_salt='', dataset_datasource_AuthDetails_password=''):\\r\\n    DSdict = {\\r\\n        'Url': dataset_datasource_Url,\\r\\n        'salt': dataset_datasource_salt,\\r\\n        'AuthDetails': {\\r\\n            'username': dataset_datasource_AuthDetails_username,\\r\\n            'password': dataset_datasource_AuthDetails_password\\r\\n        }\\r\\n    }\\r\\n    return DSdict\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""context"":[]},{""id"":""SDEcI"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-17 22:19:09"",""alias"":""Leap_Create_Ticket"",""id"":2808,""name"":""LEALP_CR80340"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_tickets;\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""786"",""position_y"":""141"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""sYFjt"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""leap_create_ticket"",""requirements"":"""",""params"":[{""name"":""leapdatasource"",""value"":""leo1311"",""type"":""Text"",""alias"":""leo1311"",""index"":""1""},{""name"":""datatable"",""value"":""leo1311_tickets"",""type"":""Text"",""alias"":""leo1311_tickets"",""index"":""2""},{""name"":""tickettype"",""value"":""Incident"",""type"":""Text"",""alias"":""Incident"",""index"":""3""},{""name"":""incidentpayload"",""value"":""{\\""shortdescription\\"":\\""test local create\\"",\\""priority\\"":{\\""displayValue\\"":\\""5\\"",\\""systemId\\"":\\""5\\""},\\""state\\"":{\\""displayValue\\"":\\""New\\"",\\""systemId\\"":\\""1\\""}, \\""category\\"":{\\""displayValue\\"":\\""Inquiry / Help\\"",\\""systemId\\"":\\""inquiry\\""},\\""impact\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""}, \\""urgency\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""},\\""configurationitem\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""description\\"":null,\\""assignmentgroup\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""assignedto\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""},\\""sop\\"":\\""Facebook Error\\"", \\""resolutionStepsClusterName\\"":\\""FacebookInsights\\""}"",""type"":""Text"",""alias"":""{\\""shortdescription\\"":\\""test local create\\"",\\""priority\\"":{\\""displayValue\\"":\\""5\\"",\\""systemId\\"":\\""5\\""},\\""state\\"":{\\""displayValue\\"":\\""New\\"",\\""systemId\\"":\\""1\\""}, \\""category\\"":{\\""displayValue\\"":\\""Inquiry / Help\\"",\\""systemId\\"":\\""inquiry\\""},\\""impact\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""}, \\""urgency\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""},\\""configurationitem\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""description\\"":null,\\""assignmentgroup\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""assignedto\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""},\\""sop\\"":\\""Facebook Error\\"", \\""resolutionStepsClusterName\\"":\\""FacebookInsights\\""}"",""index"":""4""}],""script"":[""import sys\\rimport os\\rimport mysql.connector\\rimport json\\rfrom datetime import datetime\\rfrom leaputils import Security\\rimport logging as logger\\rfrom datetime import timezone\\rfrom urllib.parse import quote_plus, urlparse\\r\\rlogger.basicConfig(level=logger.INFO,format='%(asctime)s INFO %(message)s', datefmt='%y/%m/%d %H:%M:%S')\\r\\rdef leap_create_ticket(leapdatasource, incidentpayload_param='', tickettype_param='', datatable_param='', leapdatasource_param=''):\\r    argdict={'incidentPayload': incidentpayload_param,\\r    'ticketType':tickettype_param,\\r    'dataTable':datatable_param,\\r    'LEAPDataSource':leapdatasource\\r    }\\r    icmPayload = json.loads(argdict['incidentPayload'])\\r\\r    icmPayloadMapping = {'number':'number','shortdescription':'shortdescription','priority.displayValue':'priority','state.displayValue':'state',\\r                            'description':'description','category.displayValue':'category','impact.displayValue':'impact',\\r                            'assignmentgroup.displayValue':'assignmentgroup','configurationitem.displayValue':'configurationItem','assignedto.displayValue':'assignedto',\\r                            'sop':'sop','resolutionStepsClusterName':'workflow', 'caller.displayValue':'caller'}\\r    #mapColumns\\r    foundColumns = []\\r    row = {}\\r    for key in icmPayloadMapping.keys():\\r        jv = icmPayload\\r        icmColumn = key.split('.')\\r        for item in icmColumn:\\r            try:\\r                jv = jv[item]\\r                row[icmPayloadMapping[key]] = jv\\r            except:\\r                a='No mapping'\\r\\r    row['number'] = 'INC{0}'.format(int(datetime.now().replace(tzinfo=timezone.utc).timestamp()))\\r    row['source'] = 'LEAP'\\r    row['type'] = argdict['ticketType']\\r    row['lastUpdated']=datetime.now()\\r    row['openedDate']=datetime.now()\\r    row['updatedDate']=datetime.now()\\r    row['createdDate']=datetime.now()\\r    row['createdby'] = 'admin'\\r    print('Incident number is: ', row['number'])\\r\\r    return [row]\\r\\r    \\r\\r\\r\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","poornasai.nagendra@ad.infosys.com","Leap_Create_Ticket","2023-08-28 06:17:51","LEALP_CR30379","leo1311","DragNDropLite","57","""""","NULL","{""57"":{""taskId"":""b42412be-c750-49df-8206-8e4e11db1072""}}"
"admin","2023-07-27 12:01:01.734000","\0","Predict Cluster using Trained Model","NULL","{""elements"":[{""id"":""qrKkE"",""alias"":""Dataset  Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 02:37:43"",""alias"":""NewTickets"",""id"":2747,""name"":""LEANWTCK13837"",""description"":""New Tickets to predict cluster"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""115"",""position_y"":""161"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""RccnB"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""RccnB"",""alias"":""Pre Processing Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PreProcessingScript"",""requirements"":"""",""params"":[{""name"":""incidentid"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""}],""script"":[""\\rimport pandas\\rimport logging\\r# dataset=dataset.toPandas()\\r\\rdef PreProcessingScript(dataset,incidentid_param=''):\\r    incidentId = incidentid_param\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    if incidentId !='':\\r        dataset = dataset[dataset['number'] == incidentId]\\r    # dataset.values.tolist()\\r    return dataset\\r\\r""]},""position_x"":""352"",""position_y"":""159"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""qrKkE"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""YxKrK"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 02:37:43"",""alias"":""NewTickets"",""id"":2747,""name"":""LEANWTCK13837"",""description"":""New Tickets to predict cluster"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""KwjUo"",""alias"":""Post Processing Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PostProcessingScript"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\r\\rdef PostProcessingScript(dataset, label_encoder_model_path):\\r    dataset = dataset[['number','prediction']]\\r\\r    if label_encoder_model_path.split('.')[-1] == 'pkl':\\r        unpickler = pickle.Unpickler(open(label_encoder_model_path, 'rb'))\\r        label_encoder_model = unpickler.load()\\r    elif label_encoder_model_path.split('.')[-1] == 'joblib':\\r        label_encoder_model = joblib.load(label_encoder_model_path)\\r\\r    dataset['cluster_classification_label']=label_encoder_model.inverse_transform(dataset['prediction'])\\r    dataset = dataset[['number','cluster_classification_label']]\\r    dataset['last_updated']=pd.todatetime(datetime.now())\\r    \\r    return dataset.to_dict('records')\\r""]},""position_x"":""870"",""position_y"":""158"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""JdmQJ"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""YxKrK"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""},{""FunctionName"":""Inference"",""requirements"":"""",""params"":[],""script"":[""def Inference(dataset, download_path):\\r"",""    if isinstance(dataset,str):\\r"",""        dataset, download_path = download_path, dataset\\r"",""    print('download_path: ', download_path)\\r"",""    # load the model\\r"",""    if download_path.split('.')[-1] == 'pkl':\\r"",""        unpickler = pickle.Unpickler(open(download_path, 'rb'))\\r"",""        load_model = unpickler.load()\\r"",""        print(load_model)\\r"",""    elif download_path.split('.')[-1] == 'joblib':\\r"",""        load_model = joblib.load(download_path)\\r"",""    pipe_pred = load_model.predict(dataset['X_test'])\\r"",""    print(pipe_pred)\\r"",""    \\r"",""    # OUT\\r"",""    if isinstance(pipe_pred,tuple):\\r"",""        pipe_pred, prediction_results = pipe_pred\\r"",""        output=pd.DataFrame({'Id':dataset['dataset_id']})\\r"",""        output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\r"",""        dataset = output\\r"",""        print('out', output.to_dict('records'))\\r"",""    else:\\r"",""        dataset['prediction'] = pipe_pred\\r"",""    os.remove(download_path)\\r"",""    return dataset\\r""]},{""FunctionName"":""PreProcessingScript"",""requirements"":"""",""params"":[{""name"":""incidentid"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""}],""script"":[""\\rimport pandas\\rimport logging\\r# dataset=dataset.toPandas()\\r\\rdef PreProcessingScript(dataset,incidentid_param=''):\\r    incidentId = incidentid_param\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    if incidentId !='':\\r        dataset = dataset[dataset['number'] == incidentId]\\r    # dataset.values.tolist()\\r    return dataset\\r\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 02:37:43"",""alias"":""NewTickets"",""id"":2747,""name"":""LEANWTCK13837"",""description"":""New Tickets to predict cluster"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""}]},{""id"":""JdmQJ"",""alias"":""ModelSource"",""name"":""ModelSource"",""classname"":""ModelSourceConfig"",""category"":""ModelBaseConfig"",""attributes"":{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""},""position_x"":""812"",""position_y"":""22"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""KwjUo"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""bucket"":""text"",""fileName"":""text"",""Destination_directory"":""text"",""fileId"":""text""},""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os""],""script"":""\\ndef ModelSource_<id>(bucket_param='Demo', filename_param='your_model_name.pkl', destination_directory_param='models/classicmlpoc', fileid_param=''):\\n    fileId, fileName, bucket = fileid_param, filename_param, bucket_param\\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    print('FILE_SERVER_URL: ', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    print('FILE_SERVER_TOKEN: ', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ['JOB_DIRECTORY'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + '/api/lastcount/' + fileId + '?bucket=' + bucket,\\n                                           headers={'access-token': FILE_SERVER_TOKEN},\\n                                           proxies={'http': '', 'https': ''})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search('<Integer>(.*?)</Integer>', totalChunksResponse.text).group(1)\\n            totalChunks = int('0')\\n        logger.info('Total Chunks: ' + str(totalChunks + 1))\\n\\n        # create a folder 'chunkfiles'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, 'chunkfiles')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info('dir already exists...')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model's chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            filenam = 'file' if fileId[0]=='f' else filename_param\\n            url = (FILE_SERVER_URL + '/api/download/' + fileId + '/') + filenam + '?bucket=' + bucket\\n            print(url)\\n            filedata = requests.get(url, headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n            open(chunkfilePath + '/' + str(i), 'wb').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, 'wb') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, 'rb')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n""},""context"":[]},{""id"":""DTXpz"",""alias"":""ModelSource"",""name"":""ModelSource"",""classname"":""ModelSourceConfig"",""category"":""ModelBaseConfig"",""attributes"":{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""},""position_x"":""363"",""position_y"":""19"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""YxKrK"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""bucket"":""text"",""fileName"":""text"",""Destination_directory"":""text"",""fileId"":""text""},""codeGeneration"":{""requirements"":[],""imports"":[""import requests"",""import logging as logger"",""import shutil"",""import os""],""script"":""\\ndef ModelSource_<id>(bucket_param='Demo', filename_param='your_model_name.pkl', destination_directory_param='models/classicmlpoc', fileid_param=''):\\n    fileId, fileName, bucket = fileid_param, filename_param, bucket_param\\n    FILE_SERVER_URL = os.environ['FILE_SERVER_URL']\\n    print('FILE_SERVER_URL: ', FILE_SERVER_URL)\\n    FILE_SERVER_TOKEN = os.environ['FILE_SERVER_TOKEN']\\n    print('FILE_SERVER_TOKEN: ', FILE_SERVER_TOKEN)\\n    PYTHON_JOB_TEMP = os.path.join(os.environ['JOB_DIRECTORY'],destination_directory_param)\\n    def downloadChunks(fileId, bucket):\\n        # To get the last count\\n        totalChunks = 0\\n        totalChunksResponse = requests.get(FILE_SERVER_URL + '/api/lastcount/' + fileId + '?bucket=' + bucket,\\n                                           headers={'access-token': FILE_SERVER_TOKEN},\\n                                           proxies={'http': '', 'https': ''})\\n        if totalChunksResponse.status_code == 200:\\n            import re\\n            totalChunks = re.search('<Integer>(.*?)</Integer>', totalChunksResponse.text).group(1)\\n            totalChunks = int('0')\\n        logger.info('Total Chunks: ' + str(totalChunks + 1))\\n\\n        # create a folder 'chunkfiles'  in fileid folder\\n        chunkfilePath = os.path.join(PYTHON_JOB_TEMP, fileId, 'chunkfiles')\\n        if os.path.isdir(chunkfilePath):\\n            logger.info('dir already exists...')\\n        else:\\n            os.makedirs(chunkfilePath)\\n\\n        # To download the model's chunk files in chunkfiles folder\\n        for i in range(totalChunks + 1):\\n            filenam = 'file' if fileId[0]=='f' else filename_param\\n            url = (FILE_SERVER_URL + '/api/download/' + fileId + '/') + filenam + '?bucket=' + bucket\\n            print(url)\\n            filedata = requests.get(url, headers={'access-token': FILE_SERVER_TOKEN}, proxies={'http': '', 'https': ''})\\n            open(chunkfilePath + '/' + str(i), 'wb').write(filedata.content)\\n        return chunkfilePath, totalChunks\\n\\n    def mergeChunks(chunkfilePath, fileName, totalChunks):\\n        # Merging chunk files to get file\\n        readsize = 1024\\n        filePath = os.path.join(PYTHON_JOB_TEMP, fileName)\\n        if os.path.exists(filePath):\\n            os.remove(filePath)\\n\\n        with open(filePath, 'wb') as output:\\n            for filename in range(totalChunks + 1):\\n                chunkpath = os.path.join(chunkfilePath, str(filename))\\n                fileobj = open(chunkpath, 'rb')\\n                while 1:\\n                    filebytes = fileobj.read(readsize)\\n                    if not filebytes: break\\n                    output.write(filebytes)\\n                fileobj.close()\\n\\n        return filePath\\n\\n    chunkfilePath, totalChunks = downloadChunks(fileId, bucket)\\n    filePath = mergeChunks(chunkfilePath, fileName, totalChunks)\\n    # delete temp file directory\\n    shutil.rmtree(os.path.join(PYTHON_JOB_TEMP, fileId))\\n    return filePath\\n\\n\\n""},""context"":[]},{""id"":""YxKrK"",""alias"":""Inference"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""Inference"",""requirements"":"""",""params"":[],""script"":[""def Inference(dataset, download_path):\\r"",""    if isinstance(dataset,str):\\r"",""        dataset, download_path = download_path, dataset\\r"",""    print('download_path: ', download_path)\\r"",""    # load the model\\r"",""    if download_path.split('.')[-1] == 'pkl':\\r"",""        unpickler = pickle.Unpickler(open(download_path, 'rb'))\\r"",""        load_model = unpickler.load()\\r"",""        print(load_model)\\r"",""    elif download_path.split('.')[-1] == 'joblib':\\r"",""        load_model = joblib.load(download_path)\\r"",""    pipe_pred = load_model.predict(dataset['X_test'])\\r"",""    print(pipe_pred)\\r"",""    \\r"",""    # OUT\\r"",""    if isinstance(pipe_pred,tuple):\\r"",""        pipe_pred, prediction_results = pipe_pred\\r"",""        output=pd.DataFrame({'Id':dataset['dataset_id']})\\r"",""        output = pd.merge(output, pipe_pred, left_index=True, right_index=True)\\r"",""        dataset = output\\r"",""        print('out', output.to_dict('records'))\\r"",""    else:\\r"",""        dataset['prediction'] = pipe_pred\\r"",""    os.remove(download_path)\\r"",""    return dataset\\r""]},""position_x"":""568"",""position_y"":""159"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""RccnB"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""DTXpz"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""KwjUo"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""PreProcessingScript"",""requirements"":"""",""params"":[{""name"":""incidentid"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""}],""script"":[""\\rimport pandas\\rimport logging\\r# dataset=dataset.toPandas()\\r\\rdef PreProcessingScript(dataset,incidentid_param=''):\\r    incidentId = incidentid_param\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    if incidentId !='':\\r        dataset = dataset[dataset['number'] == incidentId]\\r    # dataset.values.tolist()\\r    return dataset\\r\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 02:37:43"",""alias"":""NewTickets"",""id"":2747,""name"":""LEANWTCK13837"",""description"":""New Tickets to predict cluster"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""bucket"":""Demo"",""fileName"":""your_model_name.pkl"",""Destination_directory"":""models/classicmlpoc"",""fileId"":""""}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","poornasai.nagendra@ad.infosys.com","Cluster Prediction","2023-08-25 07:11:02","LEACLSTR65830","leo1311","DragNDropLite","36","""""","NULL","NULL"
"poornasai.nagendra@ad.infosys.com","2023-07-27 12:51:41.310000","\0","Map SOP for tickets","NULL","{""elements"":[{""id"":""kMLMn"",""alias"":""sop_config"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-27 12:53:06"",""alias"":""sop_config"",""id"":2730,""name"":""LEASP_CN59375"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_sop_config;\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""118"",""position_y"":""115"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""MSSdz"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""MSSdz"",""alias"":""Post Processing Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""SOPClassification"",""requirements"":"""",""params"":[{""name"":""IncidentId"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""}],""script"":[""import re\\rimport logging as logger\\rfrom datetime import datetime\\r\\rdef SOPClassification(dataset1, dataset2, incidentid_param='2'):\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    dataset1['shortdescription'] = dataset1['shortdescription'].astype(str)\\r    incidentId = incidentid_param\\r    print('Running for incident '+ incidentId)\\r    if incidentId !='':\\r        dataset1 = dataset1[dataset1['number'] == incidentId]\\r    print(dataset1)\\r    label_list = dataset2.apply(lambda x:{'sop':x['sop'],'workflow':x['workflow'],'regex':x['regular_exp']}, axis=1).tolist()\\r    def getsop(label,text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['sop']\\r        return\\r    def getWorkflow(label , text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['workflow']\\r        return\\r    \\r\\r    dataset1['sop'] = dataset1['shortdescription'].apply(lambda x: getsop(label_list, x))\\r    dataset1 = dataset1[dataset1['sop'] != 'undefined']\\r    dataset1['sop_confidence'] = '100'\\r    dataset1['workflow'] = dataset1['shortdescription'].apply(lambda x: getWorkflow(label_list, x))\\r    dataset1['last_updated'] = datetime.now()\\r    \\r    data = dataset1[['number','sop','sop_confidence','workflow','last_updated']]\\r    \\r    print(data)\\r    return data\\r        \\r\\r\\r""]},""position_x"":""352"",""position_y"":""232"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""kMLMn"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""VgbtI"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""hQIxu"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-27 12:53:06"",""alias"":""sop_config"",""id"":2730,""name"":""LEASP_CN59375"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_sop_config;\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:46:39"",""alias"":""SOPTickets"",""id"":2764,""name"":""LEASPTCK31291"",""description"":""Tickets to classify SOP"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, shortdescription from leo1311_tickets where  shortdescription is not null and shortdescription <> \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""hQIxu"",""alias"":""MYSQL Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:46:39"",""alias"":""SOPTickets"",""id"":2764,""name"":""LEASPTCK31291"",""description"":""Tickets to classify SOP"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, shortdescription from leo1311_tickets where  shortdescription is not null and shortdescription <> \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""153"",""position_y"":""238"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""MSSdz"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""VgbtI"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-17 22:24:39"",""alias"":""output"",""id"":2809,""name"":""LEAOTPTJ98857"",""description"":""predicted sop for tickets"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, sop, sop_confidence, last_updated from leo1311_tickets_enriched;\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""558"",""position_y"":""233"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""MSSdz"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""SOPClassification"",""requirements"":"""",""params"":[{""name"":""IncidentId"",""value"":""INC0020765"",""type"":""Text"",""alias"":""INC0020765"",""index"":""1""}],""script"":[""import re\\rimport logging as logger\\rfrom datetime import datetime\\r\\rdef SOPClassification(dataset1, dataset2, incidentid_param='2'):\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    dataset1['shortdescription'] = dataset1['shortdescription'].astype(str)\\r    incidentId = incidentid_param\\r    print('Running for incident '+ incidentId)\\r    if incidentId !='':\\r        dataset1 = dataset1[dataset1['number'] == incidentId]\\r    print(dataset1)\\r    label_list = dataset2.apply(lambda x:{'sop':x['sop'],'workflow':x['workflow'],'regex':x['regular_exp']}, axis=1).tolist()\\r    def getsop(label,text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['sop']\\r        return\\r    def getWorkflow(label , text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['workflow']\\r        return\\r    \\r\\r    dataset1['sop'] = dataset1['shortdescription'].apply(lambda x: getsop(label_list, x))\\r    dataset1 = dataset1[dataset1['sop'] != 'undefined']\\r    dataset1['sop_confidence'] = '100'\\r    dataset1['workflow'] = dataset1['shortdescription'].apply(lambda x: getWorkflow(label_list, x))\\r    dataset1['last_updated'] = datetime.now()\\r    \\r    data = dataset1[['number','sop','sop_confidence','workflow','last_updated']]\\r    \\r    print(data)\\r    return data\\r        \\r\\r\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-27 12:53:06"",""alias"":""sop_config"",""id"":2730,""name"":""LEASP_CN59375"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_sop_config;\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:46:39"",""alias"":""SOPTickets"",""id"":2764,""name"":""LEASPTCK31291"",""description"":""Tickets to classify SOP"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, shortdescription from leo1311_tickets where  shortdescription is not null and shortdescription <> \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","SOPClassification","2023-08-28 06:19:21","LEASPCLS16038","leo1311","DragNDropLite","31","""""","NULL","{""26"":{""taskId"":""cf0fde06-5115-48e3-8c22-734cefa2efdf""}}"
"poornasai.nagendra@ad.infosys.com","2023-07-27 13:38:56.387000","\0","","NULL","{""elements"":[{""id"":""JyzMj"",""alias"":""LeapUpdateTicket"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""leap_update_ticket"",""requirements"":""pyodbc"",""params"":[{""name"":""leapdatasource"",""value"":""leo1311"",""type"":""Text"",""alias"":""leo1311"",""index"":""1""},{""name"":""datatable"",""value"":""leo1311_tickets"",""type"":""Text"",""alias"":""leo1311_tickets"",""index"":""2""},{""name"":""tickettype"",""value"":""Incident"",""type"":""Text"",""alias"":""Incident"",""index"":""3""},{""name"":""incidentpayload"",""value"":""{\\""number\\"":\\""INC1625226229\\"",\\""shortdescription\\"":\\""again test local update\\"",\\""priority\\"":{\\""displayValue\\"":\\""5\\"",\\""systemId\\"":\\""5\\""},\\""state\\"":{\\""displayValue\\"":\\""New\\"",\\""systemId\\"":\\""1\\""}, \\""category\\"":{\\""displayValue\\"":\\""Inquiry / Help\\"",\\""systemId\\"":\\""inquiry\\""},\\""impact\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""}, \\""urgency\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""},\\""configurationitem\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""description\\"":null,\\""assignmentgroup\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""assignedto\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""},\\""sop\\"":\\""Facebook Error\\"", \\""resolutionStepsClusterName\\"":\\""FacebookInsights\\""}"",""type"":""Text"",""alias"":""{\\""number\\"":\\""INC1625226229\\"",\\""shortdescription\\"":\\""again test local update\\"",\\""priority\\"":{\\""displayValue\\"":\\""5\\"",\\""systemId\\"":\\""5\\""},\\""state\\"":{\\""displayValue\\"":\\""New\\"",\\""systemId\\"":\\""1\\""}, \\""category\\"":{\\""displayValue\\"":\\""Inquiry / Help\\"",\\""systemId\\"":\\""inquiry\\""},\\""impact\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""}, \\""urgency\\"":{\\""displayValue\\"":\\""3\\"",\\""systemId\\"":\\""3\\""},\\""configurationitem\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""description\\"":null,\\""assignmentgroup\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""}, \\""assignedto\\"":{\\""displayValue\\"":null,\\""systemId\\"":\\""\\""},\\""sop\\"":\\""Facebook Error\\"", \\""resolutionStepsClusterName\\"":\\""FacebookInsights\\""}"",""index"":""4""}],""script"":[""import sys\\r"",""import os\\r"",""import pyodbc\\r"",""import json\\r"",""from datetime import datetime\\r"",""from leap.utils.Utilities import Utilities\\r"",""import logging as logger\\r"",""from datetime import timezone\\r"",""\\r"",""\\r"",""logger.basicConfig(level=logger.INFO,format='%(asctime)s INFO %(message)s', datefmt='%y/%m/%d %H:%M:%S')\\r"",""\\r"",""def leap_update_ticket(incidentpayload_param='', tickettype_param='', datatable_param='', leapdatasource_param=''):\\r"",""    argdict = {'incidentPayload': incidentpayload_param,\\r"",""    'ticketType':tickettype_param,\\r"",""    'dataTable':datatable_param,\\r"",""    'LEAPDataSource':leapdatasource_param\\r"",""    }\\r"",""\\r"",""\\r"",""    icmPayload = json.loads(argdict['incidentPayload'])\\r"",""\\r"",""    icmPayloadMapping = {'number':'number','shortdescription':'shortdescription','priority.displayValue':'priority','state.displayValue':'state',\\r"",""                            'description':'description','category.displayValue':'category','impact.displayValue':'impact',\\r"",""                            'assignmentgroup.displayValue':'assignmentgroup','configurationitem.displayValue':'configurationItem','assignedto.displayValue':'assignedto',\\r"",""                            'sop':'sop','resolutionStepsClusterName':'workflow', 'caller.displayValue':'caller'}\\r"",""    #mapColumns\\r"",""    foundColumns = []\\r"",""    row = {}\\r"",""    for key in icmPayloadMapping.keys():\\r"",""        jv = icmPayload\\r"",""        icmColumn = key.split('.')\\r"",""        for item in icmColumn:\\r"",""            try:\\r"",""                jv = jv[item]\\r"",""                row[icmPayloadMapping[key]] = jv\\r"",""            except:\\r"",""                a='No mapping'\\r"",""\\r"",""\\r"",""\\r"",""    row['source'] = 'LEAP'\\r"",""    row['type'] = argdict['ticketType']\\r"",""    row['lastUpdated']=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r"",""    row['openedDate']=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r"",""    row['updatedDate']=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r"",""    row['createdDate']=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r"",""    row['createdby'] = 'admin'\\r"",""\\r"",""    #LEAP Datasource\\r"",""    leapDSdict = json.loads(argdict['LEAPDataSource'])\\r"",""\\r"",""    # #save to db\\r"",""    columnList = row.keys()\\r"",""    password = Utilities.decrypt(leapDSdict['password'],leapDSdict['salt'])\\r"",""    temp1 = leapDSdict['url'].split('//')\\r"",""    temp2 = temp1[1].split(';')\\r"",""    server = temp2[0]\\r"",""    database = (temp2[1].split('='))[1]\\r"",""    isTrusted = 'no'\\r"",""    if leapDSdict['userName'] == '':\\r"",""        isTrusted = 'yes'\\r"",""\\r"",""    connectionString = 'DRIVER={0};SERVER={1}; ' \\\\\\r"",""                    'DATABASE={2};UID={3};PWD={4}; trusted_connection={5}'.format(\\r"",""        'ODBC Driver 17 for SQL SERVER', server, database, leapDSdict['userName'], password, isTrusted)\\r"",""    cnx = pyodbc.connect(connectionString)\\r"",""    mycursor = cnx.cursor()\\r"",""\\r"",""    tablename= argdict['dataTable'] \\r"",""    # get Primary key columns of table\\r"",""    primarykeyColumns = []\\r"",""    q = '''SELECT Col.Column_Name from INFORMATION_SCHEMA.TABLE_CONSTRAINTS Tab,\\\\\\r"",""            INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE Col \\\\\\r"",""            WHERE Col.Constraint_Name = Tab.Constraint_Name AND Col.Table_Name = Tab.Table_Name \\\\\\r"",""            AND Constraint_Type = 'PRIMARY KEY' AND Col.Table_Name = '{0}' '''.format(tablename)\\r"",""    mycursor.execute(q)\\r"",""    for res in mycursor.fetchall():\\r"",""        primarykeyColumns.append(res[0])\\r"",""\\r"",""    col_value = []\\r"",""    values = []\\r"",""    paramsDict = {}\\r"",""    for key in row.keys():\\r"",""        paramsDict[key] = row[key]\\r"",""        values.append(row[key])\\r"",""        col_value.append('''[{0}]='{1}' '''.format(key, row[key]))\\r"",""\\r"",""    columns = ', '.join(columnList)\\r"",""    col_values = ', '.join(col_value)\\r"",""    if len(primarykeyColumns) > 0:\\r"",""        joinon = ' AND '.join('t.[{0}] = s.[{0}]'.format(col) for col in primarykeyColumns)\\r"",""    else:\\r"",""        joinon = ' AND '.join('t.[{0}] = s.[{0}]'.format(col) for col in columnList)\\r"",""    updateValues = ', '.join('[{0}] = s.[{0}]'.format(col) for col in columnList)\\r"",""    insertValues = ', '.join('s.[{0}]'.format(col) for col in columnList)\\r"",""    query = 'MERGE  INTO [{0}] AS t USING (SELECT {1}) AS s  ON {2} WHEN MATCHED THEN UPDATE SET {3} WHEN NOT MATCHED THEN INSERT({4}) VALUES({5});'.format(\\r"",""        tablename, col_values, joinon, updateValues, columns, insertValues)\\r"",""\\r"",""    mycursor.execute(query)\\r"",""\\r"",""    cnx.commit()\\r"",""    mycursor.close()\\r"",""    cnx.close()\\r"",""    logger.info('Completed')\\r""]},""position_x"":""458"",""position_y"":""165"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","Leap_Update_Ticket","2023-08-01 04:15:02","LEALP_PD42081","leo1311","DragNDropLite","14","""""","NULL","{""14"":{""taskId"":""0e158756-539f-4fc0-9931-f752e770accb""}}"
"admin","2023-07-27 15:44:13.595000","\0","","NULL","{""elements"":[{""id"":""ycBwD"",""alias"":""Tickets"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""Tickets"",""name"":""ST0PI7S7VA"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number,priority ,createdDate FROM leo1311_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM leo1311_tickets_enriched WHERE resolution_SLA IS NOT NULL)"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":"""",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""132"",""position_y"":""69"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""yBwDd"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""vBTnp"",""alias"":""SLA Configuration"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""SLA Configuration"",""name"":""CQUUB59FAE"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select * from leo1311_sla_configuration"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_sla_configuration"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""128"",""position_y"":""180"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""yBwDd"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""yBwDd"",""alias"":""ScriptTransformer"",""name"":""ScriptTransformer"",""classname"":""ScriptTransformerConfig"",""category"":""TransformerConfig"",""attributes"":{""requirements"":"""",""script"":[""def slaCalculation(dataset1,dataset2):\\r"",""\\tdataset=pd.merge(dataset1,dataset2, on='priority', how='outer')\\r"",""\\tdataset['responseSLA']=dataset['responseSLA']*3600.0\\r"",""\\tdataset['resolutionSLA']=dataset['resolutionSLA']*3600.0\\r"",""\\tdataset=dataset['id','priority','number','responseSLA','resolutionSLA','createdDate']\\r"",""\\tdataset['response_SLA'] = dataset.apply(lambda x: x.ix['createdDate'] + dt.timedelta(seconds=x.ix['responseSLA']), axis=1)\\r"",""\\tdataset['resolution_SLA'] = dataset.apply(lambda x: x.ix['createdDate'] + dt.timedelta(seconds=x.ix['resolutionSLA']), axis=1)\\r"",""\\t\\r"",""\\tdataset['last_updated']= datetime.now()\\r"",""\\tdataset['response_SLA_confidence']= 100\\r"",""\\tdataset['resolution_SLA_confidence']= 100\\r"",""\\t\\r"",""\\tdataset= dataset['number','response_SLA','resolution_SLA','response_SLA_confidence','resolution_SLA_confidence','last_updated']\\r"",""\\t\\r"",""\\treturn dataset\\r""]},""position_x"":""397"",""position_y"":""180"",""connectors"":[{""type"":""target"",""endpoint"":""in2"",""position"":""TopCenter"",""elementId"":""ycBwD"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""in1"",""position"":""LeftMiddle"",""elementId"":""vBTnp"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""AOLaG"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""requirements"":""text"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""""},""context"":[{""dataset"":{""alias"":""Tickets"",""name"":""ST0PI7S7VA"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number,priority ,createdDate FROM leo1311_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM leo1311_tickets_enriched WHERE resolution_SLA IS NOT NULL)"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":"""",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},{""dataset"":{""alias"":""SLA Configuration"",""name"":""CQUUB59FAE"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select * from leo1311_sla_configuration"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_sla_configuration"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}}]},{""id"":""AOLaG"",""alias"":""Output"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""alias"":""SLATickets"",""name"":""COBGWD0OVC"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select number, response_SLA, resolution_SLA, response_SLA_confidence, resolution_SLA_confidence, last_updated from leo1311_tickets_enriched"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_tickets_enriched"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""645"",""position_y"":""180"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""yBwDd"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""requirements"":"""",""script"":[""def slaCalculation(dataset1,dataset2):\\r"",""\\tdataset=pd.merge(dataset1,dataset2, on='priority', how='outer')\\r"",""\\tdataset['responseSLA']=dataset['responseSLA']*3600.0\\r"",""\\tdataset['resolutionSLA']=dataset['resolutionSLA']*3600.0\\r"",""\\tdataset=dataset['id','priority','number','responseSLA','resolutionSLA','createdDate']\\r"",""\\tdataset['response_SLA'] = dataset.apply(lambda x: x.ix['createdDate'] + dt.timedelta(seconds=x.ix['responseSLA']), axis=1)\\r"",""\\tdataset['resolution_SLA'] = dataset.apply(lambda x: x.ix['createdDate'] + dt.timedelta(seconds=x.ix['resolutionSLA']), axis=1)\\r"",""\\t\\r"",""\\tdataset['last_updated']= datetime.now()\\r"",""\\tdataset['response_SLA_confidence']= 100\\r"",""\\tdataset['resolution_SLA_confidence']= 100\\r"",""\\t\\r"",""\\tdataset= dataset['number','response_SLA','resolution_SLA','response_SLA_confidence','resolution_SLA_confidence','last_updated']\\r"",""\\t\\r"",""\\treturn dataset\\r""]},{""dataset"":{""alias"":""Tickets"",""name"":""ST0PI7S7VA"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number,priority ,createdDate FROM leo1311_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM leo1311_tickets_enriched WHERE resolution_SLA IS NOT NULL)"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":"""",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},{""dataset"":{""alias"":""SLA Configuration"",""name"":""CQUUB59FAE"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select * from leo1311_sla_configuration"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_sla_configuration"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","SLA_Calculation","2023-08-08 14:52:00","LEASL_CL95259","leo1311","DragNDropLite","12","""""","NULL","{""10"":{""taskId"":""727f1ae3-9859-4e3f-a2e0-35b4239ef9b6""}}"
"admin","2023-07-27 15:58:48.321000","\0","","NULL","{""elements"":[{""id"":""jlgmn"",""alias"":""ScriptTransformer"",""name"":""ScriptTransformer"",""classname"":""ScriptTransformerConfig"",""category"":""TransformerConfig"",""attributes"":{""requirements"":"""",""script"":[""import logging"",""import spacy"",""import pytextrank"",""from datetime import datetime"",""def phraseExtraction(self, dataset):"",""    nlp = spacy.load('en_core_web_sm')"",""    nlp.add_pipe('textrank')"",""    timenow = datetime.now()"",""    totalRecords = len(dataset)"",""    logging.info('Fetched {0} records'.format(totalRecords))"",""    count =0"",""    textPhraseMappings = {}"",""    for row in dataset:"",""        try:"",""            text = row['clean_text']"",""            if textPhraseMappings.get(text,'') != '':"",""                row['extracted_phrase'] = textPhraseMappings[text]"",""                break"",""            doc = nlp(text)"",""            phrase =''"",""            if len(doc._.phrases)>0:"",""                for item in doc._.phrases:"",""                    withoutSpace = item.text.replace(' ' ,'')"",""                    if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:"",""                        phrase = item.text"",""                        break"",""            if phrase != '':"",""                row['extracted_phrase'] = phrase"",""            else:"",""                row['extracted_phrase'] = text"",""        except Exception as ex:"",""            logging.info(ex)"",""            row['extracted_phrase'] = text"",""        row['last_updated'] = timenow"",""        row['mapped_phrase'] = ''"",""        row['mapped_phrase_confidennce'] = ''"",""        count = count+1"",""        percentage = (count/totalRecords)*100 "",""        if percentage%10 ==0:"",""            logging.info('{0}% Completed'.format(percentage))"",""    "",""    return dataset""]},""position_x"":""563"",""position_y"":""130"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""GZYQR"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""in1"",""position"":""LeftMiddle"",""elementId"":""Ruyoi"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in1"",""in2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""requirements"":""text"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""""},""context"":[{""dataset"":{""alias"":""Tickets"",""name"":""2VDT84XH8R"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number, CAST(clean_text as CHAR) as clean_text FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != ''  Order by Clean_text"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_tickets_enriched"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}}]},{""id"":""Ruyoi"",""alias"":""MYSQL Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""Tickets"",""name"":""2VDT84XH8R"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number, CAST(clean_text as CHAR) as clean_text FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != ''  Order by Clean_text"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_tickets_enriched"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""284"",""position_y"":""130"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""jlgmn"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""GZYQR"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""alias"":""extracted_phrases"",""name"":""EOSZF8RTDN"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""select number, extracted_phrase, mapped_phrase, mapped_phrase_confidennce, last_updated from leo1311_phrase_extraction"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_phrase_extraction"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""781"",""position_y"":""126"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""jlgmn"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""requirements"":"""",""script"":[""import logging"",""import spacy"",""import pytextrank"",""from datetime import datetime"",""def phraseExtraction(self, dataset):"",""    nlp = spacy.load('en_core_web_sm')"",""    nlp.add_pipe('textrank')"",""    timenow = datetime.now()"",""    totalRecords = len(dataset)"",""    logging.info('Fetched {0} records'.format(totalRecords))"",""    count =0"",""    textPhraseMappings = {}"",""    for row in dataset:"",""        try:"",""            text = row['clean_text']"",""            if textPhraseMappings.get(text,'') != '':"",""                row['extracted_phrase'] = textPhraseMappings[text]"",""                break"",""            doc = nlp(text)"",""            phrase =''"",""            if len(doc._.phrases)>0:"",""                for item in doc._.phrases:"",""                    withoutSpace = item.text.replace(' ' ,'')"",""                    if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:"",""                        phrase = item.text"",""                        break"",""            if phrase != '':"",""                row['extracted_phrase'] = phrase"",""            else:"",""                row['extracted_phrase'] = text"",""        except Exception as ex:"",""            logging.info(ex)"",""            row['extracted_phrase'] = text"",""        row['last_updated'] = timenow"",""        row['mapped_phrase'] = ''"",""        row['mapped_phrase_confidennce'] = ''"",""        count = count+1"",""        percentage = (count/totalRecords)*100 "",""        if percentage%10 ==0:"",""            logging.info('{0}% Completed'.format(percentage))"",""    "",""    return dataset""]},{""dataset"":{""alias"":""Tickets"",""name"":""2VDT84XH8R"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number, CAST(clean_text as CHAR) as clean_text FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != ''  Order by Clean_text"",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_tickets_enriched"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","Phrase_Extraction","2023-08-11 10:19:30","LEAPHRS_49469","leo1311","DragNDropLite","7","""""","NULL","{""6"":{""taskId"":""ce1c9dee-39ac-43b7-ae6f-457dba0ae339""}}"
"admin","2023-07-28 10:21:38.309000","\0","","NULL","{""elements"":[{""id"":""WIoBW"",""alias"":""StopWordRemover"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""no_stopwords"",""requirements"":"""",""params"":[{""name"":""custom_stopwords"",""value"":""a,an,the"",""type"":""Text"",""alias"":""a,an,the"",""index"":""1""}],""script"":[""import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.corpus import wordnet\\rfrom nltk.stem import WordNetLemmatizer\\rfrom datetime import datetime \\r\\rdef alphanum(text):\\r    alphanumeric = ''    \\r    for character in text:        \\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += ' '    \\r    return alphanumeric\\r    #return ''.join(char for char in text if char.isalnum() or char.isspace())\\r\\rdef token_generate(text):\\r    token = word_tokenize(text)\\r    tokens = [t for t in token if t.isalpha() or t.isspace()]\\r    return ' '.join(tokens)\\r\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    #words = set(stopwords.words('english'))\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stop_words = set(stopwords.words('english'))| set(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\r\\rdef lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\r\\rdef no_stopwords(dataset,custom_stopwords_param=''):\\r    print('inside no stopwords')\\r    dataset = pd.DataFrame(dataset)\\r    dataset['cleanText'] = dataset['shortdescription'].apply(alphanum)\\r    dataset['tokens'] = dataset['cleanText'].apply(token_generate)\\r    dataset['cleanWords'] = dataset['tokens'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    dataset['clean_Text'] = dataset['cleanWords'].apply(lematize)\\r    dataset['lastUpdated'] = datetime.now() \\r    dataset = dataset[['NUMBER','cleanText']]    \\r    print(dataset)\\r    data = dataset.to_dict(orient='index')  \\r    print(data)\\r    return data\\r    \\r""]},""position_x"":""475"",""position_y"":""87"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""hABXO"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""jZDQu"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-14 04:51:59"",""alias"":""Clean_Text"",""id"":2777,""name"":""LEACLN_T33845"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription, group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\"" \\\\\\"" AND clean_text IS NOT NULL\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""jZDQu"",""alias"":""Dataset Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-14 04:51:59"",""alias"":""Clean_Text"",""id"":2777,""name"":""LEACLN_T33845"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription, group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\"" \\\\\\"" AND clean_text IS NOT NULL\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""249"",""position_y"":""83"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""WIoBW"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""hABXO"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-16 12:17:16"",""alias"":""cleanText_stopword"",""id"":2803,""name"":""LEACLNTX84344"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, clean_text, group_by_field, last_updated from leo1311_tickets_enriched\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""update\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""683"",""position_y"":""89"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""WIoBW"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""no_stopwords"",""requirements"":"""",""params"":[{""name"":""custom_stopwords"",""value"":""a,an,the"",""type"":""Text"",""alias"":""a,an,the"",""index"":""1""}],""script"":[""import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.corpus import wordnet\\rfrom nltk.stem import WordNetLemmatizer\\rfrom datetime import datetime \\r\\rdef alphanum(text):\\r    alphanumeric = ''    \\r    for character in text:        \\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += ' '    \\r    return alphanumeric\\r    #return ''.join(char for char in text if char.isalnum() or char.isspace())\\r\\rdef token_generate(text):\\r    token = word_tokenize(text)\\r    tokens = [t for t in token if t.isalpha() or t.isspace()]\\r    return ' '.join(tokens)\\r\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    #words = set(stopwords.words('english'))\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stop_words = set(stopwords.words('english'))| set(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\r\\rdef lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\r\\rdef no_stopwords(dataset,custom_stopwords_param=''):\\r    print('inside no stopwords')\\r    dataset = pd.DataFrame(dataset)\\r    dataset['cleanText'] = dataset['shortdescription'].apply(alphanum)\\r    dataset['tokens'] = dataset['cleanText'].apply(token_generate)\\r    dataset['cleanWords'] = dataset['tokens'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    dataset['clean_Text'] = dataset['cleanWords'].apply(lematize)\\r    dataset['lastUpdated'] = datetime.now() \\r    dataset = dataset[['NUMBER','cleanText']]    \\r    print(dataset)\\r    data = dataset.to_dict(orient='index')  \\r    print(data)\\r    return data\\r    \\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-14 04:51:59"",""alias"":""Clean_Text"",""id"":2777,""name"":""LEACLN_T33845"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription, group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\"" \\\\\\"" AND clean_text IS NOT NULL\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","StopWordRemover","2023-08-28 08:03:16","LEASTPWR30075","leo1311","DragNDropLite","160","NULL","NULL","{""160"":{""taskId"":""ea6b0668-3968-4e20-92e7-121d23da4699""}}"
"poornasai.nagendra@ad.infosys.com","2023-07-28 10:25:56.478000","\0","Create incident, service request, incident task, change request in SNOW and then in  icm_tickets","NULL","{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEASNW_C26690_Leap.py""],""arguments"":[{""name"":""api"",""value"":""/api/now/table/"",""type"":""Text"",""alias"":""/api/now/table/"",""index"":""1""},{""name"":""params"",""value"":""sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident"",""type"":""Text"",""alias"":""sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident"",""index"":""2""},{""name"":""setProxy"",""value"":""True"",""type"":""Text"",""alias"":""True"",""index"":""3""},{""name"":""incidentPayload"",""value"":""{\\""shortdescription\\"": \\""VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\"", \\""priority\\"": \\""5\\"", \\""state\\"": \\""1\\""}"",""type"":""Text"",""alias"":""{\\""shortdescription\\"": \\""VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\"", \\""priority\\"": \\""5\\"", \\""state\\"": \\""1\\""}"",""index"":""4""},{""name"":""LEAPDataSource"",""value"":""undefined"",""type"":""Datasource"",""alias"":""undefined"",""index"":""5""},{""name"":""ticketType"",""value"":""Incident"",""type"":""Text"",""alias"":""Incident"",""index"":""6""},{""name"":""dataTable"",""value"":""leo1311_tickets"",""type"":""Text"",""alias"":""leo1311_tickets"",""index"":""7""},{""name"":""SnowDataSource"",""value"":""SNOW"",""type"":""Datasource"",""alias"":""SNOW"",""index"":""8""}],""dataset"":[]}}]}","poornasai.nagendra@ad.infosys.com","SNOW_Create_API","2023-07-31 11:26:50","LEASNW_C26690","leo1311","NativeScript","3","""""","NULL","NULL"
"admin","2023-07-28 11:15:53.440000","\0","Named Entity Extraction","NULL","{""elements"":[{""id"":""FImSm"",""alias"":""ShortDescription"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""alias"":""Ticket Shortdescription"",""name"":""2JMU72CD0Y"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number,  shortdescription  FROM leo1311_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL "",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_tickets"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}},""position_x"":""218"",""position_y"":""187"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""hPwPT"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""hPwPT"",""alias"":""Post Processing Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""extractNER"",""requirements"":"""",""params"":[{""name"":""incidentId"",""value"":"""",""type"":""Text"",""alias"":"""",""index"":""1""}],""script"":[""import pandas as pd\\r"",""from nltk.tokenize import RegexpTokenizer\\r"",""from nltk.stem import WordNetLemmatizer\\r"",""def extractNER(dataset,incidentId_param=''):\\r"",""    dataset=pd.DataFrame(dataset)\\r"",""    incidentId = incidentid_param\\r"",""    print('Running for incident '+ incidentId)\\r"",""    if incidentId !='':\\r"",""        dataset = dataset(dataset['number'] == incidentId)\\r"",""    \\r"",""    def lemmatize_words(text):\\r"",""       words = text.split()\\r"",""       words = [lemmatizer.lemmatize(word,pos='v') for word in words]\\r"",""       return ' '.join(words)\\r"",""    \\r"",""    def regexTokenizer(dataset):\\r"",""        tokens=RegexpTokenizer(r'\\\\w+')\\r"",""        dataset['tokens'] = dataset['lemmetized_text'].apply(tokens.tokenize)\\r"",""        return dataset\\r"",""        \\r"",""    def filterTokens(tokens):\\r"",""        d = enchant.Dict('en_US')\\r"",""        tokens = [t for t in tokens if d.check(t) == False]\\r"",""        return tokens\\r"",""\\r"",""    dfFilterTokens = dataset.apply(filterTokens).astype(str)             \\r"",""    dataset['lemmetized_text'] = dataset['shortdescription'].apply(lambda x: lemmatize_words(x))\\r"",""    dataset= regexTokenizer(dataset)\\r"",""    dataset['filter_tokens']=dataset[tokens].apply(lambda tokens: dfFilterTokens(tokens))\\r"",""    dataset['namedEntities']=dataset['filterTokens'].unique()\\r"",""    dataset['namedEntities']=dataset['filter_tokens'].apply(lambda x:list(set(x)))\\r"",""    dataset['entityValue']=dataset['namedEntities'].explode()\\r"",""    dataset=dataset.sort_values(by=['number','entityValue'])\\r"",""    dataset['entityId']=dataset.groupby('number').cumcount()+1\\r"",""    dataset['entityName']='entity'+ dataset['entityId'].astype(str)\\r"",""    dataset = dataset['number','entityName','entityValue']\\r"",""    return dataset\\r""]},""position_x"":""471"",""position_y"":""188"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""FImSm"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""alias"":""Ticket Shortdescription"",""name"":""2JMU72CD0Y"",""description"":"""",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""schema"":"""",""type"":""rw"",""views"":"""",""attributes"":{""filter"":"""",""mode"":""query"",""Query"":""SELECT number,  shortdescription  FROM leo1311_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL "",""isStreaming"":""false"",""defaultValues"":"""",""writeMode"":""append"",""params"":""{}"",""tableName"":""leo1311_tickets"",""uniqueIdentifier"":""""},""expStatus"":0,""interfacetype"":null,""isAuditRequired"":false,""isPermissionManaged"":false,""isApprovalRequired"":false,""isInboxRequired"":false,""IsArchivalEnabled"":false,""backingDataset"":""""}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","Extract_NER","2023-08-08 14:46:46","LEAEXTRC81653","leo1311","DragNDropLite","5","""""","NULL","{""5"":{""taskId"":""0eaea1f9-e5b6-4f2f-9993-a5a2bea462cd""}}"
"poornasai.nagendra@ad.infosys.com","2023-07-31 00:56:41.733000","\0","Cluster human created tickets on short description","NULL","{""elements"":[{""id"":""NehEi"",""alias"":""PreProcesData"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""filterdata"",""requirements"":"""",""params"":[{""name"":""output_col"",""value"":""filtereddf"",""type"":""Text"",""alias"":""filtereddf"",""index"":""1""}],""script"":[""def filterdata(dataset, output_col_param = ''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    dataset['cluster_Type'] = 'LDA'\\r"",""    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r"",""    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r"",""    filtereddf = dataset['group_by_field'].isin(ciList)\\r"",""    dataset[output_col_param] = filtereddf\\r"",""    dataset = dataset[dataset[output_col_param] == True]\\r"",""    return dataset""]},""position_x"":""216"",""position_y"":""99"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""bjasw"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""iaoHi"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:36:32"",""alias"":""Input_lda"",""id"":2746,""name"":""LEAINPT_60376"",""description"":""Tickets for LDA Clustering"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field  FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""bjasw"",""alias"":""Tokenizer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""tokenize"",""requirements"":""nltk"",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},""position_x"":""452"",""position_y"":""105"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""NehEi"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""jgaSQ"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""FunctionName"":""filterdata"",""requirements"":"""",""params"":[{""name"":""output_col"",""value"":""filtereddf"",""type"":""Text"",""alias"":""filtereddf"",""index"":""1""}],""script"":[""def filterdata(dataset, output_col_param = ''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    dataset['cluster_Type'] = 'LDA'\\r"",""    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r"",""    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r"",""    filtereddf = dataset['group_by_field'].isin(ciList)\\r"",""    dataset[output_col_param] = filtereddf\\r"",""    dataset = dataset[dataset[output_col_param] == True]\\r"",""    return dataset""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:36:32"",""alias"":""Input_lda"",""id"":2746,""name"":""LEAINPT_60376"",""description"":""Tickets for LDA Clustering"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field  FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""jgaSQ"",""alias"":""StopWordsRemoval"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},""position_x"":""657"",""position_y"":""101"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""bjasw"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""whzxk"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""FunctionName"":""tokenize"",""requirements"":""nltk"",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""filterdata"",""requirements"":"""",""params"":[{""name"":""output_col"",""value"":""filtereddf"",""type"":""Text"",""alias"":""filtereddf"",""index"":""1""}],""script"":[""def filterdata(dataset, output_col_param = ''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    dataset['cluster_Type'] = 'LDA'\\r"",""    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r"",""    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r"",""    filtereddf = dataset['group_by_field'].isin(ciList)\\r"",""    dataset[output_col_param] = filtereddf\\r"",""    dataset = dataset[dataset[output_col_param] == True]\\r"",""    return dataset""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:36:32"",""alias"":""Input_lda"",""id"":2746,""name"":""LEAINPT_60376"",""description"":""Tickets for LDA Clustering"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field  FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""whzxk"",""alias"":""CountVectorizer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""countvectorize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""1""}],""script"":[""def countvectorize(dataset, input_col_param=''):\\r"",""    from sklearn.feature_extraction.text import CountVectorizer\\r"",""    count_vectorizer = CountVectorizer(stop_words='english')\\r"",""    count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r"",""    words = count_vectorizer.get_feature_names_out()\\r"",""    return {'data': count_data, 'words': words}""]},""position_x"":""846"",""position_y"":""120"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""jgaSQ"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""ROGFv"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},{""FunctionName"":""tokenize"",""requirements"":""nltk"",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""filterdata"",""requirements"":"""",""params"":[{""name"":""output_col"",""value"":""filtereddf"",""type"":""Text"",""alias"":""filtereddf"",""index"":""1""}],""script"":[""def filterdata(dataset, output_col_param = ''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    dataset['cluster_Type'] = 'LDA'\\r"",""    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r"",""    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r"",""    filtereddf = dataset['group_by_field'].isin(ciList)\\r"",""    dataset[output_col_param] = filtereddf\\r"",""    dataset = dataset[dataset[output_col_param] == True]\\r"",""    return dataset""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:36:32"",""alias"":""Input_lda"",""id"":2746,""name"":""LEAINPT_60376"",""description"":""Tickets for LDA Clustering"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field  FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""ROGFv"",""alias"":""LDAClustering"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""lda_cluster"",""requirements"":""scikit-learn"",""params"":[],""script"":[""def lda_cluster(dataset):\\r"",""    from sklearn.decomposition import LatentDirichletAllocation as LDA\\r"",""    import numpy as np\\r"",""    number_topics = 10\\r"",""    number_words = 2\\r"",""    count_data, words = dataset['data'], dataset['words']\\r"",""    # Create and fit the LDA model\\r"",""    lda = LDA(n_components=number_topics, n_jobs=-1)\\r"",""    lda.fit(count_data)\\r"",""    #Get topics from model. They are represented as a list e.g. ['military','army']\\r"",""    topics = [[words[i] for i in topic.argsort()[:-number_words - 1:-1]] for (topic_idx, topic) in enumerate(lda.components_)]\\r"",""    topics = np.array(topics).ravel()\\r"",""    return topics""]},""position_x"":""503"",""position_y"":""257"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""whzxk"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n""},""context"":[{""FunctionName"":""countvectorize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""1""}],""script"":[""def countvectorize(dataset, input_col_param=''):\\r"",""    from sklearn.feature_extraction.text import CountVectorizer\\r"",""    count_vectorizer = CountVectorizer(stop_words='english')\\r"",""    count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r"",""    words = count_vectorizer.get_feature_names_out()\\r"",""    return {'data': count_data, 'words': words}""]},{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},{""FunctionName"":""tokenize"",""requirements"":""nltk"",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""filterdata"",""requirements"":"""",""params"":[{""name"":""output_col"",""value"":""filtereddf"",""type"":""Text"",""alias"":""filtereddf"",""index"":""1""}],""script"":[""def filterdata(dataset, output_col_param = ''):\\r"",""    dataset = pd.DataFrame(dataset)\\r"",""    dataset['cluster_Type'] = 'LDA'\\r"",""    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r"",""    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r"",""    filtereddf = dataset['group_by_field'].isin(ciList)\\r"",""    dataset[output_col_param] = filtereddf\\r"",""    dataset = dataset[dataset[output_col_param] == True]\\r"",""    return dataset""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:36:32"",""alias"":""Input_lda"",""id"":2746,""name"":""LEAINPT_60376"",""description"":""Tickets for LDA Clustering"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field  FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""iaoHi"",""alias"":""MYSQL Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-31 11:36:32"",""alias"":""Input_lda"",""id"":2746,""name"":""LEAINPT_60376"",""description"":""Tickets for LDA Clustering"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field  FROM leo1311_tickets_enriched  WHERE clean_text is not null and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""20"",""position_y"":""100"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""NehEi"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","poornasai.nagendra@ad.infosys.com","Clustering LDA","2023-08-01 01:45:08","LEACLSTR37477","leo1311","DragNDropLite","49","""""","NULL","{""47"":{""taskId"":""cf71ef8e-c346-4d3c-b014-802d98ad5728""}}"
"poornasai.nagendra@ad.infosys.com","2023-07-31 02:19:21.110000","\0","Train a classification model to predict clusters","NULL","{""elements"":[{""id"":""bjasw"",""alias"":""Tokenizer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""tokenize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},""position_x"":""450"",""position_y"":""107"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""jgaSQ"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""NRaSG"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""jgaSQ"",""alias"":""StopWordsRemoval"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},""position_x"":""663"",""position_y"":""104"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""bjasw"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""whzxk"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""tokenize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""whzxk"",""alias"":""CountVectorizer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""countvectorize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""1""}],""script"":[""def countvectorize(dataset, input_col_param=''):\\r"",""    from sklearn.feature_extraction.text import CountVectorizer\\r"",""    count_vectorizer = CountVectorizer(stop_words='english')\\r"",""    count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r"",""    words = count_vectorizer.get_feature_names_out()\\r"",""    return {'data': count_data, 'words': words}\\r""]},""position_x"":""859"",""position_y"":""102"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""jgaSQ"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""rMlav"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},{""FunctionName"":""tokenize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""ROGFv"",""alias"":""LabelIndexer"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""labelindexer"",""requirements"":"""",""params"":[],""script"":[""def labelindexer(labels):\\r"",""    from sklearn.preprocessing import LabelEncoder\\r"",""    label_encoder = LabelEncoder()\\r"",""    y_encoded = label_encoder.fit_transform(labels)\\r"",""    return y_encoded, label_encoder""]},""position_x"":""368"",""position_y"":""255"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""rMlav"",""elementPosition"":""LeftMiddle""},{""type"":""source"",""endpoint"":""out2"",""position"":""BottomCenter"",""elementId"":""PYoYN"",""elementPosition"":""TopCenter""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""NRaSG"",""elementPosition"":""BottomCenter""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""NRaSG"",""alias"":""Pre Processing Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},""position_x"":""269"",""position_y"":""105"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""bjasw"",""elementPosition"":""LeftMiddle""},{""type"":""source"",""endpoint"":""out2"",""position"":""BottomCenter"",""elementId"":""ROGFv"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""DDMwD"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""rMlav"",""alias"":""RandomForestClassification"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""randomforest"",""requirements"":"""",""params"":[],""script"":[""def randomforest(y_train, X_train):\\r    from sklearn.ensemble import RandomForestClassifier\\r    rf_claf = RandomForestClassifier()\\r    print('training model')\\r    rf_claf.fit(X_train['data'], y_train)\\r    return rf_claf""]},""position_x"":""645"",""position_y"":""244"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""whzxk"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""ROGFv"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out2"",""position"":""BottomCenter"",""elementId"":""eXiHL"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""countvectorize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""1""}],""script"":[""def countvectorize(dataset, input_col_param=''):\\r"",""    from sklearn.feature_extraction.text import CountVectorizer\\r"",""    count_vectorizer = CountVectorizer(stop_words='english')\\r"",""    count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r"",""    words = count_vectorizer.get_feature_names_out()\\r"",""    return {'data': count_data, 'words': words}\\r""]},{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},{""FunctionName"":""tokenize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""FunctionName"":""labelindexer"",""requirements"":"""",""params"":[],""script"":[""def labelindexer(labels):\\r"",""    from sklearn.preprocessing import LabelEncoder\\r"",""    label_encoder = LabelEncoder()\\r"",""    y_encoded = label_encoder.fit_transform(labels)\\r"",""    return y_encoded, label_encoder""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""DDMwD"",""alias"":""MYSQL Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""68"",""position_y"":""108"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""NRaSG"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""PYoYN"",""alias"":""PublishModel"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PublishModel"",""requirements"":"""",""params"":[{""name"":""modelname"",""value"":""labelencoder.joblib"",""type"":""Text"",""alias"":""labelencoder.joblib"",""index"":""1""},{""name"":""experimentname"",""value"":""ClusterClassificationTraining"",""type"":""Text"",""alias"":""ClusterClassificationTraining"",""index"":""2""},{""name"":""typeofmodel"",""value"":""joblib"",""type"":""Text"",""alias"":""joblib"",""index"":""3""}],""script"":[""def PublishModel(model, modelname_param='', experimentname_param='', typeofmodel_param=''):\\r\\\\n"",""    import torch\\r\\\\n"",""    import joblib\\r\\\\n"",""    import pickle\\r\\\\n"",""    #SaveModel\\r\\\\n"",""    modelPath = os.path.join('.','models',experimentname_param)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if not os.path.exists(modelPath):\\r\\\\n"",""        os.makedirs(modelPath)\\r\\\\n"",""    modelPath = os.path.join(modelPath)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if modelname_param.split('.')[-1] == 'pkl':\\r\\\\n"",""        pickle.dump(model, open(os.path.join(modelPath, modelname_param), 'wb'))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'pt':\\r\\\\n"",""        torch.save(model,os.path.join(modelPath,modelname_param))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'joblib':\\r\\\\n"",""        joblib.dump(model, open(os.path.join(modelPath,modelname_param), 'wb'))\\r\\\\n"",""    else:\\r\\\\n"",""        model.write().overwrite().save(os.path.join(modelPath,modelname_param))\\r\\\\n"",""    modelPath=os.path.join(modelPath,modelname_param)\\r\\\\n"",""\\r\\\\n"",""    return modelPath""]},""position_x"":""238"",""position_y"":""406"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""ROGFv"",""elementPosition"":""BottomCenter""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""gAosB"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""labelindexer"",""requirements"":"""",""params"":[],""script"":[""def labelindexer(labels):\\r"",""    from sklearn.preprocessing import LabelEncoder\\r"",""    label_encoder = LabelEncoder()\\r"",""    y_encoded = label_encoder.fit_transform(labels)\\r"",""    return y_encoded, label_encoder""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""eXiHL"",""alias"":""PublishModel"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PublishModel"",""requirements"":"""",""params"":[{""name"":""modelname"",""value"":""randomforest.joblib"",""type"":""Text"",""alias"":""randomforest.joblib"",""index"":""1""},{""name"":""experimentname"",""value"":""ClusterClassificationTraining"",""type"":""Text"",""alias"":""ClusterClassificationTraining"",""index"":""2""},{""name"":""typeofmodel"",""value"":""joblib"",""type"":""Text"",""alias"":""joblib"",""index"":""3""}],""script"":[""def PublishModel(model, modelname_param='', experimentname_param='', typeofmodel_param=''):\\r\\\\n"",""    import torch\\r\\\\n"",""    import joblib\\r\\\\n"",""    import pickle\\r\\\\n"",""    #SaveModel\\r\\\\n"",""    modelPath = os.path.join('.','models',experimentname_param)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if not os.path.exists(modelPath):\\r\\\\n"",""        os.makedirs(modelPath)\\r\\\\n"",""    modelPath = os.path.join(modelPath)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if modelname_param.split('.')[-1] == 'pkl':\\r\\\\n"",""        pickle.dump(model, open(os.path.join(modelPath, modelname_param), 'wb'))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'pt':\\r\\\\n"",""        torch.save(model,os.path.join(modelPath,modelname_param))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'joblib':\\r\\\\n"",""        joblib.dump(model, open(os.path.join(modelPath,modelname_param), 'wb'))\\r\\\\n"",""    else:\\r\\\\n"",""        model.write().overwrite().save(os.path.join(modelPath,modelname_param))\\r\\\\n"",""    modelPath=os.path.join(modelPath,modelname_param)\\r\\\\n"",""\\r\\\\n"",""    return modelPath""]},""position_x"":""696"",""position_y"":""387"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""rMlav"",""elementPosition"":""BottomCenter""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""KxJks"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""FunctionName"":""randomforest"",""requirements"":"""",""params"":[],""script"":[""def randomforest(y_train, X_train):\\r    from sklearn.ensemble import RandomForestClassifier\\r    rf_claf = RandomForestClassifier()\\r    print('training model')\\r    rf_claf.fit(X_train['data'], y_train)\\r    return rf_claf""]},{""FunctionName"":""countvectorize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""1""}],""script"":[""def countvectorize(dataset, input_col_param=''):\\r"",""    from sklearn.feature_extraction.text import CountVectorizer\\r"",""    count_vectorizer = CountVectorizer(stop_words='english')\\r"",""    count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r"",""    words = count_vectorizer.get_feature_names_out()\\r"",""    return {'data': count_data, 'words': words}\\r""]},{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},{""FunctionName"":""tokenize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""FunctionName"":""labelindexer"",""requirements"":"""",""params"":[],""script"":[""def labelindexer(labels):\\r"",""    from sklearn.preprocessing import LabelEncoder\\r"",""    label_encoder = LabelEncoder()\\r"",""    y_encoded = label_encoder.fit_transform(labels)\\r"",""    return y_encoded, label_encoder""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""gAosB"",""alias"":"" ModelLoader"",""name"":""Pre Processing Script"",""classname"":""PreProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""ModelLoader"",""requirements"":"""",""params"":[{""name"":""dataset_datasource_connectiondetails_url_param"",""value"":""http://10.85.12.143:31262/"",""type"":""Text"",""alias"":""http://10.85.12.143:31262/"",""index"":""1""},{""name"":""dataset_datasource_connectiondetails_accesskey_param"",""value"":""minio"",""type"":""Text"",""alias"":""minio"",""index"":""2""},{""name"":""dataset_datasource_connectiondetails_secretkey_param"",""value"":""miniosecret"",""type"":""Text"",""alias"":""miniosecret"",""index"":""3""},{""name"":""dataset_attributes_object_param"",""value"":""labelencoder.joblib"",""type"":""Text"",""alias"":""labelencoder.joblib"",""index"":""4""}],""script"":[""def ModelLoader(file_path, dataset_datasource_connectiondetails_url_param='', dataset_datasource_connectiondetails_accesskey_param='', dataset_datasource_connectiondetails_secretkey_param='', dataset_attributes_object_param='',dataset_attributes_bucket_param=''):\\r\\\\n"",""    URL = dataset_datasource_connectiondetails_url_param\\r\\\\n"",""    secure = True if urlparse(URL).scheme == 'https' else False\\r\\\\n"",""    client =Minio(urlparse(URL).hostname+':'+str(urlparse(URL).port),access_key=dataset_datasource_connectiondetails_accesskey_param,secret_key=dataset_datasource_connectiondetails_secretkey_param,secure=secure)\\r\\\\n"",""\\r\\\\n"",""    result = client.fput_object(dataset_attributes_bucket_param,dataset_attributes_object_param, file_path)""]},""position_x"":""495"",""position_y"":""367"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""PYoYN"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""PublishModel"",""requirements"":"""",""params"":[{""name"":""modelname"",""value"":""labelencoder.joblib"",""type"":""Text"",""alias"":""labelencoder.joblib"",""index"":""1""},{""name"":""experimentname"",""value"":""ClusterClassificationTraining"",""type"":""Text"",""alias"":""ClusterClassificationTraining"",""index"":""2""},{""name"":""typeofmodel"",""value"":""joblib"",""type"":""Text"",""alias"":""joblib"",""index"":""3""}],""script"":[""def PublishModel(model, modelname_param='', experimentname_param='', typeofmodel_param=''):\\r\\\\n"",""    import torch\\r\\\\n"",""    import joblib\\r\\\\n"",""    import pickle\\r\\\\n"",""    #SaveModel\\r\\\\n"",""    modelPath = os.path.join('.','models',experimentname_param)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if not os.path.exists(modelPath):\\r\\\\n"",""        os.makedirs(modelPath)\\r\\\\n"",""    modelPath = os.path.join(modelPath)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if modelname_param.split('.')[-1] == 'pkl':\\r\\\\n"",""        pickle.dump(model, open(os.path.join(modelPath, modelname_param), 'wb'))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'pt':\\r\\\\n"",""        torch.save(model,os.path.join(modelPath,modelname_param))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'joblib':\\r\\\\n"",""        joblib.dump(model, open(os.path.join(modelPath,modelname_param), 'wb'))\\r\\\\n"",""    else:\\r\\\\n"",""        model.write().overwrite().save(os.path.join(modelPath,modelname_param))\\r\\\\n"",""    modelPath=os.path.join(modelPath,modelname_param)\\r\\\\n"",""\\r\\\\n"",""    return modelPath""]},{""FunctionName"":""labelindexer"",""requirements"":"""",""params"":[],""script"":[""def labelindexer(labels):\\r"",""    from sklearn.preprocessing import LabelEncoder\\r"",""    label_encoder = LabelEncoder()\\r"",""    y_encoded = label_encoder.fit_transform(labels)\\r"",""    return y_encoded, label_encoder""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""KxJks"",""alias"":""ModelLoader"",""name"":""Pre Processing Script"",""classname"":""PreProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""ModelLoader"",""requirements"":"""",""params"":[{""name"":""endpoint"",""value"":""https://10.82.53.110/"",""type"":""Text"",""alias"":""https://10.82.53.110/"",""index"":""1""},{""name"":""access_key"",""value"":""GISeSU7xd6WBnXrU-QbffBee7WsCxaE2"",""type"":""Text"",""alias"":""GISeSU7xd6WBnXrU-QbffBee7WsCxaE2"",""index"":""2""},{""name"":""secret_key"",""value"":""g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0"",""type"":""Text"",""alias"":""g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0"",""index"":""3""},{""name"":""bucket"",""value"":""aicloudprd"",""type"":""Text"",""alias"":""aicloudprd"",""index"":""4""},{""name"":""key"",""value"":""randomforestclusterclassify.joblib"",""type"":""Text"",""alias"":""randomforestclusterclassify.joblib"",""index"":""5""}],""script"":[""def ModelLoader(model_path, endpoint_param='', access_key_param='', secret_key_param='', bucket_param='', key_param=''):\\r\\\\n"",""    import boto3\\r\\\\n"",""    endpoint=endpoint_param\\r\\\\n"",""    access_key=access_key_param\\r\\\\n"",""    secret_key=secret_key_param\\r\\\\n"",""    bucket = bucket_param\\r\\\\n"",""    key = 'Aip_models/' + key_param\\r\\\\n"",""    print('key is: ', key)\\r\\\\n"",""    s3 = boto3.resource(service_name='s3', endpoint_url=endpoint,aws_access_key_id=access_key,aws_secret_access_key=secret_key,verify=False)\\r\\\\n"",""    bucket_object = s3.Bucket(bucket)\\r\\\\n"",""\\r\\\\n"",""    if os.stat(model_path).st_size > 0:\\r\\\\n"",""        bucket_object.upload_file(Key=key, Filename=model_path)""]},""position_x"":""869"",""position_y"":""357"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""eXiHL"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n\\n\\n""},""context"":[{""FunctionName"":""PublishModel"",""requirements"":"""",""params"":[{""name"":""modelname"",""value"":""randomforest.joblib"",""type"":""Text"",""alias"":""randomforest.joblib"",""index"":""1""},{""name"":""experimentname"",""value"":""ClusterClassificationTraining"",""type"":""Text"",""alias"":""ClusterClassificationTraining"",""index"":""2""},{""name"":""typeofmodel"",""value"":""joblib"",""type"":""Text"",""alias"":""joblib"",""index"":""3""}],""script"":[""def PublishModel(model, modelname_param='', experimentname_param='', typeofmodel_param=''):\\r\\\\n"",""    import torch\\r\\\\n"",""    import joblib\\r\\\\n"",""    import pickle\\r\\\\n"",""    #SaveModel\\r\\\\n"",""    modelPath = os.path.join('.','models',experimentname_param)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if not os.path.exists(modelPath):\\r\\\\n"",""        os.makedirs(modelPath)\\r\\\\n"",""    modelPath = os.path.join(modelPath)\\r\\\\n"",""    print('Saving Model at path:'+modelPath )\\r\\\\n"",""    if modelname_param.split('.')[-1] == 'pkl':\\r\\\\n"",""        pickle.dump(model, open(os.path.join(modelPath, modelname_param), 'wb'))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'pt':\\r\\\\n"",""        torch.save(model,os.path.join(modelPath,modelname_param))\\r\\\\n"",""    elif modelname_param.split('.')[-1] == 'joblib':\\r\\\\n"",""        joblib.dump(model, open(os.path.join(modelPath,modelname_param), 'wb'))\\r\\\\n"",""    else:\\r\\\\n"",""        model.write().overwrite().save(os.path.join(modelPath,modelname_param))\\r\\\\n"",""    modelPath=os.path.join(modelPath,modelname_param)\\r\\\\n"",""\\r\\\\n"",""    return modelPath""]},{""FunctionName"":""randomforest"",""requirements"":"""",""params"":[],""script"":[""def randomforest(y_train, X_train):\\r    from sklearn.ensemble import RandomForestClassifier\\r    rf_claf = RandomForestClassifier()\\r    print('training model')\\r    rf_claf.fit(X_train['data'], y_train)\\r    return rf_claf""]},{""FunctionName"":""countvectorize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""1""}],""script"":[""def countvectorize(dataset, input_col_param=''):\\r"",""    from sklearn.feature_extraction.text import CountVectorizer\\r"",""    count_vectorizer = CountVectorizer(stop_words='english')\\r"",""    count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r"",""    words = count_vectorizer.get_feature_names_out()\\r"",""    return {'data': count_data, 'words': words}\\r""]},{""FunctionName"":""remove_stop_words"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""1""},{""name"":""output_col"",""value"":""cleantext"",""type"":""Text"",""alias"":""cleantext"",""index"":""2""},{""name"":""custom_stops_words"",""value"":""a,and"",""type"":""Text"",""alias"":""a,and"",""index"":""3""}],""script"":[""def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r"",""    from nltk.corpus import stopwords\\r"",""    '''\\r"",""    Returns text without stop words\\r"",""    '''\\r"",""    def removesw(input):\\r"",""        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r"",""\\r"",""    custom_stops_words = custom_stops_words_param.split(',')\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r"",""    return dataset\\r""]},{""FunctionName"":""tokenize"",""requirements"":"""",""params"":[{""name"":""input_col"",""value"":""clean_text"",""type"":""Text"",""alias"":""clean_text"",""index"":""1""},{""name"":""output_col"",""value"":""tokenize"",""type"":""Text"",""alias"":""tokenize"",""index"":""2""}],""script"":[""def tokenize(dataset, input_col_param='', output_col_param = ''):\\r"",""    from nltk.tokenize import word_tokenize\\r"",""    '''\\r"",""    Returns tokenized version of text\\r"",""    '''\\r"",""    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r"",""    return dataset\\r""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""FunctionName"":""labelindexer"",""requirements"":"""",""params"":[],""script"":[""def labelindexer(labels):\\r"",""    from sklearn.preprocessing import LabelEncoder\\r"",""    label_encoder = LabelEncoder()\\r"",""    y_encoded = label_encoder.fit_transform(labels)\\r"",""    return y_encoded, label_encoder""]},{""FunctionName"":""preprocessing"",""requirements"":"""",""params"":[],""script"":[""def preprocessing(dataset):\\r    os.environ['JOB_DIRECTORY'] = '.'\\r    dataset = pd.DataFrame(dataset)\\r    dataset.rename(columns={'text': 'clean_text'}, inplace=True)\\r    return dataset, dataset['clusterName']\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-01 02:30:17"",""alias"":""ClusterClassify"",""id"":2772,""name"":""LEACLSTR32975"",""description"":""clustered Tickets with short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM leo1311_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <>\\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","poornasai.nagendra@ad.infosys.com","Cluster Classification Training","2023-08-28 07:33:59","LEACLSTR98551","leo1311","DragNDropLite","80","""""","NULL","{""46"":{""taskId"":""597b5986-91df-4abc-a13a-6900af918fff""}}"
"admin","2023-07-31 04:30:32.176000","\0","","NULL","{""elements"":[{""id"":""xcDtt"",""alias"":""cleanTickets"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""cleanTickets"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\rimport nltk\\rnltk.download('stopwords')\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rdef alphaNum(text):   \\r    alphanumeric = ''    \\r    for character in text:\\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += ' '    \\r    finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r    return ' '.join(finalTokens) \\r    \\rdef stopword_remover(tokens):\\r    stop_words = set(stopwords.words('english'))\\r    #stop_words = ['in','for','an','a','from','is','not','and','to','the','of','on','my','we','has','been','or','as','it','all']\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef lematize(text):\\r    #lematizer = WordNetLemmatizer()\\r    #tokens = text\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef cleanTickets(dataset):\\r    #print('dataset main  ', dataset)\\r    dataset = pd.DataFrame(dataset)\\r    print(type(dataset))  \\r    dataset['alpha_text'] = dataset['shortdescription'].apply(alphaNum)\\r    #print(dataset)\\r    dataset['noStopword_text'] = dataset['alpha_text'].apply(stopword_remover)\\r    #print(dataset)\\r    dataset['CleanText'] = dataset['noStopword_text'].apply(lematize)\\r    print(dataset)\\r    dataset = dataset[['number', 'shortdescription','CleanText']]\\r    #print(dataset)\\r    return dataset\\r""]},""position_x"":""452"",""position_y"":""33"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""AxniL"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""WofXt"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-08 06:34:08"",""alias"":""Clean_Tickets"",""id"":2778,""name"":""LEACLN_T91544"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription, configurationItem AS group_by_field FROM leo1311_tickets WHERE shortdescription <> \\\\\\"" \\\\\\"" AND shortdescription IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_ticket\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""WofXt"",""alias"":""Dataset Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-08 06:34:08"",""alias"":""Clean_Tickets"",""id"":2778,""name"":""LEACLN_T91544"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription, configurationItem AS group_by_field FROM leo1311_tickets WHERE shortdescription <> \\\\\\"" \\\\\\"" AND shortdescription IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_ticket\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""185"",""position_y"":""33"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""xcDtt"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""AxniL"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-16 12:15:08"",""alias"":""result_Tickets"",""id"":2802,""name"":""LEARSLT_17026"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, clean_text, group_by_field, last_updated from leo1311_tickets_enriched\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""overwrite\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""686"",""position_y"":""116"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""xcDtt"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""cleanTickets"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\rimport nltk\\rnltk.download('stopwords')\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rdef alphaNum(text):   \\r    alphanumeric = ''    \\r    for character in text:\\r        if character.isalnum():            \\r            alphanumeric += character        \\r        else:            \\r            alphanumeric += ' '    \\r    finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r    return ' '.join(finalTokens) \\r    \\rdef stopword_remover(tokens):\\r    stop_words = set(stopwords.words('english'))\\r    #stop_words = ['in','for','an','a','from','is','not','and','to','the','of','on','my','we','has','been','or','as','it','all']\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef lematize(text):\\r    #lematizer = WordNetLemmatizer()\\r    #tokens = text\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef cleanTickets(dataset):\\r    #print('dataset main  ', dataset)\\r    dataset = pd.DataFrame(dataset)\\r    print(type(dataset))  \\r    dataset['alpha_text'] = dataset['shortdescription'].apply(alphaNum)\\r    #print(dataset)\\r    dataset['noStopword_text'] = dataset['alpha_text'].apply(stopword_remover)\\r    #print(dataset)\\r    dataset['CleanText'] = dataset['noStopword_text'].apply(lematize)\\r    print(dataset)\\r    dataset = dataset[['number', 'shortdescription','CleanText']]\\r    #print(dataset)\\r    return dataset\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-08 06:34:08"",""alias"":""Clean_Tickets"",""id"":2778,""name"":""LEACLN_T91544"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, shortdescription, configurationItem AS group_by_field FROM leo1311_tickets WHERE shortdescription <> \\\\\\"" \\\\\\"" AND shortdescription IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_ticket\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","cleanTickets","2023-08-28 06:16:27","LEACLNTC62252","leo1311","DragNDropLite","152","NULL","NULL","{""152"":{""taskId"":""8798a621-5f76-446a-b35f-9431f7fbf822""}}"
"admin","2023-07-31 04:36:21.123000","\0","","NULL","{""elements"":[{""id"":""ptuPQ"",""alias"":""Create Clusters"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""createClusters"",""requirements"":"""",""params"":[],""script"":[""# importing required packages\\rfrom datetime import datetime\\rimport pandas as pd\\rdef createClusters(dataset):\\r    print(type(dataset))\\r    dataset = pd.DataFrame(dataset)\\r    sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    numberList = pd.NamedAgg(column='number',aggfunc=list),\\r        textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r    ).reset_index()\\r    sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r    sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    sound_Df = sound_Df.drop(columns=['textList'])\\r    sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    sound_Df['lastUpdated'] = datetime.now()\\r    sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_clusters'})\\r    return sound_Df\\r""]},""position_x"":""496"",""position_y"":""6"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""EyGPQ"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""AqUnw"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-04 05:30:53"",""alias"":""Tickets_cluster"",""id"":2776,""name"":""LEATCKTS64318"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, Soundex(clean_text) as sound, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE clean_text is not null and  Soundex(clean_text) != \\\\\\""\\\\\\"" and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""AqUnw"",""alias"":""DatasetExtractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-04 05:30:53"",""alias"":""Tickets_cluster"",""id"":2776,""name"":""LEATCKTS64318"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, Soundex(clean_text) as sound, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE clean_text is not null and  Soundex(clean_text) != \\\\\\""\\\\\\"" and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""211"",""position_y"":""6"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""ptuPQ"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""EyGPQ"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-16 12:20:22"",""alias"":""Clusters_soundex"",""id"":2804,""name"":""LEACLSTR41446"",""description"":""Tickets with soundex of Short description"",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""Select  number , soundex_cluster, last_updated from leo1311_soundex\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""overwrite\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\"" leo1311_soundex\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""839"",""position_y"":""6"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""ptuPQ"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""createClusters"",""requirements"":"""",""params"":[],""script"":[""# importing required packages\\rfrom datetime import datetime\\rimport pandas as pd\\rdef createClusters(dataset):\\r    print(type(dataset))\\r    dataset = pd.DataFrame(dataset)\\r    sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r    numberList = pd.NamedAgg(column='number',aggfunc=list),\\r        textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r    ).reset_index()\\r    sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r    sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r    sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r    sound_Df = sound_Df.drop(columns=['textList'])\\r    sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r    sound_Df['lastUpdated'] = datetime.now()\\r    sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_clusters'})\\r    return sound_Df\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-04 05:30:53"",""alias"":""Tickets_cluster"",""id"":2776,""name"":""LEATCKTS64318"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, Soundex(clean_text) as sound, CASE WHEN group_by_field IS NULL THEN \\\\\\""\\\\\\"" ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE clean_text is not null and  Soundex(clean_text) != \\\\\\""\\\\\\"" and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":"""",""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","Clustering soundex","2023-08-23 10:24:07","LEACLSTR70385","leo1311","DragNDropLite","65","NULL","NULL","{""59"":{""taskId"":""4daf3827-0d9f-4fa7-a5fa-93f1ae579128""}}"
"poornasai.nagendra@ad.infosys.com","2023-07-31 12:13:46.632000","\0","Create incident, service request, incident task, change request in SNOW and then in  icm_tickets","NULL","{""elements"":[{""id"":""HcDTl"",""alias"":""SnowCreateApi"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""snow_create_api"",""requirements"":"""",""params"":[{""name"":""api"",""value"":""/api/now/table/"",""type"":""Text"",""alias"":""/api/now/table/"",""index"":""1""},{""name"":""params"",""value"":""sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident"",""type"":""Text"",""alias"":""sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident"",""index"":""2""},{""name"":""setProxy"",""value"":""True"",""type"":""Text"",""alias"":""True"",""index"":""3""},{""name"":""incidentPayload"",""value"":""{\\""shortdescription\\"": \\""VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\"", \\""priority\\"": \\""5\\"", \\""state\\"": \\""1\\""}"",""type"":""Text"",""alias"":""{\\""shortdescription\\"": \\""VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\"", \\""priority\\"": \\""5\\"", \\""state\\"": \\""1\\""}"",""index"":""4""},{""name"":""ticketType"",""value"":""Incident"",""type"":""Text"",""alias"":""Incident"",""index"":""5""},{""name"":""dataTable"",""value"":""leo1311_tickets"",""type"":""Text"",""alias"":""leo1311_tickets"",""index"":""6""}],""script"":[""import requests\\rimport sys\\rimport os\\rimport json\\rimport datetime\\rfrom urllib.parse import quote_plus, urlparse\\rfrom leaputils import Security\\rimport logging as logger\\rimport numpy as np\\r\\r\\ros.environ['HTTP_PROXY']='http://10.219.2.222:80' \\ros.environ['HTTPS_PROXY']='http://10.219.2.222:80'\\r\\rdef snow_create_api(snow_data_source, leapdatasource, api_param='', \\r                    params_param='', setproxy_param='', \\r                    incidentpayload_param='', tickettype_param='', datatable_param=''):\\r    argdict={'api': api_param,\\r    'params': params_param,\\r    'setProxy': setproxy_param,\\r    'incidentPayload': incidentpayload_param,\\r    'LEAPDataSource': leapdatasource,\\r    'ticketType': tickettype_param,\\r    'dataTable': datatable_param,\\r    'SnowDataSource': snow_data_source\\r    }\\r    #SNow Datasource\\r    snowds = argdict['SnowDataSource']\\r    snowDSdict=snowds\\r\\r    # Set the request parameters\\r    url = snowDSdict['Url']\\r    user = snowDSdict['AuthDetails']['username']\\r    pwd = Security.decrypt(snowDSdict['AuthDetails']['password'], snowDSdict['salt'])\\r    pwd = 'qwer1234'\\r    api = argdict['api']\\r    params =argdict['params']\\r    ticketType = argdict['ticketType']\\r    tablename = argdict['dataTable']\\r\\r    if(ticketType.lower() == 'incident'):\\r        snowtable = 'incident'\\r    elif(ticketType.lower()=='changerequest-normal'):\\r        snowtable = 'change_request'\\r    elif(ticketType.lower() == 'servicerequest'):\\r        snowtable = 'sc_request'\\r    elif(ticketType.lower() == 'incidenttask'):\\r        snowtable = 'incident_task'\\r\\r    icmPayload = argdict['incidentPayload']\\r    icmPayload = json.loads(icmPayload)\\r    icmPayloadMapping = {'number':'number','shortdescription':'short_description','priority.systemId':'priority','state.systemId':'state','description':'description',\\r                            'sysId':'sys_id','category.systemId':'category','impact.systemId':'impact','assignmentgroup.systemId':'assignment_group','assignedto.systemId':'assigned_to',\\r                            'configurationitem.systemId':'cmdb_ci','urgency.systemId':'urgency' }\\r\\r    snowPayload = {}\\r    for key in icmPayloadMapping.keys():\\r        jv = icmPayload\\r        icmColumn = key.split('.')\\r        for item in icmColumn:\\r            try:\\r                jv = jv[item]\\r                snowPayload[icmPayloadMapping[key]] = jv\\r            except:\\r                a='No mapping'\\r        \\r\\r    snowPayload = json.dumps(snowPayload)\\r\\r    #setproxy\\r    proxyDict ={}\\r    if argdict['setProxy'] == 'True':\\r        proxyDict = {\\r                    'http'  : os.environ['HTTP_PROXY'],\\r                    'https' : os.environ['HTTPS_PROXY']      \\r                    }\\r    #set headers\\r    headers = {'Content-Type':'application/json','Accept':'application/json'}\\r    url = url+api+snowtable+'?'+params\\r    # Do the HTTP request\\r    response = requests.post(url, auth=(user, pwd), headers=headers ,data=snowPayload,proxies = proxyDict, verify = False )\\r\\r    # Check for HTTP codes other than 201\\r    if response.status_code != 201:\\r        logger.info('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:',response.json())\\r        exit()\\r\\r    # Decode the JSON response into a dictionary and use the data\\r    data = response.json()\\r\\r    jsonStringArray = []\\r\\r    row = data['result']\\r    if(ticketType.lower() == 'servicerequest'):\\r        row['price'] = float(row['price'][1:])\\r    jsonstr =json.dumps(row)\\r    # jsonStringArray.append(jsonstr)\\r    jsonStringArray.append(json.loads(jsonstr))\\r    print('jsonStringArray', jsonStringArray)\\r\\r    # spark = SparkSession.builder.master('local').appName('sla').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\\r    # #convert json to spark dataframe\\r    # df = spark.read.json(spark.sparkContext.parallelize(jsonStringArray))\\r    df = pd.DataFrame(jsonStringArray)\\r\\r    snowToIcmColumnMapping = {'number':'number','short_description':'shortdescription','priority':'priority','state':'state','description':'description',\\r                            'sys_id':'sysId','opened_at':'openedDate','sys_created_on':'createdDate','sys_updated_on':'updatedDate',\\r                            'sla_due':'sladueDate','closed_at':'closedDate','due_date':'duedate','sys_created_by':'createdby',\\r                            'reopened_time':'reopenedDate','resolved_at':'resolvedDate','category':'category',\\r                            'close_code':'closecode','close_code':'resolutionCategory','impact':'impact','requested_for.display_value':'requested_for',\\r                            'assignment_group.display_value':'assignmentgroup','caller_id.display_value':'caller','assigned_to.display_value':'assignedto',\\r                            'resolved_by.display_value':'resolvedby','closed_by.display_value':'closedby','cmdb_ci.display_value':'configurationItem',\\r                            'close_notes':'closenotes','close_notes':'resolution_steps','location.display_value':'location','request_state':'request_state','price':'price',\\r                            'special_instructions':'special_instructions','approval':'approval','business_service':'business_service',\\r                            'risk':'risk','type':'type','requested_by.display_value':'requested_by','incident.display_value':'parent_Incident',\\r                            'urgency':'severity' }\\r    #mapColumns\\r    foundColumns = []\\r    for key in snowToIcmColumnMapping.keys():\\r        try:\\r            icmColumn =snowToIcmColumnMapping[key]\\r            snowColumn= key\\r            df[icmColumn] = np.where(df[snowColumn] == '', None, df[snowColumn])\\r            foundColumns.append(icmColumn)\\r        except:\\r            error = 'Column Not found'\\r            \\r    print('foundColumns', foundColumns)\\r\\r    df = df[foundColumns]\\r\\r    # Add static columns - ICM specific\\r\\r    df['source'] = 'SNOW'\\r\\r    def recordType(i):\\r        switcher = {\\r            'incident': 'Incident',\\r            'change_request': 'ChangeRequest-Normal',\\r            'incident_task': 'Task',\\r            'sc_request': 'ServiceRequest'\\r        }\\r        return switcher.get(i, 'Incident')\\r    df['type'] = recordType(snowtable.lower())\\r    df['lastUpdated'] = datetime.datetime.now()\\r\\r    print(df)\\r    # logger.info(df)\\r\\r    \\r    return df.to_dict('records')\\r""]},""position_x"":""433"",""position_y"":""171"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""iwbCo"",""elementPosition"":""LeftMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""QgDqu"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""LPcSS"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-30 22:49:23"",""alias"":""Get Incidents"",""id"":2745,""name"":""LEAGTNCD43053"",""description"":""GET  /api/now/table/incident"",""schema"":null,""attributes"":""{\\""bodyType\\"":\\""Text\\"",\\""Cacheable\\"":false,\\""transformData\\"":false,\\""RequestMethod\\"":\\""GET\\"",\\""TransformationScript\\"":\\""\\\\\\""import groovy.json.*\\\\\\\\n\\\\\\\\ndef inpResponse = \\\\\\\\\\\\\\""$inputJson\\\\\\\\\\\\\\""\\\\\\\\n//start your code from here\\\\\\\\n\\\\\\""\\"",\\""Headers\\"":\\""\\"",\\""bodyOption\\"":\\""raw\\"",\\""pretransformData\\"":false,\\""QueryParams\\"":[{\\""value\\"":\\""5\\"",\\""key\\"":\\""sysparm_limit\\""}],\\""preTransformationScript\\"":\\""\\\\\\""import groovy.json.*\\\\\\\\n\\\\\\\\ndef inpBody = \\\\\\\\\\\\\\""$Body\\\\\\\\\\\\\\""\\\\\\\\ndef inpQueryParams = \\\\\\\\\\\\\\""$QueryParams\\\\\\\\\\\\\\""\\\\\\\\ndef inpHeaders = \\\\\\\\\\\\\\""$Headers\\\\\\\\\\\\\\""\\\\\\\\ndef inpbodyType = \\\\\\\\\\\\\\""$bodyType\\\\\\\\\\\\\\""\\\\\\\\ndef inpPathVariables = \\\\\\\\\\\\\\""$PathVariables\\\\\\\\\\\\\\""\\\\\\\\ndef inpUrl = \\\\\\\\\\\\\\""$Url\\\\\\\\\\\\\\""\\\\\\\\ndef inpConfigVariables = \\\\\\\\\\\\\\""$ConfigVariables\\\\\\\\\\\\\\""\\\\\\\\n\\\\\\\\n//start your code from here\\\\\\\\n\\\\\\\\nreturn JsonOutput.prettyPrint(JsonOutput.toJson([Body:{preprocessed_Body}, QueryParams:{preprocessed_QueryParams},Headers:{preprocessed_Headers},bodyType:{preprocessed_bodyType},PathVariables:{preprocessed_PathVariables},Url:{preprocessed_Url},ConfigVariables:{preprocessed_ConfigVariables}]))\\\\\\\\n\\\\\\""\\"",\\""Body\\"":\\""\\"",\\""Url\\"":\\""/api/now/table/incident\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-30 22:49:24"",""alias"":""SNOW"",""id"":541,""name"":""LEASNWSP56549"",""description"":""Service Now"",""type"":""REST"",""connectionDetails"":""{\\""NoProxy\\"":\\""true\\"",\\""ConnectionType\\"":\\""ApiRequest\\"",\\""testDataset\\"":{\\""name\\"":\\""\\"",\\""attributes\\"":{\\""bodyType\\"":\\""raw\\"",\\""Endpoint\\"":\\""\\"",\\""RequestMethod\\"":\\""GET\\"",\\""Headers\\"":\\""\\"",\\""LeapParams\\"":[],\\""QueryParams\\"":\\""\\"",\\""Body\\"":\\""\\""}},\\""AuthDetails\\"":{\\""password\\"":\\""encQ0iETSgsUqcjR4DpfRcaq1JHTqADQpoc\\"",\\""authParams\\"":{\\""grant_type\\"":\\""\\"",\\""client_secret\\"":\\""\\"",\\""client_id\\"":\\""\\""},\\""username\\"":\\""ICSP_icap_user\\""},\\""AuthType\\"":\\""BasicAuth\\"",\\""Url\\"":\\""https://infosysq3dev1.service-now.com\\"",\\""fileId\\"":\\""\\"",\\""tokenExpirationTime\\"":\\""\\""}"",""salt"":""wSj9Bf2qHFKpn+acHaJ4C0/DFSzjAzjW55bCpdBLrA2gnXHZ+x9jIsT3S4QWzlm666fWc59dJskXOeErVM/baw=="",""organization"":""leo1311"",""dshashcode"":""4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7"",""activetime"":""2023-07-30 22:49:23"",""category"":""REST"",""extras"":""{\\""apispec\\"":{\\""openapi\\"":\\""3.0.2\\"",\\""info\\"":{\\""version\\"":1,\\""title\\"":\\""SNOW\\"",\\""description\\"":\\""GET  /api/now/table/incident\\""},\\""servers\\"":[{\\""url\\"":\\""https://ai-platform\\""}],\\""paths\\"":{\\""/api/service/REST/SNOW/Get Incidents\\"":{\\""get\\"":{\\""dataset\\"":\\""LEAGTNCD43053\\"",\\""parameters\\"":[{\\""name\\"":\\""sysparm_limit\\"",\\""value\\"":\\""5\\"",\\""description\\"":\\""sysparm_limit\\"",\\""required\\"":\\""true\\"",\\""type\\"":\\""string\\"",\\""in\\"":\\""params\\""}],\\""responses\\"":{\\""200\\"":{\\""description\\"":\\""response\\"",\\""content\\"":{\\""application/json\\"":{\\""schema\\"":{\\""type\\"":\\""string\\""}}}},\\""default\\"":{\\""description\\"":\\""error\\"",\\""content\\"":{\\""application/json\\"":{\\""schema\\"":{\\""type\\"":\\""string\\""}}}}}}}},\\""components\\"":{\\""schemas\\"":{\\""input\\"":{\\""properties\\"":{}}},\\""securitySchemes\\"":{\\""bearerAuth\\"":{\\""type\\"":\\""http\\"",\\""scheme\\"":\\""bearer\\"",\\""bearerFormat\\"":\\""JWT\\""},\\""basicAuth\\"":{\\""type\\"":\\""http\\"",\\""scheme\\"":\\""basic\\""}},\\""security\\"":[{\\""bearerAuth\\"":[],\\""basicAuth\\"":[]}]}},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-03 13:16:01"",""alias"":""Tickets"",""id"":2775,""name"":""LEATCKTS50134"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, Soundex(clean_text) as sound, CASE WHEN group_by_field IS NULL THEN '' ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE clean_text is not null and  Soundex(clean_text) != \\\\\\""\\\\\\"" and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""QgDqu"",""alias"":""DataSourcedict"",""name"":""DataSourcedict"",""classname"":""DatasetSourceConfig"",""category"":""DatasetSourceConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-30 22:49:23"",""alias"":""Get Incidents"",""id"":2745,""name"":""LEAGTNCD43053"",""description"":""GET  /api/now/table/incident"",""schema"":null,""attributes"":""{\\""bodyType\\"":\\""Text\\"",\\""Cacheable\\"":false,\\""transformData\\"":false,\\""RequestMethod\\"":\\""GET\\"",\\""TransformationScript\\"":\\""\\\\\\""import groovy.json.*\\\\\\\\n\\\\\\\\ndef inpResponse = \\\\\\\\\\\\\\""$inputJson\\\\\\\\\\\\\\""\\\\\\\\n//start your code from here\\\\\\\\n\\\\\\""\\"",\\""Headers\\"":\\""\\"",\\""bodyOption\\"":\\""raw\\"",\\""pretransformData\\"":false,\\""QueryParams\\"":[{\\""value\\"":\\""5\\"",\\""key\\"":\\""sysparm_limit\\""}],\\""preTransformationScript\\"":\\""\\\\\\""import groovy.json.*\\\\\\\\n\\\\\\\\ndef inpBody = \\\\\\\\\\\\\\""$Body\\\\\\\\\\\\\\""\\\\\\\\ndef inpQueryParams = \\\\\\\\\\\\\\""$QueryParams\\\\\\\\\\\\\\""\\\\\\\\ndef inpHeaders = \\\\\\\\\\\\\\""$Headers\\\\\\\\\\\\\\""\\\\\\\\ndef inpbodyType = \\\\\\\\\\\\\\""$bodyType\\\\\\\\\\\\\\""\\\\\\\\ndef inpPathVariables = \\\\\\\\\\\\\\""$PathVariables\\\\\\\\\\\\\\""\\\\\\\\ndef inpUrl = \\\\\\\\\\\\\\""$Url\\\\\\\\\\\\\\""\\\\\\\\ndef inpConfigVariables = \\\\\\\\\\\\\\""$ConfigVariables\\\\\\\\\\\\\\""\\\\\\\\n\\\\\\\\n//start your code from here\\\\\\\\n\\\\\\\\nreturn JsonOutput.prettyPrint(JsonOutput.toJson([Body:{preprocessed_Body}, QueryParams:{preprocessed_QueryParams},Headers:{preprocessed_Headers},bodyType:{preprocessed_bodyType},PathVariables:{preprocessed_PathVariables},Url:{preprocessed_Url},ConfigVariables:{preprocessed_ConfigVariables}]))\\\\\\\\n\\\\\\""\\"",\\""Body\\"":\\""\\"",\\""Url\\"":\\""/api/now/table/incident\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-30 22:49:24"",""alias"":""SNOW"",""id"":541,""name"":""LEASNWSP56549"",""description"":""Service Now"",""type"":""REST"",""connectionDetails"":""{\\""NoProxy\\"":\\""true\\"",\\""ConnectionType\\"":\\""ApiRequest\\"",\\""testDataset\\"":{\\""name\\"":\\""\\"",\\""attributes\\"":{\\""bodyType\\"":\\""raw\\"",\\""Endpoint\\"":\\""\\"",\\""RequestMethod\\"":\\""GET\\"",\\""Headers\\"":\\""\\"",\\""LeapParams\\"":[],\\""QueryParams\\"":\\""\\"",\\""Body\\"":\\""\\""}},\\""AuthDetails\\"":{\\""password\\"":\\""encQ0iETSgsUqcjR4DpfRcaq1JHTqADQpoc\\"",\\""authParams\\"":{\\""grant_type\\"":\\""\\"",\\""client_secret\\"":\\""\\"",\\""client_id\\"":\\""\\""},\\""username\\"":\\""ICSP_icap_user\\""},\\""AuthType\\"":\\""BasicAuth\\"",\\""Url\\"":\\""https://infosysq3dev1.service-now.com\\"",\\""fileId\\"":\\""\\"",\\""tokenExpirationTime\\"":\\""\\""}"",""salt"":""wSj9Bf2qHFKpn+acHaJ4C0/DFSzjAzjW55bCpdBLrA2gnXHZ+x9jIsT3S4QWzlm666fWc59dJskXOeErVM/baw=="",""organization"":""leo1311"",""dshashcode"":""4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7"",""activetime"":""2023-07-30 22:49:23"",""category"":""REST"",""extras"":""{\\""apispec\\"":{\\""openapi\\"":\\""3.0.2\\"",\\""info\\"":{\\""version\\"":1,\\""title\\"":\\""SNOW\\"",\\""description\\"":\\""GET  /api/now/table/incident\\""},\\""servers\\"":[{\\""url\\"":\\""https://ai-platform\\""}],\\""paths\\"":{\\""/api/service/REST/SNOW/Get Incidents\\"":{\\""get\\"":{\\""dataset\\"":\\""LEAGTNCD43053\\"",\\""parameters\\"":[{\\""name\\"":\\""sysparm_limit\\"",\\""value\\"":\\""5\\"",\\""description\\"":\\""sysparm_limit\\"",\\""required\\"":\\""true\\"",\\""type\\"":\\""string\\"",\\""in\\"":\\""params\\""}],\\""responses\\"":{\\""200\\"":{\\""description\\"":\\""response\\"",\\""content\\"":{\\""application/json\\"":{\\""schema\\"":{\\""type\\"":\\""string\\""}}}},\\""default\\"":{\\""description\\"":\\""error\\"",\\""content\\"":{\\""application/json\\"":{\\""schema\\"":{\\""type\\"":\\""string\\""}}}}}}}},\\""components\\"":{\\""schemas\\"":{\\""input\\"":{\\""properties\\"":{}}},\\""securitySchemes\\"":{\\""bearerAuth\\"":{\\""type\\"":\\""http\\"",\\""scheme\\"":\\""bearer\\"",\\""bearerFormat\\"":\\""JWT\\""},\\""basicAuth\\"":{\\""type\\"":\\""http\\"",\\""scheme\\"":\\""basic\\""}},\\""security\\"":[{\\""bearerAuth\\"":[],\\""basicAuth\\"":[]}]}},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""214"",""position_y"":""172"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""HcDTl"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out1""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DataSourcedictREST(dataset_datasource_Url='', dataset_datasource_AuthDetails_username='', dataset_datasource_salt='', dataset_datasource_AuthDetails_password=''):\\r\\n    DSdict = {\\r\\n        'Url': dataset_datasource_Url,\\r\\n        'salt': dataset_datasource_salt,\\r\\n        'AuthDetails': {\\r\\n            'username': dataset_datasource_AuthDetails_username,\\r\\n            'password': dataset_datasource_AuthDetails_password\\r\\n        }\\r\\n    }\\r\\n    return DSdict\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""context"":[]},{""id"":""LPcSS"",""alias"":""DataSourcedict"",""name"":""DataSourcedict"",""classname"":""DatasetSourceConfig"",""category"":""DatasetSourceConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-03 13:16:01"",""alias"":""Tickets"",""id"":2775,""name"":""LEATCKTS50134"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, Soundex(clean_text) as sound, CASE WHEN group_by_field IS NULL THEN '' ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE clean_text is not null and  Soundex(clean_text) != \\\\\\""\\\\\\"" and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""321"",""position_y"":""48"",""connectors"":[{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""HcDTl"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out1""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""requirements"":[],""imports"":[],""script"":""def DataSourcedictREST(dataset_datasource_Url='', dataset_datasource_AuthDetails_username='', dataset_datasource_salt='', dataset_datasource_AuthDetails_password=''):\\r\\n    DSdict = {\\r\\n        'Url': dataset_datasource_Url,\\r\\n        'salt': dataset_datasource_salt,\\r\\n        'AuthDetails': {\\r\\n            'username': dataset_datasource_AuthDetails_username,\\r\\n            'password': dataset_datasource_AuthDetails_password\\r\\n        }\\r\\n    }\\r\\n    return DSdict\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""context"":[]},{""id"":""iwbCo"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-08-17 21:29:27"",""alias"":""SNOW_Create_API"",""id"":2807,""name"":""LEASNW_C55246"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_tickets;\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""677"",""position_y"":""169"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""HcDTl"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{""imports"":[""from urllib.parse import urlparse"",""import requests"",""from requests.auth import HTTPBasicAuth"",""from requests import auth"",""from leaputils import Security"",""import json""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    connection_type = \\""<dataset.datasource.connectionDetails.ConnectionType>\\""\\n    auth_type = \\""<dataset.datasource.connectionDetails.AuthType>\\""\\n    auth_details = \\""<dataset.datasource.connectionDetails.AuthDetails>\\""\\n    test_dataset = \\""<dataset.datasource.connectionDetails.testDataset>\\""\\n    noProxy = \\""<dataset.datasource.connectionDetails.noProxy>\\""\\n    salt = \\""<dataset.datasource.connectionDetails.salt>\\""\\n    url = \\""<dataset.attributes.Url>\\""\\n    method = \\""<dataset.attributes.RequestMethod>\\""\\n    path = \\""<dataset.attributes.EndPoint>\\""\\n    params = \\""<dataset.attributes.QueryParams>\\""\\n    headers = \\""<dataset.attributes.Headers>\\""\\n    requestBody = \\""<dataset.attributes.Body>\\""\\n    documentElement = \\""<TransformationScript>\\""\\n    \\n    if connection_type.lower() == \\""apirequest\\"":\\n        URL = url\\n    elif connection_type.lower() == \\""apispec\\"":\\n        URL = url + path\\n    logging.info(\\""Connecting to URL {0}\\"".format(URL))\\n\\n    PROXIES = {}\\n    hostname = urlparse(URL).hostname\\n    if (hostname != '' and hostname in os.environ.get(\\""NO_PROXY\\"",\\""\\"").split(',')) or (noProxy.lower() == 'true'):\\n        logging.info(\\""Removing Proxy\\"")\\n        PROXIES['http'] = ''\\n        PROXIES['https'] = ''\\n    auth_details=auth_details\\n    auth_token=\\""\\""\\n\\n    header_prefix = \\""Bearer\\""\\n    response = \\""\\""\\n\\n    params = {}\\n    HEADERS = {}\\n    if params != '':\\n        params_list = params\\n        for item in params_list:\\n            item_object = item\\n            params[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if headers != '':\\n        headers_list=headers\\n        for item in headers_list:\\n            item_object=item\\n            HEADERS[item_object.get(\\""key\\"")] = item_object.get(\\""value\\"")\\n\\n    if auth_type.lower() == \\""basicauth\\"":\\n\\n        username = auth_details.get(\\""username\\"")\\n        enc_password = auth_details.get(\\""password\\"")\\n        password=enc_password\\n        if str(enc_password).startswith('enc'):\\n            password = Security.decrypt(enc_password, salt)\\n\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, auth=HTTPBasicAuth(username, password), verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    elif auth_type.lower() == \\""bearertoken\\"":\\n        auth_token = auth_details.get(\\""authToken\\"")\\n\\n    elif auth_type.lower() == \\""oauth\\"":\\n        auth_url = auth_details.get(\\""authUrl\\"")\\n        auth_params = auth_details.get(\\""authParams\\"")\\n        auth_headers = auth_details.get(\\""authHeaders\\"")\\n        header_prefix = auth_details.get(\\""HeaderPrefix\\"")\\n        auth_method = auth_details.get(\\""authMethod\\"" , \\""GET\\"")\\n        token_element = auth_details.get(\\""tokenElement\\"", \\""\\"")\\n\\n        authResponse = requests.request(method=auth_method, url=auth_url ,params=auth_params, headers = auth_headers,\\n                                        timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n        if token_element!=\\""\\"":\\n            auth_token = json.loads(str(authResponse)).get(token_element)\\n\\n        else:\\n            auth_token= authResponse.json()\\n\\n    elif auth_type.lower() == \\""noauth\\"":\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    if auth_token!= \\""\\"":\\n        HEADERS['Authorization'] = header_prefix + \\"" \\"" + auth_token\\n        response = requests.request(method=method, url=URL, headers=HEADERS, params=params,\\n                                    proxies=PROXIES, verify=False, data=dataset,\\n                                    timeout=(int(os.environ.get(\\""CONNECT_TIMEOUT\\"",\\""30\\"")), int(os.environ.get(\\""READ_TIMEOUT\\"",\\""30\\""))))\\n\\n    logging.info(\\""Response Code: {0}\\"".format(response.status_code))\\n""},""MYSQL"":{""imports"":[""import mysql.connector"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n    \\n\\n    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""Drop table IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""`{0}` TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""`{0}`\\"".format(k) for k in paramsDict)\\n            duplicates = \\"", \\"".join(\\""{0}=VALUES({0})\\"".format(k) for k in paramsDict)\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n            data.append(values)\\n        \\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()\\n    cnx.close()""},""MSSQL"":{""imports"":[""from leap.core.iLoader import Loader"",""from leap.utils.Utilities import Utilities"",""import logging as logger"",""from leap.utils import vault"",""import pyodbc"",""import re"",""from datetime import datetime"",""import os""],""script"":""def DatasetLoader_<id>(dataset):\\n\\n\\n    mode = \\\\\\""<dataset.attributes.writeMode>\\\\\\""\\n\\n    url=\\\\\\""<dataset.datasource.connectionDetails.url>\\\\\\""\\n\\n    tablename = \\\\\\""<dataset.attributes.tableName>\\\\\\""\\n\\n    username = \\\\\\""<dataset.datasource.connectionDetails.userName>\\\\\\""\\n\\n    password = Security.decrypt(\\\\\\""<dataset.datasource.connectionDetails.password>\\\\\\"",\\\\\\""<dataset.datasource.salt>\\\\\\"")\\n\\n    temp1 = self.url.split(\\""//\\"")\\n\\n    temp2 = temp1[1].split(\\"";\\"")\\n\\n    server = temp2[0]\\n\\n    database = (temp2[1].split(\\""=\\""))[1]\\n\\n    isTrusted = \\""no\\""\\n\\n    if username == \\""\\"":\\n\\n    isTrusted = \\""yes\\""\\n\\n    regex = \\\\\\""^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\\\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$\\\\\\""\\n\\n\\n    if(re.search(regex, server.split(\\"":\\"")[0])):\\n\\n        server=server.replace(\\"":\\"",\\"",\\"")\\n\\n\\n    connectionString = \\\\\\""DRIVER={0};SERVER={1}; \\\\\\""\\n\\n                       \\\\\\""DATABASE={2};UID={3};PWD={4}; trusted_connection={5}\\\\\\"".format(\\n\\n        \\""ODBC Driver 17 for SQL SERVER\\"", server, database, username, password, isTrusted)\\n\\n    connection = pyodbc.connect(connectionString)\\n\\n    cursor = connection.cursor()\\n\\n    \\n\\n    if dataset != None and len(dataset) > 0:\\n\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\\\\\""overwrite\\\\\\"":\\n\\n        cursor.execute(\\\\\\""Drop table IF EXISTS {0}\\\\\\"".format(tablename))\\n\\n\\n    # create table if not exists\\n\\n    column_definition = \\\\\\"", \\\\\\"".join([\\\\\\""`{0}` TEXT\\\\\\"".format(c) for c in columnList])\\n\\n    createQuery = \\\\\\"" CREATE TABLE IF NOT EXISTS {0} ({1})\\\\\\"".format(tablename, column_definition)\\n\\n    cursor.execute(createQuery)\\n\\n \\n\\n    data = []\\n\\n    for row in dataset:\\n\\n        try:\\n\\n            paramsDict = {}\\n\\n            values = []\\n\\n            for i in range(0, len(columnList)):\\n\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n\\n                values.append(row[columnList[i]])\\n\\n            columns = \\\\\\"", \\\\\\"".join(\\\\\\""`{0}`\\\\\\"".format(k) for k in paramsDict)\\n\\n            duplicates = \\\\\\"", \\\\\\"".join(\\\\\\""{0}=VALUES({0})\\\\\\"".format(k) for k in paramsDict)\\n\\n            place_holders = \\\\\\"", \\\\\\"".join(\\\\\\""%s\\\\\\"".format(k) for k in paramsDict)\\n\\n            query = \\\\\\""INSERT INTO {0} ({1}) VALUES ({2})\\\\\\"".format(tablename, columns, place_holders)\\n\\n            if mode in (\\\\\\""update\\\\\\""):\\n\\n                query = \\\\\\""{0} ON DUPLICATE KEY UPDATE {1}\\\\\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n\\n            logging.error(\\\\\\""{0}:{1}\\\\\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n\\n        cursor.executemany(query, data)\\n\\n        connection.commit()\\n\\n      \\n\\n    cursor.close()\\n\\n    connection.close()""},""AWS"":{""imports"":[""import pandas as pd"",""import pickle"",""import os""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    url = \\""<dataset.attributes.Url>\\""\\n    filename = url.split('/')[-1]\\n    extension = filename.split('.')[-1]\\n\\n    data_directory = \\""/opt/ml/processing/output\\""\\n    file_path = os.path.join(data_directory, filename)\\n    print(\\""Saving data\\"")\\n    if extension == '.csv':\\n        dataset.to_csv(file_path)\\n    elif extension == 'pkl':\\n        pickle.dumps(dataset, open(file_path, 'wb'))\\n    else:\\n        with open(file_path, 'w') as f:\\n            f.writelines(dataset)\\n\\n""},""POSTGRESQL"":{""imports"":[""import psycopg2"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\ndef DatasetLoader_<id>(dataset):\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    url=\\""<dataset.datasource.connectionDetails.url>\\""\\n    tablename = \\""<dataset.attributes.tableName>\\""\\n    username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n    password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n    host = urlparse(url[5:]).hostname\\n    port = urlparse(url[5:]).port\\n    database = urlparse(url[5:]).path.rsplit(\\""/\\"", 1)[1]\\n\\n    cnx = psycopg2.connect(user=username, password=password, host=host, port=port, database=database)\\n    mycursor = cnx.cursor()\\n\\n    if dataset != None and len(dataset) > 0:\\n        columnList = list(dataset[0].keys())\\n\\n    if mode in \\""overwrite\\"":\\n        mycursor.execute(\\""DROP TABLE IF EXISTS {0}\\"".format(tablename))\\n\\n    # create table if not exists\\n    column_definition = \\"", \\"".join([\\""{0} TEXT\\"".format(c) for c in columnList])\\n    createQuery = \\""CREATE TABLE IF NOT EXISTS {0} ({1})\\"".format(tablename, column_definition)\\n    mycursor.execute(createQuery)\\n    data = []\\n\\n    for row in dataset:\\n        try:\\n            paramsDict = {}\\n            values = []\\n            for i in range(0, len(columnList)):\\n                paramsDict[columnList[i]] = row[columnList[i]]\\n                values.append(row[columnList[i]])\\n\\n            columns = \\"", \\"".join(\\""{0}\\"".format(k) for k in paramsDict.keys())\\n            duplicates = \\"", \\"".join(\\""{0}=EXCLUDED.{0}\\"".format(k) for k in paramsDict.keys())\\n            place_holders = \\"", \\"".join(\\""%s\\"".format(k) for k in paramsDict)\\n\\n            query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n            if mode in (\\""update\\""):\\n                query = \\""{0} ON CONFLICT DO UPDATE SET {1}\\"".format(query, duplicates)\\n\\n            data.append(values)\\n\\n        except Exception as e:\\n            print(\\""{0}:{1}\\"".format(e,row))\\n\\n    if(len(data) > 0):\\n        mycursor.executemany(query, data)\\n        cnx.commit()\\n\\n    mycursor.close()""}},""context"":[{""FunctionName"":""snow_create_api"",""requirements"":"""",""params"":[{""name"":""api"",""value"":""/api/now/table/"",""type"":""Text"",""alias"":""/api/now/table/"",""index"":""1""},{""name"":""params"",""value"":""sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident"",""type"":""Text"",""alias"":""sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident"",""index"":""2""},{""name"":""setProxy"",""value"":""True"",""type"":""Text"",""alias"":""True"",""index"":""3""},{""name"":""incidentPayload"",""value"":""{\\""shortdescription\\"": \\""VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\"", \\""priority\\"": \\""5\\"", \\""state\\"": \\""1\\""}"",""type"":""Text"",""alias"":""{\\""shortdescription\\"": \\""VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\"", \\""priority\\"": \\""5\\"", \\""state\\"": \\""1\\""}"",""index"":""4""},{""name"":""ticketType"",""value"":""Incident"",""type"":""Text"",""alias"":""Incident"",""index"":""5""},{""name"":""dataTable"",""value"":""leo1311_tickets"",""type"":""Text"",""alias"":""leo1311_tickets"",""index"":""6""}],""script"":[""import requests\\rimport sys\\rimport os\\rimport json\\rimport datetime\\rfrom urllib.parse import quote_plus, urlparse\\rfrom leaputils import Security\\rimport logging as logger\\rimport numpy as np\\r\\r\\ros.environ['HTTP_PROXY']='http://10.219.2.222:80' \\ros.environ['HTTPS_PROXY']='http://10.219.2.222:80'\\r\\rdef snow_create_api(snow_data_source, leapdatasource, api_param='', \\r                    params_param='', setproxy_param='', \\r                    incidentpayload_param='', tickettype_param='', datatable_param=''):\\r    argdict={'api': api_param,\\r    'params': params_param,\\r    'setProxy': setproxy_param,\\r    'incidentPayload': incidentpayload_param,\\r    'LEAPDataSource': leapdatasource,\\r    'ticketType': tickettype_param,\\r    'dataTable': datatable_param,\\r    'SnowDataSource': snow_data_source\\r    }\\r    #SNow Datasource\\r    snowds = argdict['SnowDataSource']\\r    snowDSdict=snowds\\r\\r    # Set the request parameters\\r    url = snowDSdict['Url']\\r    user = snowDSdict['AuthDetails']['username']\\r    pwd = Security.decrypt(snowDSdict['AuthDetails']['password'], snowDSdict['salt'])\\r    pwd = 'qwer1234'\\r    api = argdict['api']\\r    params =argdict['params']\\r    ticketType = argdict['ticketType']\\r    tablename = argdict['dataTable']\\r\\r    if(ticketType.lower() == 'incident'):\\r        snowtable = 'incident'\\r    elif(ticketType.lower()=='changerequest-normal'):\\r        snowtable = 'change_request'\\r    elif(ticketType.lower() == 'servicerequest'):\\r        snowtable = 'sc_request'\\r    elif(ticketType.lower() == 'incidenttask'):\\r        snowtable = 'incident_task'\\r\\r    icmPayload = argdict['incidentPayload']\\r    icmPayload = json.loads(icmPayload)\\r    icmPayloadMapping = {'number':'number','shortdescription':'short_description','priority.systemId':'priority','state.systemId':'state','description':'description',\\r                            'sysId':'sys_id','category.systemId':'category','impact.systemId':'impact','assignmentgroup.systemId':'assignment_group','assignedto.systemId':'assigned_to',\\r                            'configurationitem.systemId':'cmdb_ci','urgency.systemId':'urgency' }\\r\\r    snowPayload = {}\\r    for key in icmPayloadMapping.keys():\\r        jv = icmPayload\\r        icmColumn = key.split('.')\\r        for item in icmColumn:\\r            try:\\r                jv = jv[item]\\r                snowPayload[icmPayloadMapping[key]] = jv\\r            except:\\r                a='No mapping'\\r        \\r\\r    snowPayload = json.dumps(snowPayload)\\r\\r    #setproxy\\r    proxyDict ={}\\r    if argdict['setProxy'] == 'True':\\r        proxyDict = {\\r                    'http'  : os.environ['HTTP_PROXY'],\\r                    'https' : os.environ['HTTPS_PROXY']      \\r                    }\\r    #set headers\\r    headers = {'Content-Type':'application/json','Accept':'application/json'}\\r    url = url+api+snowtable+'?'+params\\r    # Do the HTTP request\\r    response = requests.post(url, auth=(user, pwd), headers=headers ,data=snowPayload,proxies = proxyDict, verify = False )\\r\\r    # Check for HTTP codes other than 201\\r    if response.status_code != 201:\\r        logger.info('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:',response.json())\\r        exit()\\r\\r    # Decode the JSON response into a dictionary and use the data\\r    data = response.json()\\r\\r    jsonStringArray = []\\r\\r    row = data['result']\\r    if(ticketType.lower() == 'servicerequest'):\\r        row['price'] = float(row['price'][1:])\\r    jsonstr =json.dumps(row)\\r    # jsonStringArray.append(jsonstr)\\r    jsonStringArray.append(json.loads(jsonstr))\\r    print('jsonStringArray', jsonStringArray)\\r\\r    # spark = SparkSession.builder.master('local').appName('sla').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\\r    # #convert json to spark dataframe\\r    # df = spark.read.json(spark.sparkContext.parallelize(jsonStringArray))\\r    df = pd.DataFrame(jsonStringArray)\\r\\r    snowToIcmColumnMapping = {'number':'number','short_description':'shortdescription','priority':'priority','state':'state','description':'description',\\r                            'sys_id':'sysId','opened_at':'openedDate','sys_created_on':'createdDate','sys_updated_on':'updatedDate',\\r                            'sla_due':'sladueDate','closed_at':'closedDate','due_date':'duedate','sys_created_by':'createdby',\\r                            'reopened_time':'reopenedDate','resolved_at':'resolvedDate','category':'category',\\r                            'close_code':'closecode','close_code':'resolutionCategory','impact':'impact','requested_for.display_value':'requested_for',\\r                            'assignment_group.display_value':'assignmentgroup','caller_id.display_value':'caller','assigned_to.display_value':'assignedto',\\r                            'resolved_by.display_value':'resolvedby','closed_by.display_value':'closedby','cmdb_ci.display_value':'configurationItem',\\r                            'close_notes':'closenotes','close_notes':'resolution_steps','location.display_value':'location','request_state':'request_state','price':'price',\\r                            'special_instructions':'special_instructions','approval':'approval','business_service':'business_service',\\r                            'risk':'risk','type':'type','requested_by.display_value':'requested_by','incident.display_value':'parent_Incident',\\r                            'urgency':'severity' }\\r    #mapColumns\\r    foundColumns = []\\r    for key in snowToIcmColumnMapping.keys():\\r        try:\\r            icmColumn =snowToIcmColumnMapping[key]\\r            snowColumn= key\\r            df[icmColumn] = np.where(df[snowColumn] == '', None, df[snowColumn])\\r            foundColumns.append(icmColumn)\\r        except:\\r            error = 'Column Not found'\\r            \\r    print('foundColumns', foundColumns)\\r\\r    df = df[foundColumns]\\r\\r    # Add static columns - ICM specific\\r\\r    df['source'] = 'SNOW'\\r\\r    def recordType(i):\\r        switcher = {\\r            'incident': 'Incident',\\r            'change_request': 'ChangeRequest-Normal',\\r            'incident_task': 'Task',\\r            'sc_request': 'ServiceRequest'\\r        }\\r        return switcher.get(i, 'Incident')\\r    df['type'] = recordType(snowtable.lower())\\r    df['lastUpdated'] = datetime.datetime.now()\\r\\r    print(df)\\r    # logger.info(df)\\r\\r    \\r    return df.to_dict('records')\\r""]},{""dataset"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-30 22:49:23"",""alias"":""Get Incidents"",""id"":2745,""name"":""LEAGTNCD43053"",""description"":""GET  /api/now/table/incident"",""schema"":null,""attributes"":""{\\""bodyType\\"":\\""Text\\"",\\""Cacheable\\"":false,\\""transformData\\"":false,\\""RequestMethod\\"":\\""GET\\"",\\""TransformationScript\\"":\\""\\\\\\""import groovy.json.*\\\\\\\\n\\\\\\\\ndef inpResponse = \\\\\\\\\\\\\\""$inputJson\\\\\\\\\\\\\\""\\\\\\\\n//start your code from here\\\\\\\\n\\\\\\""\\"",\\""Headers\\"":\\""\\"",\\""bodyOption\\"":\\""raw\\"",\\""pretransformData\\"":false,\\""QueryParams\\"":[{\\""value\\"":\\""5\\"",\\""key\\"":\\""sysparm_limit\\""}],\\""preTransformationScript\\"":\\""\\\\\\""import groovy.json.*\\\\\\\\n\\\\\\\\ndef inpBody = \\\\\\\\\\\\\\""$Body\\\\\\\\\\\\\\""\\\\\\\\ndef inpQueryParams = \\\\\\\\\\\\\\""$QueryParams\\\\\\\\\\\\\\""\\\\\\\\ndef inpHeaders = \\\\\\\\\\\\\\""$Headers\\\\\\\\\\\\\\""\\\\\\\\ndef inpbodyType = \\\\\\\\\\\\\\""$bodyType\\\\\\\\\\\\\\""\\\\\\\\ndef inpPathVariables = \\\\\\\\\\\\\\""$PathVariables\\\\\\\\\\\\\\""\\\\\\\\ndef inpUrl = \\\\\\\\\\\\\\""$Url\\\\\\\\\\\\\\""\\\\\\\\ndef inpConfigVariables = \\\\\\\\\\\\\\""$ConfigVariables\\\\\\\\\\\\\\""\\\\\\\\n\\\\\\\\n//start your code from here\\\\\\\\n\\\\\\\\nreturn JsonOutput.prettyPrint(JsonOutput.toJson([Body:{preprocessed_Body}, QueryParams:{preprocessed_QueryParams},Headers:{preprocessed_Headers},bodyType:{preprocessed_bodyType},PathVariables:{preprocessed_PathVariables},Url:{preprocessed_Url},ConfigVariables:{preprocessed_ConfigVariables}]))\\\\\\\\n\\\\\\""\\"",\\""Body\\"":\\""\\"",\\""Url\\"":\\""/api/now/table/incident\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-30 22:49:24"",""alias"":""SNOW"",""id"":541,""name"":""LEASNWSP56549"",""description"":""Service Now"",""type"":""REST"",""connectionDetails"":""{\\""NoProxy\\"":\\""true\\"",\\""ConnectionType\\"":\\""ApiRequest\\"",\\""testDataset\\"":{\\""name\\"":\\""\\"",\\""attributes\\"":{\\""bodyType\\"":\\""raw\\"",\\""Endpoint\\"":\\""\\"",\\""RequestMethod\\"":\\""GET\\"",\\""Headers\\"":\\""\\"",\\""LeapParams\\"":[],\\""QueryParams\\"":\\""\\"",\\""Body\\"":\\""\\""}},\\""AuthDetails\\"":{\\""password\\"":\\""encQ0iETSgsUqcjR4DpfRcaq1JHTqADQpoc\\"",\\""authParams\\"":{\\""grant_type\\"":\\""\\"",\\""client_secret\\"":\\""\\"",\\""client_id\\"":\\""\\""},\\""username\\"":\\""ICSP_icap_user\\""},\\""AuthType\\"":\\""BasicAuth\\"",\\""Url\\"":\\""https://infosysq3dev1.service-now.com\\"",\\""fileId\\"":\\""\\"",\\""tokenExpirationTime\\"":\\""\\""}"",""salt"":""wSj9Bf2qHFKpn+acHaJ4C0/DFSzjAzjW55bCpdBLrA2gnXHZ+x9jIsT3S4QWzlm666fWc59dJskXOeErVM/baw=="",""organization"":""leo1311"",""dshashcode"":""4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7"",""activetime"":""2023-07-30 22:49:23"",""category"":""REST"",""extras"":""{\\""apispec\\"":{\\""openapi\\"":\\""3.0.2\\"",\\""info\\"":{\\""version\\"":1,\\""title\\"":\\""SNOW\\"",\\""description\\"":\\""GET  /api/now/table/incident\\""},\\""servers\\"":[{\\""url\\"":\\""https://ai-platform\\""}],\\""paths\\"":{\\""/api/service/REST/SNOW/Get Incidents\\"":{\\""get\\"":{\\""dataset\\"":\\""LEAGTNCD43053\\"",\\""parameters\\"":[{\\""name\\"":\\""sysparm_limit\\"",\\""value\\"":\\""5\\"",\\""description\\"":\\""sysparm_limit\\"",\\""required\\"":\\""true\\"",\\""type\\"":\\""string\\"",\\""in\\"":\\""params\\""}],\\""responses\\"":{\\""200\\"":{\\""description\\"":\\""response\\"",\\""content\\"":{\\""application/json\\"":{\\""schema\\"":{\\""type\\"":\\""string\\""}}}},\\""default\\"":{\\""description\\"":\\""error\\"",\\""content\\"":{\\""application/json\\"":{\\""schema\\"":{\\""type\\"":\\""string\\""}}}}}}}},\\""components\\"":{\\""schemas\\"":{\\""input\\"":{\\""properties\\"":{}}},\\""securitySchemes\\"":{\\""bearerAuth\\"":{\\""type\\"":\\""http\\"",\\""scheme\\"":\\""bearer\\"",\\""bearerFormat\\"":\\""JWT\\""},\\""basicAuth\\"":{\\""type\\"":\\""http\\"",\\""scheme\\"":\\""basic\\""}},\\""security\\"":[{\\""bearerAuth\\"":[],\\""basicAuth\\"":[]}]}},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":"""",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-03 13:16:01"",""alias"":""Tickets"",""id"":2775,""name"":""LEATCKTS50134"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, clean_text, Soundex(clean_text) as sound, CASE WHEN group_by_field IS NULL THEN '' ELSE group_by_field END AS group_by_field FROM leo1311_tickets_enriched  WHERE clean_text is not null and  Soundex(clean_text) != \\\\\\""\\\\\\"" and clean_text != \\\\\\""\\\\\\""\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","poornasai.nagendra@ad.infosys.com","SNOW_Create_API_","2023-08-28 05:59:10","LEASNW_C32819","leo1311","DragNDropLite","52","""""","NULL","{""52"":{""taskId"":""ff14d08c-0572-442d-858f-5b07069ffbb3""}}"
"admin","2023-08-17 07:45:28.625000","\0","","NULL","{""elements"":[{""id"":""nwocU"",""alias"":""MYSQL Extractor"",""name"":""MYSQL Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-14 04:52:45"",""alias"":""cleantext"",""id"":2774,""name"":""LEACLNTX34353"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\"" \\\\\\"" AND   clean_text IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""228"",""position_y"":""47"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""nxaCY"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown""},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""SnQdq"",""alias"":""MYSQL Loader"",""name"":""MYSQL Loader"",""category"":""DatasetLoaderConfig"",""attributes"":{"""":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-16 12:32:45"",""alias"":""InvertedIndex"",""id"":2806,""name"":""LEAINVRT90611"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_invertedindex order by frequency\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_invertedindex\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""696"",""position_y"":""43"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""nxaCY"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[],""formats"":{"""":"""",""dataset"":""dropdown""},""codeGeneration"":{""requirements"":[""response"",""requests""],""imports"":[""import sys"",""import urllib3"",""import requests"",""import json""],""script"":""def Profanity(inputText):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/safety/profanity/censor'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {\\r\\n    'inputText': inputText\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""},""context"":[{""FunctionName"":""Extract_words"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rdef Extract_words( dataset): \\r    dataset = pd.DataFrame(dataset)\\r    def clean_text(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        return alphanumeric  \\r    def tokenizer(text):       \\r        token = nltk.word_tokenize(text)        \\r        tokens = [t for t in token if t.isalpha()]        \\r        tokens = ''.join(tokens)        \\r        return tokens \\r    def remove_stopwords(text):                \\r        words = [t for t in text if t.lower() not in stopwords.words('english')]        \\r        text = ''.join(words)       \\r        return text \\r    def lemmatize_text(text):       \\r        words = text.split() \\r        wordnet_lemmatizer = WordNetLemmatizer()\\r        words = [wordnet_lemmatizer.lemmatize(word, pos = 'v') for word in words]        \\r        return ''.join(words)\\r    def remove_numbers(tokens):        \\r        filter_tokens = []        \\r        for t in tokens:            \\r            if not t.isdigit() and len(t)>2:               \\r                filter_tokens.append(t)        \\r        return filter_tokens     \\r    \\r    dataset['cleanText'] = [clean_text(desc) for desc in dataset['shortdescription']]\\r    dataset['tokens'] = dataset['cleanText'].apply(tokenizer)\\r    dataset['cleanWords'] = dataset['tokens'].apply(remove_stopwords)\\r    dataset['cleanText'] = dataset['cleanWords'].apply(lemmatize_text) \\r    print(dataset)\\r    dataset['filteredTokens'] = dataset['cleanText'].apply(remove_numbers)\\r    dataset = dataset[['filteredTokens','NUMBER']]   \\r    dataset['words'] = dataset.explode('filteredTokens')    \\r    dataset = dataset.drop_duplicates()    \\r    dataset = dataset.groupby(['words']).agg(numberList = pd.NameAgg(column = 'number',aggfunc=list))   \\r    dataset['frequency'] = dataset['numberList'].apply(lambda x: len(x))    \\r    dataset['numberList'] = dataset['numberList'].apply(lambda x: ','.join(x)) \\r    return dataset\\r""]},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-14 04:52:45"",""alias"":""cleantext"",""id"":2774,""name"":""LEACLNTX34353"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\"" \\\\\\"" AND   clean_text IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]},{""id"":""nxaCY"",""alias"":""Post Processing Script"",""name"":""Post Processing Script"",""classname"":""PostProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""Extract_words"",""requirements"":"""",""params"":[],""script"":[""import pandas as pd\\rimport nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.stem.wordnet import WordNetLemmatizer\\rdef Extract_words( dataset): \\r    dataset = pd.DataFrame(dataset)\\r    def clean_text(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        return alphanumeric  \\r    def tokenizer(text):       \\r        token = nltk.word_tokenize(text)        \\r        tokens = [t for t in token if t.isalpha()]        \\r        tokens = ''.join(tokens)        \\r        return tokens \\r    def remove_stopwords(text):                \\r        words = [t for t in text if t.lower() not in stopwords.words('english')]        \\r        text = ''.join(words)       \\r        return text \\r    def lemmatize_text(text):       \\r        words = text.split() \\r        wordnet_lemmatizer = WordNetLemmatizer()\\r        words = [wordnet_lemmatizer.lemmatize(word, pos = 'v') for word in words]        \\r        return ''.join(words)\\r    def remove_numbers(tokens):        \\r        filter_tokens = []        \\r        for t in tokens:            \\r            if not t.isdigit() and len(t)>2:               \\r                filter_tokens.append(t)        \\r        return filter_tokens     \\r    \\r    dataset['cleanText'] = [clean_text(desc) for desc in dataset['shortdescription']]\\r    dataset['tokens'] = dataset['cleanText'].apply(tokenizer)\\r    dataset['cleanWords'] = dataset['tokens'].apply(remove_stopwords)\\r    dataset['cleanText'] = dataset['cleanWords'].apply(lemmatize_text) \\r    print(dataset)\\r    dataset['filteredTokens'] = dataset['cleanText'].apply(remove_numbers)\\r    dataset = dataset[['filteredTokens','NUMBER']]   \\r    dataset['words'] = dataset.explode('filteredTokens')    \\r    dataset = dataset.drop_duplicates()    \\r    dataset = dataset.groupby(['words']).agg(numberList = pd.NameAgg(column = 'number',aggfunc=list))   \\r    dataset['frequency'] = dataset['numberList'].apply(lambda x: len(x))    \\r    dataset['numberList'] = dataset['numberList'].apply(lambda x: ','.join(x)) \\r    return dataset\\r""]},""position_x"":""457"",""position_y"":""55"",""connectors"":[{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""nwocU"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out1"",""position"":""RightMiddle"",""elementId"":""SnQdq"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2""],""outputEndpoints"":[""out1"",""out2""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-14 04:52:45"",""alias"":""cleantext"",""id"":2774,""name"":""LEACLNTX34353"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NUMBER, clean_text AS shortdescription , group_by_field FROM leo1311_tickets_enriched WHERE clean_text <> \\\\\\"" \\\\\\"" AND   clean_text IS NOT NULL \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}}]}]}","admin","TEST","2023-08-17 07:49:15","LEATSTKH45900","leo1311","DragNDropLite","6","""""","NULL","NULL"
"admin","2023-08-17 10:48:16.200000","\0","","NULL","{""elements"":[{""id"":""wvOlb"",""alias"":""AnalyzeImage"",""name"":""AnalyzeImage"",""category"":""RAI"",""attributes"":{},""position_x"":""486"",""position_y"":""103"",""connectors"":[],""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""formats"":{},""codeGeneration"":{""requirements"":[""response"",""requests""],""imports"":[""import sys"",""import urllib3"",""import requests"",""import json""],""script"":""def Profanity(inputText):\\r\\n    url = 'https://api-aicloud.ad.infosys.com/api/v1/safety/profanity/censor'\\r\\n    headers = {'content-type': 'application/json'}\\r\\n    data = {\\r\\n    'inputText': inputText\\r\\n    }\\r\\n    print(url)\\r\\n    response = requests.post(url, data=json.dumps(data), headers=headers, verify=False)\\r\\n    print(response.status_code)\\r\\n    print('response:', '\\\\n', response.text)\\r\\n    return response.text\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\n""}}]}","admin","test","2023-08-17 10:48:36","LEATSTCX78026","leo1311","DragNDropLite","0","""""","NULL","NULL"
"admin","2023-08-24 11:38:48.470000","\0","","NULL","{""elements"":[{""id"":""hHQsZ"",""alias"":""User Availability"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 08:18:51"",""alias"":""User Availability"",""id"":2823,""name"":""LEAUSRVL16671"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT [Name],assignmentgroup,COALESCE(w.assignedweight,0) AS AssignedWeight, [Shift], CONVERT(varchar, [DATE], 101) AS DATE FROM leo1311_useravailability LEFT JOIN (SELECT predicted_assignee AS \\\\\\""USER\\\\\\"" , SUM(CAST(CAST(Weight AS varchar)AS int)) AS AssignedWeight FROM leo1311_tickets t JOIN leo1311_tickets_enriched e ON t.number= e.number JOIN leo1311_priorityweightage p ON t.priority = CAST(p.priority AS varchar) WHERE t.state NOT IN ('Closed','Resolved') AND predicted_assignee IS NOT NULL GROUP BY predicted_assignee) w ON CAST([NAME] AS varchar) = w.[user] WHERE CAST([Shift] AS varchar=(SELECT CAST(reassignedShift AS varchar) AS NextShift  FROM leo1311_rosterdefinition WHERE CONVERT(varchar,ShiftStart,108) <= CONVERT(VARCHAR, GETDATE(), 108) AND CONVERT(varchar,ShiftEnd,108) >= CONVERT(VARCHAR, GETDATE(), 108) AND userAvailable=1 AND CONVERT(varchar, [DATE], 101)=CONVERT(varchar, GETDATE(), 101);\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""212"",""position_y"":""6"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""MaXog"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""lvsWw"",""alias"":""Unassigned  Tickets and Priority"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:44:39"",""alias"":""Tickets and Priority"",""id"":2824,""name"":""LEATCKTS80877"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, a.priority, assignmentgroup,weight FROM leo1311_tickets e JOIN leo1311_priorityweightage a ON e.priority=a.priority WHERE NUMBER NOT IN( SELECT NUMBER FROM leo1311_tickets_enriched WHERE predicted_assignee IS NOT NULL ) AND (assignmentgroup IS NOT NULL OR assignmentgroup != 'NULL')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""211"",""position_y"":""197"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""MaXog"",""elementPosition"":""BottomCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""MaXog"",""alias"":""Python Script"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""NextShiftAssign"",""requirements"":"""",""params"":[],""script"":[""import logging as logger\\r\\\\n"",""from datetime import datetime\\r\\\\n"",""import time\\r\\\\n"",""def NextShiftAssign(self, UnAssignedTickets, current_shift_emp):\\r\\\\n"",""    print(UnAssignedTickets)\\r\\\\n"",""    if len(UnAssignedTickets) == 0:\\r\\\\n"",""        print('No Tickets to assign')\\r\\\\n"",""        print('Completed')\\r\\\\n"",""        exit()\\r\\\\n"",""    current_time = datetime.now().time().strftime('%H:%M:%S')\\r\\\\n"",""    print('CurrentTime',current_time)\\r\\\\n"",""    print('current_Shift', current_shift_emp)\\r\\\\n"",""    techList= list(set([ sub['assignmentgroup'] for sub in current_shift_emp ]))\\r\\\\n"",""    print('AssignmentGroup',techList)\\r\\\\n"",""    assignedTickets= []\\r\\\\n"",""    for item in techList:\\r\\\\n"",""        print('Assigning for', item)\\r\\\\n"",""        techTickets= [e for e in UnAssignedTickets if e['assignmentgroup']== item]\\r\\\\n"",""        techEmpList = [e for e in current_shift_emp if e['assignmentgroup']== item]\\r\\\\n"",""        print('Available Employees', techEmpList)\\r\\\\n"",""        if len(techEmpList)>0:\\r\\\\n"",""            for ticket in techTickets:\\r\\\\n"",""                #print(techEmpList)\\r\\\\n"",""                employee= min(techEmpList, key= lambda x:x['AssignedWeight']) \\r\\\\n"",""                techEmployee= next(x for x in techEmpList if x['Name']== employee['Name'])\\r\\\\n"",""                e= next(x for x in techEmpList if x['Name']== employee['Name'])\\r\\\\n"",""                techEmployee['AssignedWeight'] = techEmployee['AssignedWeight']+int(ticket['weight'])\\r\\\\n"",""                ticket['assignedTo']= employee['Name']\\r\\\\n"",""                assignedTickets.append({'number': ticket['number'], 'predicted_assignee': ticket['assignedTo'], 'last_updated':datetime.now()})\\r\\\\n"",""                print('Ticket {0} assigned to {1}. Current Weight {2}'.format(ticket['number'],ticket['assignedTo'],techEmployee['AssignedWeight']))\\r\\\\n"",""                    \\r\\\\n"",""    if len(assignedTickets)>0:\\r\\\\n"",""        print('Assignment completed')\\r\\\\n"",""        print(assignedTickets)\\r\\\\n"",""        return assignedTickets\\r\\\\n"",""    else:\\r\\\\n"",""        print('No Tickets Assigned')\\r\\\\n"",""        print('Completed')""]},""position_x"":""515"",""position_y"":""95"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""hHQsZ"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset3"",""position"":""BottomCenter"",""elementId"":""lvsWw"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 08:18:51"",""alias"":""User Availability"",""id"":2823,""name"":""LEAUSRVL16671"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT [Name],assignmentgroup,COALESCE(w.assignedweight,0) AS AssignedWeight, [Shift], CONVERT(varchar, [DATE], 101) AS DATE FROM leo1311_useravailability LEFT JOIN (SELECT predicted_assignee AS \\\\\\""USER\\\\\\"" , SUM(CAST(CAST(Weight AS varchar)AS int)) AS AssignedWeight FROM leo1311_tickets t JOIN leo1311_tickets_enriched e ON t.number= e.number JOIN leo1311_priorityweightage p ON t.priority = CAST(p.priority AS varchar) WHERE t.state NOT IN ('Closed','Resolved') AND predicted_assignee IS NOT NULL GROUP BY predicted_assignee) w ON CAST([NAME] AS varchar) = w.[user] WHERE CAST([Shift] AS varchar=(SELECT CAST(reassignedShift AS varchar) AS NextShift  FROM leo1311_rosterdefinition WHERE CONVERT(varchar,ShiftStart,108) <= CONVERT(VARCHAR, GETDATE(), 108) AND CONVERT(varchar,ShiftEnd,108) >= CONVERT(VARCHAR, GETDATE(), 108) AND userAvailable=1 AND CONVERT(varchar, [DATE], 101)=CONVERT(varchar, GETDATE(), 101);\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":""[]"",""tags"":""\\""\\"""",""interfacetype"":null}},{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:44:39"",""alias"":""Tickets and Priority"",""id"":2824,""name"":""LEATCKTS80877"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, a.priority, assignmentgroup,weight FROM leo1311_tickets e JOIN leo1311_priorityweightage a ON e.priority=a.priority WHERE NUMBER NOT IN( SELECT NUMBER FROM leo1311_tickets_enriched WHERE predicted_assignee IS NOT NULL ) AND (assignmentgroup IS NOT NULL OR assignmentgroup != 'NULL')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}}]}],""pipeline_attributes"":[{""key"":""storageType"",""value"":""s3""}]}","admin","NextShiftAssignment","2023-08-28 04:19:04","LEANXTSH25686","leo1311","DragNDropLite","80","""""","NULL","{""47"":{""taskId"":""a9e54bb7-4e4f-462f-9fa9-289ba3c592ae""}}"
"admin","2023-08-25 06:28:25.875000","\0","","NULL","{""elements"":[{""id"":""KEwSo"",""alias"":""Roster data"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:30:56"",""alias"":""Roster data"",""id"":2819,""name"":""LEARSTRD63823"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NAME,track,shift,STR_TO_DATE(DATE, '%m/%d/%Y') AS DATE,userAvailable,shiftStart,shiftEnd,reassignAt,reassignedShift FROM leo1311_useravailability rostern JOIN leo1311_rosterdefinition def ON def.shiftName=roster.shift WHERE STR_TO_DATE(DATE, '%m/%d/%Y')=CURRENT_DATE AND userAvailable=1\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},""position_x"":""74"",""position_y"":""27"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""UbDdd"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""formats"":{""isValidation"":""checkbox"",""dataset"":""dropdown"",""samplingRatio"":""text"",""applySchema"":""checkbox""},""codeGeneration"":{""MYSQL"":{""imports"":[""from leaputils import Security""],""script"":""    dataset_<id> = spark.read.format(\\""jdbc\\"").options(url=\\""<dataset.datasource.connectionDetails.url>\\"",dbtable=\\""(<dataset.attributes.Query>)t1\\"",user=\\""<dataset.datasource.connectionDetails.userName>\\"",password=Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")).load()""},""FILE"":{""CSV"":{""imports"":[],""script"":""    dataset_<id> = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',\\n                                                                               delimiter=\\""<dataset.attributes.delimiter>\\"").load(\\""<dataset.attributes.path>\\"")\\n    ""}}},""context"":[]},{""id"":""mQFFC"",""alias"":""classified tickets"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:32:19"",""alias"":""Newdump categories"",""id"":2820,""name"":""LEANWDMP75806"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_techclassifiedtickets WHERE (State = 'New' OR State = 'Open') AND (Category IS NOT NULL OR Category != '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_techclassifiedtickets\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},""position_x"":""76"",""position_y"":""112"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""UbDdd"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""formats"":{""isValidation"":""checkbox"",""dataset"":""dropdown"",""samplingRatio"":""text"",""applySchema"":""checkbox""},""codeGeneration"":{""MYSQL"":{""imports"":[""from leaputils import Security""],""script"":""    dataset_<id> = spark.read.format(\\""jdbc\\"").options(url=\\""<dataset.datasource.connectionDetails.url>\\"",dbtable=\\""(<dataset.attributes.Query>)t1\\"",user=\\""<dataset.datasource.connectionDetails.userName>\\"",password=Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")).load()""},""FILE"":{""CSV"":{""imports"":[],""script"":""    dataset_<id> = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',\\n                                                                               delimiter=\\""<dataset.attributes.delimiter>\\"").load(\\""<dataset.attributes.path>\\"")\\n    ""}}},""context"":[]},{""id"":""VyFqZ"",""alias"":""ticket weightage"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:22:03"",""alias"":"" Ticket Weightage "",""id"":2821,""name"":""LEA TCKT79784"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_priorityweightage\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_priorityweightage\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},""position_x"":""75"",""position_y"":""207"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""UbDdd"",""elementPosition"":""BottomCenter""}],""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""formats"":{""isValidation"":""checkbox"",""dataset"":""dropdown"",""samplingRatio"":""text"",""applySchema"":""checkbox""},""codeGeneration"":{""MYSQL"":{""imports"":[""from leaputils import Security""],""script"":""    dataset_<id> = spark.read.format(\\""jdbc\\"").options(url=\\""<dataset.datasource.connectionDetails.url>\\"",dbtable=\\""(<dataset.attributes.Query>)t1\\"",user=\\""<dataset.datasource.connectionDetails.userName>\\"",password=Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")).load()""},""FILE"":{""CSV"":{""imports"":[],""script"":""    dataset_<id> = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',\\n                                                                               delimiter=\\""<dataset.attributes.delimiter>\\"").load(\\""<dataset.attributes.path>\\"")\\n    ""}}},""context"":[]},{""id"":""UbDdd"",""alias"":""Pre Processing Script"",""name"":""Pre Processing Script"",""classname"":""PreProcessingScriptConfig"",""category"":""BaseConfig"",""attributes"":{""params"":""{\\""IncidentId\\"":\\""\\"""",""script"":[""import logging as logger\\r\\\\n"",""from datetime import datetime\\r\\\\n"",""from pyspark.sql import SparkSession\\r\\\\n"",""from pyspark.sql.functions import *\\r\\\\n"",""from pyspark.sql.types import *\\r\\\\n"",""import datetime\\r\\\\n"",""import time\\r\\\\n"",""import pandas as pd\\r\\\\n"",""\\r\\\\n"",""class CustomPythonClass():\\r\\\\n"",""    def __main__(self, df1, df2, df3):\\r\\\\n"",""        incidentId = self.params['IncidentId']\\r\\\\n"",""        if incidentId !='':\\r\\\\n"",""            df1 = df1.filter(df1['Number'] == incidentId)\\r\\\\n"",""        \\r\\\\n"",""        spark = SparkSession.builder.master('local[*]').appName('ShiftRosterAssignment').getOrCreate()\\r\\\\n"",""\\r\\\\n"",""        def assignTicket(empWithNoOfTickets, priority, tech, ticketAssignedToNextShift,ticketWeight):\\r\\\\n"",""            \\r\\\\n"",""            print('Assigning tickets for', tech[0],priority[0],'Open')\\r\\\\n"",""            try:\\r\\\\n"",""           \\r\\\\n"",""                ticketdf = df1.filter(\\r\\\\n"",""                    (df1['Category'] == tech[0]) & (df1['Priority'] == priority[0]) & ((df1['State'] == 'New')| (df1['State'] == 'Open') ))\\r\\\\n"",""                #ticketdf = ticketdf.where(col('Assignee').isNull())\\r\\\\n"",""                \\r\\\\n"",""                #Change Assignee to Null instead of ,,\\r\\\\n"",""                currPointer=0\\r\\\\n"",""                ticketAssigned = {}\\r\\\\n"",""               \\r\\\\n"",""                if(ticketdf.count()>0):\\r\\\\n"",""                    print('Assigning tickets to employees for ', priority[0], ' priority and ', tech[0], 'tech')\\r\\\\n"",""                else:\\r\\\\n"",""                    print('No tickets to assign for ', priority[0], ' priority and ', tech[0], 'tech')\\r\\\\n"",""                for i in ticketdf.collect():\\r\\\\n"",""                    \\r\\\\n"",""                    empWithNoOfTickets.sort(key=lambda empWithNoOfTickets: empWithNoOfTickets[0])\\r\\\\n"",""                    \\r\\\\n"",""                    #use min instead of sort\\r\\\\n"",""                    # i[3] = 'Open'\\r\\\\n"",""                    \\r\\\\n"",""                    \\r\\\\n"",""                    \\r\\\\n"",""                    if empWithNoOfTickets[currPointer][1] in ticketAssigned.keys():\\r\\\\n"",""                        ticketAssigned[empWithNoOfTickets[currPointer][1]].append([i[0],i[1],i[2],i[3],i[4],i[5],empWithNoOfTickets[currPointer][1]])\\r\\\\n"",""                    else:\\r\\\\n"",""                        \\r\\\\n"",""                        \\r\\\\n"",""                        ticketAssigned[empWithNoOfTickets[currPointer][1]] = [[i[0],i[1],i[2],i[3],i[4],i[5],empWithNoOfTickets[currPointer][1]]]\\r\\\\n"",""\\r\\\\n"",""                    ticketAssignedToNextShift.append([i[0], i[1], i[2], i[3], i[4], i[5], empWithNoOfTickets[currPointer][1]])\\r\\\\n"",""                    empWithNoOfTickets[currPointer][0]+=(ticketWeight[priority[0]]/len(empListInNextShift))\\r\\\\n"",""                    #print(empWithNoOfTickets,i[2])\\r\\\\n"",""                    \\r\\\\n"",""                return ticketAssignedToNextShift\\r\\\\n"",""            except ex:\\r\\\\n"",""                return ''\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""        trackList = df2.select('track').distinct().collect()\\r\\\\n"",""        priorityList = df1.select('Priority').distinct().collect()\\r\\\\n"",""        priorityList.sort()\\r\\\\n"",""\\r\\\\n"",""        empListdf = df2.groupBy('track', 'shift').agg(array_distinct(collect_list('Name')).alias('EmpInTech'),\\r\\\\n"",""                                                      first('shiftStart').alias('shiftStart'),\\r\\\\n"",""                                                      first('shiftEnd').alias('shiftEnd'),\\r\\\\n"",""                                                      first('reassignAt').alias('reassignAt'),\\r\\\\n"",""                                                      first('reassignedShift').alias('reassignedShift'))\\r\\\\n"",""        empListdf = empListdf.withColumn('EmpInTechSize', size('EmpInTech'))\\r\\\\n"",""\\r\\\\n"",""        current_time = datetime.datetime.now().time().strftime('%H:%M:%S')\\r\\\\n"",""        # Logic for adding shift in db?\\r\\\\n"",""        shiftsdf = df2.select('shift', 'shiftStart', 'shiftEnd', 'reassignedShift').distinct()\\r\\\\n"",""\\r\\\\n"",""        cons_shift = shiftsdf.filter((shiftsdf['shiftStart'] <= current_time) &(shiftsdf['shiftEnd'] > current_time)).select(col('shift').alias('current_shift'), col('reassignedShift').alias('next_shift')).collect()\\r\\\\n"",""\\r\\\\n"",""        ticketAssignedToNextShift=[]\\r\\\\n"",""        ticketWeight = {}\\r\\\\n"",""        #for converting weight df to dict\\r\\\\n"",""        for priority in priorityList:\\r\\\\n"",""            try:\\r\\\\n"",""                weightageList = df3.filter((df3['Priority'] == priority[0])).select('Weight').collect()\\r\\\\n"",""                ticketWeight[priority[0]] = int(weightageList[0][0])\\r\\\\n"",""                \\r\\\\n"",""            except:\\r\\\\n"",""                a=1\\r\\\\n"",""\\r\\\\n"",""        Assigneddata = []\\r\\\\n"",""\\r\\\\n"",""        for tech in trackList:\\r\\\\n"",""            try:\\r\\\\n"",""               # print('Fetching employees in next shift for ', tech[0], cons_shift[0]['next_shift'])\\r\\\\n"",""                if len(cons_shift) <= 0:\\r\\\\n"",""                    print('No employees in next shift for tech ', tech[0])\\r\\\\n"",""                else:\\r\\\\n"",""                        \\r\\\\n"",""                    empInNextShift = empListdf.filter((empListdf['Track'] == tech[0]) &(empListdf['shift'] == cons_shift[0]['next_shift']))\\r\\\\n"",""                    if empInNextShift.count()<=0:\\r\\\\n"",""                        print('No employees in next shift for tech ', tech[0])\\r\\\\n"",""                    else:\\r\\\\n"",""                        empListInNextShift = empInNextShift.select(explode('EmpInTech').alias('EmpInTech')).collect()\\r\\\\n"",""        \\r\\\\n"",""                        empWithNoOfTickets = []\\r\\\\n"",""                        for priority in priorityList:\\r\\\\n"",""                            for emp in empListInNextShift:\\r\\\\n"",""        \\r\\\\n"",""                                empInList = any(emp[0] in sublist[1] for sublist in empWithNoOfTickets)\\r\\\\n"",""                                if(not empInList):\\r\\\\n"",""                                    intitalWeight =0\\r\\\\n"",""                                    empWithNoOfTickets.append([intitalWeight, emp[0]])\\r\\\\n"",""                            Assigneddata = assignTicket(empWithNoOfTickets, priority, tech, ticketAssignedToNextShift,ticketWeight)\\r\\\\n"",""            except:\\r\\\\n"",""                a=1\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""        if(len(Assigneddata)>0):\\r\\\\n"",""            headers = ['Number', 'Category', 'Priority', 'State', 'Subject', 'cleanText', 'Assignee']\\r\\\\n"",""            updated = spark.createDataFrame(Assigneddata, headers)\\r\\\\n"",""            #updated = updated.withColumn('Shift',lit(None).cast(StringType()))\\r\\\\n"",""            print('Assignment completed')\\r\\\\n"",""            updated.show()\\r\\\\n"",""            \\r\\\\n"",""            return updated\\r\\\\n"",""        else:\\r\\\\n"",""            df1=df1.withColumn('Assignee',lit(None).cast(StringType()))\\r\\\\n"",""            print('not assigned')\\r\\\\n"",""            return df1\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n""]},""position_x"":""357"",""position_y"":""112"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""KEwSo"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset1"",""position"":""LeftMiddle"",""elementId"":""mQFFC"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset3"",""position"":""BottomCenter"",""elementId"":""VyFqZ"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""tWXcU"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""params"":""textarea"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""""},""context"":[{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:30:56"",""alias"":""Roster data"",""id"":2819,""name"":""LEARSTRD63823"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NAME,track,shift,STR_TO_DATE(DATE, '%m/%d/%Y') AS DATE,userAvailable,shiftStart,shiftEnd,reassignAt,reassignedShift FROM leo1311_useravailability rostern JOIN leo1311_rosterdefinition def ON def.shiftName=roster.shift WHERE STR_TO_DATE(DATE, '%m/%d/%Y')=CURRENT_DATE AND userAvailable=1\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:32:19"",""alias"":""Newdump categories"",""id"":2820,""name"":""LEANWDMP75806"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_techclassifiedtickets WHERE (State = 'New' OR State = 'Open') AND (Category IS NOT NULL OR Category != '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_techclassifiedtickets\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:22:03"",""alias"":"" Ticket Weightage "",""id"":2821,""name"":""LEA TCKT79784"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_priorityweightage\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_priorityweightage\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false}]},{""id"":""tWXcU"",""alias"":""Python Script Transformer"",""name"":""Python Script Transformer"",""classname"":""PythonScriptTransformerConfig"",""category"":""TransformerConfig"",""attributes"":{""script"":[""import logging\\r\\\\n"",""import datetime\\r\\\\n"",""class CustomPythonClass():\\r\\\\n"",""    def __main__(self, dataset):\\r\\\\n"",""        dataset = dataset.select(col('Number').alias('number'),col('Assignee').alias('predicted_assignee'))\\r\\\\n"",""        dataset = dataset.withColumn('last_updated',lit(datetime.datetime.now()))\\r\\\\n"",""        return dataset\\r\\\\n"",""\\r\\\\n""]},""position_x"":""569"",""position_y"":""112"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""UbDdd"",""elementPosition"":""RightMiddle""},{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""BqCTE"",""elementPosition"":""LeftMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""formats"":{""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""""},""context"":[{""params"":""{\\""IncidentId\\"":\\""\\"""",""script"":[""import logging as logger\\r\\\\n"",""from datetime import datetime\\r\\\\n"",""from pyspark.sql import SparkSession\\r\\\\n"",""from pyspark.sql.functions import *\\r\\\\n"",""from pyspark.sql.types import *\\r\\\\n"",""import datetime\\r\\\\n"",""import time\\r\\\\n"",""import pandas as pd\\r\\\\n"",""\\r\\\\n"",""class CustomPythonClass():\\r\\\\n"",""    def __main__(self, df1, df2, df3):\\r\\\\n"",""        incidentId = self.params['IncidentId']\\r\\\\n"",""        if incidentId !='':\\r\\\\n"",""            df1 = df1.filter(df1['Number'] == incidentId)\\r\\\\n"",""        \\r\\\\n"",""        spark = SparkSession.builder.master('local[*]').appName('ShiftRosterAssignment').getOrCreate()\\r\\\\n"",""\\r\\\\n"",""        def assignTicket(empWithNoOfTickets, priority, tech, ticketAssignedToNextShift,ticketWeight):\\r\\\\n"",""            \\r\\\\n"",""            print('Assigning tickets for', tech[0],priority[0],'Open')\\r\\\\n"",""            try:\\r\\\\n"",""           \\r\\\\n"",""                ticketdf = df1.filter(\\r\\\\n"",""                    (df1['Category'] == tech[0]) & (df1['Priority'] == priority[0]) & ((df1['State'] == 'New')| (df1['State'] == 'Open') ))\\r\\\\n"",""                #ticketdf = ticketdf.where(col('Assignee').isNull())\\r\\\\n"",""                \\r\\\\n"",""                #Change Assignee to Null instead of ,,\\r\\\\n"",""                currPointer=0\\r\\\\n"",""                ticketAssigned = {}\\r\\\\n"",""               \\r\\\\n"",""                if(ticketdf.count()>0):\\r\\\\n"",""                    print('Assigning tickets to employees for ', priority[0], ' priority and ', tech[0], 'tech')\\r\\\\n"",""                else:\\r\\\\n"",""                    print('No tickets to assign for ', priority[0], ' priority and ', tech[0], 'tech')\\r\\\\n"",""                for i in ticketdf.collect():\\r\\\\n"",""                    \\r\\\\n"",""                    empWithNoOfTickets.sort(key=lambda empWithNoOfTickets: empWithNoOfTickets[0])\\r\\\\n"",""                    \\r\\\\n"",""                    #use min instead of sort\\r\\\\n"",""                    # i[3] = 'Open'\\r\\\\n"",""                    \\r\\\\n"",""                    \\r\\\\n"",""                    \\r\\\\n"",""                    if empWithNoOfTickets[currPointer][1] in ticketAssigned.keys():\\r\\\\n"",""                        ticketAssigned[empWithNoOfTickets[currPointer][1]].append([i[0],i[1],i[2],i[3],i[4],i[5],empWithNoOfTickets[currPointer][1]])\\r\\\\n"",""                    else:\\r\\\\n"",""                        \\r\\\\n"",""                        \\r\\\\n"",""                        ticketAssigned[empWithNoOfTickets[currPointer][1]] = [[i[0],i[1],i[2],i[3],i[4],i[5],empWithNoOfTickets[currPointer][1]]]\\r\\\\n"",""\\r\\\\n"",""                    ticketAssignedToNextShift.append([i[0], i[1], i[2], i[3], i[4], i[5], empWithNoOfTickets[currPointer][1]])\\r\\\\n"",""                    empWithNoOfTickets[currPointer][0]+=(ticketWeight[priority[0]]/len(empListInNextShift))\\r\\\\n"",""                    #print(empWithNoOfTickets,i[2])\\r\\\\n"",""                    \\r\\\\n"",""                return ticketAssignedToNextShift\\r\\\\n"",""            except ex:\\r\\\\n"",""                return ''\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""        trackList = df2.select('track').distinct().collect()\\r\\\\n"",""        priorityList = df1.select('Priority').distinct().collect()\\r\\\\n"",""        priorityList.sort()\\r\\\\n"",""\\r\\\\n"",""        empListdf = df2.groupBy('track', 'shift').agg(array_distinct(collect_list('Name')).alias('EmpInTech'),\\r\\\\n"",""                                                      first('shiftStart').alias('shiftStart'),\\r\\\\n"",""                                                      first('shiftEnd').alias('shiftEnd'),\\r\\\\n"",""                                                      first('reassignAt').alias('reassignAt'),\\r\\\\n"",""                                                      first('reassignedShift').alias('reassignedShift'))\\r\\\\n"",""        empListdf = empListdf.withColumn('EmpInTechSize', size('EmpInTech'))\\r\\\\n"",""\\r\\\\n"",""        current_time = datetime.datetime.now().time().strftime('%H:%M:%S')\\r\\\\n"",""        # Logic for adding shift in db?\\r\\\\n"",""        shiftsdf = df2.select('shift', 'shiftStart', 'shiftEnd', 'reassignedShift').distinct()\\r\\\\n"",""\\r\\\\n"",""        cons_shift = shiftsdf.filter((shiftsdf['shiftStart'] <= current_time) &(shiftsdf['shiftEnd'] > current_time)).select(col('shift').alias('current_shift'), col('reassignedShift').alias('next_shift')).collect()\\r\\\\n"",""\\r\\\\n"",""        ticketAssignedToNextShift=[]\\r\\\\n"",""        ticketWeight = {}\\r\\\\n"",""        #for converting weight df to dict\\r\\\\n"",""        for priority in priorityList:\\r\\\\n"",""            try:\\r\\\\n"",""                weightageList = df3.filter((df3['Priority'] == priority[0])).select('Weight').collect()\\r\\\\n"",""                ticketWeight[priority[0]] = int(weightageList[0][0])\\r\\\\n"",""                \\r\\\\n"",""            except:\\r\\\\n"",""                a=1\\r\\\\n"",""\\r\\\\n"",""        Assigneddata = []\\r\\\\n"",""\\r\\\\n"",""        for tech in trackList:\\r\\\\n"",""            try:\\r\\\\n"",""               # print('Fetching employees in next shift for ', tech[0], cons_shift[0]['next_shift'])\\r\\\\n"",""                if len(cons_shift) <= 0:\\r\\\\n"",""                    print('No employees in next shift for tech ', tech[0])\\r\\\\n"",""                else:\\r\\\\n"",""                        \\r\\\\n"",""                    empInNextShift = empListdf.filter((empListdf['Track'] == tech[0]) &(empListdf['shift'] == cons_shift[0]['next_shift']))\\r\\\\n"",""                    if empInNextShift.count()<=0:\\r\\\\n"",""                        print('No employees in next shift for tech ', tech[0])\\r\\\\n"",""                    else:\\r\\\\n"",""                        empListInNextShift = empInNextShift.select(explode('EmpInTech').alias('EmpInTech')).collect()\\r\\\\n"",""        \\r\\\\n"",""                        empWithNoOfTickets = []\\r\\\\n"",""                        for priority in priorityList:\\r\\\\n"",""                            for emp in empListInNextShift:\\r\\\\n"",""        \\r\\\\n"",""                                empInList = any(emp[0] in sublist[1] for sublist in empWithNoOfTickets)\\r\\\\n"",""                                if(not empInList):\\r\\\\n"",""                                    intitalWeight =0\\r\\\\n"",""                                    empWithNoOfTickets.append([intitalWeight, emp[0]])\\r\\\\n"",""                            Assigneddata = assignTicket(empWithNoOfTickets, priority, tech, ticketAssignedToNextShift,ticketWeight)\\r\\\\n"",""            except:\\r\\\\n"",""                a=1\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""        if(len(Assigneddata)>0):\\r\\\\n"",""            headers = ['Number', 'Category', 'Priority', 'State', 'Subject', 'cleanText', 'Assignee']\\r\\\\n"",""            updated = spark.createDataFrame(Assigneddata, headers)\\r\\\\n"",""            #updated = updated.withColumn('Shift',lit(None).cast(StringType()))\\r\\\\n"",""            print('Assignment completed')\\r\\\\n"",""            updated.show()\\r\\\\n"",""            \\r\\\\n"",""            return updated\\r\\\\n"",""        else:\\r\\\\n"",""            df1=df1.withColumn('Assignee',lit(None).cast(StringType()))\\r\\\\n"",""            print('not assigned')\\r\\\\n"",""            return df1\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n""]},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:30:56"",""alias"":""Roster data"",""id"":2819,""name"":""LEARSTRD63823"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NAME,track,shift,STR_TO_DATE(DATE, '%m/%d/%Y') AS DATE,userAvailable,shiftStart,shiftEnd,reassignAt,reassignedShift FROM leo1311_useravailability rostern JOIN leo1311_rosterdefinition def ON def.shiftName=roster.shift WHERE STR_TO_DATE(DATE, '%m/%d/%Y')=CURRENT_DATE AND userAvailable=1\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:32:19"",""alias"":""Newdump categories"",""id"":2820,""name"":""LEANWDMP75806"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_techclassifiedtickets WHERE (State = 'New' OR State = 'Open') AND (Category IS NOT NULL OR Category != '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_techclassifiedtickets\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:22:03"",""alias"":"" Ticket Weightage "",""id"":2821,""name"":""LEA TCKT79784"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_priorityweightage\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_priorityweightage\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false}]},{""id"":""BqCTE"",""alias"":""Dataset Loader"",""name"":""Dataset Loader"",""classname"":""DatasetLoaderConfig"",""category"":""LoaderConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:35:24"",""alias"":""Assigned Tickets"",""id"":2822,""name"":""LEAASGND79193"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select number, predicted_assignee from leo1311_tickets_enriched where  predicted_assignee is not null order by last_updated desc\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_tickets_enriched\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""applySchema"":false},""position_x"":""855"",""position_y"":""112"",""connectors"":[{""type"":""target"",""endpoint"":""in"",""position"":""LeftMiddle"",""elementId"":""tWXcU"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""in""],""outputEndpoints"":[""out""],""formats"":{""dataset"":""dropdown"",""applySchema"":""checkbox""},""codeGeneration"":{""MYSQL"":{""imports"":[""import mysql"",""from urllib.parse import urlparse"",""from leaputils import Security""],""script"":""\\n    mode = \\""<dataset.attributes.writeMode>\\""\\n    if mode.lower() in ('overwrite', 'append', 'error', 'errorifexists', 'ignore'):\\n        dataset_<id>.write.format('jdbc').options(\\n            url=\\""<dataset.datasource.connectionDetails.url>\\"",\\n            dbtable=\\""<dataset.attributes.tableName>\\"",\\n            user=\\""<dataset.datasource.connectionDetails.userName>\\"",\\n            password=Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")).mode(mode).save()\\n    \\n    elif mode.lower() in ('update'):\\n        columnList = dataset_<id>.columns\\n        url=\\""<dataset.datasource.connectionDetails.url>\\""\\n        tablename = \\""<dataset.attributes.tableName>\\""\\n        username = \\""<dataset.datasource.connectionDetails.userName>\\""\\n        password = Security.decrypt(\\""<dataset.datasource.connectionDetails.password>\\"",\\""<dataset.datasource.salt>\\"")\\n        host = urlparse(url[5:]).hostname\\n        port = urlparse(url[5:]).port\\n        database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\n    \\n        def process_partition(iterator):\\n            cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\n            mycursor = cnx.cursor()\\n            data_list = []\\n            for row in iterator:\\n                paramsDict = {}\\n                values = []\\n                for i in range(0, len(columnList)):\\n                    paramsDict[columnList[i]] = row[i]\\n                    values.append(row[i])\\n    \\n                columns = ', '.join('`{0}`'.format(k) for k in paramsDict)\\n                duplicates = ', '.join('{0}=VALUES({0})'.format(k) for k in paramsDict)\\n                place_holders = ', '.join('%s'.format(k) for k in paramsDict)\\n    \\n                query = \\""INSERT INTO {0} ({1}) VALUES ({2})\\"".format(tablename, columns, place_holders)\\n                query = \\""{0} ON DUPLICATE KEY UPDATE {1}\\"".format(query, duplicates)\\n                data_list.append(values)\\n            if len(data_list) > 0:\\n                mycursor.executemany(query, data_list)\\n                cnx.commit()\\n    \\n            mycursor.close()\\n            cnx.close()\\n    \\n        dataset_<id>.foreachPartition(process_partition)""},""FILE"":{""CSV"":{""imports"":[],""script"":""    dataset.coalesce(1).write.format(\\""com.databricks.spark.csv\\"").mode('overwrite').option(\\""header\\"", \\""true\\"").save(<dataset.attributes.path>)""}}},""context"":[{""script"":[""import logging\\r\\\\n"",""import datetime\\r\\\\n"",""class CustomPythonClass():\\r\\\\n"",""    def __main__(self, dataset):\\r\\\\n"",""        dataset = dataset.select(col('Number').alias('number'),col('Assignee').alias('predicted_assignee'))\\r\\\\n"",""        dataset = dataset.withColumn('last_updated',lit(datetime.datetime.now()))\\r\\\\n"",""        return dataset\\r\\\\n"",""\\r\\\\n""]},{""params"":""{\\""IncidentId\\"":\\""\\"""",""script"":[""import logging as logger\\r\\\\n"",""from datetime import datetime\\r\\\\n"",""from pyspark.sql import SparkSession\\r\\\\n"",""from pyspark.sql.functions import *\\r\\\\n"",""from pyspark.sql.types import *\\r\\\\n"",""import datetime\\r\\\\n"",""import time\\r\\\\n"",""import pandas as pd\\r\\\\n"",""\\r\\\\n"",""class CustomPythonClass():\\r\\\\n"",""    def __main__(self, df1, df2, df3):\\r\\\\n"",""        incidentId = self.params['IncidentId']\\r\\\\n"",""        if incidentId !='':\\r\\\\n"",""            df1 = df1.filter(df1['Number'] == incidentId)\\r\\\\n"",""        \\r\\\\n"",""        spark = SparkSession.builder.master('local[*]').appName('ShiftRosterAssignment').getOrCreate()\\r\\\\n"",""\\r\\\\n"",""        def assignTicket(empWithNoOfTickets, priority, tech, ticketAssignedToNextShift,ticketWeight):\\r\\\\n"",""            \\r\\\\n"",""            print('Assigning tickets for', tech[0],priority[0],'Open')\\r\\\\n"",""            try:\\r\\\\n"",""           \\r\\\\n"",""                ticketdf = df1.filter(\\r\\\\n"",""                    (df1['Category'] == tech[0]) & (df1['Priority'] == priority[0]) & ((df1['State'] == 'New')| (df1['State'] == 'Open') ))\\r\\\\n"",""                #ticketdf = ticketdf.where(col('Assignee').isNull())\\r\\\\n"",""                \\r\\\\n"",""                #Change Assignee to Null instead of ,,\\r\\\\n"",""                currPointer=0\\r\\\\n"",""                ticketAssigned = {}\\r\\\\n"",""               \\r\\\\n"",""                if(ticketdf.count()>0):\\r\\\\n"",""                    print('Assigning tickets to employees for ', priority[0], ' priority and ', tech[0], 'tech')\\r\\\\n"",""                else:\\r\\\\n"",""                    print('No tickets to assign for ', priority[0], ' priority and ', tech[0], 'tech')\\r\\\\n"",""                for i in ticketdf.collect():\\r\\\\n"",""                    \\r\\\\n"",""                    empWithNoOfTickets.sort(key=lambda empWithNoOfTickets: empWithNoOfTickets[0])\\r\\\\n"",""                    \\r\\\\n"",""                    #use min instead of sort\\r\\\\n"",""                    # i[3] = 'Open'\\r\\\\n"",""                    \\r\\\\n"",""                    \\r\\\\n"",""                    \\r\\\\n"",""                    if empWithNoOfTickets[currPointer][1] in ticketAssigned.keys():\\r\\\\n"",""                        ticketAssigned[empWithNoOfTickets[currPointer][1]].append([i[0],i[1],i[2],i[3],i[4],i[5],empWithNoOfTickets[currPointer][1]])\\r\\\\n"",""                    else:\\r\\\\n"",""                        \\r\\\\n"",""                        \\r\\\\n"",""                        ticketAssigned[empWithNoOfTickets[currPointer][1]] = [[i[0],i[1],i[2],i[3],i[4],i[5],empWithNoOfTickets[currPointer][1]]]\\r\\\\n"",""\\r\\\\n"",""                    ticketAssignedToNextShift.append([i[0], i[1], i[2], i[3], i[4], i[5], empWithNoOfTickets[currPointer][1]])\\r\\\\n"",""                    empWithNoOfTickets[currPointer][0]+=(ticketWeight[priority[0]]/len(empListInNextShift))\\r\\\\n"",""                    #print(empWithNoOfTickets,i[2])\\r\\\\n"",""                    \\r\\\\n"",""                return ticketAssignedToNextShift\\r\\\\n"",""            except ex:\\r\\\\n"",""                return ''\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""        trackList = df2.select('track').distinct().collect()\\r\\\\n"",""        priorityList = df1.select('Priority').distinct().collect()\\r\\\\n"",""        priorityList.sort()\\r\\\\n"",""\\r\\\\n"",""        empListdf = df2.groupBy('track', 'shift').agg(array_distinct(collect_list('Name')).alias('EmpInTech'),\\r\\\\n"",""                                                      first('shiftStart').alias('shiftStart'),\\r\\\\n"",""                                                      first('shiftEnd').alias('shiftEnd'),\\r\\\\n"",""                                                      first('reassignAt').alias('reassignAt'),\\r\\\\n"",""                                                      first('reassignedShift').alias('reassignedShift'))\\r\\\\n"",""        empListdf = empListdf.withColumn('EmpInTechSize', size('EmpInTech'))\\r\\\\n"",""\\r\\\\n"",""        current_time = datetime.datetime.now().time().strftime('%H:%M:%S')\\r\\\\n"",""        # Logic for adding shift in db?\\r\\\\n"",""        shiftsdf = df2.select('shift', 'shiftStart', 'shiftEnd', 'reassignedShift').distinct()\\r\\\\n"",""\\r\\\\n"",""        cons_shift = shiftsdf.filter((shiftsdf['shiftStart'] <= current_time) &(shiftsdf['shiftEnd'] > current_time)).select(col('shift').alias('current_shift'), col('reassignedShift').alias('next_shift')).collect()\\r\\\\n"",""\\r\\\\n"",""        ticketAssignedToNextShift=[]\\r\\\\n"",""        ticketWeight = {}\\r\\\\n"",""        #for converting weight df to dict\\r\\\\n"",""        for priority in priorityList:\\r\\\\n"",""            try:\\r\\\\n"",""                weightageList = df3.filter((df3['Priority'] == priority[0])).select('Weight').collect()\\r\\\\n"",""                ticketWeight[priority[0]] = int(weightageList[0][0])\\r\\\\n"",""                \\r\\\\n"",""            except:\\r\\\\n"",""                a=1\\r\\\\n"",""\\r\\\\n"",""        Assigneddata = []\\r\\\\n"",""\\r\\\\n"",""        for tech in trackList:\\r\\\\n"",""            try:\\r\\\\n"",""               # print('Fetching employees in next shift for ', tech[0], cons_shift[0]['next_shift'])\\r\\\\n"",""                if len(cons_shift) <= 0:\\r\\\\n"",""                    print('No employees in next shift for tech ', tech[0])\\r\\\\n"",""                else:\\r\\\\n"",""                        \\r\\\\n"",""                    empInNextShift = empListdf.filter((empListdf['Track'] == tech[0]) &(empListdf['shift'] == cons_shift[0]['next_shift']))\\r\\\\n"",""                    if empInNextShift.count()<=0:\\r\\\\n"",""                        print('No employees in next shift for tech ', tech[0])\\r\\\\n"",""                    else:\\r\\\\n"",""                        empListInNextShift = empInNextShift.select(explode('EmpInTech').alias('EmpInTech')).collect()\\r\\\\n"",""        \\r\\\\n"",""                        empWithNoOfTickets = []\\r\\\\n"",""                        for priority in priorityList:\\r\\\\n"",""                            for emp in empListInNextShift:\\r\\\\n"",""        \\r\\\\n"",""                                empInList = any(emp[0] in sublist[1] for sublist in empWithNoOfTickets)\\r\\\\n"",""                                if(not empInList):\\r\\\\n"",""                                    intitalWeight =0\\r\\\\n"",""                                    empWithNoOfTickets.append([intitalWeight, emp[0]])\\r\\\\n"",""                            Assigneddata = assignTicket(empWithNoOfTickets, priority, tech, ticketAssignedToNextShift,ticketWeight)\\r\\\\n"",""            except:\\r\\\\n"",""                a=1\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""        if(len(Assigneddata)>0):\\r\\\\n"",""            headers = ['Number', 'Category', 'Priority', 'State', 'Subject', 'cleanText', 'Assignee']\\r\\\\n"",""            updated = spark.createDataFrame(Assigneddata, headers)\\r\\\\n"",""            #updated = updated.withColumn('Shift',lit(None).cast(StringType()))\\r\\\\n"",""            print('Assignment completed')\\r\\\\n"",""            updated.show()\\r\\\\n"",""            \\r\\\\n"",""            return updated\\r\\\\n"",""        else:\\r\\\\n"",""            df1=df1.withColumn('Assignee',lit(None).cast(StringType()))\\r\\\\n"",""            print('not assigned')\\r\\\\n"",""            return df1\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n"",""\\r\\\\n""]},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:30:56"",""alias"":""Roster data"",""id"":2819,""name"":""LEARSTRD63823"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT NAME,track,shift,STR_TO_DATE(DATE, '%m/%d/%Y') AS DATE,userAvailable,shiftStart,shiftEnd,reassignAt,reassignedShift FROM leo1311_useravailability rostern JOIN leo1311_rosterdefinition def ON def.shiftName=roster.shift WHERE STR_TO_DATE(DATE, '%m/%d/%Y')=CURRENT_DATE AND userAvailable=1\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 04:32:19"",""alias"":""Newdump categories"",""id"":2820,""name"":""LEANWDMP75806"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_techclassifiedtickets WHERE (State = 'New' OR State = 'Open') AND (Category IS NOT NULL OR Category != '')\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_techclassifiedtickets\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false},{""isValidation"":"""",""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-25 06:22:03"",""alias"":"" Ticket Weightage "",""id"":2821,""name"":""LEA TCKT79784"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""select * from leo1311_priorityweightage\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""leo1311_priorityweightage\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null},""samplingRatio"":"""",""applySchema"":false}]}],""pipeline_attributes"":[{""key"":"""",""value"":""""}]}","admin","test_NextShift","2023-08-25 07:35:04","LEATST_N12573","leo1311","DragAndDrop","15","""""","NULL","NULL"
"admin","2023-08-25 07:24:10.223000","\0","","NULL","{""elements"":[{""id"":""QKsUX"",""alias"":""Python Script"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""PythonScript"",""requirements"":"""",""params"":[],""script"":[""print('Hello')""]},""position_x"":""842"",""position_y"":""106"",""connectors"":[],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""}}]}","admin","testEMRPipeline","2023-08-25 07:25:30","LEATSTMR83573","leo1311","DragNDropLite","0","""""","NULL","NULL"
"admin","2023-08-25 11:08:08.360000","\0","","NULL","{""elements"":[{""attributes"":{""filetype"":""Python3"",""files"":[""LEASNWCN40863_Leap.py""],""arguments"":[],""dataset"":[]}}]}","admin","SnowConnection","2023-08-25 11:24:28","LEASNWCN40863","leo1311","NativeScript","17","""""","NULL","{""17"":{""taskId"":""ea10bf1e-b5b1-418a-8c1c-d59ee2f92b96""}}"
"admin","2023-08-28 04:34:42.191000","\0","","NULL","{""elements"":[{""id"":""TcePf"",""alias"":""Dataset Extractor"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-28 04:28:14"",""alias"":""UserAvailability_current"",""id"":2826,""name"":""LEAUSRVL59473"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT [Name],assignmentgroup,[Shift] AS CurrentShift ,convert(varchar, [DATE], 101) AS DATE, userAvailable,shiftStart,shiftEnd, COALESCE(w.AssignedWeight,0) AS AssignedWeight FROM leo1311_useravailability roster  JOIN leo1311_rosterdefinition def ON cast(def.shiftName as varchar)=cast(roster.[Shift] as varchar) LEFT JOIN (SELECT predicted_assignee AS \\\\\\""USER\\\\\\"" , SUM(cast(cast(weight as varchar)as int)) AS AssignedWeight FROM leo1311_tickets t JOIN leo1311_tickets_enriched e ON t.number= e.number JOIN  leo1311_priorityweightage p ON t.priority = cast(p.priority as varchar) WHERE t.state NOT IN ('Closed','Resolved')  AND predicted_assignee IS NOT NULL GROUP BY predicted_assignee ) w ON cast([Name] as varchar) = w.[user] WHERE convert(varchar, [DATE], 101)=CONVERT(varchar, GETDATE(), 101) AND cast(userAvailable as varchar)='1'  AND CONVERT(VARCHAR, ShiftStart, 108) <= CONVERT(VARCHAR, GETDATE(), 108) AND CONVERT(VARCHAR, ShiftEnd, 108) > CONVERT(VARCHAR, GETDATE(), 108)\\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""r"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""52"",""position_y"":""42"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""zDXSb"",""elementPosition"":""TopCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""DeouB"",""alias"":""Dataset Extractor"",""name"":""Dataset Extractor"",""classname"":""DatasetExtractorConfig"",""category"":""ExtractorConfig"",""attributes"":{""dataset"":{""lastmodifiedby"":""admin"",""lastmodifieddate"":""2023-08-28 04:30:55"",""alias"":""UnassignedTickets_current"",""id"":2827,""name"":""LEAUNSGN80261"",""description"":"""",""schema"":null,""attributes"":""{\\""filter\\"":\\""\\"",\\""mode\\"":\\""query\\"",\\""Query\\"":\\""SELECT number, a.priority, assignmentgroup,weight FROM leo1311_tickets e JOIN leo1311_priorityweightage a ON e.priority=a.priority WHERE NUMBER NOT IN( SELECT NUMBER FROM leo1311_tickets_enriched  WHERE  predicted_assignee IS NOT NULL ) AND  (assignmentgroup IS NOT NULL OR assignmentgroup != 'NULL') \\"",\\""Cacheable\\"":false,\\""isStreaming\\"":\\""false\\"",\\""Headers\\"":\\""\\"",\\""defaultValues\\"":\\""\\"",\\""QueryParams\\"":\\""\\"",\\""writeMode\\"":\\""append\\"",\\""params\\"":\\""{}\\"",\\""tableName\\"":\\""\\"",\\""uniqueIdentifier\\"":\\""\\""}"",""dashboard"":null,""type"":""rw"",""datasource"":{""lastmodifiedby"":""poornasai.nagendra@ad.infosys.com"",""lastmodifieddate"":""2023-07-28 05:55:39"",""alias"":""leo1311"",""id"":524,""name"":""LEAL131Q55868"",""description"":""Leo data scource"",""type"":""MYSQL"",""connectionDetails"":""{\\""password\\"":\\""enciZeumvRF0Vrzfk9aLh6+lb8cuFb42Pc5\\"",\\""userName\\"":\\""leapuser\\"",\\""url\\"":\\""jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\""}"",""salt"":""2Yi2EXDeOGSOGdBiQWu6vSk7OydfusUEvLRFkOW5zX+BhGzGWifHsJBBuf67ShrxYyTfyDVUBShHGyzbuf4uNw=="",""organization"":""leo1311"",""dshashcode"":""4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126"",""activetime"":""2023-07-28 05:55:38"",""category"":""SQL"",""extras"":""{\\""apispec\\"":{},\\""apispectemplate\\"":{}}"",""interfacetype"":null},""backingDataset"":null,""organization"":""leo1311"",""expStatus"":0,""views"":""Table View"",""archivalConfig"":null,""isArchivalEnabled"":null,""isApprovalRequired"":false,""isPermissionManaged"":false,""isAuditRequired"":false,""isInboxRequired"":false,""metadata"":null,""modeltype"":null,""taskdetails"":null,""tags"":""\\""\\"""",""interfacetype"":null}},""position_x"":""18"",""position_y"":""148"",""connectors"":[{""type"":""source"",""endpoint"":""out"",""position"":""RightMiddle"",""elementId"":""zDXSb"",""elementPosition"":""BottomCenter""}],""inputEndpoints"":[],""outputEndpoints"":[""out""],""formats"":{""dataset"":[""dropdown""]},""codeGeneration"":{""REST"":{},""servicenow"":{},""MYSQL"":{},""H2"":{},""MSSQL"":{},""AWS"":{},""POSTGRESQL"":{}},""context"":[]},{""id"":""zDXSb"",""alias"":""Python Script"",""name"":""Python Script"",""classname"":""PythonScriptConfig"",""category"":""BaseConfig"",""attributes"":{""FunctionName"":""CurrentShiftAssign"",""requirements"":"""",""params"":[],""script"":[""import logging as logger\\r\\\\n"",""from datetime import datetime\\r\\\\n"",""import time\\r\\\\n"",""def CurrentShiftAssign(self, UnAssignedTickets, current_shift_emp):\\r\\\\n"",""    #print(UnAssignedTickets)\\r\\\\n"",""    if len(UnAssignedTickets) == 0:\\r\\\\n"",""        print('No Tickets to assign')\\r\\\\n"",""        print('Completed')\\r\\\\n"",""        exit()\\r\\\\n"",""    current_time = datetime.now().time().strftime('%H:%M:%S')\\r\\\\n"",""    print('CurrentTime',current_time)\\r\\\\n"",""    print('current_Shift', current_shift_emp)\\r\\\\n"",""    techList= list(set([ sub['assignmentgroup'] for sub in current_shift_emp ]))\\r\\\\n"",""    print('AssignmentGroup',techList)\\r\\\\n"",""    assignedTickets= []\\r\\\\n"",""    for item in techList:\\r\\\\n"",""        print('Assigning for', item)\\r\\\\n"",""        techTickets= [e for e in UnAssignedTickets if e['assignmentgroup']== item]\\r\\\\n"",""        techEmpList = [e for e in current_shift_emp if e['assignmentgroup']== item]\\r\\\\n"",""        print('Available Employees', techEmpList)\\r\\\\n"",""        if len(techEmpList)>0:\\r\\\\n"",""            for ticket in techTickets:\\r\\\\n"",""                employee= min(techEmpList, key= lambda x:x['AssignedWeight']) \\r\\\\n"",""                techEmployee= next(x for x in techEmpList if x['Name']== employee['Name'])\\r\\\\n"",""                e= next(x for x in techEmpList if x['Name']== employee['Name'])\\r\\\\n"",""                techEmployee['AssignedWeight'] = techEmployee['AssignedWeight']+int(ticket['weight'])\\r\\\\n"",""                ticket['assignedTo']= employee['Name']\\r\\\\n"",""                assignedTickets.append({'number': ticket['number'], 'predicted_assignee': ticket['assignedTo'], 'last_updated':datetime.now()})\\r\\\\n"",""                #print('Ticket {0} assigned to {1}. Current Weight {2}'.format(ticket['number'],ticket['assignedTo'],techEmployee['AssignedWeight']))\\r\\\\n"",""                    \\r\\\\n"",""    if len(assignedTickets)>0:\\r\\\\n"",""        print('Assignment completed')\\r\\\\n"",""        print(assignedTickets)\\r\\\\n"",""        return assignedTickets\\r\\\\n"",""    else:\\r\\\\n"",""        print('No Tickets Assigned')\\r\\\\n"",""        print('Completed')\\r\\\\n"",""        exit()""]},""position_x"":""265"",""position_y"":""90"",""connectors"":[{""type"":""target"",""endpoint"":""dataset2"",""position"":""TopCenter"",""elementId"":""TcePf"",""elementPosition"":""RightMiddle""},{""type"":""target"",""endpoint"":""dataset3"",""position"":""BottomCenter"",""elementId"":""DeouB"",""elementPosition"":""RightMiddle""}],""inputEndpoints"":[""dataset1"",""dataset2"",""dataset3""],""outputEndpoints"":[""out""],""formats"":{""FunctionName"":""text"",""requirements"":""textarea"",""params"":""list"",""script"":""textarea""},""codeGeneration"":{""imports"":[],""script"":""\\n\\n\\n\\n\\n\\n""},""context"":[]}]}","admin","CurrentShidtAssign","2023-08-28 04:37:09","LEACRNTS80546","leo1311","DragNDropLite","6","""""","NULL","NULL"
