"created_by","created_date","deleted","description","job_id","json_content","lastmodifiedby","alias","lastmodifieddate","name","organization","type","version","tags","interfacetype","pipeline_metadata"
"admin","2021-04-05 09:47:02.150000","false","Train a classification model to predict clusters","NULL","{\"elements\":[{\"id\":\"nvSqf\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false},\"position_x\":\"50\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"oksqS\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"aPhFu\",\"alias\":\"Count  Vectorizer\",\"name\":\"Count  Vectorizer\",\"classname\":\"CountVectorizerFeatureExtractorConfig\",\"category\":\"FeatureExtractorConfig\",\"attributes\":{\"inputCol\":\"clean_tokens\",\"outputCol\":\"features\",\"vocabSize\":\"1000000\"},\"position_x\":\"710\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"UkSyH\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"acJqw\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"vocabSize\":\"text\"},\"context\":[{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"KoEyF\",\"alias\":\"Random  Forest  Classifier\",\"name\":\"Random  Forest  Classifier\",\"classname\":\"RandomForestClassifierConfig\",\"category\":\"AnalyzerConfig\",\"attributes\":{\"labelCol\":\"cluster_idx\",\"featuresCol\":\"features\",\"numTrees\":\"10\"},\"position_x\":\"60\",\"position_y\":\"240\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"PwxMg\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"UkSyH\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"labelCol\":\"text\",\"featuresCol\":\"text\",\"numTrees\":\"text\"},\"context\":[{\"inputCols\":\"clusterName\",\"outputCols\":\"cluster_idx\"},{\"inputCol\":\"clean_tokens\",\"outputCol\":\"features\",\"vocabSize\":\"1000000\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"PwxMg\",\"alias\":\"Model  Sink\",\"name\":\"Model  Sink\",\"classname\":\"ModelSinkConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"modelName\":\"IIAClusterClassification\"},\"position_x\":\"330\",\"position_y\":\"240\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"KoEyF\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"kOIVY\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"modelName\":\"text\"},\"context\":[{\"labelCol\":\"cluster_idx\",\"featuresCol\":\"features\",\"numTrees\":\"10\"},{\"inputCols\":\"clusterName\",\"outputCols\":\"cluster_idx\"},{\"inputCol\":\"clean_tokens\",\"outputCol\":\"features\",\"vocabSize\":\"1000000\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"kOIVY\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"from datetime import datetime\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"        print(dataset)\\r\",\"        data = dataset.select('number','prediction')\\r\",\"        indexer = model.stages[3]\\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='cluster_classification_label', labels=indexer.labels)\\r\",\"        dataset = labeler.transform(data)\\r\",\"        dataset = dataset.drop('prediction')\\r\",\"        dataset = dataset.withColumn('last_updated', lit(datetime.now()))\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"600\",\"position_y\":\"240\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"PwxMg\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"kqYQR\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"modelName\":\"IIAClusterClassification\"},{\"labelCol\":\"cluster_idx\",\"featuresCol\":\"features\",\"numTrees\":\"10\"},{\"inputCols\":\"clusterName\",\"outputCols\":\"cluster_idx\"},{\"inputCol\":\"clean_tokens\",\"outputCol\":\"features\",\"vocabSize\":\"1000000\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"kqYQR\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"id\":39038,\"name\":\"ClassifiedClusters\",\"alias\":\"ClassifiedClusters\",\"description\":\"Classified Clusters for training data\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, cluster_classification_label,  last_updated from @projectname_tickets_enriched  \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_enriched\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"groups\":\"\",\"expStatus\":0,\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"860\",\"position_y\":\"240\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"kOIVY\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"from datetime import datetime\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"        print(dataset)\\r\",\"        data = dataset.select('number','prediction')\\r\",\"        indexer = model.stages[3]\\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='cluster_classification_label', labels=indexer.labels)\\r\",\"        dataset = labeler.transform(data)\\r\",\"        dataset = dataset.drop('prediction')\\r\",\"        dataset = dataset.withColumn('last_updated', lit(datetime.now()))\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"modelName\":\"IIAClusterClassification\"},{\"labelCol\":\"cluster_idx\",\"featuresCol\":\"features\",\"numTrees\":\"10\"},{\"inputCols\":\"clusterName\",\"outputCols\":\"cluster_idx\"},{\"inputCol\":\"clean_tokens\",\"outputCol\":\"features\",\"vocabSize\":\"1000000\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"UkSyH\",\"alias\":\"String  Indexer\",\"name\":\"String  Indexer\",\"classname\":\"StringIndexerTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCols\":\"clusterName\",\"outputCols\":\"cluster_idx\"},\"position_x\":\"930\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"aPhFu\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"KoEyF\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCols\":\"text\",\"outputCols\":\"text\"},\"context\":[{\"inputCol\":\"clean_tokens\",\"outputCol\":\"features\",\"vocabSize\":\"1000000\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"acJqw\",\"alias\":\"Stop  Words  Remover\",\"name\":\"Stop  Words  Remover\",\"classname\":\"StopWordsRemoverTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"\"},\"position_x\":\"490\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"aPhFu\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"oksqS\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"stopWords\":\"text\"},\"context\":[{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"oksqS\",\"alias\":\"Tokenizer\",\"name\":\"Tokenizer\",\"classname\":\"TokenizerTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"text\",\"outputCol\":\"tokens\"},\"position_x\":\"260\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"nvSqf\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"acJqw\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"context\":[{\"dataset\":{\"id\":39039,\"name\":\"Clusters\",\"alias\":\"Clusters\",\"description\":\"clustered Tickets with short description\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:03:07\",\"alias\":\"Cluster Classification\",\"id\":13,\"name\":\"ACMCLSTR30039\",\"description\":\"\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isprimarykey\\\":true,\\\"isautoincrement\\\":false,\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"text\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\",\\\"isprimarykey\\\":false,\\\"isautoincrement\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\",\"formtemplate\":\"[]\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number , clean_text as text, post_ranking_cluster AS clusterName FROM @projectname_tickets_enriched WHERE post_ranking_cluster IS NOT NULL AND post_ranking_cluster <> ''\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]}]}","admin","Cluster Classification Training","2021-06-21 11:15:34","ClusterClassification_Training","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-04-05 09:52:16.408000","false","Predict cluster using trained model","NULL","{\"elements\":[{\"id\":\"nvSqf\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"id\":39048,\"name\":\"NewTickets\",\"alias\":\"NewTickets\",\"description\":\"New Tickets to predict cluster\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":true},\"position_x\":\"50\",\"position_y\":\"150\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"YNfFg\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"kOIVY\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"from datetime import datetime\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"        data = dataset.select('number','prediction')\\r\",\"        indexer = model.stages[3]\\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='cluster_classification_label', labels=indexer.labels)\\r\",\"        dataset = labeler.transform(data)\\r\",\"        dataset = dataset.select('number','cluster_classification_label')\\r\",\"        dataset = dataset.withColumn('last_updated', lit(datetime.now()))\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"730\",\"position_y\":\"150\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"kqYQR\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"NjHpk\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"modelName\":\"IIAClusterClassification\"},{\"params\":\"{'WorkflowName': '', 'IncidentId': ''}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        dataset.collect()\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39048,\"name\":\"NewTickets\",\"alias\":\"NewTickets\",\"description\":\"New Tickets to predict cluster\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":true}]},{\"id\":\"kqYQR\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"id\":39038,\"name\":\"ClassifiedClusters\",\"alias\":\"ClassifiedClusters\",\"description\":\"Classified Clusters for training data\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,cluster_classification_label,  last_updated from @projectname_tickets_enriched\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_enriched\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"groups\":\"\",\"expStatus\":0,\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"950\",\"position_y\":\"150\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"kOIVY\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"from datetime import datetime\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"        data = dataset.select('number','prediction')\\r\",\"        indexer = model.stages[3]\\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='cluster_classification_label', labels=indexer.labels)\\r\",\"        dataset = labeler.transform(data)\\r\",\"        dataset = dataset.select('number','cluster_classification_label')\\r\",\"        dataset = dataset.withColumn('last_updated', lit(datetime.now()))\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"modelName\":\"IIAClusterClassification\"},{\"params\":\"{'WorkflowName': '', 'IncidentId': ''}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        dataset.collect()\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39048,\"name\":\"NewTickets\",\"alias\":\"NewTickets\",\"description\":\"New Tickets to predict cluster\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":true}]},{\"id\":\"NjHpk\",\"alias\":\"Model  Source\",\"name\":\"Model  Source\",\"classname\":\"ModelSourceConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"modelName\":\"IIAClusterClassification\"},\"position_x\":\"510\",\"position_y\":\"150\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"kOIVY\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"YNfFg\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"modelName\":\"text\"},\"context\":[{\"params\":\"{'WorkflowName': '', 'IncidentId': ''}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        dataset.collect()\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39048,\"name\":\"NewTickets\",\"alias\":\"NewTickets\",\"description\":\"New Tickets to predict cluster\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":true}]},{\"id\":\"YNfFg\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"{'WorkflowName': '', 'IncidentId': ''}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        dataset.collect()\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"300\",\"position_y\":\"150\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"nvSqf\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"NjHpk\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"id\":39048,\"name\":\"NewTickets\",\"alias\":\"NewTickets\",\"description\":\"New Tickets to predict cluster\",\"schema\":{\"id\":10106,\"name\":\"ClusterClassification\",\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"configurationItem\\\",\\\"recordcolumndisplayname\\\":\\\"configurationItem\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"clusterName\\\",\\\"recordcolumndisplayname\\\":\\\"clusterName\\\"},{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"projectId\\\",\\\"recordcolumndisplayname\\\":\\\"projectId\\\"},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"shortDescription\\\",\\\"recordcolumndisplayname\\\":\\\"shortDescription\\\"}]\",\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, shortdescription as text, configurationItem, ''AS clusterName FROM  @projectname_tickets WHERE (shortdescription is not null and shortdescription <> '')\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-21 11:01:56\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encnOsN7Ge7HZuvB9Z4Tobpr5LfVWGgdkxS\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"gBL2oW1CXmYVlFMotQnqjQeGevFjul91p7wutQx8N8Jj71UuoY7eMWasQ9jK5zhTCIwqod4DB9VqFlIC8n2W6w==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-21 11:01:55\",\"category\":\"SQL\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":true}]}]}","admin","Cluster Prediction","2021-06-21 11:13:54","Cluster_Prediction","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-05-25 15:06:10.595000","false","MAP Key Phrases","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"ACMKYPHR40814_leo1311.py\"],\"arguments\":[{\"name\":\"LEAPDS\",\"value\":\"leo1311\",\"type\":\"Datasource\",\"index\":\"1\"},{\"name\":\"TableName\",\"value\":\"@projectname_phrase_extraction\",\"type\":\"Text\",\"index\":\"2\",\"alias\":\"@projectname_phrase_extraction\"}],\"dataset\":[]}}]}","NULL","Key Phrase Mapping","NULL","ACMKYPHR40814","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-04-05 11:38:22.204000","false","Named Entity Extraction","NULL","{\"elements\":[{\"id\":\"fgAKW\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false},\"position_x\":\"60\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"QUUgg\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"wtuSh\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.sql import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, dataset):\\r\",\"        \\r\",\"        dataset = dataset.withColumn('namedEntities', array_distinct('filter_tokens'))\\r\",\"        dataset = dataset.withColumn('entityValue', explode('namedEntities'))\\r\",\"        dataset = dataset.withColumn('entityId', dense_rank().over(Window.partitionBy('number').orderBy('entityValue')))\\r\",\"        dataset = dataset.withColumn('entityName', concat(lit('entity'), 'entityId'))\\r\",\"        dataset = dataset.select('number','entityName','entityValue')\\r\",\"        return dataset\\r\"]},\"position_x\":\"330\",\"position_y\":\"250\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"CNAxZ\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"HWHBP\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"script\":[\"from pyspark.sql.types import *\\r\",\"from pyspark.sql.functions import *\\r\",\"import enchant\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        def filterTokens(tokens):\\r\",\"            d = enchant.Dict('en_US')\\r\",\"            tokens = [t for t in tokens if d.check(t) == False]\\r\",\"            return tokens\\r\",\"\\r\",\"        udfFilterTokens = udf(filterTokens,ArrayType(StringType()))\\r\",\"        dataset = dataset.withColumn('filter_tokens',udfFilterTokens('tokens'))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"inputCol\":\"lemmetized_text\",\"outputCol\":\"tokens\",\"pattern\":\"\\\\W\"},{\"inputCol\":\"shortDescription\",\"outputCol\":\"lemmetized_text\",\"tags\":\"\"},{\"params\":\"{\\\"IncidentId\\\":\\\"\\\"}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"CNAxZ\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"id\":39046,\"name\":\"aio_tkt_ner\",\"alias\":\"aio_tkt_ner\",\"description\":\"NER for Tickets\",\"schema\":{\"id\":10105,\"name\":\"iamp\",\"schemavalue\":[],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select * from @projectname_tickets_ner\",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_ner\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"groups\":\"\",\"expStatus\":0,\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"540\",\"position_y\":\"250\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"wtuSh\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.sql import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, dataset):\\r\",\"        \\r\",\"        dataset = dataset.withColumn('namedEntities', array_distinct('filter_tokens'))\\r\",\"        dataset = dataset.withColumn('entityValue', explode('namedEntities'))\\r\",\"        dataset = dataset.withColumn('entityId', dense_rank().over(Window.partitionBy('number').orderBy('entityValue')))\\r\",\"        dataset = dataset.withColumn('entityName', concat(lit('entity'), 'entityId'))\\r\",\"        dataset = dataset.select('number','entityName','entityValue')\\r\",\"        return dataset\\r\"]},{\"script\":[\"from pyspark.sql.types import *\\r\",\"from pyspark.sql.functions import *\\r\",\"import enchant\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        def filterTokens(tokens):\\r\",\"            d = enchant.Dict('en_US')\\r\",\"            tokens = [t for t in tokens if d.check(t) == False]\\r\",\"            return tokens\\r\",\"\\r\",\"        udfFilterTokens = udf(filterTokens,ArrayType(StringType()))\\r\",\"        dataset = dataset.withColumn('filter_tokens',udfFilterTokens('tokens'))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"inputCol\":\"lemmetized_text\",\"outputCol\":\"tokens\",\"pattern\":\"\\\\W\"},{\"inputCol\":\"shortDescription\",\"outputCol\":\"lemmetized_text\",\"tags\":\"\"},{\"params\":\"{\\\"IncidentId\\\":\\\"\\\"}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"fjZcE\",\"alias\":\"Word  Lemmetizer\",\"name\":\"Word  Lemmetizer\",\"classname\":\"WordLemmetizerTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"shortDescription\",\"outputCol\":\"lemmetized_text\",\"tags\":\"\"},\"position_x\":\"520\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"uWRrr\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"QUUgg\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"textarea\",\"outputCol\":\"textarea\",\"tags\":\"textarea\"},\"context\":[{\"params\":\"{\\\"IncidentId\\\":\\\"\\\"}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"HWHBP\",\"alias\":\"Filter Tokens\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"from pyspark.sql.types import *\\r\",\"from pyspark.sql.functions import *\\r\",\"import enchant\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        def filterTokens(tokens):\\r\",\"            d = enchant.Dict('en_US')\\r\",\"            tokens = [t for t in tokens if d.check(t) == False]\\r\",\"            return tokens\\r\",\"\\r\",\"        udfFilterTokens = udf(filterTokens,ArrayType(StringType()))\\r\",\"        dataset = dataset.withColumn('filter_tokens',udfFilterTokens('tokens'))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"900\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wtuSh\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"uWRrr\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\"},\"context\":[{\"inputCol\":\"lemmetized_text\",\"outputCol\":\"tokens\",\"pattern\":\"\\\\W\"},{\"inputCol\":\"shortDescription\",\"outputCol\":\"lemmetized_text\",\"tags\":\"\"},{\"params\":\"{\\\"IncidentId\\\":\\\"\\\"}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"uWRrr\",\"alias\":\"Regex  Tokenizer\",\"name\":\"Regex  Tokenizer\",\"classname\":\"RegexTokenizerTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"lemmetized_text\",\"outputCol\":\"tokens\",\"pattern\":\"\\\\W\"},\"position_x\":\"720\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"fjZcE\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"HWHBP\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"pattern\":\"text\"},\"context\":[{\"inputCol\":\"shortDescription\",\"outputCol\":\"lemmetized_text\",\"tags\":\"\"},{\"params\":\"{\\\"IncidentId\\\":\\\"\\\"}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]},{\"id\":\"QUUgg\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"{\\\"IncidentId\\\":\\\"\\\"}\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        incidentId = self.params['IncidentId']\\r\",\"        if incidentId !='':\\r\",\"            dataset = dataset.filter(dataset['number'] == incidentId)\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"290\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"fgAKW\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"fjZcE\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"id\":39045,\"name\":\"ticket_shortDescription\",\"alias\":\"ticket_shortDescription\",\"description\":\"tickets with short description\",\"schema\":{\"id\":10108,\"name\":\"NERExtraction\",\"alias\":\"NERExtraction\",\"schemavalue\":[{\"columntype\":\"string\",\"columnorder\":1,\"recordcolumnname\":\"number\",\"recordcolumndisplayname\":\"number\"},{\"columntype\":\"string\",\"columnorder\":2,\"recordcolumnname\":\"shortDescription\",\"recordcolumndisplayname\":\"shortDescription\"},{\"columntype\":\"string\",\"columnorder\":3,\"recordcolumnname\":\"namedEntity\",\"recordcolumndisplayname\":\"namedEntity\"}],\"organization\":\"leo1311\"},\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,  shortdescription  FROM @projectname_tickets WHERE shortdescription != '' AND shortdescription IS NOT NULL \",\"Cacheable\":\"false\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-10 17:08:39\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"enciJA+QyHJIh3qn/lHXQKhmbxHNp5c5qQe\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"uZEL8wN8KBpjxlGp7vmogYHrv8muq+383eAKOoU2hkXJQNfJUGqELqf7JlPfgX01Q7913nPE5X8sLO1Xxu1y9g==\",\"organization\":\"leo1311\",\"dshashcode\":\"6dff27e9115bd0514b0e559d24ac28b5567259110cd168ea4378ceb12269b063\",\"activetime\":\"2021-06-10T17:08:38.614+00:00\",\"category\":\"SQL\"},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"datasetType\":\"MYSQL\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false,\"restnode\":false}]}]}","admin","Extract_NER","2021-06-10 17:09:57","Extract_NER","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-04-05T11:43:44.820","false","Detect Anomalies in CPU Utilization","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python2\",\"files\":[\"AnomalyDetection_Core.py\"],\"arguments\":[],\"dataset\":[]}}]}","admin","AnomalyDetection","2021-04-05T13:57:46","AnomalyDetection","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-05-03T05:32:23.355","false","Create incident, service request, incident task, change request in SNOW and then in  icm_tickets","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"SNOW_Create_API_Acme.py\"],\"arguments\":[{\"name\":\"SnowDataSource\",\"value\":\"ACMSNWQB82627\",\"index\":\"1\",\"type\":\"Datasource\",\"alias\":\"SNOW\"},{\"name\":\"api\",\"value\":\"/api/now/table/\",\"index\":\"2\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"4\"},{\"name\":\"setProxy\",\"value\":\"True\",\"index\":\"7\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"index\":\"8\"},{\"name\":\"LEAPDataSource\",\"value\":\"leo1311\",\"index\":\"9\",\"type\":\"Datasource\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"index\":\"9\"},{\"name\":\"dataTable\",\"value\":\"@projectname_tickets\",\"type\":\"Text\",\"index\":\"9\"}]}}]}","NULL","SNOW_Create_API","NULL","SNOW_Create_API","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-05-03T05:32:52.837","false","Update incident, service request, change request, incident task in SNOW","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"SNOW_Update_API_Acme.py\"],\"arguments\":[{\"name\":\"SnowDataSource\",\"value\":\"ACMSNWQB82627\",\"index\":\"1\",\"type\":\"Datasource\",\"alias\":\"SNOW\"},{\"name\":\"api\",\"value\":\"/api/now/table/\",\"index\":\"2\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"4\"},{\"name\":\"setProxy\",\"value\":\"True\",\"index\":\"5\"},{\"name\":\"LEAPDataSource\",\"value\":\"leo1311\",\"index\":\"10\",\"type\":\"Datasource\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"shortdescription\\\":\\\"test update\\\",\\\"sysId\\\":\\\"d581f578dbf8501064b334523996193d\\\",\\\"priority\\\":{\\\"displayvalue\\\":\\\"1\\\",\\\"systemId\\\":\\\"1\\\"},\\\"state\\\":{\\\"displayvalue\\\":\\\"In Progress\\\",\\\"systemId\\\":\\\"2\\\"},\\\"category\\\":{\\\"displayvalue\\\":\\\"Inquiry / Help\\\",\\\"systemId\\\":\\\"inquiry\\\"},\\\"impact\\\":{\\\"displayvalue\\\":\\\"1\\\",\\\"systemId\\\":\\\"1\\\"},\\\"urgency\\\":{\\\"systemId\\\":\\\"\\\"},\\\"configurationitem\\\":{\\\"displayvalue\\\":null,\\\"systemId\\\":\\\"\\\"},\\\"description\\\":\\\"P3 - AccessAuthentication - Multiple Login Failure from same User - Windows\\\\\\\\n containing Failure Audit: An account failed to log on\\\\\\\\n\\\",\\\"assignmentgroup\\\":{\\\"displayvalue\\\":\\\"RMA Approvers\\\",\\\"systemId\\\":\\\"1c590685c0a8018b2a473a7159ff5d9a\\\"},\\\"assignedto\\\":{\\\"displayvalue\\\":\\\"Ameya Shetti\\\",\\\"systemId\\\":\\\"01d0c837374da34084720ba754990e0c\\\"},\\\"sop\\\":null,\\\"workflow\\\":null,\\\"resolutionsteps\\\":null}\",\"index\":\"13\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"index\":\"10\"},{\"name\":\"dataTable\",\"value\":\"@projectname_tickets\",\"type\":\"Text\",\"index\":\"9\"}]}}]}","NULL","SNOW_Update_API","NULL","SNOW_Update_API","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-05-25 09:07:03.255000","false","Processing short description and extracting clean tickets","NULL","{\"elements\":[{\"id\":\"UEeVc\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select number, shortdescription, configurationItem as group_by_field from @projectname_tickets where shortdescription <> '' and   shortdescription is not Null  \",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"40\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ffQic\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"HYOWn\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"CleanText\",\"alias\":\"Clean text\",\"description\":\"Clean Text\",\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, clean_text, group_by_field, last_updated from @projectname_tickets_enriched\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_enriched\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"780\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"fPAbp\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from nltk.stem import WordNetLemmatizer\\r\",\"from datetime import datetime\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"      \\r\",\"        def lemmatize(tokens):\\r\",\"            wordnet_lemmatizer = WordNetLemmatizer()\\r\",\"            lemmatizedTokens = []\\r\",\"            for word in tokens:\\r\",\"                lemmatizedTokens.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\\r\",\"            return ' '.join(lemmatizedTokens)\\r\",\"        \\r\",\"        udfLemmetize = udf(lemmatize, StringType())\\r\",\"        dataset = dataset.withColumn('clean_text',udfLemmetize('clean_tokens'))\\r\",\"        dataset = dataset.select('number','clean_text', 'group_by_field')\\r\",\"        dataset = dataset.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"lemmatizedText\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('shortdescription'))\\r\",\"        dataset = dataset.select('number','shortdescription','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,shortdescription,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select number, shortdescription, configurationItem as group_by_field from @projectname_tickets where shortdescription <> '' and   shortdescription is not Null  \",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"ffQic\",\"alias\":\"Clean Text\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('shortdescription'))\\r\",\"        dataset = dataset.select('number','shortdescription','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,shortdescription,cleanText,group_by_field\"},\"position_x\":\"260\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"UEeVc\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"cMPGE\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\",\"outputCols\":\"text\"},\"context\":[{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select number, shortdescription, configurationItem as group_by_field from @projectname_tickets where shortdescription <> '' and   shortdescription is not Null  \",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"cMPGE\",\"alias\":\"Tokenizer\",\"name\":\"Tokenizer\",\"classname\":\"TokenizerTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},\"position_x\":\"520\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"ffQic\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"IgGTx\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('shortdescription'))\\r\",\"        dataset = dataset.select('number','shortdescription','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,shortdescription,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select number, shortdescription, configurationItem as group_by_field from @projectname_tickets where shortdescription <> '' and   shortdescription is not Null  \",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"IgGTx\",\"alias\":\"Stop  Words  Remover\",\"name\":\"Stop  Words  Remover\",\"classname\":\"StopWordsRemoverTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},\"position_x\":\"760\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"cMPGE\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"fPAbp\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"stopWords\":\"text\"},\"context\":[{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('shortdescription'))\\r\",\"        dataset = dataset.select('number','shortdescription','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,shortdescription,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select number, shortdescription, configurationItem as group_by_field from @projectname_tickets where shortdescription <> '' and   shortdescription is not Null  \",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"fPAbp\",\"alias\":\"Lemmetizer\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from nltk.stem import WordNetLemmatizer\\r\",\"from datetime import datetime\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"      \\r\",\"        def lemmatize(tokens):\\r\",\"            wordnet_lemmatizer = WordNetLemmatizer()\\r\",\"            lemmatizedTokens = []\\r\",\"            for word in tokens:\\r\",\"                lemmatizedTokens.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\\r\",\"            return ' '.join(lemmatizedTokens)\\r\",\"        \\r\",\"        udfLemmetize = udf(lemmatize, StringType())\\r\",\"        dataset = dataset.withColumn('clean_text',udfLemmetize('clean_tokens'))\\r\",\"        dataset = dataset.select('number','clean_text', 'group_by_field')\\r\",\"        dataset = dataset.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"lemmatizedText\"},\"position_x\":\"70\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"IgGTx\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"HYOWn\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\",\"outputCols\":\"text\"},\"context\":[{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('shortdescription'))\\r\",\"        dataset = dataset.select('number','shortdescription','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,shortdescription,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select number, shortdescription, configurationItem as group_by_field from @projectname_tickets where shortdescription <> '' and   shortdescription is not Null  \",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]}]}","NULL","Clean Tickets","NULL","ACMCLNTC99308","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-05-25T14:13:54.009","false","TelemetryLogging","NULL","{\"elements\":[{\"id\":\"qqkeR\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"20\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"NiwAo\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"NiwAo\",\"alias\":\"Python  Script  Transforme\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},\"position_x\":\"220\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"qqkeR\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"mlzTs\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"FxLFX\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},\"position_x\":\"410\",\"position_y\":\"100\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out2\",\"position\":\"BottomCenter\",\"elementId\":\"twFuJ\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"sccMW\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"mlzTs\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"twFuJ\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"# Load EndBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('EB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('EB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['SB_mId', 'SB_ets', 'SB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},\"position_x\":\"410\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"FxLFX\",\"elementPosition\":\"BottomCenter\"},{\"type\":\"source\",\"endpoint\":\"out2\",\"position\":\"BottomCenter\",\"elementId\":\"vGfMK\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"iRGSJ\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"mlzTs\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"410\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"NiwAo\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out2\",\"position\":\"BottomCenter\",\"elementId\":\"FxLFX\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"blpXu\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"vGfMK\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"# Load SkipEvent & FailEvent\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        \\r\",\"        if ('SE_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SE_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['FE_mId', 'FE_ets', 'FE_timestamp','SB_mId', 'SB_ets', 'SB_timestamp', 'EB_mId', 'EB_ets', 'EB_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        \\r\",\"        if ('FE_mId' in data.columns) :\\r\",\"            dataset2 = data.where(col('FE_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp','SB_mId', 'SB_ets', 'SB_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset2.columns]\\r\",\"            dataset2 = dataset2.drop(*drop_columns)\\r\",\"            dataset2 = dataset2.repartition(10)\\r\",\"        else :\\r\",\"            dataset2 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1,dataset2\\r\"]},\"position_x\":\"410\",\"position_y\":\"300\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"twFuJ\",\"elementPosition\":\"BottomCenter\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"GleaE\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"source\",\"endpoint\":\"out2\",\"position\":\"BottomCenter\",\"elementId\":\"BGJcB\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"params\":\"\",\"script\":[\"# Load EndBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('EB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('EB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['SB_mId', 'SB_ets', 'SB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"blpXu\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"telemetry_errors\",\"description\":\"telemetry_errors\",\"schema\":\"\",\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM telemetry_errors\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"telemetry_errors\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"telemetry_errors\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"640\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"mlzTs\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"sccMW\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"SB_data\",\"description\":\"\",\"schema\":\"\",\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from telemetry_logging\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"telemetry_logging\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"SB_data\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"640\",\"position_y\":\"100\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"FxLFX\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"iRGSJ\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"EB_data\",\"description\":\"\",\"schema\":\"\",\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from telemetry_logging\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"telemetry_logging\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"EB_data\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"640\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"twFuJ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"# Load EndBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('EB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('EB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['SB_mId', 'SB_ets', 'SB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"GleaE\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"SE_data\",\"description\":\"\",\"schema\":\"\",\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from telemetry_logging\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"telemetry_logging\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"SE_data\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"640\",\"position_y\":\"300\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"vGfMK\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"# Load SkipEvent & FailEvent\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        \\r\",\"        if ('SE_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SE_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['FE_mId', 'FE_ets', 'FE_timestamp','SB_mId', 'SB_ets', 'SB_timestamp', 'EB_mId', 'EB_ets', 'EB_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        \\r\",\"        if ('FE_mId' in data.columns) :\\r\",\"            dataset2 = data.where(col('FE_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp','SB_mId', 'SB_ets', 'SB_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset2.columns]\\r\",\"            dataset2 = dataset2.drop(*drop_columns)\\r\",\"            dataset2 = dataset2.repartition(10)\\r\",\"        else :\\r\",\"            dataset2 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1,dataset2\\r\"]},{\"params\":\"\",\"script\":[\"# Load EndBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('EB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('EB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['SB_mId', 'SB_ets', 'SB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"BGJcB\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"FE_Data\",\"description\":\"\",\"schema\":\"\",\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from telemetry_logging\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"telemetry_logging\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\" FE_Data\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"640\",\"position_y\":\"380\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"vGfMK\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"# Load SkipEvent & FailEvent\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        \\r\",\"        if ('SE_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SE_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['FE_mId', 'FE_ets', 'FE_timestamp','SB_mId', 'SB_ets', 'SB_timestamp', 'EB_mId', 'EB_ets', 'EB_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        \\r\",\"        if ('FE_mId' in data.columns) :\\r\",\"            dataset2 = data.where(col('FE_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp','SB_mId', 'SB_ets', 'SB_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset2.columns]\\r\",\"            dataset2 = dataset2.drop(*drop_columns)\\r\",\"            dataset2 = dataset2.repartition(10)\\r\",\"        else :\\r\",\"            dataset2 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1,dataset2\\r\"]},{\"params\":\"\",\"script\":[\"# Load EndBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('EB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('EB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['SB_mId', 'SB_ets', 'SB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"# Load StartBot\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        data = data['out2']\\r\",\"        if ('SB_mId' in data.columns) :\\r\",\"            dataset1 = data.where(col('SB_mId').isNotNull())\\r\",\"\\r\",\"            to_drop = ['EB_mId', 'EB_ets', 'EB_timestamp', 'FE_mId', 'FE_ets', 'FE_timestamp', 'SE_mId', 'SE_ets', 'SE_timestamp']\\r\",\"            drop_columns = [i for i in to_drop if i in dataset1.columns]\\r\",\"            dataset1 = dataset1.drop(*drop_columns)\\r\",\"            dataset1 = dataset1.repartition(10)\\r\",\"        else :\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"\\r\",\"        return dataset1, data\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model, data):\\r\",\"        \\r\",\"        if ('message' in data.columns):\\r\",\"            dataset1 = data.where(col('message').isNotNull()).select('eid', 'message')\\r\",\"            data = data.where(col('message').isNull())\\r\",\"            data = data.drop('message')\\r\",\"        else:\\r\",\"            dataset1 = data.where(col('eid').isNull())\\r\",\"        return dataset1, data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql import SparkSession\\r\",\"import json\\r\",\"import re\\r\",\"from datetime import datetime\\r\",\"import mysql.connector\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        responseList = []\\r\",\"        eidList = []\\r\",\"\\r\",\"                \\r\",\"        for item in dataset:\\r\",\"            try:\\r\",\"                row = {}\\r\",\"                message = item.get('message')\\r\",\"                msg_dict = message.replace('\\\\n', '')\\r\",\"                newStr = re.sub('\\\\s+', ' ', msg_dict)\\r\",\"                jsonStr = newStr\\r\",\"                data = json.loads(jsonStr.strip())\\r\",\"                date_time = str(item.get('@timestamp'))\\r\",\"                time_stamp = date_time.replace('T', ' ').replace('Z', '')\\r\",\"                format = '%Y-%m-%d %H:%M:%S.%f'  # The format\\r\",\"                datetime_str = datetime.strptime(time_stamp, format)\\r\",\"                row['action_timestamp'] = datetime_str\\r\",\"                \\r\",\"                if ((data['btfdata']['project_name'] == None or data['btfdata']['portfolio_name'] == None or data['btfdata']['workflow_name'] == None or data['btfdata']['workflow_name'] == '') and not(data['mId'].startswith('Skipped Event'))):\\r\",\"                    # logging.info('Ignoring None: ' + str(data['eid']))\\r\",\"                    continue\\r\",\"        \\r\",\"                for key in data.keys():\\r\",\"                    if key in ('eid', 'mId', 'ets'):\\r\",\"                        row[key] = str(data.get(key))\\r\",\"                    else:\\r\",\"                        if key.lower() == 'context':\\r\",\"                            context = data.get('context')\\r\",\"                            row['channel'] = str(context.get('channel', ''))\\r\",\"                            row['env'] = str(context.get('env', ''))\\r\",\"\\r\",\"                            cdata = context.get('cdata')\\r\",\"                            if (type(cdata)) == dict:\\r\",\"                                row['ActivityInstanceId'] = str(cdata.get('ActivityInstanceId', ''))\\r\",\"                                try:\\r\",\"                                    row['Duration'] = str(cdata.get('Duration', ''))\\r\",\"                                except ValueError:\\r\",\"                                    row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = str(cdata.get('CurrentActivityName', ''))\\r\",\"                                row['ProcessInstanceId'] = str(cdata.get('ProcessInstanceId', ''))\\r\",\"                                row['Component'] = str(cdata.get('Component', ''))\\r\",\"                            else:\\r\",\"                                row['ActivityInstanceId'] = ''\\r\",\"                                row['Duration'] = ''\\r\",\"                                row['CurrentActivityName'] = ''\\r\",\"                                row['ProcessInstanceId'] = ''\\r\",\"                                row['Component'] = ''\\r\",\"\\r\",\"                        elif key.lower() == 'edata':\\r\",\"                            edata = data.get('edata')\\r\",\"                            row['pageid'] = str(edata.get('pageid', ''))\\r\",\"                            row['type'] = str(edata.get('type', ''))\\r\",\"                            row['stageto'] = str(edata.get('stageto', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'actor':\\r\",\"                            actor = data.get('actor')\\r\",\"                            row['actor_id'] = str(actor.get('id', ''))\\r\",\"                            row['actor_type'] = str(actor.get('type', ''))\\r\",\"\\r\",\"                        elif key.lower() == 'btfdata':\\r\",\"                            btfdata = data.get('btfdata')\\r\",\"                            row['user_name'] = str(btfdata.get('user_name', ''))\\r\",\"                            row['project_name'] = str(btfdata.get('project_name', ''))\\r\",\"                            row['portfolio_name'] = str(btfdata.get('portfolio_name', ''))\\r\",\"                            row['workflow_name'] = str(btfdata.get('workflow_name', ''))\\r\",\"                            row['bot_name'] = str(btfdata.get('bot_name', ''))\\r\",\"                            row['schedule_id'] = str(btfdata.get('schedule_id', ''))\\r\",\"                            row['remote_server'] = str(btfdata.get('remote_server', ''))\\r\",\"                            row['execution_type'] = str(btfdata.get('execution_type', ''))\\r\",\"                            row['error_message'] = str(btfdata.get('error_message', ''))\\r\",\"\\r\",\"                if row['eid'] in eidList:\\r\",\"                    for item in responseList:\\r\",\"                        if item['eid'] == row['eid']:\\r\",\"                            responseList.remove(item)\\r\",\"                else:\\r\",\"                    eidList.append(row['eid'])\\r\",\"                \\r\",\"                mId = row['mId']\\r\",\"                if (mId.startswith('START Bot')):\\r\",\"                    row['SB_ets'] = data.get('ets')\\r\",\"                    row['SB_mId'] = mId.split(':')[1]\\r\",\"                    row['SB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'In Progress'\\r\",\"                elif (mId.startswith('END Bot')):\\r\",\"                    row['EB_ets'] = data.get('ets')\\r\",\"                    row['EB_mId'] = mId.split(':')[1]\\r\",\"                    row['EB_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Completed'\\r\",\"                elif (mId.startswith('Fail Event')):\\r\",\"                    row['FE_ets'] = data.get('ets')\\r\",\"                    row['FE_mId'] = mId.split(':')[1]\\r\",\"                    row['FE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Failed'\\r\",\"                elif (mId.startswith('Skipped Event')):\\r\",\"                    row['SE_ets'] = data.get('ets')\\r\",\"                    row['SE_mId'] = mId.split(':')[1]\\r\",\"                    row['SE_timestamp'] = datetime_str\\r\",\"                    row['Status'] = 'Skipped'\\r\",\"                else:\\r\",\"                    continue\\r\",\"\\r\",\"                responseList.append(row)\\r\",\"            except:\\r\",\"                error = {}\\r\",\"                error['eid'] = data['eid']\\r\",\"                error['message'] = str(data)\\r\",\"                responseList.append(error)\\r\",\"        \\r\",\"            \\r\",\"        if responseList is None or len(responseList) == 0:\\r\",\"            logging.info('No New events fetched')\\r\",\"            print('Completed')\\r\",\"            exit()\\r\",\"        spark = SparkSession.builder.appName('').config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(responseList)\\r\",\"        \\r\",\"        logging.info('returning dataframe')\\r\",\"        return df\\r\"]},{\"dataset\":{\"name\":\"elasticExtractor\",\"description\":\"\",\"schema\":{\"id\":810,\"name\":\"iamp\",\"schemavalue\":\"[]\",\"organization\":\"leo1311\"},\"type\":\"r\",\"attributes\":{\"get_count\":\"false\",\"Cacheable\":\"\",\"Query\":\"\",\"index\":\"bot-factory-telemetry-<jobParams:CurrentExecutionTime;%Y.%m.%d>\",\"QueryParams\":\"{}\",\"IncludeFields\":\"\",\"Limit\":\"3000\",\"ExcludeFields\":\"\",\"params\":\"{}\"},\"expStatus\":0,\"datasetType\":\"ELASTICSEARCH\",\"groups\":\"\",\"backingDataset\":\"\",\"alias\":\"elasticExtractor\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-25 14:08:31\",\"alias\":\"ElasticBtf\",\"id\":18,\"name\":\"ACMELSTC74624\",\"description\":\"ElasticBtf\",\"type\":\"ELASTICSEARCH\",\"connectionDetails\":\"{\\\"password\\\":\\\"\\\",\\\"port\\\":\\\"9200\\\",\\\"host\\\":\\\"vimppnz01-07\\\",\\\"AuthType\\\":\\\"NoAuth\\\",\\\"username\\\":\\\"\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2021-05-25T14:08:30.698+00:00\",\"category\":\"ELASTICSEARCH\"}},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]}]}","NULL","TelemetryLogging","NULL","TelemetryLogging","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-05-27 10:20:38.522000","false","Calculate response and resolution SLA for tickets","NULL","{\"elements\":[{\"id\":\"AJXpS\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"sla configuration\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from @projectname_sla_configuration\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"140\",\"position_y\":\"220\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"XyhZO\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"LOhNn\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"Tickets\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,priority ,createdDate FROM @projectname_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM @projectname_tickets_enriched WHERE resolution_SLA IS NOT NULL)\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"120\",\"position_y\":\"50\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"XyhZO\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"ePRxe\",\"alias\":\"Python  Script  Transformer\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"import logging\\r\",\"from datetime import datetime\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        dataset = dataset.withColumn('response_SLA' ,(unix_timestamp('createdDate') + col('responseSLA')).cast('timestamp'))\\r\",\"        dataset = dataset.withColumn('resolution_SLA',(unix_timestamp('createdDate') + col('resolutionSLA')).cast('timestamp'))\\r\",\"    \\r\",\"    \\r\",\"        dataset = dataset.withColumn('last_updated', lit(datetime.now()))\\r\",\"        dataset = dataset.withColumn('response_SLA_confidence', lit(100))\\r\",\"        dataset = dataset.withColumn('resolution_SLA_confidence', lit(100))\\r\",\"\\r\",\"        dataset = dataset.select('number','response_SLA','resolution_SLA','response_SLA_confidence','resolution_SLA_confidence','last_updated' )\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\"]},\"position_x\":\"629\",\"position_y\":\"94\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"LdAsF\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"XyhZO\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        dataset = dataset1.join(dataset2, 'priority')\\r\",\"        dataset = dataset.withColumn('responseSLA', (col('responseSLA') * 3600.0))\\r\",\"        dataset = dataset.withColumn('resolutionSLA', (col('resolutionSLA') * 3600.0))\\r\",\"        dataset = dataset.select('id','priority','number','responseSLA','resolutionSLA','createdDate')\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\",\"\\r\"]},{\"dataset\":{\"alias\":\"sla configuration\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from @projectname_sla_configuration\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"Tickets\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,priority ,createdDate FROM @projectname_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM @projectname_tickets_enriched WHERE resolution_SLA IS NOT NULL)\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"LdAsF\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"alias\":\"SLA Tickets\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, response_SLA, resolution_SLA, response_SLA_confidence, resolution_SLA_confidence, last_updated from @projectname_tickets_enriched\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_enriched\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"applySchema\":false},\"position_x\":\"836\",\"position_y\":\"156\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"ePRxe\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"script\":[\"import logging\\r\",\"from datetime import datetime\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        dataset = dataset.withColumn('response_SLA' ,(unix_timestamp('createdDate') + col('responseSLA')).cast('timestamp'))\\r\",\"        dataset = dataset.withColumn('resolution_SLA',(unix_timestamp('createdDate') + col('resolutionSLA')).cast('timestamp'))\\r\",\"    \\r\",\"    \\r\",\"        dataset = dataset.withColumn('last_updated', lit(datetime.now()))\\r\",\"        dataset = dataset.withColumn('response_SLA_confidence', lit(100))\\r\",\"        dataset = dataset.withColumn('resolution_SLA_confidence', lit(100))\\r\",\"\\r\",\"        dataset = dataset.select('number','response_SLA','resolution_SLA','response_SLA_confidence','resolution_SLA_confidence','last_updated' )\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\"]},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        dataset = dataset1.join(dataset2, 'priority')\\r\",\"        dataset = dataset.withColumn('responseSLA', (col('responseSLA') * 3600.0))\\r\",\"        dataset = dataset.withColumn('resolutionSLA', (col('resolutionSLA') * 3600.0))\\r\",\"        dataset = dataset.select('id','priority','number','responseSLA','resolutionSLA','createdDate')\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\",\"\\r\"]},{\"dataset\":{\"alias\":\"sla configuration\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from @projectname_sla_configuration\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"Tickets\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,priority ,createdDate FROM @projectname_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM @projectname_tickets_enriched WHERE resolution_SLA IS NOT NULL)\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"XyhZO\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        dataset = dataset1.join(dataset2, 'priority')\\r\",\"        dataset = dataset.withColumn('responseSLA', (col('responseSLA') * 3600.0))\\r\",\"        dataset = dataset.withColumn('resolutionSLA', (col('resolutionSLA') * 3600.0))\\r\",\"        dataset = dataset.select('id','priority','number','responseSLA','resolutionSLA','createdDate')\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\",\"\\r\"]},\"position_x\":\"370\",\"position_y\":\"140\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"AJXpS\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"LOhNn\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ePRxe\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"alias\":\"sla configuration\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from @projectname_sla_configuration\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"Tickets\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number,priority ,createdDate FROM @projectname_tickets WHERE createdDate IS NOT NULL AND\\n number NOT IN (SELECT NUMBER FROM @projectname_tickets_enriched WHERE resolution_SLA IS NOT NULL)\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]}]}","NULL","SLA Calculation","NULL","ACMSLCLC50990","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-05-28 15:46:38.547000","false","Predict Assignment group and assignee for the ticket","NULL","{\"elements\":[{\"id\":\"BRljv\",\"alias\":\"unassigned ticktes\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"unassigned ticktes\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, configurationItem FROM @projectname_tickets\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"110\",\"position_y\":\"160\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"BYjfm\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"thKAM\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"alias\":\"assigned tickets\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, predicted_assignment_group, predicted_assignee from @projectname_tickets_enriched\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_enriched\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"applySchema\":false},\"position_x\":\"890\",\"position_y\":\"130\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"WUowg\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        dataset = dataset.select('number',col('assignmentgroup').alias('predicted_assignment_group'),col('user').alias('predicted_assignee'))    \\r\",\"        return dataset\\r\",\"\\r\"]},{\"params\":\"\",\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1,dataset2,dataset3):\\r\",\"        dataset = dataset1.join(dataset3, ('configurationItem'))\\r\",\"        dataset.show()\\r\",\"        dataset = dataset.join(dataset2, ('assignmentgroup'))\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\"]},{\"dataset\":{\"alias\":\"unassigned ticktes\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, configurationItem FROM @projectname_tickets\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"shift roster\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT assignmentgroup , user , isAvailable FROM @projectname_shift_roster WHERE user <> '' AND assignmentgroup <> ''  AND assignmentgroup IS NOT NULL group by assignmentgroup\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"Assignment Config\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentgroup  FROM @projectname_assignment_config\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"BYjfm\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1,dataset2,dataset3):\\r\",\"        dataset = dataset1.join(dataset3, ('configurationItem'))\\r\",\"        dataset.show()\\r\",\"        dataset = dataset.join(dataset2, ('assignmentgroup'))\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\"]},\"position_x\":\"350\",\"position_y\":\"120\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"BRljv\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"WUowg\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"nuYSY\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset3\",\"position\":\"BottomCenter\",\"elementId\":\"qOpVw\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"alias\":\"unassigned ticktes\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, configurationItem FROM @projectname_tickets\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"shift roster\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT assignmentgroup , user , isAvailable FROM @projectname_shift_roster WHERE user <> '' AND assignmentgroup <> ''  AND assignmentgroup IS NOT NULL group by assignmentgroup\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"Assignment Config\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentgroup  FROM @projectname_assignment_config\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"nuYSY\",\"alias\":\"shift roster\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"shift roster\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT assignmentgroup , user , isAvailable FROM @projectname_shift_roster WHERE user <> '' AND assignmentgroup <> ''  AND assignmentgroup IS NOT NULL group by assignmentgroup\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"100\",\"position_y\":\"60\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"BYjfm\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"qOpVw\",\"alias\":\"assignment config\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"Assignment Config\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentgroup  FROM @projectname_assignment_config\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"100\",\"position_y\":\"310\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"BYjfm\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"WUowg\",\"alias\":\"Python  Script  Transformer\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        dataset = dataset.select('number',col('assignmentgroup').alias('predicted_assignment_group'),col('user').alias('predicted_assignee'))    \\r\",\"        return dataset\\r\",\"\\r\"]},\"position_x\":\"570\",\"position_y\":\"130\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"BYjfm\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"thKAM\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1,dataset2,dataset3):\\r\",\"        dataset = dataset1.join(dataset3, ('configurationItem'))\\r\",\"        dataset.show()\\r\",\"        dataset = dataset.join(dataset2, ('assignmentgroup'))\\r\",\"        dataset.show()\\r\",\"        return dataset\\r\"]},{\"dataset\":{\"alias\":\"unassigned ticktes\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT number, configurationItem FROM @projectname_tickets\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"shift roster\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT assignmentgroup , user , isAvailable FROM @projectname_shift_roster WHERE user <> '' AND assignmentgroup <> ''  AND assignmentgroup IS NOT NULL group by assignmentgroup\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"Assignment Config\",\"name\":\"\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 12:33:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentgroup  FROM @projectname_assignment_config\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"groups\":\"\",\"backingDataset\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]}]}","NULL","Auto Assignment","NULL","ACMATSGN51350","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-06-15 05:48:03.224000","false","Create clusters for resolution steps","NULL","{\"elements\":[{\"id\":\"UEeVc\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"40\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ffQic\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"HYOWn\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"CleanText\",\"alias\":\"Clusters\",\"description\":\"Clusters\",\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, cluster, group_by_field, last_updated from @projectname_resolution_clusters\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"overwrite\",\"params\":\"{}\",\"tableName\":\"@projectname_resolution_clusters\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"datasetType\":\"MYSQL\",\"groups\":\"\",\"backingDataset\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"}},\"applySchema\":false},\"position_x\":\"780\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"tgmni\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging\\r\",\"from datetime import datetime\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.sql.functions import *\\r\",\"import re\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self,model, dataset):\\r\",\"        dataset = dataset.withColumn('sound', soundex('clean_text'))\\r\",\"        dataset.show()\\r\",\"        soundDf = dataset.groupBy('group_by_field','sound').agg(collect_list('number').alias('numberList'), collect_list('clean_text').alias('text_list'))\\r\",\"       \\r\",\"        soundDf = soundDf.withColumn('numberListSize', size('numberList'))\\r\",\"        soundDf = soundDf.filter(soundDf['numberListSize']>=5)\\r\",\"        soundDf = soundDf.withColumn('cluster' , soundDf['text_list'].getItem(0)).drop('text_list')\\r\",\"        \\r\",\"        soundDf = soundDf.select('group_by_field', 'cluster', 'numberList','sound')\\r\",\"        soundDf = soundDf.withColumn('number',explode('numberList')).drop('numberList')\\r\",\"        soundDf = soundDf.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        soundDf = soundDf.select('number',col('cluster').alias('soundex_cluster'),'last_updated')\\r\",\"\\r\",\"        \\r\",\"        soundDf = soundDf.repartition(50)\\r\",\"        return soundDf\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from nltk.stem import WordNetLemmatizer\\r\",\"from datetime import datetime\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"      \\r\",\"        def lemmatize(tokens):\\r\",\"            wordnet_lemmatizer = WordNetLemmatizer()\\r\",\"            lemmatizedTokens = []\\r\",\"            for word in tokens:\\r\",\"                lemmatizedTokens.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\\r\",\"            return ' '.join(lemmatizedTokens)\\r\",\"        \\r\",\"        udfLemmetize = udf(lemmatize, StringType())\\r\",\"        dataset = dataset.withColumn('clean_text',udfLemmetize('clean_tokens'))\\r\",\"        dataset = dataset.select('number','clean_text', 'group_by_field')\\r\",\"        dataset = dataset.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"lemmatizedText\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('resolution_steps'))\\r\",\"        dataset = dataset.select('number','resolution_steps','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,resolution_steps,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"ffQic\",\"alias\":\"Clean Text\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('resolution_steps'))\\r\",\"        dataset = dataset.select('number','resolution_steps','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,resolution_steps,cleanText,group_by_field\"},\"position_x\":\"260\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"UEeVc\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"cMPGE\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\",\"outputCols\":\"text\"},\"context\":[{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"cMPGE\",\"alias\":\"Tokenizer\",\"name\":\"Tokenizer\",\"classname\":\"TokenizerTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},\"position_x\":\"460\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"ffQic\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"IgGTx\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('resolution_steps'))\\r\",\"        dataset = dataset.select('number','resolution_steps','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,resolution_steps,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"IgGTx\",\"alias\":\"Stop  Words  Remover\",\"name\":\"Stop  Words  Remover\",\"classname\":\"StopWordsRemoverTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},\"position_x\":\"800\",\"position_y\":\"40\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"cMPGE\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"fPAbp\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"inputCol\":\"text\",\"outputCol\":\"text\",\"stopWords\":\"text\"},\"context\":[{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('resolution_steps'))\\r\",\"        dataset = dataset.select('number','resolution_steps','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,resolution_steps,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"fPAbp\",\"alias\":\"Lemmetizer\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from nltk.stem import WordNetLemmatizer\\r\",\"from datetime import datetime\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"      \\r\",\"        def lemmatize(tokens):\\r\",\"            wordnet_lemmatizer = WordNetLemmatizer()\\r\",\"            lemmatizedTokens = []\\r\",\"            for word in tokens:\\r\",\"                lemmatizedTokens.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\\r\",\"            return ' '.join(lemmatizedTokens)\\r\",\"        \\r\",\"        udfLemmetize = udf(lemmatize, StringType())\\r\",\"        dataset = dataset.withColumn('clean_text',udfLemmetize('clean_tokens'))\\r\",\"        dataset = dataset.select('number','clean_text', 'group_by_field')\\r\",\"        dataset = dataset.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"lemmatizedText\"},\"position_x\":\"70\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"IgGTx\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"tgmni\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\",\"outputCols\":\"text\"},\"context\":[{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('resolution_steps'))\\r\",\"        dataset = dataset.select('number','resolution_steps','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,resolution_steps,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"tgmni\",\"alias\":\"Cluster\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging\\r\",\"from datetime import datetime\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.sql.functions import *\\r\",\"import re\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self,model, dataset):\\r\",\"        dataset = dataset.withColumn('sound', soundex('clean_text'))\\r\",\"        dataset.show()\\r\",\"        soundDf = dataset.groupBy('group_by_field','sound').agg(collect_list('number').alias('numberList'), collect_list('clean_text').alias('text_list'))\\r\",\"       \\r\",\"        soundDf = soundDf.withColumn('numberListSize', size('numberList'))\\r\",\"        soundDf = soundDf.filter(soundDf['numberListSize']>=5)\\r\",\"        soundDf = soundDf.withColumn('cluster' , soundDf['text_list'].getItem(0)).drop('text_list')\\r\",\"        \\r\",\"        soundDf = soundDf.select('group_by_field', 'cluster', 'numberList','sound')\\r\",\"        soundDf = soundDf.withColumn('number',explode('numberList')).drop('numberList')\\r\",\"        soundDf = soundDf.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        soundDf = soundDf.select('number',col('cluster').alias('soundex_cluster'),'last_updated')\\r\",\"\\r\",\"        \\r\",\"        soundDf = soundDf.repartition(50)\\r\",\"        return soundDf\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"570\",\"position_y\":\"200\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"fPAbp\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"HYOWn\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"script\":[\"import logging\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from nltk.stem import WordNetLemmatizer\\r\",\"from datetime import datetime\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"      \\r\",\"        def lemmatize(tokens):\\r\",\"            wordnet_lemmatizer = WordNetLemmatizer()\\r\",\"            lemmatizedTokens = []\\r\",\"            for word in tokens:\\r\",\"                lemmatizedTokens.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\\r\",\"            return ' '.join(lemmatizedTokens)\\r\",\"        \\r\",\"        udfLemmetize = udf(lemmatize, StringType())\\r\",\"        dataset = dataset.withColumn('clean_text',udfLemmetize('clean_tokens'))\\r\",\"        dataset = dataset.select('number','clean_text', 'group_by_field')\\r\",\"        dataset = dataset.withColumn('last_updated',lit(datetime.now()))\\r\",\"\\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"lemmatizedText\"},{\"inputCol\":\"tokens\",\"outputCol\":\"clean_tokens\",\"stopWords\":\"id\"},{\"inputCol\":\"cleanText\",\"outputCol\":\"tokens\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"import re\\r\",\"\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        \\r\",\"        def alphaNum(text):\\r\",\"            alphanumeric = ''\\r\",\"            for character in text:\\r\",\"                if character.isalnum():\\r\",\"                    alphanumeric += character\\r\",\"                else:\\r\",\"                    alphanumeric += ' '\\r\",\"            alphanumeric = re.sub(' +', ' ', alphanumeric)\\r\",\"            finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r\",\"            return ' '.join(finalTokens)\\r\",\"        udfAlpaNum = udf(alphaNum, StringType())\\r\",\"        dataset = dataset.withColumn('cleanText',udfAlpaNum('resolution_steps'))\\r\",\"        dataset = dataset.select('number','resolution_steps','group_by_field', 'cleanText' )\\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"number,resolution_steps,cleanText,group_by_field\"},{\"dataset\":{\"name\":\"Tickets\",\"alias\":\"Tickets\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":2,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encDFA7GGtjoS9sUXcj8Fa63LyB3Z3LRMzb\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://10.212.115.65:31692/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\"},\"schema\":null,\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select t.number, resolution_steps,  post_ranking_cluster as group_by_field from @projectname_tickets t join @projectname_tickets_enriched te on t.number = te.number where resolution_steps <> '' and   resolution_steps is not Null   and post_ranking_cluster is not null and post_ranking_cluster <> ''\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"datasetType\":\"MYSQL\",\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"groups\":\"\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]}]}","admin","Resolution Steps Clustering","2021-06-15 06:23:22","ACMRSLTN82640","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-07-02T06:29:20.701","false","Update Ticket in Leap","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"ACMLP_PD72695_Acme.py\"],\"arguments\":[{\"name\":\"LEAPDataSource\",\"value\":\"leo1311\",\"type\":\"Datasource\",\"alias\":\"leo1311\",\"index\":\"1\"},{\"name\":\"dataTable\",\"value\":\"@projectname_tickets\",\"type\":\"Text\",\"alias\":\"@projectname_tickets\",\"index\":\"2\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"number\\\":\\\"INC1625226229\\\",\\\"shortdescription\\\":\\\"Leap local update\\\",\\\"priority\\\":{\\\"displayValue\\\":\\\"5\\\",\\\"systemId\\\":\\\"5\\\"},\\\"state\\\":{\\\"displayValue\\\":\\\"New\\\",\\\"systemId\\\":\\\"1\\\"}, \\\"category\\\":{\\\"displayValue\\\":\\\"Inquiry / Help\\\",\\\"systemId\\\":\\\"inquiry\\\"},\\\"impact\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"}, \\\"urgency\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"},\\\"configurationitem\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"description\\\":null,\\\"assignmentgroup\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"assignedto\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"},\\\"sop\\\":\\\"Facebook Error\\\", \\\"resolutionStepsClusterName\\\":\\\"FacebookInsights\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"number\\\":\\\"INC1625226229\\\",\\\"shortdescription\\\":\\\"again test local update\\\",\\\"priority\\\":{\\\"displayValue\\\":\\\"5\\\",\\\"systemId\\\":\\\"5\\\"},\\\"state\\\":{\\\"displayValue\\\":\\\"New\\\",\\\"systemId\\\":\\\"1\\\"}, \\\"category\\\":{\\\"displayValue\\\":\\\"Inquiry / Help\\\",\\\"systemId\\\":\\\"inquiry\\\"},\\\"impact\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"}, \\\"urgency\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"},\\\"configurationitem\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"description\\\":null,\\\"assignmentgroup\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"assignedto\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"},\\\"sop\\\":\\\"Facebook Error\\\", \\\"resolutionStepsClusterName\\\":\\\"FacebookInsights\\\"}\",\"index\":\"3\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"alias\":\"Incident\",\"index\":\"4\"}],\"dataset\":[]}}]}","admin","Leap_Update_Ticket","2021-07-02T06:37:19.646041","ACMLP_PD72695","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-08-17T06:53:42.241","false","NULL","NULL","{\"elements\":[{\"id\":\"dBHhv\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"import pyspark.sql.functions as F\\r\",\"from pyspark.sql import SparkSession\\r\",\"from pyspark.sql.types import *\\r\",\"from datetime import datetime\\r\",\"from functools import partial\\r\",\"import re\\r\",\"import logging as logger\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        logger.error(dataset1)\\r\",\"        data2 = dataset2.collect()\\r\",\"        data1 = dataset1.collect()\\r\",\"        logger.error(data1)\\r\",\"        btmClassify = []\\r\",\"        for ticket in data2:\\r\",\"            for category in data1:\\r\",\"                matched = False\\r\",\"                if(category['selection']==False):\\r\",\"                    #logger.error('false')\\r\",\"                    #logger.error(eval(category['category_regex_json']))\\r\",\"                    for x in eval(category['category_regex_json']) :\\r\",\"                        casejson=eval(ticket['caseJson'])\\r\",\"                        #logger.error(x['inputfield'])\\r\",\"                        #logger.error(x['category_regex'])\\r\",\"                        matchData = casejson.get(x['inputfield'])\\r\",\"                        if(matchData is not None):\\r\",\"                            #logger.error('regex')\\r\",\"                            ismatch = re.match(x['category_regex'],matchData.lower())\\r\",\"                            if ismatch != None:\\r\",\"                                #logger.error('regex matched')\\r\",\"                                #logger.error(ticket['status'])\\r\",\"                                #logger.error(type(ticket))\\r\",\"                                \\r\",\"                                testdict = ticket.asDict()\\r\",\"                                #logger.error(type(testdict))\\r\",\"                                testdict['workflow_id'] = category['resolver_workflow_id']\\r\",\"                                testdict['status']='Classified'\\r\",\"                                #logger.error('test1')\\r\",\"                                btmClassify.append(testdict)\\r\",\"                                matched = True\\r\",\"                                #logger.error('test2')\\r\",\"                                #logger.error(testdict)\\r\",\"                                #btmClassify.append({'workflow_id':category['resolver_workflow_id'],'application_id':category['application_id'],'case_id':ticket['case_id']})\\r\",\"                                break;\\r\",\"                    if(matched!=True) :            \\r\",\"                        testdict = ticket.asDict()\\r\",\"                        testdict['workflow_id'] = None\\r\",\"                        testdict['status']='Extracted not Classified'\\r\",\"                        btmClassify.append(testdict)\\r\",\"                else:\\r\",\"                    logger.error('true')\\r\",\"        #logger.error(btmClassify)            \\r\",\"        spark = SparkSession.builder.config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(btmClassify)  \\r\",\"        logger.error('test123')\\r\",\"        \\r\",\"        #df.show()\\r\",\"        # logger.error(btmClassify)            \\r\",\"        return df\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"440\",\"position_y\":\"90\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"yqfEz\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"DoaFw\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"JEUuc\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"alias\":\"btm\",\"name\":\"9CHTQE0WHI\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_cases WHERE application_id=10000 AND STATUS='Extracted'\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"btm_category\",\"name\":\"22OWG5ICZC\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_category_details WHERE application_id=10000\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"yqfEz\",\"alias\":\"Python  Script  Transformer\",\"name\":\"Python  Script  Transformer\",\"classname\":\"PythonScriptTransformerConfig\",\"category\":\"TransformerConfig\",\"attributes\":{\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        print('python transformer')\\r\",\"        #dataset.show()    \\r\",\"        return dataset\\r\"]},\"position_x\":\"600\",\"position_y\":\"90\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"dBHhv\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"qNIZO\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"script\":\"textarea\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"import pyspark.sql.functions as F\\r\",\"from pyspark.sql import SparkSession\\r\",\"from pyspark.sql.types import *\\r\",\"from datetime import datetime\\r\",\"from functools import partial\\r\",\"import re\\r\",\"import logging as logger\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        logger.error(dataset1)\\r\",\"        data2 = dataset2.collect()\\r\",\"        data1 = dataset1.collect()\\r\",\"        logger.error(data1)\\r\",\"        btmClassify = []\\r\",\"        for ticket in data2:\\r\",\"            for category in data1:\\r\",\"                matched = False\\r\",\"                if(category['selection']==False):\\r\",\"                    #logger.error('false')\\r\",\"                    #logger.error(eval(category['category_regex_json']))\\r\",\"                    for x in eval(category['category_regex_json']) :\\r\",\"                        casejson=eval(ticket['caseJson'])\\r\",\"                        #logger.error(x['inputfield'])\\r\",\"                        #logger.error(x['category_regex'])\\r\",\"                        matchData = casejson.get(x['inputfield'])\\r\",\"                        if(matchData is not None):\\r\",\"                            #logger.error('regex')\\r\",\"                            ismatch = re.match(x['category_regex'],matchData.lower())\\r\",\"                            if ismatch != None:\\r\",\"                                #logger.error('regex matched')\\r\",\"                                #logger.error(ticket['status'])\\r\",\"                                #logger.error(type(ticket))\\r\",\"                                \\r\",\"                                testdict = ticket.asDict()\\r\",\"                                #logger.error(type(testdict))\\r\",\"                                testdict['workflow_id'] = category['resolver_workflow_id']\\r\",\"                                testdict['status']='Classified'\\r\",\"                                #logger.error('test1')\\r\",\"                                btmClassify.append(testdict)\\r\",\"                                matched = True\\r\",\"                                #logger.error('test2')\\r\",\"                                #logger.error(testdict)\\r\",\"                                #btmClassify.append({'workflow_id':category['resolver_workflow_id'],'application_id':category['application_id'],'case_id':ticket['case_id']})\\r\",\"                                break;\\r\",\"                    if(matched!=True) :            \\r\",\"                        testdict = ticket.asDict()\\r\",\"                        testdict['workflow_id'] = None\\r\",\"                        testdict['status']='Extracted not Classified'\\r\",\"                        btmClassify.append(testdict)\\r\",\"                else:\\r\",\"                    logger.error('true')\\r\",\"        #logger.error(btmClassify)            \\r\",\"        spark = SparkSession.builder.config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(btmClassify)  \\r\",\"        logger.error('test123')\\r\",\"        \\r\",\"        #df.show()\\r\",\"        # logger.error(btmClassify)            \\r\",\"        return df\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"alias\":\"btm\",\"name\":\"9CHTQE0WHI\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_cases WHERE application_id=10000 AND STATUS='Extracted'\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"btm_category\",\"name\":\"22OWG5ICZC\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_category_details WHERE application_id=10000\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"qNIZO\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"class CustomPythonClass():\\r\",\"  def __main__(self, model,dataset):\\r\",\"      #data = dataset.select('orderId','IsOrderDeliveredOnTime','prediction')\\r\",\"      #index =  2 #<index of StringIndexer used to convert IsOrderDeliveredOnTime to index>\\r\",\"      #indexer = model.stages[index]\\r\",\"      #labeler = IndexToString(inputCol='prediction', outputCol='prediction_label', labels=indexer.labels)\\r\",\"      #dataset = labeler.transform(data)\\r\",\"      #dataset = dataset.drop('prediction')\\r\",\"      #dataset.show()\\r\",\"      data=dataset.select([c for c in dataset.columns if c!='workflow_id'])\\r\",\"      data = data.withColumn('last_updated_dts',lit(current_timestamp()))\\r\",\"      data.show()\\r\",\"      data2=dataset.select('case_id','workflow_id','application_id')\\r\",\"      data2 = data2.dropna(subset=['workflow_id'])\\r\",\"      \\r\",\"      data2.show()\\r\",\"      return data,data2\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"780\",\"position_y\":\"70\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"yqfEz\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"GiUsS\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"source\",\"endpoint\":\"out2\",\"position\":\"BottomCenter\",\"elementId\":\"OdDRx\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        print('python transformer')\\r\",\"        #dataset.show()    \\r\",\"        return dataset\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"import pyspark.sql.functions as F\\r\",\"from pyspark.sql import SparkSession\\r\",\"from pyspark.sql.types import *\\r\",\"from datetime import datetime\\r\",\"from functools import partial\\r\",\"import re\\r\",\"import logging as logger\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        logger.error(dataset1)\\r\",\"        data2 = dataset2.collect()\\r\",\"        data1 = dataset1.collect()\\r\",\"        logger.error(data1)\\r\",\"        btmClassify = []\\r\",\"        for ticket in data2:\\r\",\"            for category in data1:\\r\",\"                matched = False\\r\",\"                if(category['selection']==False):\\r\",\"                    #logger.error('false')\\r\",\"                    #logger.error(eval(category['category_regex_json']))\\r\",\"                    for x in eval(category['category_regex_json']) :\\r\",\"                        casejson=eval(ticket['caseJson'])\\r\",\"                        #logger.error(x['inputfield'])\\r\",\"                        #logger.error(x['category_regex'])\\r\",\"                        matchData = casejson.get(x['inputfield'])\\r\",\"                        if(matchData is not None):\\r\",\"                            #logger.error('regex')\\r\",\"                            ismatch = re.match(x['category_regex'],matchData.lower())\\r\",\"                            if ismatch != None:\\r\",\"                                #logger.error('regex matched')\\r\",\"                                #logger.error(ticket['status'])\\r\",\"                                #logger.error(type(ticket))\\r\",\"                                \\r\",\"                                testdict = ticket.asDict()\\r\",\"                                #logger.error(type(testdict))\\r\",\"                                testdict['workflow_id'] = category['resolver_workflow_id']\\r\",\"                                testdict['status']='Classified'\\r\",\"                                #logger.error('test1')\\r\",\"                                btmClassify.append(testdict)\\r\",\"                                matched = True\\r\",\"                                #logger.error('test2')\\r\",\"                                #logger.error(testdict)\\r\",\"                                #btmClassify.append({'workflow_id':category['resolver_workflow_id'],'application_id':category['application_id'],'case_id':ticket['case_id']})\\r\",\"                                break;\\r\",\"                    if(matched!=True) :            \\r\",\"                        testdict = ticket.asDict()\\r\",\"                        testdict['workflow_id'] = None\\r\",\"                        testdict['status']='Extracted not Classified'\\r\",\"                        btmClassify.append(testdict)\\r\",\"                else:\\r\",\"                    logger.error('true')\\r\",\"        #logger.error(btmClassify)            \\r\",\"        spark = SparkSession.builder.config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(btmClassify)  \\r\",\"        logger.error('test123')\\r\",\"        \\r\",\"        #df.show()\\r\",\"        # logger.error(btmClassify)            \\r\",\"        return df\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"alias\":\"btm\",\"name\":\"9CHTQE0WHI\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_cases WHERE application_id=10000 AND STATUS='Extracted'\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"btm_category\",\"name\":\"22OWG5ICZC\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_category_details WHERE application_id=10000\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"DoaFw\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"btm\",\"name\":\"9CHTQE0WHI\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_cases WHERE application_id=10000 AND STATUS='Extracted'\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"150\",\"position_y\":\"30\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"dBHhv\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"JEUuc\",\"alias\":\"category\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"alias\":\"btm_category\",\"name\":\"22OWG5ICZC\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_category_details WHERE application_id=10000\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"150\",\"position_y\":\"170\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"dBHhv\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]},{\"id\":\"GiUsS\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"alias\":\"test\",\"name\":\"OCQSRTE6M0\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from btm_cases\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"btm_cases\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"applySchema\":false},\"position_x\":\"970\",\"position_y\":\"100\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"qNIZO\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"class CustomPythonClass():\\r\",\"  def __main__(self, model,dataset):\\r\",\"      #data = dataset.select('orderId','IsOrderDeliveredOnTime','prediction')\\r\",\"      #index =  2 #<index of StringIndexer used to convert IsOrderDeliveredOnTime to index>\\r\",\"      #indexer = model.stages[index]\\r\",\"      #labeler = IndexToString(inputCol='prediction', outputCol='prediction_label', labels=indexer.labels)\\r\",\"      #dataset = labeler.transform(data)\\r\",\"      #dataset = dataset.drop('prediction')\\r\",\"      #dataset.show()\\r\",\"      data=dataset.select([c for c in dataset.columns if c!='workflow_id'])\\r\",\"      data = data.withColumn('last_updated_dts',lit(current_timestamp()))\\r\",\"      data.show()\\r\",\"      data2=dataset.select('case_id','workflow_id','application_id')\\r\",\"      data2 = data2.dropna(subset=['workflow_id'])\\r\",\"      \\r\",\"      data2.show()\\r\",\"      return data,data2\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        print('python transformer')\\r\",\"        #dataset.show()    \\r\",\"        return dataset\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"import pyspark.sql.functions as F\\r\",\"from pyspark.sql import SparkSession\\r\",\"from pyspark.sql.types import *\\r\",\"from datetime import datetime\\r\",\"from functools import partial\\r\",\"import re\\r\",\"import logging as logger\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        logger.error(dataset1)\\r\",\"        data2 = dataset2.collect()\\r\",\"        data1 = dataset1.collect()\\r\",\"        logger.error(data1)\\r\",\"        btmClassify = []\\r\",\"        for ticket in data2:\\r\",\"            for category in data1:\\r\",\"                matched = False\\r\",\"                if(category['selection']==False):\\r\",\"                    #logger.error('false')\\r\",\"                    #logger.error(eval(category['category_regex_json']))\\r\",\"                    for x in eval(category['category_regex_json']) :\\r\",\"                        casejson=eval(ticket['caseJson'])\\r\",\"                        #logger.error(x['inputfield'])\\r\",\"                        #logger.error(x['category_regex'])\\r\",\"                        matchData = casejson.get(x['inputfield'])\\r\",\"                        if(matchData is not None):\\r\",\"                            #logger.error('regex')\\r\",\"                            ismatch = re.match(x['category_regex'],matchData.lower())\\r\",\"                            if ismatch != None:\\r\",\"                                #logger.error('regex matched')\\r\",\"                                #logger.error(ticket['status'])\\r\",\"                                #logger.error(type(ticket))\\r\",\"                                \\r\",\"                                testdict = ticket.asDict()\\r\",\"                                #logger.error(type(testdict))\\r\",\"                                testdict['workflow_id'] = category['resolver_workflow_id']\\r\",\"                                testdict['status']='Classified'\\r\",\"                                #logger.error('test1')\\r\",\"                                btmClassify.append(testdict)\\r\",\"                                matched = True\\r\",\"                                #logger.error('test2')\\r\",\"                                #logger.error(testdict)\\r\",\"                                #btmClassify.append({'workflow_id':category['resolver_workflow_id'],'application_id':category['application_id'],'case_id':ticket['case_id']})\\r\",\"                                break;\\r\",\"                    if(matched!=True) :            \\r\",\"                        testdict = ticket.asDict()\\r\",\"                        testdict['workflow_id'] = None\\r\",\"                        testdict['status']='Extracted not Classified'\\r\",\"                        btmClassify.append(testdict)\\r\",\"                else:\\r\",\"                    logger.error('true')\\r\",\"        #logger.error(btmClassify)            \\r\",\"        spark = SparkSession.builder.config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(btmClassify)  \\r\",\"        logger.error('test123')\\r\",\"        \\r\",\"        #df.show()\\r\",\"        # logger.error(btmClassify)            \\r\",\"        return df\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"alias\":\"btm\",\"name\":\"9CHTQE0WHI\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_cases WHERE application_id=10000 AND STATUS='Extracted'\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"btm_category\",\"name\":\"22OWG5ICZC\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_category_details WHERE application_id=10000\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"OdDRx\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"alias\":\"btm_classifier\",\"name\":\"0OVJMW70P6\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from btm_classifier\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"btm_classifier\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"applySchema\":false},\"position_x\":\"950\",\"position_y\":\"220\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"qNIZO\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"class CustomPythonClass():\\r\",\"  def __main__(self, model,dataset):\\r\",\"      #data = dataset.select('orderId','IsOrderDeliveredOnTime','prediction')\\r\",\"      #index =  2 #<index of StringIndexer used to convert IsOrderDeliveredOnTime to index>\\r\",\"      #indexer = model.stages[index]\\r\",\"      #labeler = IndexToString(inputCol='prediction', outputCol='prediction_label', labels=indexer.labels)\\r\",\"      #dataset = labeler.transform(data)\\r\",\"      #dataset = dataset.drop('prediction')\\r\",\"      #dataset.show()\\r\",\"      data=dataset.select([c for c in dataset.columns if c!='workflow_id'])\\r\",\"      data = data.withColumn('last_updated_dts',lit(current_timestamp()))\\r\",\"      data.show()\\r\",\"      data2=dataset.select('case_id','workflow_id','application_id')\\r\",\"      data2 = data2.dropna(subset=['workflow_id'])\\r\",\"      \\r\",\"      data2.show()\\r\",\"      return data,data2\\r\",\"\\r\",\"\\r\"]},{\"script\":[\"import logging\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"        print('python transformer')\\r\",\"        #dataset.show()    \\r\",\"        return dataset\\r\"]},{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"import pyspark.sql.functions as F\\r\",\"from pyspark.sql import SparkSession\\r\",\"from pyspark.sql.types import *\\r\",\"from datetime import datetime\\r\",\"from functools import partial\\r\",\"import re\\r\",\"import logging as logger\\r\",\"\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset1, dataset2):\\r\",\"        logger.error(dataset1)\\r\",\"        data2 = dataset2.collect()\\r\",\"        data1 = dataset1.collect()\\r\",\"        logger.error(data1)\\r\",\"        btmClassify = []\\r\",\"        for ticket in data2:\\r\",\"            for category in data1:\\r\",\"                matched = False\\r\",\"                if(category['selection']==False):\\r\",\"                    #logger.error('false')\\r\",\"                    #logger.error(eval(category['category_regex_json']))\\r\",\"                    for x in eval(category['category_regex_json']) :\\r\",\"                        casejson=eval(ticket['caseJson'])\\r\",\"                        #logger.error(x['inputfield'])\\r\",\"                        #logger.error(x['category_regex'])\\r\",\"                        matchData = casejson.get(x['inputfield'])\\r\",\"                        if(matchData is not None):\\r\",\"                            #logger.error('regex')\\r\",\"                            ismatch = re.match(x['category_regex'],matchData.lower())\\r\",\"                            if ismatch != None:\\r\",\"                                #logger.error('regex matched')\\r\",\"                                #logger.error(ticket['status'])\\r\",\"                                #logger.error(type(ticket))\\r\",\"                                \\r\",\"                                testdict = ticket.asDict()\\r\",\"                                #logger.error(type(testdict))\\r\",\"                                testdict['workflow_id'] = category['resolver_workflow_id']\\r\",\"                                testdict['status']='Classified'\\r\",\"                                #logger.error('test1')\\r\",\"                                btmClassify.append(testdict)\\r\",\"                                matched = True\\r\",\"                                #logger.error('test2')\\r\",\"                                #logger.error(testdict)\\r\",\"                                #btmClassify.append({'workflow_id':category['resolver_workflow_id'],'application_id':category['application_id'],'case_id':ticket['case_id']})\\r\",\"                                break;\\r\",\"                    if(matched!=True) :            \\r\",\"                        testdict = ticket.asDict()\\r\",\"                        testdict['workflow_id'] = None\\r\",\"                        testdict['status']='Extracted not Classified'\\r\",\"                        btmClassify.append(testdict)\\r\",\"                else:\\r\",\"                    logger.error('true')\\r\",\"        #logger.error(btmClassify)            \\r\",\"        spark = SparkSession.builder.config('spark.ui.showConsoleProgress', 'false').getOrCreate()\\r\",\"        df = spark.createDataFrame(btmClassify)  \\r\",\"        logger.error('test123')\\r\",\"        \\r\",\"        #df.show()\\r\",\"        # logger.error(btmClassify)            \\r\",\"        return df\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"dataset\":{\"alias\":\"btm\",\"name\":\"9CHTQE0WHI\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_cases WHERE application_id=10000 AND STATUS='Extracted'\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},{\"dataset\":{\"alias\":\"btm_category\",\"name\":\"22OWG5ICZC\",\"description\":\"\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-09-21 05:41:49\",\"alias\":\"leapmaster\",\"id\":24329,\"name\":\"LEOBTMTQ65374\",\"description\":\"btm\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encB7uhUO6Nv79TkK4YQBoi9DXGkTHVwgffvvR0\\\",\\\"userName\\\":\\\"bot-factory\\\",\\\"url\\\":\\\"jdbc:mysql://victadpst-21:3306/leapmaster\\\"}\",\"salt\":\"146MmrUVqn8lUbBxIQWmn2nypcODrYqQdR1AJa+cFMkpRoLbW19T7K6biN1dTSUhtirp1Zx9uig7KCG0EC7K4A==\",\"organization\":\"leo1311\",\"dshashcode\":\"7f78ce5cd189a027927f5afb41fe8b32a779fb9cfd72d246a249d1d93bd0f7f9\",\"activetime\":\"2021-09-21 05:41:49\",\"category\":\"SQL\"},\"schema\":\"\",\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * FROM btm_category_details WHERE application_id=10000\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"leo1311\"},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]}]}","admin","btm_classify","2021-09-29T05:43:16","LEOBTM_C22107","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2021-08-17T06:53:42.241","false","NULL","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"LEOBTM_S61592_leo1311.py\"],\"arguments\":[{\"name\":\"SnowDataSource\",\"value\":\"ACMSNWQB82627\",\"type\":\"Datasource\",\"alias\":\"SNOW\",\"index\":\"1\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"2\"},{\"name\":\"LEAPDataSource\",\"value\":\"LEOBTMTQ65374\",\"type\":\"Datasource\",\"alias\":\"leapmaster\",\"index\":\"3\"},{\"name\":\"params\",\"value\":\"state=1&sysparm_query=active=true^short_descriptionLIKEProcess_Invoice^ORshort_descriptionLIKEretail%20replenishment\",\"type\":\"Text\",\"alias\":\"state=1&sysparm_query=active=true^short_descriptionLIKEProcess_Invoice^ORshort_descriptionLIKEretail%20replenishment\",\"index\":\"4\"},{\"name\":\"dataTable\",\"value\":\"btm_cases\",\"type\":\"Text\",\"alias\":\"btm_cases\",\"index\":\"5\"},{\"name\":\"fullLoad\",\"value\":\"False\",\"type\":\"Text\",\"alias\":\"False\",\"index\":\"6\"},{\"name\":\"TimeDelta(hrs)\",\"value\":\"1\",\"type\":\"Text\",\"alias\":\"1\",\"index\":\"7\"},{\"name\":\"limit\",\"value\":\"1000\",\"type\":\"Text\",\"alias\":\"1000\",\"index\":\"8\"},{\"name\":\"offset\",\"value\":\"0\",\"type\":\"Text\",\"alias\":\"0\",\"index\":\"9\"},{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"10\"},{\"name\":\"snowTable\",\"value\":\"incident\",\"type\":\"Text\",\"alias\":\"incident\",\"index\":\"11\"},{\"name\":\"applicationId\",\"value\":\"10000\",\"type\":\"Text\",\"alias\":\"10000\",\"index\":\"12\"}],\"dataset\":[]}}]}","admin","btm_extractor","2021-09-29T05:43:16","LEOBTM_S61592","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-04-28T09:28:53.964","false","Get metadata from ticketing tool","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"SNOW_Get_Metadata_API_Acme.py\"],\"arguments\":[{\"name\":\"SnowDataSource\",\"value\":\"ACMSNWQB82627\",\"index\":\"1\",\"type\":\"Datasource\",\"alias\":\"SNOW\"},{\"name\":\"api\",\"value\":\"/api/now/table/\",\"index\":\"2\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true\",\"index\":\"4\"},{\"name\":\"LEAPDataSource\",\"value\":\"leo1311\",\"index\":\"8\",\"type\":\"Datasource\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"index\":\"14\"},{\"name\":\"dataTable\",\"value\":\"@projectname_tool_metadata\",\"type\":\"Text\",\"index\":\"6\"}]}}]}","NULL","SNOW_Get_Metadata_API","NULL","SNOW_Get_Metadata_API","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-06-07T06:02:41.157","false","Generic Network Extraction","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"ACMGNRCN39596_Acme.py\"],\"arguments\":[{\"name\":\"projectId\",\"value\":\"1\",\"type\":\"Text\",\"alias\":\"1\",\"index\":\"1\"},{\"name\":\"rawdatasource\",\"value\":\"leo1311\",\"type\":\"Datasource\",\"alias\":\"leo1311\",\"index\":\"2\"},{\"name\":\"additionalMapping\",\"value\":\"[\\\"txtPU\\\",\\\"txtDeskName\\\",\\\"dtLoad\\\"]\",\"type\":\"Text\",\"alias\":\"[\\\"txtPU\\\",\\\"txtDeskName\\\",\\\"dtLoad\\\"]\",\"index\":\"3\"},{\"name\":\"mandatoryParams\",\"value\":\"{\\\"id\\\":\\\"intClaimid\\\",\\\"node\\\":\\\"txtStatusName\\\",\\\"actiontimestamp\\\":\\\"dtDateActionTakenOn\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"id\\\":\\\"intClaimid\\\",\\\"node\\\":\\\"txtStatusName\\\",\\\"actiontimestamp\\\":\\\"dtDateActionTakenOn\\\"}\",\"index\":\"4\"},{\"name\":\"edgetablename\",\"value\":\"edge_table\",\"type\":\"Text\",\"alias\":\"edge_table\",\"index\":\"5\"},{\"name\":\"graphtablename\",\"value\":\"graph_table\",\"type\":\"Text\",\"alias\":\"graph_table\",\"index\":\"6\"},{\"name\":\"rawTableName\",\"value\":\"raw_process\",\"type\":\"Text\",\"alias\":\"raw_process\",\"index\":\"7\"},{\"name\":\"type\",\"value\":\"claims\",\"type\":\"Text\",\"alias\":\"claims\",\"index\":\"8\"}],\"dataset\":[]}}]}","admin","Generic Network Extraction","2021-06-07T06:05:33","ACMGNRCN39596","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-07-01T11:19:57.357","false","Create tickets in Leap","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"ACMLP_CR65949_Acme.py\"],\"arguments\":[{\"name\":\"LEAPDataSource\",\"value\":\"leo1311\",\"type\":\"Datasource\",\"alias\":\"leo1311\",\"index\":\"1\"},{\"name\":\"dataTable\",\"value\":\"@projectname_tickets\",\"type\":\"Text\",\"alias\":\"@projectname_tickets\",\"index\":\"2\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"shortdescription\\\":\\\"Leap Local create\\\",\\\"priority\\\":{\\\"displayValue\\\":\\\"5\\\",\\\"systemId\\\":\\\"5\\\"},\\\"state\\\":{\\\"displayValue\\\":\\\"New\\\",\\\"systemId\\\":\\\"1\\\"}, \\\"category\\\":{\\\"displayValue\\\":\\\"Inquiry / Help\\\",\\\"systemId\\\":\\\"inquiry\\\"},\\\"impact\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"}, \\\"urgency\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"},\\\"configurationitem\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"description\\\":null,\\\"assignmentgroup\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"assignedto\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"},\\\"sop\\\":\\\"Facebook Error\\\", \\\"resolutionStepsClusterName\\\":\\\"FacebookInsights\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"shortdescription\\\":\\\"test local create\\\",\\\"priority\\\":{\\\"displayValue\\\":\\\"5\\\",\\\"systemId\\\":\\\"5\\\"},\\\"state\\\":{\\\"displayValue\\\":\\\"New\\\",\\\"systemId\\\":\\\"1\\\"}, \\\"category\\\":{\\\"displayValue\\\":\\\"Inquiry / Help\\\",\\\"systemId\\\":\\\"inquiry\\\"},\\\"impact\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"}, \\\"urgency\\\":{\\\"displayValue\\\":\\\"3\\\",\\\"systemId\\\":\\\"3\\\"},\\\"configurationitem\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"description\\\":null,\\\"assignmentgroup\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"}, \\\"assignedto\\\":{\\\"displayValue\\\":null,\\\"systemId\\\":\\\"\\\"},\\\"sop\\\":\\\"Facebook Error\\\", \\\"resolutionStepsClusterName\\\":\\\"FacebookInsights\\\"}\",\"index\":\"3\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"alias\":\"Incident\",\"index\":\"4\"}],\"dataset\":[]}}]}","admin","Leap_Create_Ticket","2021-07-02T06:13:49","ACMLP_CR65949","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2021-05-03T05:32:52.837","false","Get Incidents, service requests, ChangeRequest, IncidentTask","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"SNOW_Get_API_Acme.py\"],\"arguments\":[{\"name\":\"SnowDataSource\",\"value\":\"ACMSNWQB82627\",\"type\":\"Datasource\",\"index\":\"1\",\"alias\":\"SNOW\"},{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"index\":\"2\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"index\":\"3\"},{\"name\":\"snowTable\",\"value\":\"incident\",\"type\":\"Text\",\"index\":\"5\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"index\":\"6\"},{\"name\":\"LEAPDataSource\",\"value\":\"leo1311\",\"type\":\"Datasource\",\"index\":\"7\"},{\"name\":\"fullLoad\",\"value\":\"False\",\"type\":\"Text\",\"index\":\"8\"},{\"name\":\"TimeDelta(hrs)\",\"value\":\"1\",\"type\":\"Text\",\"index\":\"9\"},{\"name\":\"dataTable\",\"value\":\"@projectname_tickets\",\"type\":\"Text\",\"index\":\"9\"},{\"name\":\"limit\",\"value\":\"1000\",\"type\":\"Text\",\"index\":\"10\"},{\"name\":\"offset\",\"value\":\"0\",\"type\":\"Text\",\"index\":\"11\"}],\"dataset\":[]}}]}","NULL","SNOW_GET_API","NULL","SNOW_GET_API","leo1311","NativeScript","NULL","NULL","pipeline","NULL"
"admin","2023-10-13T07:05:31.131","false","Create NGram clusters","NULL","{\"elements\":[{\"id\":\"VxGPt\",\"alias\":\"NGram Clusters\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"ngram_clusters\",\"requirements\":\"scikit-learn\",\"params\":[],\"script\":[\"\\rimport logging\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram_clusters(dataset):\\r    dataset=pd.DataFrame(dataset)\\r    txt1 =[]\\r    for index, row in dataset.iterrows():\\r        txt1.append(row['clean_text'])\\r    logging.info('Generating NGrams')\\r    # Getting trigrams \\r    vectorizer = CountVectorizer(ngram_range = (3,3))\\r    X1 = vectorizer.fit_transform(txt1) \\r    features = (vectorizer.get_feature_names_out())\\r    # Applying TFIDF\\r    vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r    X2 = vectorizer.fit_transform(txt1)\\r    scores = (X2.toarray())\\r    # Getting top ranking features\\r    logging.info('Getting Top 50 Grams')\\r    sums = X2.sum(axis = 0)\\r    data1 = []\\r    for col, term in enumerate(features):\\r        data1.append( (term, sums[0,col] ))\\r    ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r    words = (ranking.sort_values('rank', ascending = False))\\r    top50df = words.nlargest(50,'rank')\\r    top50 = words['term'].tolist()\\r    \\r    distinctText = list(set(txt1))\\r    count=0\\r    totalRecords = len(distinctText)\\r    logging.info('Mapping Text to Gram')\\r    logging.info('Total Unique Values: {0}'.format(totalRecords))\\r    ngramDict = {}\\r    for item in distinctText:\\r        matchingGrams = [gram for gram in top50 if gram in item]\\r        if len(matchingGrams) >0:\\r            ngramDict[item] = matchingGrams[0]\\r        count = count+1\\r        if count%1000 ==0:\\r            logging.info('{0} rows mapped'.format(count))\\r\\r    logging.info('Mapping Tickets')\\r    dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    dataset['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r    dataset = dataset[['number',  'ngram', 'last_updated']]\\r    dataset = dataset.to_dict('records')\\r    \\r    \\r    return dataset\\r\\r\\r\\r\\r\"]},\"position_x\":\"422\",\"position_y\":\"79\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"NUMXf\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"VzbvE\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"NUMXf\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"144\",\"position_y\":\"92\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"VxGPt\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error(\\r            'EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\n\"}},{\"id\":\"VzbvE\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"731\",\"position_y\":\"74\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"VxGPt\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"}}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}]}","admin","NGRam Clusters","2023-10-20T10:22:40","LEONGRMC63016","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"31\":{\"taskId\":\"a2483776-d8c6-473d-ae2e-cc9d29331a97\"}}"
"admin","2023-10-13T09:22:42.076","false","extract phases","NULL","{\"elements\":[{\"id\":\"yAZat\",\"alias\":\"PhraseExtraction\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"phrase_extraction\",\"requirements\":\"spacy, pytextrank\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def phrase_extraction(dataset):\\r\\\\n\",\"    nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"    nlp.add_pipe('textrank')\\r\\\\n\",\"    timenow = datetime.now()\\r\\\\n\",\"    totalRecords = len(dataset)\\r\\\\n\",\"    logging.info('Fetched {0} records'.format(totalRecords))\\r\\\\n\",\"    count =0\\r\\\\n\",\"    textPhraseMappings = {}\\r\\\\n\",\"    for row in dataset:\\r\\\\n\",\"        try:\\r\\\\n\",\"            text = row['clean_text']\\r\\\\n\",\"            if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                break\\r\\\\n\",\"            doc = nlp(text)\\r\\\\n\",\"            phrase =''\\r\\\\n\",\"            if len(doc._.phrases)>0:\\r\\\\n\",\"                for item in doc._.phrases:\\r\\\\n\",\"                    withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                    if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                        phrase = item.text\\r\\\\n\",\"                        break\\r\\\\n\",\"            if phrase != '':\\r\\\\n\",\"                row['extracted_phrase'] = phrase\\r\\\\n\",\"            else:\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"        except Exception as ex:\\r\\\\n\",\"            logging.info(ex)\\r\\\\n\",\"            row['extracted_phrase'] = text\\r\\\\n\",\"        row['last_updated'] = timenow\\r\\\\n\",\"        count = count+1\\r\\\\n\",\"        percentage = (count/totalRecords)*100 \\r\\\\n\",\"        if percentage%10 ==0:\\r\\\\n\",\"            logging.info('{0}% Completed'.format(percentage))\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},\"position_x\":\"506\",\"position_y\":\"103\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"OyQqY\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"QHeIl\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"QHeIl\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"271\",\"position_y\":\"80\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"yAZat\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error(\\r            'EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"OyQqY\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"720\",\"position_y\":\"108\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"yAZat\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"phrase_extraction\",\"requirements\":\"spacy, pytextrank\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def phrase_extraction(dataset):\\r\\\\n\",\"    nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"    nlp.add_pipe('textrank')\\r\\\\n\",\"    timenow = datetime.now()\\r\\\\n\",\"    totalRecords = len(dataset)\\r\\\\n\",\"    logging.info('Fetched {0} records'.format(totalRecords))\\r\\\\n\",\"    count =0\\r\\\\n\",\"    textPhraseMappings = {}\\r\\\\n\",\"    for row in dataset:\\r\\\\n\",\"        try:\\r\\\\n\",\"            text = row['clean_text']\\r\\\\n\",\"            if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                break\\r\\\\n\",\"            doc = nlp(text)\\r\\\\n\",\"            phrase =''\\r\\\\n\",\"            if len(doc._.phrases)>0:\\r\\\\n\",\"                for item in doc._.phrases:\\r\\\\n\",\"                    withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                    if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                        phrase = item.text\\r\\\\n\",\"                        break\\r\\\\n\",\"            if phrase != '':\\r\\\\n\",\"                row['extracted_phrase'] = phrase\\r\\\\n\",\"            else:\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"        except Exception as ex:\\r\\\\n\",\"            logging.info(ex)\\r\\\\n\",\"            row['extracted_phrase'] = text\\r\\\\n\",\"        row['last_updated'] = timenow\\r\\\\n\",\"        count = count+1\\r\\\\n\",\"        percentage = (count/totalRecords)*100 \\r\\\\n\",\"        if percentage%10 ==0:\\r\\\\n\",\"            logging.info('{0}% Completed'.format(percentage))\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}]}","admin","Phrase Extraction","2023-10-20T10:40:50","LEOPHRSX53223","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"42\":{\"taskId\":\"f8273742-ffd3-4c0e-b4e1-26d8bf69a2fe\"}}"
"admin","2023-10-13T10:01:27.525","false","Map key phrases to the extracted phrases","NULL","{\"elements\":[{\"id\":\"UDHiU\",\"alias\":\"Map Phrases\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    keywords = []\\r    for item in ease:\\r        word = item['Key_Word']\\r        if word not in keywords:\\r            keywords.append(word)\\r    phrases = []\\r    for item in dataset:\\r        phrase = item['extracted_phrase']\\r        if phrase not in phrases:\\r            phrases.append(phrase)\\r    \\r        \\r    def getSimilar(sentences, keywords):\\r        corpus = sentences + keywords\\r        keywordstartIndex = len(sentences)\\r        tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r        pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r        arr = pairwise_similarity.toarray()\\r        np.fill_diagonal(arr, np.nan)\\r        results = {}\\r        for s in sentences:\\r            input_idx = sentences.index(s)\\r            result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r            match = arr[input_idx][keywordstartIndex + result_idx]\\r            r = keywords[result_idx]\\r            if match > 0:\\r                results[s] = r + ':' + str(match)\\r            else:\\r                results[s] = 'NO MATCH:0'\\r        return results\\r\\r\\r    def getEASE(phrases,keywords):\\r        try:\\r            results = getSimilar(phrases, keywords)\\r            mappings = {}\\r            for pattern in results.keys():\\r                kw = results[pattern].split(':')[0]\\r                score = results[pattern].split(':')[-1]\\r                if kw != 'NO MATCH':\\r                    mappings[pattern] = {'keyword':kw, 'score':score}\\r            return mappings\\r        except Exception as ex:\\r            logger.warning(ex)\\r            return False\\r    \\r    totallen = len(phrases)\\r    logger.info('Total Records: {0}'.format(totallen))\\r    start = 0\\r    step = 1000\\r    results = []\\r    for i in range(start, totallen, step):\\r        stop = i + step\\r        if (stop > totallen):\\r            stop = totallen\\r        mappedphrases = getEASE(phrases[i:stop],keywords)\\r        results.append(mappedphrases)\\r        for row in dataset:\\r            phrase =row['extracted_phrase']\\r            if mappedphrases.get(phrase,'') !='':\\r                row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                row['mapped_phrase_confidennce'] =  mappedphrases[phrase]['score']\\r    print(dataset[0:5])\\r    return dataset\"]},\"position_x\":\"430\",\"position_y\":\"123\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"yjxYV\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"lDkee\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"lMdAF\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 10:51:55\",\"alias\":\"Extracted Phrases\",\"id\":862,\"name\":\"LEOEXTRC41937\",\"description\":\"Phrases extracted from tickets\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, extracted_phrase, mapped_phrase, mapped_phrase_confidennce, last_updated from @projectname_phrase_extraction\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"overwrite\",\"params\":\"{}\",\"tableName\":\"@projectname_phrase_extraction\",\"uniqueIdentifier\":\"\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":[],\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"lDkee\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"216\",\"position_y\":\"21\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"UDHiU\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"lMdAF\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 10:51:55\",\"alias\":\"Extracted Phrases\",\"id\":862,\"name\":\"LEOEXTRC41937\",\"description\":\"Phrases extracted from tickets\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, extracted_phrase, mapped_phrase, mapped_phrase_confidennce, last_updated from @projectname_phrase_extraction\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"overwrite\",\"params\":\"{}\",\"tableName\":\"@projectname_phrase_extraction\",\"uniqueIdentifier\":\"\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":[],\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"207\",\"position_y\":\"124\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"UDHiU\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"yjxYV\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 10:51:55\",\"alias\":\"Extracted Phrases\",\"id\":862,\"name\":\"LEOEXTRC41937\",\"description\":\"Phrases extracted from tickets\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select number, extracted_phrase, mapped_phrase, mapped_phrase_confidence, last_updated from @projectname_phrase_extraction\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"overwrite\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_phrase_extraction\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"696\",\"position_y\":\"124\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"UDHiU\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    keywords = []\\r    for item in ease:\\r        word = item['Key_Word']\\r        if word not in keywords:\\r            keywords.append(word)\\r    phrases = []\\r    for item in dataset:\\r        phrase = item['extracted_phrase']\\r        if phrase not in phrases:\\r            phrases.append(phrase)\\r    \\r        \\r    def getSimilar(sentences, keywords):\\r        corpus = sentences + keywords\\r        keywordstartIndex = len(sentences)\\r        tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r        pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r        arr = pairwise_similarity.toarray()\\r        np.fill_diagonal(arr, np.nan)\\r        results = {}\\r        for s in sentences:\\r            input_idx = sentences.index(s)\\r            result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r            match = arr[input_idx][keywordstartIndex + result_idx]\\r            r = keywords[result_idx]\\r            if match > 0:\\r                results[s] = r + ':' + str(match)\\r            else:\\r                results[s] = 'NO MATCH:0'\\r        return results\\r\\r\\r    def getEASE(phrases,keywords):\\r        try:\\r            results = getSimilar(phrases, keywords)\\r            mappings = {}\\r            for pattern in results.keys():\\r                kw = results[pattern].split(':')[0]\\r                score = results[pattern].split(':')[-1]\\r                if kw != 'NO MATCH':\\r                    mappings[pattern] = {'keyword':kw, 'score':score}\\r            return mappings\\r        except Exception as ex:\\r            logger.warning(ex)\\r            return False\\r    \\r    totallen = len(phrases)\\r    logger.info('Total Records: {0}'.format(totallen))\\r    start = 0\\r    step = 1000\\r    results = []\\r    for i in range(start, totallen, step):\\r        stop = i + step\\r        if (stop > totallen):\\r            stop = totallen\\r        mappedphrases = getEASE(phrases[i:stop],keywords)\\r        results.append(mappedphrases)\\r        for row in dataset:\\r            phrase =row['extracted_phrase']\\r            if mappedphrases.get(phrase,'') !='':\\r                row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                row['mapped_phrase_confidennce'] =  mappedphrases[phrase]['score']\\r    print(dataset[0:5])\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 10:51:55\",\"alias\":\"Extracted Phrases\",\"id\":862,\"name\":\"LEOEXTRC41937\",\"description\":\"Phrases extracted from tickets\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select number, extracted_phrase, mapped_phrase, mapped_phrase_confidennce, last_updated from @projectname_phrase_extraction\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"overwrite\",\"params\":\"{}\",\"tableName\":\"@projectname_phrase_extraction\",\"uniqueIdentifier\":\"\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":[],\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Key Phrase Mapping","2023-12-19T12:08:06","LEOKYPHR67439","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"12\":{\"taskId\":\"4e2b24b3-c708-469b-ba4d-c37bc7661f37\"}}"
"admin","2023-10-13T11:20:17.503","false","Assign prioritized cluster to a ticket","NULL","{\"elements\":[{\"id\":\"asRvM\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rfrom datetime import datetime\\rdef PythonScript(dataset):\\r    print(\\\"Dataset\\\")\\r    print(dataset[0:5])\\r    results = []\\r    def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r            cluster = ''\\r            if tags is not None and tags != '':\\r                cluster = tags\\r            elif ngram is not None and ngram != '':\\r                cluster = ngram\\r            elif soundex_cluster is not None and soundex_cluster != '':\\r                cluster = soundex_cluster\\r            elif mapped_phrase is not None and mapped_phrase != '':\\r                cluster = mapped_phrase\\r            elif lda_cluster is not None and lda_cluster != '':\\r                cluster = lda_cluster\\r            return cluster\\r    \\r    for row in dataset:\\r        data = {}\\r        data['number'] = row[\\\"number\\\"]\\r        data[\\\"post_ranking_cluster\\\"]= getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r        data['last_updated'] = datetime.now()\\r    \\r        results.append(data)\\r    print(results[0:5])\\r    \\r    return results\\r\\r\\r\"]},\"position_x\":\"483\",\"position_y\":\"206\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"KfpKU\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"bjZBd\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 11:58:29\",\"alias\":\"Clusters\",\"id\":884,\"name\":\"LEOCLSTR80245\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT cast(t.number as char) AS number, tags AS tags,ngram,soundex_cluster,lda_cluster,mapped_phrase FROM @projectname_tickets t LEFT JOIN @projectname_tickets_enriched te ON t.number=cast(te.number as char)\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"KfpKU\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 08:43:25\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":null,\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from @projectname_tickets_enriched\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets_enriched\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"861\",\"position_y\":\"238\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"asRvM\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rfrom datetime import datetime\\rdef PythonScript(dataset):\\r    print(\\\"Dataset\\\")\\r    print(dataset[0:5])\\r    results = []\\r    def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r            cluster = ''\\r            if tags is not None and tags != '':\\r                cluster = tags\\r            elif ngram is not None and ngram != '':\\r                cluster = ngram\\r            elif soundex_cluster is not None and soundex_cluster != '':\\r                cluster = soundex_cluster\\r            elif mapped_phrase is not None and mapped_phrase != '':\\r                cluster = mapped_phrase\\r            elif lda_cluster is not None and lda_cluster != '':\\r                cluster = lda_cluster\\r            return cluster\\r    \\r    for row in dataset:\\r        data = {}\\r        data['number'] = row[\\\"number\\\"]\\r        data[\\\"post_ranking_cluster\\\"]= getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r        data['last_updated'] = datetime.now()\\r    \\r        results.append(data)\\r    print(results[0:5])\\r    \\r    return results\\r\\r\\r\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 11:58:29\",\"alias\":\"Clusters\",\"id\":884,\"name\":\"LEOCLSTR80245\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT cast(t.number as char) AS number, tags AS tags,ngram,soundex_cluster,lda_cluster,mapped_phrase FROM @projectname_tickets t LEFT JOIN @projectname_tickets_enriched te ON t.number=cast(te.number as char)\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"bjZBd\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 11:58:29\",\"alias\":\"Clusters\",\"id\":884,\"name\":\"LEOCLSTR80245\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT cast(t.number as char) AS number, tags AS tags,ngram,soundex_cluster,lda_cluster,mapped_phrase FROM @projectname_tickets t LEFT JOIN @projectname_tickets_enriched te ON t.number=cast(te.number as char)\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"181\",\"position_y\":\"190\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"asRvM\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Cluster Prioritization","2023-12-19T12:05:18","LEOCLSTR99161","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"31\":{\"taskId\":\"3caee7c7-cfd7-4d55-90c4-296eec71451e\"}}"
"admin","2023-10-13T11:22:37.835","false","Clustering LDA","NULL","{\"elements\":[{\"id\":\"CUnsR\",\"alias\":\"Pre-Process Data\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"filterdata\",\"requirements\":\"\",\"params\":[{\"name\":\"output_col\",\"value\":\"filterddf\",\"type\":\"Text\",\"alias\":\"filterddf\",\"index\":\"1\"}],\"script\":[\"def filterdata(dataset, output_col_param = ''):\\r\\\\n\",\"    dataset = pd.DataFrame(dataset)\\r\\\\n\",\"    dataset['cluster_Type'] = 'LDA'\\r\\\\n\",\"    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r\\\\n\",\"    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r\\\\n\",\"    filtereddf = dataset['group_by_field'].isin(ciList)\\r\\\\n\",\"    dataset[output_col_param] = filtereddf\\r\\\\n\",\"    dataset = dataset[dataset[output_col_param] == True]\\r\\\\n\",\"    return dataset\"]},\"position_x\":\"218\",\"position_y\":\"46\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"IOdfo\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"yzNaQ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"IOdfo\",\"alias\":\"Tokenize\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"tokenize\",\"requirements\":\"nltk\",\"params\":[{\"name\":\"input_col\",\"value\":\"clean_text\",\"type\":\"Text\",\"alias\":\"clean_text\",\"index\":\"1\"},{\"name\":\"output_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"}],\"script\":[\"def tokenize(dataset, input_col_param='', output_col_param = ''):\\r\\\\n\",\"    from nltk.tokenize import word_tokenize\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns tokenized version of text\\r\\\\n\",\"    '''\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"    dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    return dataset\"]},\"position_x\":\"422\",\"position_y\":\"39\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"CUnsR\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"xKakm\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"filterdata\",\"requirements\":\"\",\"params\":[{\"name\":\"output_col\",\"value\":\"filterddf\",\"type\":\"Text\",\"alias\":\"filterddf\",\"index\":\"1\"}],\"script\":[\"def filterdata(dataset, output_col_param = ''):\\r\\\\n\",\"    dataset = pd.DataFrame(dataset)\\r\\\\n\",\"    dataset['cluster_Type'] = 'LDA'\\r\\\\n\",\"    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r\\\\n\",\"    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r\\\\n\",\"    filtereddf = dataset['group_by_field'].isin(ciList)\\r\\\\n\",\"    dataset[output_col_param] = filtereddf\\r\\\\n\",\"    dataset = dataset[dataset[output_col_param] == True]\\r\\\\n\",\"    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"xKakm\",\"alias\":\"StopWordsRemover\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"remove_stop_words\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stops_words\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"},{\"name\":\"input_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"},{\"name\":\"output_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"3\"}],\"script\":[\"def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r\\\\n\",\"    from nltk.corpus import stopwords\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns text without stop words\\r\\\\n\",\"    '''\\r\\\\n\",\"    def removesw(input):\\r\\\\n\",\"        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r\\\\n\",\"\\r\\\\n\",\"    custom_stops_words = custom_stops_words_param.split(',')\\r\\\\n\",\"    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r\\\\n\",\"    return dataset\"]},\"position_x\":\"643\",\"position_y\":\"39\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"IOdfo\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"jWotk\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"tokenize\",\"requirements\":\"nltk\",\"params\":[{\"name\":\"input_col\",\"value\":\"clean_text\",\"type\":\"Text\",\"alias\":\"clean_text\",\"index\":\"1\"},{\"name\":\"output_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"}],\"script\":[\"def tokenize(dataset, input_col_param='', output_col_param = ''):\\r\\\\n\",\"    from nltk.tokenize import word_tokenize\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns tokenized version of text\\r\\\\n\",\"    '''\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"    dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"filterdata\",\"requirements\":\"\",\"params\":[{\"name\":\"output_col\",\"value\":\"filterddf\",\"type\":\"Text\",\"alias\":\"filterddf\",\"index\":\"1\"}],\"script\":[\"def filterdata(dataset, output_col_param = ''):\\r\\\\n\",\"    dataset = pd.DataFrame(dataset)\\r\\\\n\",\"    dataset['cluster_Type'] = 'LDA'\\r\\\\n\",\"    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r\\\\n\",\"    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r\\\\n\",\"    filtereddf = dataset['group_by_field'].isin(ciList)\\r\\\\n\",\"    dataset[output_col_param] = filtereddf\\r\\\\n\",\"    dataset = dataset[dataset[output_col_param] == True]\\r\\\\n\",\"    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"jWotk\",\"alias\":\"CountVectorize\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"countvectorize\",\"requirements\":\"\",\"params\":[{\"name\":\"input_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"1\"}],\"script\":[\"def countvectorize(dataset, input_col_param=''):\\r\\\\n\",\"    def cv(dataset, input_col_param):\\r\\\\n\",\"        from sklearn.feature_extraction.text import CountVectorizer\\r\\\\n\",\"        count_vectorizer = CountVectorizer(stop_words='english')\\r\\\\n\",\"        count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r\\\\n\",\"        words = count_vectorizer.get_feature_names_out()\\r\\\\n\",\"        return {'data': count_data, 'words': words}\\r\\\\n\",\"    result = {}\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        result[name] = cv(dataset, input_col_param)\\r\\\\n\",\"        result[name]['number'] = dataset['number'].to_list()\\r\\\\n\",\"    return result\"]},\"position_x\":\"269\",\"position_y\":\"199\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"xKakm\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"KllMA\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"remove_stop_words\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stops_words\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"},{\"name\":\"input_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"},{\"name\":\"output_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"3\"}],\"script\":[\"def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r\\\\n\",\"    from nltk.corpus import stopwords\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns text without stop words\\r\\\\n\",\"    '''\\r\\\\n\",\"    def removesw(input):\\r\\\\n\",\"        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r\\\\n\",\"\\r\\\\n\",\"    custom_stops_words = custom_stops_words_param.split(',')\\r\\\\n\",\"    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"nltk\",\"params\":[{\"name\":\"input_col\",\"value\":\"clean_text\",\"type\":\"Text\",\"alias\":\"clean_text\",\"index\":\"1\"},{\"name\":\"output_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"}],\"script\":[\"def tokenize(dataset, input_col_param='', output_col_param = ''):\\r\\\\n\",\"    from nltk.tokenize import word_tokenize\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns tokenized version of text\\r\\\\n\",\"    '''\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"    dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"filterdata\",\"requirements\":\"\",\"params\":[{\"name\":\"output_col\",\"value\":\"filterddf\",\"type\":\"Text\",\"alias\":\"filterddf\",\"index\":\"1\"}],\"script\":[\"def filterdata(dataset, output_col_param = ''):\\r\\\\n\",\"    dataset = pd.DataFrame(dataset)\\r\\\\n\",\"    dataset['cluster_Type'] = 'LDA'\\r\\\\n\",\"    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r\\\\n\",\"    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r\\\\n\",\"    filtereddf = dataset['group_by_field'].isin(ciList)\\r\\\\n\",\"    dataset[output_col_param] = filtereddf\\r\\\\n\",\"    dataset = dataset[dataset[output_col_param] == True]\\r\\\\n\",\"    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"KllMA\",\"alias\":\"LDA\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"lda_cluster\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\r\\\\n\",\"import datetime\\r\\\\n\",\"def lda_cluster(dataset, clustercount_param=30, uniqueidcolumn_param=''):\\r\\\\n\",\"    print(dataset)\\r\\\\n\",\"    def LDARunner(dataset, clustercount_param):\\r\\\\n\",\"        from sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\\\n\",\"        import numpy as np\\r\\\\n\",\"        number_topics =  clustercount_param\\r\\\\n\",\"        count_data, words, number = dataset['data'], dataset['words'], dataset['number']\\r\\\\n\",\"        \\r\\\\n\",\"        lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\\\n\",\"        lda.fit(count_data)\\r\\\\n\",\"        documents = lda.transform(count_data)\\r\\\\n\",\"        \\r\\\\n\",\"\\r\\\\n\",\"        argmax_values = np.argmax(documents, axis=1)\\r\\\\n\",\"        newdocuments = pd.DataFrame({'cluster_Id':argmax_values, 'number':number})\\r\\\\n\",\"\\r\\\\n\",\"        \\r\\\\n\",\"        def wordsWithWeights(termIndices, termWeights, index):\\r\\\\n\",\"            terms = [words[i] for i in termIndices]\\r\\\\n\",\"            topic = [index]*len(terms)\\r\\\\n\",\"            return list(zip(terms, topic, termIndices, termWeights))\\r\\\\n\",\"        \\r\\\\n\",\"        topics = [ wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) for index, topic in enumerate(lda.components_)]\\r\\\\n\",\"        topicWords = []\\r\\\\n\",\"        \\r\\\\n\",\"        for topic in topics:\\r\\\\n\",\"            topicWords.extend(topic)\\r\\\\n\",\"        finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\\\n\",\"        finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\\\n\",\"        finalTopic.columns = ['topic', 'word', 'weight']\\r\\\\n\",\"        \\r\\\\n\",\"        # newdocuments = newdocuments.merge(finalTopic, left_on='cluster_Id', right_index=True).drop(columns='cluster_Id')\\r\\\\n\",\"        \\r\\\\n\",\"        topicwordsdf = finalTopic.groupby('topic')['word'].apply(list).reset_index()\\r\\\\n\",\"        topicwordsdf.rename(columns={'word': 'cluster_Name'}, inplace=True)\\r\\\\n\",\"        topicwordsdf = topicwordsdf[['topic', 'cluster_Name']]\\r\\\\n\",\"        newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\\\n\",\"        newdocuments = newdocuments.drop(columns='topic')\\r\\\\n\",\"        return newdocuments, finalTopic\\r\\\\n\",\"    \\r\\\\n\",\"    newdocuments_result = {}\\r\\\\n\",\"    finalTopic_result = {}\\r\\\\n\",\"    for name, datasets in dataset.items():\\r\\\\n\",\"        newdocuments, finalTopic = LDARunner(datasets, clustercount_param)\\r\\\\n\",\"        finalTopic['group_by_field'] = name\\r\\\\n\",\"        newdocuments['last_updated'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\\\n\",\"        newdocuments_result[name] = newdocuments\\r\\\\n\",\"        finalTopic_result[name] = finalTopic\\r\\\\n\",\"\\r\\\\n\",\"    \\r\\\\n\",\"    resultsdf = pd.concat(newdocuments_result.values(), ignore_index=True)\\r\\\\n\",\"    topicsdf = pd.concat(finalTopic_result.values(), ignore_index=True)\\r\\\\n\",\"    topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({'word': list, 'weight': list}).reset_index()\\r\\\\n\",\"    topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"    topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(list(map(lambda y: str(y), x))))\\r\\\\n\",\"    topicsdf['alias'] = topicsdf['words']\\r\\\\n\",\"    topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"    resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"    resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r\\\\n\",\"    # resultsdf['number'] = uniqueidcolumn_param\\r\\\\n\",\"    resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']] \\r\\\\n\",\"   \\r\\\\n\",\"    return  resultsdf.to_dict('records')\"]},\"position_x\":\"508\",\"position_y\":\"220\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"jWotk\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"NOpBq\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"countvectorize\",\"requirements\":\"\",\"params\":[{\"name\":\"input_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"1\"}],\"script\":[\"def countvectorize(dataset, input_col_param=''):\\r\\\\n\",\"    def cv(dataset, input_col_param):\\r\\\\n\",\"        from sklearn.feature_extraction.text import CountVectorizer\\r\\\\n\",\"        count_vectorizer = CountVectorizer(stop_words='english')\\r\\\\n\",\"        count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r\\\\n\",\"        words = count_vectorizer.get_feature_names_out()\\r\\\\n\",\"        return {'data': count_data, 'words': words}\\r\\\\n\",\"    result = {}\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        result[name] = cv(dataset, input_col_param)\\r\\\\n\",\"        result[name]['number'] = dataset['number'].to_list()\\r\\\\n\",\"    return result\"]},{\"FunctionName\":\"remove_stop_words\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stops_words\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"},{\"name\":\"input_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"},{\"name\":\"output_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"3\"}],\"script\":[\"def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r\\\\n\",\"    from nltk.corpus import stopwords\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns text without stop words\\r\\\\n\",\"    '''\\r\\\\n\",\"    def removesw(input):\\r\\\\n\",\"        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r\\\\n\",\"\\r\\\\n\",\"    custom_stops_words = custom_stops_words_param.split(',')\\r\\\\n\",\"    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"nltk\",\"params\":[{\"name\":\"input_col\",\"value\":\"clean_text\",\"type\":\"Text\",\"alias\":\"clean_text\",\"index\":\"1\"},{\"name\":\"output_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"}],\"script\":[\"def tokenize(dataset, input_col_param='', output_col_param = ''):\\r\\\\n\",\"    from nltk.tokenize import word_tokenize\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns tokenized version of text\\r\\\\n\",\"    '''\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"    dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"filterdata\",\"requirements\":\"\",\"params\":[{\"name\":\"output_col\",\"value\":\"filterddf\",\"type\":\"Text\",\"alias\":\"filterddf\",\"index\":\"1\"}],\"script\":[\"def filterdata(dataset, output_col_param = ''):\\r\\\\n\",\"    dataset = pd.DataFrame(dataset)\\r\\\\n\",\"    dataset['cluster_Type'] = 'LDA'\\r\\\\n\",\"    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r\\\\n\",\"    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r\\\\n\",\"    filtereddf = dataset['group_by_field'].isin(ciList)\\r\\\\n\",\"    dataset[output_col_param] = filtereddf\\r\\\\n\",\"    dataset = dataset[dataset[output_col_param] == True]\\r\\\\n\",\"    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"yzNaQ\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"2\",\"position_y\":\"94\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"CUnsR\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"NOpBq\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"750\",\"position_y\":\"206\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"KllMA\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"lda_cluster\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\r\\\\n\",\"import datetime\\r\\\\n\",\"def lda_cluster(dataset, clustercount_param=30, uniqueidcolumn_param=''):\\r\\\\n\",\"    print(dataset)\\r\\\\n\",\"    def LDARunner(dataset, clustercount_param):\\r\\\\n\",\"        from sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\\\n\",\"        import numpy as np\\r\\\\n\",\"        number_topics =  clustercount_param\\r\\\\n\",\"        count_data, words, number = dataset['data'], dataset['words'], dataset['number']\\r\\\\n\",\"        \\r\\\\n\",\"        lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\\\n\",\"        lda.fit(count_data)\\r\\\\n\",\"        documents = lda.transform(count_data)\\r\\\\n\",\"        \\r\\\\n\",\"\\r\\\\n\",\"        argmax_values = np.argmax(documents, axis=1)\\r\\\\n\",\"        newdocuments = pd.DataFrame({'cluster_Id':argmax_values, 'number':number})\\r\\\\n\",\"\\r\\\\n\",\"        \\r\\\\n\",\"        def wordsWithWeights(termIndices, termWeights, index):\\r\\\\n\",\"            terms = [words[i] for i in termIndices]\\r\\\\n\",\"            topic = [index]*len(terms)\\r\\\\n\",\"            return list(zip(terms, topic, termIndices, termWeights))\\r\\\\n\",\"        \\r\\\\n\",\"        topics = [ wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) for index, topic in enumerate(lda.components_)]\\r\\\\n\",\"        topicWords = []\\r\\\\n\",\"        \\r\\\\n\",\"        for topic in topics:\\r\\\\n\",\"            topicWords.extend(topic)\\r\\\\n\",\"        finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\\\n\",\"        finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\\\n\",\"        finalTopic.columns = ['topic', 'word', 'weight']\\r\\\\n\",\"        \\r\\\\n\",\"        # newdocuments = newdocuments.merge(finalTopic, left_on='cluster_Id', right_index=True).drop(columns='cluster_Id')\\r\\\\n\",\"        \\r\\\\n\",\"        topicwordsdf = finalTopic.groupby('topic')['word'].apply(list).reset_index()\\r\\\\n\",\"        topicwordsdf.rename(columns={'word': 'cluster_Name'}, inplace=True)\\r\\\\n\",\"        topicwordsdf = topicwordsdf[['topic', 'cluster_Name']]\\r\\\\n\",\"        newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\\\n\",\"        newdocuments = newdocuments.drop(columns='topic')\\r\\\\n\",\"        return newdocuments, finalTopic\\r\\\\n\",\"    \\r\\\\n\",\"    newdocuments_result = {}\\r\\\\n\",\"    finalTopic_result = {}\\r\\\\n\",\"    for name, datasets in dataset.items():\\r\\\\n\",\"        newdocuments, finalTopic = LDARunner(datasets, clustercount_param)\\r\\\\n\",\"        finalTopic['group_by_field'] = name\\r\\\\n\",\"        newdocuments['last_updated'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\\\n\",\"        newdocuments_result[name] = newdocuments\\r\\\\n\",\"        finalTopic_result[name] = finalTopic\\r\\\\n\",\"\\r\\\\n\",\"    \\r\\\\n\",\"    resultsdf = pd.concat(newdocuments_result.values(), ignore_index=True)\\r\\\\n\",\"    topicsdf = pd.concat(finalTopic_result.values(), ignore_index=True)\\r\\\\n\",\"    topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({'word': list, 'weight': list}).reset_index()\\r\\\\n\",\"    topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"    topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(list(map(lambda y: str(y), x))))\\r\\\\n\",\"    topicsdf['alias'] = topicsdf['words']\\r\\\\n\",\"    topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"    resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"    resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r\\\\n\",\"    # resultsdf['number'] = uniqueidcolumn_param\\r\\\\n\",\"    resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']] \\r\\\\n\",\"   \\r\\\\n\",\"    return  resultsdf.to_dict('records')\"]},{\"FunctionName\":\"countvectorize\",\"requirements\":\"\",\"params\":[{\"name\":\"input_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"1\"}],\"script\":[\"def countvectorize(dataset, input_col_param=''):\\r\\\\n\",\"    def cv(dataset, input_col_param):\\r\\\\n\",\"        from sklearn.feature_extraction.text import CountVectorizer\\r\\\\n\",\"        count_vectorizer = CountVectorizer(stop_words='english')\\r\\\\n\",\"        count_data = count_vectorizer.fit_transform(dataset[input_col_param].to_list())\\r\\\\n\",\"        words = count_vectorizer.get_feature_names_out()\\r\\\\n\",\"        return {'data': count_data, 'words': words}\\r\\\\n\",\"    result = {}\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        result[name] = cv(dataset, input_col_param)\\r\\\\n\",\"        result[name]['number'] = dataset['number'].to_list()\\r\\\\n\",\"    return result\"]},{\"FunctionName\":\"remove_stop_words\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stops_words\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"},{\"name\":\"input_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"},{\"name\":\"output_col\",\"value\":\"cleantext\",\"type\":\"Text\",\"alias\":\"cleantext\",\"index\":\"3\"}],\"script\":[\"def remove_stop_words(dataset, input_col_param='', output_col_param = '', custom_stops_words_param=''):\\r\\\\n\",\"    from nltk.corpus import stopwords\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns text without stop words\\r\\\\n\",\"    '''\\r\\\\n\",\"    def removesw(input):\\r\\\\n\",\"        return ' '.join([word for word in input if word not in stopwords.words('english') and word not in custom_stops_words])\\r\\\\n\",\"\\r\\\\n\",\"    custom_stops_words = custom_stops_words_param.split(',')\\r\\\\n\",\"    dataset[output_col_param] = dataset[input_col_param].apply(lambda input: removesw(input)) \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"nltk\",\"params\":[{\"name\":\"input_col\",\"value\":\"clean_text\",\"type\":\"Text\",\"alias\":\"clean_text\",\"index\":\"1\"},{\"name\":\"output_col\",\"value\":\"tokenize\",\"type\":\"Text\",\"alias\":\"tokenize\",\"index\":\"2\"}],\"script\":[\"def tokenize(dataset, input_col_param='', output_col_param = ''):\\r\\\\n\",\"    from nltk.tokenize import word_tokenize\\r\\\\n\",\"    '''\\r\\\\n\",\"    Returns tokenized version of text\\r\\\\n\",\"    '''\\r\\\\n\",\"    grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"    grouped_df = {}\\r\\\\n\",\"    for name, group in grouped:\\r\\\\n\",\"        grouped_df[name] = group\\r\\\\n\",\"\\r\\\\n\",\"    for name, dataset in grouped_df.items():\\r\\\\n\",\"        dataset[output_col_param] = dataset[input_col_param].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"    dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"filterdata\",\"requirements\":\"\",\"params\":[{\"name\":\"output_col\",\"value\":\"filterddf\",\"type\":\"Text\",\"alias\":\"filterddf\",\"index\":\"1\"}],\"script\":[\"def filterdata(dataset, output_col_param = ''):\\r\\\\n\",\"    dataset = pd.DataFrame(dataset)\\r\\\\n\",\"    dataset['cluster_Type'] = 'LDA'\\r\\\\n\",\"    filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r\\\\n\",\"    ciList = filteredCIs[filteredCIs['count'] >=50]['group_by_field'].tolist()\\r\\\\n\",\"    filtereddf = dataset['group_by_field'].isin(ciList)\\r\\\\n\",\"    dataset[output_col_param] = filtereddf\\r\\\\n\",\"    dataset = dataset[dataset[output_col_param] == True]\\r\\\\n\",\"    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-13 09:09:56\",\"alias\":\"Clean tickets -Cluster Input\",\"id\":861,\"name\":\"LEOCLNTC47521\",\"description\":\"Clean Tickets for clustering\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select number, clean_text , group_by_field from @projectname_tickets_enriched\\\\nwhere clean_text != \\\\\\\"\\\\\\\"\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\",\"name\":\"storageType\"}]}","admin","Clustering LDA","2023-10-26T08:13:42","LEOCLSTR81583","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"89\":{\"taskId\":\"4010f2cd-4214-4884-89be-d7ce6c6fe83e\"}}"
"admin","2023-10-16T09:16:47.458","false","Using Isolation Forest algorithm for Anomaly detection Isolation Forest does it by introducing (an ensemble of) binary trees that recursively generates partitions by randomly selecting a feature and then randomly selecting a split value for the feature. The partitioning process will continue until it separates all the data points from the rest of the samples.","NULL","{\"elements\":[{\"id\":\"LswBs\",\"alias\":\"IsolationForest\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"detect\",\"requirements\":\"pandas\",\"params\":[],\"script\":[\"import pandas as pd\\rfrom sklearn.ensemble import IsolationForest\\rfrom datetime import datetime\\rdef PythonScript( dataset):    #python-script Data\\r    print(dataset)\\r    df =pd.DataFrame(dataset)\\r    df['timestamp']=pd.to_datetime(df['run_timestamp'])\\r    #Adding more features\\r    df['hour']=df.timestamp.dt.hour\\r    df['weekday']=pd.Categorical(df.timestamp.dt.strftime('%A'), categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday'], ordered=True)\\r    model =  IsolationForest(contamination=0.004)\\r    model.fit(df[['value']])\\r    df['outliers']=pd.Series(model.predict(df[['value']])).apply(lambda x: 'yes' if (x == -1) else 'no' )\\r    print(df)\\r    return df\"]},\"position_x\":\"369\",\"position_y\":\"147\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"Udgkm\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"GVmDv\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-12 10:42:27\",\"alias\":\"CPU_Percent\",\"id\":92,\"name\":\"CPU_Percent\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT run_timestamp, value FROM @projectname_anomaly_cpu  ORDER BY run_timestamp\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}],\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"}},{\"id\":\"GVmDv\",\"alias\":\"Time Series Data\",\"name\":\"MYSQL Extractor\",\"classname\":\"MYSQLExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-12 10:42:27\",\"alias\":\"CPU_Percent\",\"id\":92,\"name\":\"CPU_Percent\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT run_timestamp, value FROM @projectname_anomaly_cpu  ORDER BY run_timestamp\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"95\",\"position_y\":\"147\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"LswBs\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":\"dropdown\"},\"context\":[],\"codeGeneration\":{\"requirements\":[\"mysql-connector-python\"],\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"import json\",\"from leaputils import Security\"],\"script\":\"def MYSQLExtractor(dataset_param=''):\\r    attributes = json.loads(dataset_param['attributes'])\\r    datasource = json.loads(dataset_param['datasource']['connectionDetails'])\\r    def getConnection():\\r        username = datasource['userName']\\r        password = Security.decrypt(datasource['password'], dataset_param['datasource']['salt'])\\r        url = datasource['url']\\r        host = urlparse(url[5:]).hostname\\r        port = urlparse(url[5:]).port\\r        database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port=port)\\r        return connection\\r    connection = getConnection()\\r\\r    query = attributes['Query']  # self.mapQueryParams()\\r    cursor = connection.cursor(dictionary=True)\\r    cursor.execute(query)\\r    results = cursor.fetchall()\\r    return results\\r\\r\\r\\r\\n\"}},{\"id\":\"Udgkm\",\"alias\":\"Anomalies\",\"name\":\"MYSQL Loader\",\"classname\":\"MYSQLLoaderConfig\",\"category\":\"DatasetLoaderConfig\",\"attributes\":{\"dataset\":\"\"},\"position_x\":\"597\",\"position_y\":\"147\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"LswBs\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"context\":[{\"FunctionName\":\"detect\",\"requirements\":\"pandas\",\"params\":[],\"script\":[\"import pandas as pd\\rfrom sklearn.ensemble import IsolationForest\\rfrom datetime import datetime\\rdef PythonScript( dataset):    #python-script Data\\r    print(dataset)\\r    df =pd.DataFrame(dataset)\\r    df['timestamp']=pd.to_datetime(df['run_timestamp'])\\r    #Adding more features\\r    df['hour']=df.timestamp.dt.hour\\r    df['weekday']=pd.Categorical(df.timestamp.dt.strftime('%A'), categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday'], ordered=True)\\r    model =  IsolationForest(contamination=0.004)\\r    model.fit(df[['value']])\\r    df['outliers']=pd.Series(model.predict(df[['value']])).apply(lambda x: 'yes' if (x == -1) else 'no' )\\r    print(df)\\r    return df\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-12 10:42:27\",\"alias\":\"CPU_Percent\",\"id\":92,\"name\":\"CPU_Percent\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT run_timestamp, value FROM @projectname_anomaly_cpu  ORDER BY run_timestamp\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}],\"codeGeneration\":{\"requirements\":[\"mysql-connector-python\"],\"imports\":[\"import mysql\",\"import json\",\"from leaputils import Security\",\"from urllib.parse import urlparse\"],\"script\":\"def MYSQLLoader(dataset, dataset_param=''):\\r    attributes = json.loads(dataset_param['attributes'])\\r    datasource = json.loads(dataset_param['datasource']['connectionDetails'])\\r    mode = attributes['writeMode']\\r    url = datasource['url']\\r    tablename = attributes['tableName']\\r    username = datasource['userName']\\r    password = Security.decrypt(datasource['password'], dataset_param['datasource']['salt'])\\r    host = urlparse(url[5:]).hostname\\r    port = urlparse(url[5:]).port\\r    database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r\\r    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\r    mycursor = cnx.cursor()\\r    if dataset != None and len(dataset) > 0:\\r        columnList = list(dataset[0].keys())\\r    if mode in 'overwrite':\\r        mycursor.execute('Drop table IF EXISTS {0}'.format(tablename))\\r\\r    # create table if not exists\\r    column_definition = ', '.join(['`{0}` TEXT'.format(c) for c in columnList])\\r    createQuery = ' CREATE TABLE IF NOT EXISTS {0} ({1})'.format(tablename, column_definition)\\r    mycursor.execute(createQuery)\\r    data = []\\r    for row in dataset:\\r        try:\\r            paramsDict = {}\\r            values = []\\r            for i in range(0, len(columnList)):\\r                paramsDict[columnList[i]] = row[columnList[i]]\\r                values.append(row[columnList[i]])\\r\\r            columns = ', '.join('`{0}`'.format(k) for k in paramsDict)\\r            duplicates = ', '.join('{0}=VALUES({0})'.format(k) for k in paramsDict)\\r            place_holders = ', '.join('%s'.format(k) for k in paramsDict)\\r\\r            query = 'INSERT INTO {0} ({1}) VALUES ({2})'.format(tablename, columns, place_holders)\\r            if mode in ('update'):\\r                query = '{0} ON DUPLICATE KEY UPDATE {1}'.format(query, duplicates)\\r            data.append(values)\\r\\r        except Exception as e:\\r            logger.error('{0}:{1}'.format(e, row))\\r    if (len(data) > 0):\\r        mycursor.executemany(query, data)\\r        cnx.commit()\\r\\r    mycursor.close()\\r    cnx.close()\\r\\r\\r\\r\\r\\r\\n\"}}]}","admin","Anomaly Detection- Isolation Forest","2023-10-17T04:38:13","LEOANMLY92723","leo1311","DragNDropLite","NULL","NULL","pipeline","NULL"
"admin","2023-10-16T09:34:11.259","false","The Amazon SageMaker Random Cut Forest (RCF) algorithm is an unsupervised algorithm for detecting anomalous data points within a dataset. ","NULL","{\"elements\":[{\"id\":\"JvAJf\",\"alias\":\"Prepare Training Data\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas,boto3,sagemaker\",\"params\":[{\"name\":\"data_file_name\",\"value\":\"CPU_Metic.csv\",\"type\":\"Text\",\"alias\":\"CPU_Metic.csv\",\"index\":\"1\"}],\"script\":[\"import pandas\\rdef prepare_data(data_file_name_param='' ): \\r    #python-script Data\\r    dataset = pandas.read_csv(data_file_name_param, delimiter=',')\\r    def convert_and_upload_training_data(ndarray, bucket, prefix, filename='data.pbr'):\\r        import boto3\\r        import os\\r        from sagemaker.amazon.common import numpy_to_record_serializer\\r    \\r        # convert Numpy array to Protobuf RecordIO format\\r        serializer = numpy_to_record_serializer()\\r        buffer = serializer(ndarray)\\r    \\r        # upload to S3\\r        s3_object = os.path.join(prefix, 'train', filename)\\r        boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(buffer)\\r        s3_path = 's3://{}/{}'.format(bucket, s3_object)\\r        return s3_path\\r\\r    bucket = '<my-s3-bucket>' # <-- use your own bucket, here\\r    prefix = 'sagemaker/randomcutforest'\\r    s3_train_data = convert_and_upload_training_data(\\r        dataset.value.as_matrix().reshape(-1,1),\\r        bucket,\\r        prefix)\\r    return s3_train_data\"]},\"position_x\":\"160\",\"position_y\":\"152\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"QHuPA\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"QHuPA\",\"alias\":\"Train\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"Train\",\"requirements\":\"\",\"params\":[],\"script\":[\"import boto3\\rimport sagemaker\\rdef Train( dataset):    #python-script Data\\r\\r\\r    containers = {\\r        'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/randomcutforest:latest',\\r        'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/randomcutforest:latest',\\r        'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/randomcutforest:latest',\\r        'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/randomcutforest:latest'}\\r    region_name = boto3.Session().region_name\\r    container = containers[region_name]\\r    \\r    session = sagemaker.Session()\\r    \\r    rcf = sagemaker.estimator.Estimator(\\r        container,\\r        sagemaker.get_execution_role(),\\r        output_path='s3://{}/{}/output'.format(bucket, prefix),\\r        train_instance_count=1,\\r        train_instance_type='ml.c5.xlarge',\\r        sagemaker_session=session)\\r    \\r    rcf.set_hyperparameters(\\r        num_samples_per_tree=200,\\r        num_trees=50,\\r        feature_size=1)\\r    \\r    s3_train_input = sagemaker.session.s3_input(\\r        s3_train_data,\\r        distribution='ShardedByS3Key',\\r        content_type='application/x-recordio-protobuf')\\r    \\r    rcf.fit({'train': s3_train_input})\\r    return rfc\"]},\"position_x\":\"419\",\"position_y\":\"152\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"JvAJf\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"QwHka\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas,boto3,sagemaker\",\"params\":[{\"name\":\"data_file_name\",\"value\":\"CPU_Metic.csv\",\"type\":\"Text\",\"alias\":\"CPU_Metic.csv\",\"index\":\"1\"}],\"script\":[\"import pandas\\rdef prepare_data(data_file_name_param='' ): \\r    #python-script Data\\r    dataset = pandas.read_csv(data_file_name_param, delimiter=',')\\r    def convert_and_upload_training_data(ndarray, bucket, prefix, filename='data.pbr'):\\r        import boto3\\r        import os\\r        from sagemaker.amazon.common import numpy_to_record_serializer\\r    \\r        # convert Numpy array to Protobuf RecordIO format\\r        serializer = numpy_to_record_serializer()\\r        buffer = serializer(ndarray)\\r    \\r        # upload to S3\\r        s3_object = os.path.join(prefix, 'train', filename)\\r        boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(buffer)\\r        s3_path = 's3://{}/{}'.format(bucket, s3_object)\\r        return s3_path\\r\\r    bucket = '<my-s3-bucket>' # <-- use your own bucket, here\\r    prefix = 'sagemaker/randomcutforest'\\r    s3_train_data = convert_and_upload_training_data(\\r        dataset.value.as_matrix().reshape(-1,1),\\r        bucket,\\r        prefix)\\r    return s3_train_data\"]}]},{\"id\":\"QwHka\",\"alias\":\"Predict\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"predict\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef predict( estimator, dataset):    #python-script Data\\r    from sagemaker.predictor import csv_serializer, json_deserializer\\r\\r    rcf_inference = rcf.deploy(\\r        initial_instance_count=1,\\r        instance_type='ml.c5.xlarge',\\r    )\\r    \\r    rcf_inference.content_type = 'text/csv'\\r    rcf_inference.serializer = csv_serializer\\r    rcf_inference.deserializer = json_deserializer\\r    results = rcf_inference.predict(dataset.value.as_matrix().reshape(-1,1))\\r    scores = [datum['score'] for datum in results['scores']]\\r    dataset['score'] = pandas.Series(scores, index=dataset.index)\\r    \\r    score_mean = dataset.score.mean()\\r    score_std = dataset.score.std()\\r    \\r    score_cutoff = score_mean + 3*score_std\\r    anomalies = dataset[dataset['score'] > score_cutoff]\\r    return dataset\"]},\"position_x\":\"640\",\"position_y\":\"152\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"QHuPA\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"NWurD\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"Train\",\"requirements\":\"\",\"params\":[],\"script\":[\"import boto3\\rimport sagemaker\\rdef Train( dataset):    #python-script Data\\r\\r\\r    containers = {\\r        'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/randomcutforest:latest',\\r        'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/randomcutforest:latest',\\r        'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/randomcutforest:latest',\\r        'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/randomcutforest:latest'}\\r    region_name = boto3.Session().region_name\\r    container = containers[region_name]\\r    \\r    session = sagemaker.Session()\\r    \\r    rcf = sagemaker.estimator.Estimator(\\r        container,\\r        sagemaker.get_execution_role(),\\r        output_path='s3://{}/{}/output'.format(bucket, prefix),\\r        train_instance_count=1,\\r        train_instance_type='ml.c5.xlarge',\\r        sagemaker_session=session)\\r    \\r    rcf.set_hyperparameters(\\r        num_samples_per_tree=200,\\r        num_trees=50,\\r        feature_size=1)\\r    \\r    s3_train_input = sagemaker.session.s3_input(\\r        s3_train_data,\\r        distribution='ShardedByS3Key',\\r        content_type='application/x-recordio-protobuf')\\r    \\r    rcf.fit({'train': s3_train_input})\\r    return rfc\"]},{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas,boto3,sagemaker\",\"params\":[{\"name\":\"data_file_name\",\"value\":\"CPU_Metic.csv\",\"type\":\"Text\",\"alias\":\"CPU_Metic.csv\",\"index\":\"1\"}],\"script\":[\"import pandas\\rdef prepare_data(data_file_name_param='' ): \\r    #python-script Data\\r    dataset = pandas.read_csv(data_file_name_param, delimiter=',')\\r    def convert_and_upload_training_data(ndarray, bucket, prefix, filename='data.pbr'):\\r        import boto3\\r        import os\\r        from sagemaker.amazon.common import numpy_to_record_serializer\\r    \\r        # convert Numpy array to Protobuf RecordIO format\\r        serializer = numpy_to_record_serializer()\\r        buffer = serializer(ndarray)\\r    \\r        # upload to S3\\r        s3_object = os.path.join(prefix, 'train', filename)\\r        boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(buffer)\\r        s3_path = 's3://{}/{}'.format(bucket, s3_object)\\r        return s3_path\\r\\r    bucket = '<my-s3-bucket>' # <-- use your own bucket, here\\r    prefix = 'sagemaker/randomcutforest'\\r    s3_train_data = convert_and_upload_training_data(\\r        dataset.value.as_matrix().reshape(-1,1),\\r        bucket,\\r        prefix)\\r    return s3_train_data\"]},{\"FunctionName\":\"PredictionData\",\"requirements\":\"\",\"params\":[{\"name\":\"data_file_name\",\"value\":\"CPU_Metric.csv\",\"type\":\"Text\",\"alias\":\"CPU_Metric.csv\",\"index\":\"1\"}],\"script\":[\"import pandas\\rdef PredictionData( data_file_name_param=''):    #python-script Data\\r    dataset = pandas.read_csv(data_file_name_param, delimiter=',')\\r    return dataset\"]}]},{\"id\":\"NWurD\",\"alias\":\"PredictionData\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"PredictionData\",\"requirements\":\"\",\"params\":[{\"name\":\"data_file_name\",\"value\":\"CPU_Metric.csv\",\"type\":\"Text\",\"alias\":\"CPU_Metric.csv\",\"index\":\"1\"}],\"script\":[\"import pandas\\rdef PredictionData( data_file_name_param=''):    #python-script Data\\r    dataset = pandas.read_csv(data_file_name_param, delimiter=',')\\r    return dataset\"]},\"position_x\":\"160\",\"position_y\":\"19\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"QwHka\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[]}]}","admin","Anomaly Detection- AWS RCF","2023-10-17T11:56:03","LEOANMLY31081","leo1311","DragNDropLite","NULL","NULL","pipeline","NULL"
"admin","2023-10-16T09:36:25.655","false","Time based correlation","NULL","{\"elements\":[{\"id\":\"sFiZx\",\"alias\":\"MYSQL Extractor\",\"name\":\"MYSQL Extractor\",\"classname\":\"MYSQLExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-10 14:16:38\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 12:16:34\",\"alias\":\"Tickets Schema\",\"id\":1,\"name\":\"TicketsSchema\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 1,\\\"recordcolumnname\\\": \\\"number\\\",\\\"recordcolumndisplayname\\\": \\\"Number\\\",\\\"isprimarykey\\\": true,\\\"isunique\\\": true,\\\"isrequired\\\": true},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 2,\\\"recordcolumnname\\\": \\\"shortdescription\\\",\\\"recordcolumndisplayname\\\": \\\"Short Description\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 3,\\\"recordcolumnname\\\": \\\"priority\\\",\\\"recordcolumndisplayname\\\": \\\"Priority\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 4,\\\"recordcolumnname\\\": \\\"type\\\",\\\"recordcolumndisplayname\\\": \\\"Type\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 5,\\\"recordcolumnname\\\": \\\"createdDate\\\",\\\"recordcolumndisplayname\\\": \\\"Created Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 6,\\\"recordcolumnname\\\": \\\"approval\\\",\\\"recordcolumndisplayname\\\": \\\"Approval\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 7,\\\"recordcolumnname\\\": \\\"assignedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Assigned Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 8,\\\"recordcolumnname\\\": \\\"assignedto\\\",\\\"recordcolumndisplayname\\\": \\\"Assigned To\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 9,\\\"recordcolumnname\\\": \\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\": \\\"Assignment Group\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 10,\\\"recordcolumnname\\\": \\\"business_service\\\",\\\"recordcolumndisplayname\\\": \\\"Business Service\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 11,\\\"recordcolumnname\\\": \\\"caller\\\",\\\"recordcolumndisplayname\\\": \\\"Caller\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 12,\\\"recordcolumnname\\\": \\\"category\\\",\\\"recordcolumndisplayname\\\": \\\"Category\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 13,\\\"recordcolumnname\\\": \\\"closecode\\\",\\\"recordcolumndisplayname\\\": \\\"Close Code\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 14,\\\"recordcolumnname\\\": \\\"closedby\\\",\\\"recordcolumndisplayname\\\": \\\"Closed By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 15,\\\"recordcolumnname\\\": \\\"closedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Closed Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 16,\\\"recordcolumnname\\\": \\\"closenotes\\\",\\\"recordcolumndisplayname\\\": \\\"Close Notes\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 17,\\\"recordcolumnname\\\": \\\"comments\\\",\\\"recordcolumndisplayname\\\": \\\"Comments\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 18,\\\"recordcolumnname\\\": \\\"configurationItem\\\",\\\"recordcolumndisplayname\\\": \\\"Configuration Item\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 19,\\\"recordcolumnname\\\": \\\"createdby\\\",\\\"recordcolumndisplayname\\\": \\\"Created By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 20,\\\"recordcolumnname\\\": \\\"description\\\",\\\"recordcolumndisplayname\\\": \\\"Description\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 21,\\\"recordcolumnname\\\": \\\"duedate\\\",\\\"recordcolumndisplayname\\\": \\\"Due Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 22,\\\"recordcolumnname\\\": \\\"impact\\\",\\\"recordcolumndisplayname\\\": \\\"Impact\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 23,\\\"recordcolumnname\\\": \\\"last_updated_by\\\",\\\"recordcolumndisplayname\\\": \\\"Last Updated By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 24,\\\"recordcolumnname\\\": \\\"lastUpdated\\\",\\\"recordcolumndisplayname\\\": \\\"Last Updated\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 25,\\\"recordcolumnname\\\": \\\"location\\\",\\\"recordcolumndisplayname\\\": \\\"Location\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 26,\\\"recordcolumnname\\\": \\\"openedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Opened Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 27,\\\"recordcolumnname\\\": \\\"price\\\",\\\"recordcolumndisplayname\\\": \\\"Price\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 28,\\\"recordcolumnname\\\": \\\"reopenedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Reopened Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 29,\\\"recordcolumnname\\\": \\\"request_state\\\",\\\"recordcolumndisplayname\\\": \\\"Request State\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 30,\\\"recordcolumnname\\\": \\\"requested_by\\\",\\\"recordcolumndisplayname\\\": \\\"Requested By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 31,\\\"recordcolumnname\\\": \\\"requested_for\\\",\\\"recordcolumndisplayname\\\": \\\"Requested For\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 32,\\\"recordcolumnname\\\": \\\"resolvedby\\\",\\\"recordcolumndisplayname\\\": \\\"Resolved By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 33,\\\"recordcolumnname\\\": \\\"resolvedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Resolved Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 34,\\\"recordcolumnname\\\": \\\"risk\\\",\\\"recordcolumndisplayname\\\": \\\"Risk\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 35,\\\"recordcolumnname\\\": \\\"severity\\\",\\\"recordcolumndisplayname\\\": \\\"Severity\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 36,\\\"recordcolumnname\\\": \\\"sladueDate\\\",\\\"recordcolumndisplayname\\\": \\\"SLA Due Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 37,\\\"recordcolumnname\\\": \\\"special_instructions\\\",\\\"recordcolumndisplayname\\\": \\\"Special Instructions\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 38,\\\"recordcolumnname\\\": \\\"resolution_Steps\\\",\\\"recordcolumndisplayname\\\": \\\"Resolution Steps\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 39,\\\"recordcolumnname\\\": \\\"state\\\",\\\"recordcolumndisplayname\\\": \\\"State\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 40,\\\"recordcolumnname\\\": \\\"sysId\\\",\\\"recordcolumndisplayname\\\": \\\"Sys Id\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 41,\\\"recordcolumnname\\\": \\\"taskType\\\",\\\"recordcolumndisplayname\\\": \\\"Task Type\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 42,\\\"recordcolumnname\\\": \\\"updatedby\\\",\\\"recordcolumndisplayname\\\": \\\"Updated By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 43,\\\"recordcolumnname\\\": \\\"updatedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Updated Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 44,\\\"recordcolumnname\\\": \\\"sop\\\",\\\"recordcolumndisplayname\\\": \\\"SOP\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 45,\\\"recordcolumnname\\\": \\\"tags\\\",\\\"recordcolumndisplayname\\\": \\\"Tags\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 46,\\\"recordcolumnname\\\": \\\"source\\\",\\\"recordcolumndisplayname\\\": \\\"Source\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 47,\\\"recordcolumnname\\\": \\\"resolutionCategory\\\",\\\"recordcolumndisplayname\\\": \\\"Resolution Category\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 48,\\\"recordcolumnname\\\": \\\"workflow\\\",\\\"recordcolumndisplayname\\\": \\\"Workflow\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 48,\\\"recordcolumnname\\\": \\\"resolutionStepsClusterName\\\",\\\"recordcolumndisplayname\\\": \\\"resolutionStepsClusterName\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"208\",\"position_y\":\"140\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"YlpwM\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"requirements\":[\"mysql-connector-python\"],\"imports\":[\"import mysql.connector\",\"from urllib.parse import urlparse\",\"import json\",\"from leaputils import Security\"],\"script\":\"def MYSQLExtractor(dataset_param=''):\\r    attributes = json.loads(dataset_param['attributes'])\\r    datasource = json.loads(dataset_param['datasource']['connectionDetails'])\\r    def getConnection():\\r        username = datasource['userName']\\r        password = Security.decrypt(datasource['password'], dataset_param['datasource']['salt'])\\r        url = datasource['url']\\r        host = urlparse(url[5:]).hostname\\r        port = urlparse(url[5:]).port\\r        database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r        connection = mysql.connector.connect(user=username, password=password, host=host, database=database, port=port)\\r        return connection\\r    connection = getConnection()\\r\\r    query = attributes['Query']  # self.mapQueryParams()\\r    cursor = connection.cursor(dictionary=True)\\r    cursor.execute(query)\\r    results = cursor.fetchall()\\r    return results\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"YlpwM\",\"alias\":\"Correlation\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"correlate\",\"requirements\":\"pyspark\",\"params\":[],\"script\":[\"import logging\\rfrom pyspark.sql.functions import *\\rfrom pyspark.sql.types import *\\rfrom pyspark.sql.window import Window\\rfrom datetime import datetime\\rimport json\\rfrom pyspark.ml.stat import Correlation\\rfrom pyspark.sql import SparkSession \\rdef correlate( data):    #python-script Data\\r    spark = SparkSession.builder.appName('correlation').getOrCreate() \\r    dataset = spark.createDataFrame(data) \\r    w = Window().orderBy(col('createdDate').cast('long')).rangeBetween(0, 1800)\\r    dataset = dataset.select(col('application').alias('ci_x'),col('createdDate'))\\r    cidf = dataset.select('ci_x',collect_set('ci_x').over(w).alias('ci_y_list'))\\r    correlatedCIs = cidf.select('ci_x',explode('ci_y_list').alias('ci_y'))\\r    correlatedCIsCount = correlatedCIs.groupBy('ci_x','ci_y').count()\\r    denominator = correlatedCIsCount.filter(col('ci_x') == col('ci_y')).select(col('ci_x').alias('ci'), col('count').alias('total'))\\r    correlatedCIsCount = correlatedCIsCount.join(denominator, correlatedCIsCount.ci_x == denominator.ci)\\r    correlatedCIsCount =correlatedCIsCount.withColumn('percentage', (col('count')*100)/col('total'))\\r    correlatedCIsCount =correlatedCIsCount.withColumn('run_timestamp', lit(datetime.now()))\\r    correlatedCIsCount =correlatedCIsCount.select('ci_x','ci_y',col('count').alias('value'),'percentage','run_timestamp')\\r    correlatedCIsCount =correlatedCIsCount.withColumn('projectId', lit(254))\\r    return correlatedCIsCount\\r \"]},\"position_x\":\"481\",\"position_y\":\"140\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"sFiZx\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"HGHsj\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-10 14:16:38\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 12:16:34\",\"alias\":\"Tickets Schema\",\"id\":1,\"name\":\"TicketsSchema\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 1,\\\"recordcolumnname\\\": \\\"number\\\",\\\"recordcolumndisplayname\\\": \\\"Number\\\",\\\"isprimarykey\\\": true,\\\"isunique\\\": true,\\\"isrequired\\\": true},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 2,\\\"recordcolumnname\\\": \\\"shortdescription\\\",\\\"recordcolumndisplayname\\\": \\\"Short Description\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 3,\\\"recordcolumnname\\\": \\\"priority\\\",\\\"recordcolumndisplayname\\\": \\\"Priority\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 4,\\\"recordcolumnname\\\": \\\"type\\\",\\\"recordcolumndisplayname\\\": \\\"Type\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 5,\\\"recordcolumnname\\\": \\\"createdDate\\\",\\\"recordcolumndisplayname\\\": \\\"Created Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": true,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 6,\\\"recordcolumnname\\\": \\\"approval\\\",\\\"recordcolumndisplayname\\\": \\\"Approval\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 7,\\\"recordcolumnname\\\": \\\"assignedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Assigned Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 8,\\\"recordcolumnname\\\": \\\"assignedto\\\",\\\"recordcolumndisplayname\\\": \\\"Assigned To\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 9,\\\"recordcolumnname\\\": \\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\": \\\"Assignment Group\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 10,\\\"recordcolumnname\\\": \\\"business_service\\\",\\\"recordcolumndisplayname\\\": \\\"Business Service\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 11,\\\"recordcolumnname\\\": \\\"caller\\\",\\\"recordcolumndisplayname\\\": \\\"Caller\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 12,\\\"recordcolumnname\\\": \\\"category\\\",\\\"recordcolumndisplayname\\\": \\\"Category\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 13,\\\"recordcolumnname\\\": \\\"closecode\\\",\\\"recordcolumndisplayname\\\": \\\"Close Code\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 14,\\\"recordcolumnname\\\": \\\"closedby\\\",\\\"recordcolumndisplayname\\\": \\\"Closed By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 15,\\\"recordcolumnname\\\": \\\"closedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Closed Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false,\\\"isautoincrement\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 16,\\\"recordcolumnname\\\": \\\"closenotes\\\",\\\"recordcolumndisplayname\\\": \\\"Close Notes\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 17,\\\"recordcolumnname\\\": \\\"comments\\\",\\\"recordcolumndisplayname\\\": \\\"Comments\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 18,\\\"recordcolumnname\\\": \\\"configurationItem\\\",\\\"recordcolumndisplayname\\\": \\\"Configuration Item\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 19,\\\"recordcolumnname\\\": \\\"createdby\\\",\\\"recordcolumndisplayname\\\": \\\"Created By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 20,\\\"recordcolumnname\\\": \\\"description\\\",\\\"recordcolumndisplayname\\\": \\\"Description\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 21,\\\"recordcolumnname\\\": \\\"duedate\\\",\\\"recordcolumndisplayname\\\": \\\"Due Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 22,\\\"recordcolumnname\\\": \\\"impact\\\",\\\"recordcolumndisplayname\\\": \\\"Impact\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 23,\\\"recordcolumnname\\\": \\\"last_updated_by\\\",\\\"recordcolumndisplayname\\\": \\\"Last Updated By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 24,\\\"recordcolumnname\\\": \\\"lastUpdated\\\",\\\"recordcolumndisplayname\\\": \\\"Last Updated\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 25,\\\"recordcolumnname\\\": \\\"location\\\",\\\"recordcolumndisplayname\\\": \\\"Location\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 26,\\\"recordcolumnname\\\": \\\"openedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Opened Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 27,\\\"recordcolumnname\\\": \\\"price\\\",\\\"recordcolumndisplayname\\\": \\\"Price\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 28,\\\"recordcolumnname\\\": \\\"reopenedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Reopened Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 29,\\\"recordcolumnname\\\": \\\"request_state\\\",\\\"recordcolumndisplayname\\\": \\\"Request State\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 30,\\\"recordcolumnname\\\": \\\"requested_by\\\",\\\"recordcolumndisplayname\\\": \\\"Requested By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 31,\\\"recordcolumnname\\\": \\\"requested_for\\\",\\\"recordcolumndisplayname\\\": \\\"Requested For\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 32,\\\"recordcolumnname\\\": \\\"resolvedby\\\",\\\"recordcolumndisplayname\\\": \\\"Resolved By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 33,\\\"recordcolumnname\\\": \\\"resolvedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Resolved Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 34,\\\"recordcolumnname\\\": \\\"risk\\\",\\\"recordcolumndisplayname\\\": \\\"Risk\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 35,\\\"recordcolumnname\\\": \\\"severity\\\",\\\"recordcolumndisplayname\\\": \\\"Severity\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 36,\\\"recordcolumnname\\\": \\\"sladueDate\\\",\\\"recordcolumndisplayname\\\": \\\"SLA Due Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 37,\\\"recordcolumnname\\\": \\\"special_instructions\\\",\\\"recordcolumndisplayname\\\": \\\"Special Instructions\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"text\\\",\\\"columnorder\\\": 38,\\\"recordcolumnname\\\": \\\"resolution_Steps\\\",\\\"recordcolumndisplayname\\\": \\\"Resolution Steps\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 39,\\\"recordcolumnname\\\": \\\"state\\\",\\\"recordcolumndisplayname\\\": \\\"State\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 40,\\\"recordcolumnname\\\": \\\"sysId\\\",\\\"recordcolumndisplayname\\\": \\\"Sys Id\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 41,\\\"recordcolumnname\\\": \\\"taskType\\\",\\\"recordcolumndisplayname\\\": \\\"Task Type\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 42,\\\"recordcolumnname\\\": \\\"updatedby\\\",\\\"recordcolumndisplayname\\\": \\\"Updated By\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"timestamp\\\",\\\"columnorder\\\": 43,\\\"recordcolumnname\\\": \\\"updatedDate\\\",\\\"recordcolumndisplayname\\\": \\\"Updated Date\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 44,\\\"recordcolumnname\\\": \\\"sop\\\",\\\"recordcolumndisplayname\\\": \\\"SOP\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 45,\\\"recordcolumnname\\\": \\\"tags\\\",\\\"recordcolumndisplayname\\\": \\\"Tags\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 46,\\\"recordcolumnname\\\": \\\"source\\\",\\\"recordcolumndisplayname\\\": \\\"Source\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 47,\\\"recordcolumnname\\\": \\\"resolutionCategory\\\",\\\"recordcolumndisplayname\\\": \\\"Resolution Category\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 48,\\\"recordcolumnname\\\": \\\"workflow\\\",\\\"recordcolumndisplayname\\\": \\\"Workflow\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false},{\\\"columntype\\\": \\\"string\\\",\\\"columnorder\\\": 48,\\\"recordcolumnname\\\": \\\"resolutionStepsClusterName\\\",\\\"recordcolumndisplayname\\\": \\\"resolutionStepsClusterName\\\",\\\"isprimarykey\\\": false,\\\"isunique\\\": false,\\\"isrequired\\\": false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"HGHsj\",\"alias\":\"MYSQL Loader\",\"name\":\"MYSQL Loader\",\"classname\":\"MYSQLLoaderConfig\",\"category\":\"DatasetLoaderConfig\",\"attributes\":{\"dataset\":\"\"},\"position_x\":\"692\",\"position_y\":\"139\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"YlpwM\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"requirements\":[\"mysql-connector-python\"],\"imports\":[\"import mysql\",\"import json\",\"from leaputils import Security\",\"from urllib.parse import urlparse\"],\"script\":\"def MYSQLLoader(dataset, dataset_param=''):\\r    attributes = json.loads(dataset_param['attributes'])\\r    datasource = json.loads(dataset_param['datasource']['connectionDetails'])\\r    mode = attributes['writeMode']\\r    url = datasource['url']\\r    tablename = attributes['tableName']\\r    username = datasource['userName']\\r    password = Security.decrypt(datasource['password'], dataset_param['datasource']['salt'])\\r    host = urlparse(url[5:]).hostname\\r    port = urlparse(url[5:]).port\\r    database = urlparse(url[5:]).path.rsplit('/', 1)[1]\\r\\r    cnx = mysql.connector.connect(user=username, password=password, host=host, port=port, database=database)\\r    mycursor = cnx.cursor()\\r    if dataset != None and len(dataset) > 0:\\r        columnList = list(dataset[0].keys())\\r    if mode in 'overwrite':\\r        mycursor.execute('Drop table IF EXISTS {0}'.format(tablename))\\r\\r    # create table if not exists\\r    column_definition = ', '.join(['`{0}` TEXT'.format(c) for c in columnList])\\r    createQuery = ' CREATE TABLE IF NOT EXISTS {0} ({1})'.format(tablename, column_definition)\\r    mycursor.execute(createQuery)\\r    data = []\\r    for row in dataset:\\r        try:\\r            paramsDict = {}\\r            values = []\\r            for i in range(0, len(columnList)):\\r                paramsDict[columnList[i]] = row[columnList[i]]\\r                values.append(row[columnList[i]])\\r\\r            columns = ', '.join('`{0}`'.format(k) for k in paramsDict)\\r            duplicates = ', '.join('{0}=VALUES({0})'.format(k) for k in paramsDict)\\r            place_holders = ', '.join('%s'.format(k) for k in paramsDict)\\r\\r            query = 'INSERT INTO {0} ({1}) VALUES ({2})'.format(tablename, columns, place_holders)\\r            if mode in ('update'):\\r                query = '{0} ON DUPLICATE KEY UPDATE {1}'.format(query, duplicates)\\r            data.append(values)\\r\\r        except Exception as e:\\r            logger.error('{0}:{1}'.format(e, row))\\r    if (len(data) > 0):\\r        mycursor.executemany(query, data)\\r        cnx.commit()\\r\\r    mycursor.close()\\r    cnx.close()\\r\\r\\r\\r\\r\\r\\n\"}}]}","admin","Application Correlation","2023-10-17T10:31:46","LEOAPLCT87207","leo1311","DragNDropLite","NULL","NULL","pipeline","NULL"
"admin","2023-10-16T09:42:44.439","false","Recommend SOP based on pattern mapping","NULL","{\"elements\":[{\"id\":\"LBiIU\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"SOPClassification\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentid\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"}],\"script\":[\"import re\\rimport logging as logger\\rfrom datetime import datetime\\rimport pandas as pd\\rdef SOPClassification(dataset1, dataset2, incidentid_param=''):\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    dataset1['shortdescription'] = dataset1['shortdescription'].astype(str)\\r    incidentId = incidentid_param\\r    \\r    if incidentId !='':\\r        logger.info('Running for incident '+ incidentId)\\r        dataset1 = dataset1[dataset1['number'] == incidentId]\\r   \\r    label_list = dataset2.apply(lambda x:{'sop':x['sop'],'workflow':x['workflow'],'regex':x['regular_exp']}, axis=1).tolist()\\r    def getsop(label,text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['sop']\\r        return\\r    def getWorkflow(label , text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['workflow']\\r        return\\r    \\r\\r    dataset1['sop'] = dataset1['shortdescription'].apply(lambda x: getsop(label_list, x))\\r    dataset1 = dataset1[dataset1['sop'] != 'undefined']\\r    dataset1['sop_confidence'] = '100'\\r    dataset1['workflow'] = dataset1['shortdescription'].apply(lambda x: getWorkflow(label_list, x))\\r    dataset1['last_updated'] = datetime.now()\\r    \\r    data = dataset1[['number','sop','sop_confidence','workflow','last_updated']]\\r    data = data.to_dict('records')\\r    print(data[0:5])\\r    return data\\r        \\r\\r\\r\"]},\"position_x\":\"373\",\"position_y\":\"178\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"VRrfF\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"TOkBa\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"gsjOY\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:56:02\",\"alias\":\"SOP Configuration\",\"id\":289,\"name\":\"ACMSPCNF36673\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:50:54\",\"alias\":\"SOP Configuration\",\"id\":12,\"name\":\"ACMSPCNF87143\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"id\\\",\\\"recordcolumndisplayname\\\":\\\"id\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"regular_exp\\\",\\\"recordcolumndisplayname\\\":\\\"regular_exp\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_sop_config\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_sop_config\\\",\\\"uniqueIdentifier\\\":\\\"id\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"TOkBa\",\"alias\":\"SOP Configuration\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:56:02\",\"alias\":\"SOP Configuration\",\"id\":289,\"name\":\"ACMSPCNF36673\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:50:54\",\"alias\":\"SOP Configuration\",\"id\":12,\"name\":\"ACMSPCNF87143\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"id\\\",\\\"recordcolumndisplayname\\\":\\\"id\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"regular_exp\\\",\\\"recordcolumndisplayname\\\":\\\"regular_exp\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_sop_config\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_sop_config\\\",\\\"uniqueIdentifier\\\":\\\"id\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"167\",\"position_y\":\"60\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"LBiIU\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"gsjOY\",\"alias\":\"Tickets\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"112\",\"position_y\":\"188\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"LBiIU\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"VRrfF\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"599\",\"position_y\":\"193\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"LBiIU\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"SOPClassification\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentid\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"}],\"script\":[\"import re\\rimport logging as logger\\rfrom datetime import datetime\\rimport pandas as pd\\rdef SOPClassification(dataset1, dataset2, incidentid_param=''):\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    dataset1['shortdescription'] = dataset1['shortdescription'].astype(str)\\r    incidentId = incidentid_param\\r    \\r    if incidentId !='':\\r        logger.info('Running for incident '+ incidentId)\\r        dataset1 = dataset1[dataset1['number'] == incidentId]\\r   \\r    label_list = dataset2.apply(lambda x:{'sop':x['sop'],'workflow':x['workflow'],'regex':x['regular_exp']}, axis=1).tolist()\\r    def getsop(label,text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['sop']\\r        return\\r    def getWorkflow(label , text):\\r        for item in label:\\r            ismatch = re.match(item['regex'],text.lower())\\r            if ismatch != None:\\r                return item['workflow']\\r        return\\r    \\r\\r    dataset1['sop'] = dataset1['shortdescription'].apply(lambda x: getsop(label_list, x))\\r    dataset1 = dataset1[dataset1['sop'] != 'undefined']\\r    dataset1['sop_confidence'] = '100'\\r    dataset1['workflow'] = dataset1['shortdescription'].apply(lambda x: getWorkflow(label_list, x))\\r    dataset1['last_updated'] = datetime.now()\\r    \\r    data = dataset1[['number','sop','sop_confidence','workflow','last_updated']]\\r    data = data.to_dict('records')\\r    print(data[0:5])\\r    return data\\r        \\r\\r\\r\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:56:02\",\"alias\":\"SOP Configuration\",\"id\":289,\"name\":\"ACMSPCNF36673\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:50:54\",\"alias\":\"SOP Configuration\",\"id\":12,\"name\":\"ACMSPCNF87143\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"id\\\",\\\"recordcolumndisplayname\\\":\\\"id\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"regular_exp\\\",\\\"recordcolumndisplayname\\\":\\\"regular_exp\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_sop_config\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_sop_config\\\",\\\"uniqueIdentifier\\\":\\\"id\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\",\"name\":\"storageType\"}],\"environment\":[]}","admin","SOP Recommendation","2023-12-19T12:12:34","LEOSPCLS56756","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"9\":{\"taskId\":\"3b4ad630-b79d-4114-b815-b6e0dd719164\"}}"
"admin","2023-10-16T10:35:33.809","false","","NULL","{\"elements\":[{\"id\":\"spKJZ\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"StatusPredictionOutput\",\"description\":\"StatusPredictionOutput\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from bjm_status_prediction_data\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"bjm_status_prediction_data\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"alias\":\"StatusPredictionOutput\",\"organization\":\"NextGenAcme\",\"groups\":[]},\"applySchema\":false},\"position_x\":\"860\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"gFJOt\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"import requests\\r\",\"import urllib\\r\",\"import json\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"       \\r\",\"        indexer = model.stages[8]\\r\",\"        \\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='predicted_status', labels=indexer.labels)\\r\",\"        transformedData = labeler.transform(dataset)\\r\",\"       \\r\",\"        transformedData = transformedData.withColumn('ji_group', col('jiGroup'))\\r\",\"        transformedData = transformedData.withColumn('api_url', col('apiUrl'))\\r\",\"        transformedData = transformedData.withColumn('execution_type', col('executionType'))\\r\",\"        transformedData = transformedData.withColumn('rundeck_group', col('rundeckGroup'))\\r\",\"        transformedData = transformedData.withColumn('job_name', col('jobName'))\\r\",\"        transformedData = transformedData.withColumn('start_time', col('startTime'))\\r\",\"        # transformedData = transformedData.withColumn('end_time', col('endTime'))\\r\",\"        transformedData = transformedData.withColumn('running_job_instance_id', col('jobInstanceId'))\\r\",\"        transformedData = transformedData.withColumn('ticket', col('ticket'))\\r\",\"        transformedData = transformedData.withColumn('server', col('server'))\\r\",\"        \\r\",\"        data = transformedData.select('user','ji_group','ticket','api_url','execution_type','rundeck_group','job_name','server','start_time','predicted_status','running_job_instance_id')\\r\",\"       \\r\",\"        #Get token\\r\",\"        def get_token():\\r\",\"            keycloakurl ='https://myzul02o:8443/auth/realms/IAMP/protocol/openid-connect/token'\\r\",\"            tokenHeaders = {'Content-Type':'application/x-www-form-urlencoded','Accept':'*/*'}\\r\",\"            data={'grant_type': 'password','username': 'admin','password': 'infy@123','scope': 'openid','client_id': 'ang-app','client_secret': ''}\\r\",\"            requests.packages.urllib3.disable_warnings()\\r\",\"            keycloakResponse = requests.post(keycloakurl, headers=tokenHeaders ,data=data,verify=False,proxies={'http':'','https':''})\\r\",\"            jwttoken = keycloakResponse.json()['access_token']\\r\",\"            #print(keycloakResponse.json())\\r\",\"            return jwttoken\\r\",\"        \\r\",\"        JWTToken = get_token()\\r\",\"        head = {'Authorization': 'Bearer {}'.format(JWTToken)}\\r\",\"        params=transformedData.select('id','job_name','start_time','predicted_status').toJSON().map(lambda j: json.loads(j)).collect()\\r\",\"        for j in params:\\r\",\"            j['prediction'] = {'predicted_status':j['predicted_status']}\\r\",\"            \\r\",\"        url= 'https://leap/api/event/trigger/update_forecast?org=NextGenAcme&param='+str({'params':params})\\r\",\"        print(url)\\r\",\"        try:\\r\",\"            resp = requests.get(url=url,headers=head,verify=False,proxies={'http':'','https':''})\\r\",\"            if(resp.status_code!=200):\\r\",\"                print(resp.status_code)\\r\",\"                exit()\\r\",\"        except Exception as e:\\r\",\"            print(e)\\r\",\"       \\r\",\"        return data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"modelName\":\"JobStatusPrediction\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"hNPFF\",\"alias\":\"Model  Source\",\"name\":\"Model  Source\",\"classname\":\"ModelSourceConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"modelName\":\"JobStatusPrediction\"},\"position_x\":\"450\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"gFJOt\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"tBouL\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"modelName\":\"text\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"gFJOt\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"import requests\\r\",\"import urllib\\r\",\"import json\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"       \\r\",\"        indexer = model.stages[8]\\r\",\"        \\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='predicted_status', labels=indexer.labels)\\r\",\"        transformedData = labeler.transform(dataset)\\r\",\"       \\r\",\"        transformedData = transformedData.withColumn('ji_group', col('jiGroup'))\\r\",\"        transformedData = transformedData.withColumn('api_url', col('apiUrl'))\\r\",\"        transformedData = transformedData.withColumn('execution_type', col('executionType'))\\r\",\"        transformedData = transformedData.withColumn('rundeck_group', col('rundeckGroup'))\\r\",\"        transformedData = transformedData.withColumn('job_name', col('jobName'))\\r\",\"        transformedData = transformedData.withColumn('start_time', col('startTime'))\\r\",\"        # transformedData = transformedData.withColumn('end_time', col('endTime'))\\r\",\"        transformedData = transformedData.withColumn('running_job_instance_id', col('jobInstanceId'))\\r\",\"        transformedData = transformedData.withColumn('ticket', col('ticket'))\\r\",\"        transformedData = transformedData.withColumn('server', col('server'))\\r\",\"        \\r\",\"        data = transformedData.select('user','ji_group','ticket','api_url','execution_type','rundeck_group','job_name','server','start_time','predicted_status','running_job_instance_id')\\r\",\"       \\r\",\"        #Get token\\r\",\"        def get_token():\\r\",\"            keycloakurl ='https://myzul02o:8443/auth/realms/IAMP/protocol/openid-connect/token'\\r\",\"            tokenHeaders = {'Content-Type':'application/x-www-form-urlencoded','Accept':'*/*'}\\r\",\"            data={'grant_type': 'password','username': 'admin','password': 'infy@123','scope': 'openid','client_id': 'ang-app','client_secret': ''}\\r\",\"            requests.packages.urllib3.disable_warnings()\\r\",\"            keycloakResponse = requests.post(keycloakurl, headers=tokenHeaders ,data=data,verify=False,proxies={'http':'','https':''})\\r\",\"            jwttoken = keycloakResponse.json()['access_token']\\r\",\"            #print(keycloakResponse.json())\\r\",\"            return jwttoken\\r\",\"        \\r\",\"        JWTToken = get_token()\\r\",\"        head = {'Authorization': 'Bearer {}'.format(JWTToken)}\\r\",\"        params=transformedData.select('id','job_name','start_time','predicted_status').toJSON().map(lambda j: json.loads(j)).collect()\\r\",\"        for j in params:\\r\",\"            j['prediction'] = {'predicted_status':j['predicted_status']}\\r\",\"            \\r\",\"        url= 'https://leap/api/event/trigger/update_forecast?org=NextGenAcme&param='+str({'params':params})\\r\",\"        print(url)\\r\",\"        try:\\r\",\"            resp = requests.get(url=url,headers=head,verify=False,proxies={'http':'','https':''})\\r\",\"            if(resp.status_code!=200):\\r\",\"                print(resp.status_code)\\r\",\"                exit()\\r\",\"        except Exception as e:\\r\",\"            print(e)\\r\",\"       \\r\",\"        return data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"670\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"hNPFF\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"spKJZ\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"modelName\":\"JobStatusPrediction\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"tBouL\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},\"position_x\":\"240\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"hNPFF\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"VlAMK\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\",\"outputCols\":\"text\"},\"context\":[{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"VlAMK\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"40\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"tBouL\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]}]}","admin","Job Status Predict","2023-10-16T10:37:01","LEOJBSTS50319","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2023-10-16T10:36:25.098","false","","NULL","{\"elements\":[{\"id\":\"spKJZ\",\"alias\":\"Dataset  Loader\",\"name\":\"Dataset  Loader\",\"classname\":\"DatasetLoaderConfig\",\"category\":\"LoaderConfig\",\"attributes\":{\"dataset\":{\"name\":\"StatusPredictionOutput\",\"description\":\"StatusPredictionOutput\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"rw\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from bjm_status_prediction_data\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"bjm_status_prediction_data\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"alias\":\"StatusPredictionOutput\",\"organization\":\"NextGenAcme\",\"groups\":[]},\"applySchema\":false},\"position_x\":\"860\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"gFJOt\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"applySchema\":\"checkbox\"},\"context\":[{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"import requests\\r\",\"import urllib\\r\",\"import json\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"       \\r\",\"        indexer = model.stages[8]\\r\",\"        \\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='predicted_status', labels=indexer.labels)\\r\",\"        transformedData = labeler.transform(dataset)\\r\",\"       \\r\",\"        transformedData = transformedData.withColumn('ji_group', col('jiGroup'))\\r\",\"        transformedData = transformedData.withColumn('api_url', col('apiUrl'))\\r\",\"        transformedData = transformedData.withColumn('execution_type', col('executionType'))\\r\",\"        transformedData = transformedData.withColumn('rundeck_group', col('rundeckGroup'))\\r\",\"        transformedData = transformedData.withColumn('job_name', col('jobName'))\\r\",\"        transformedData = transformedData.withColumn('start_time', col('startTime'))\\r\",\"        # transformedData = transformedData.withColumn('end_time', col('endTime'))\\r\",\"        transformedData = transformedData.withColumn('running_job_instance_id', col('jobInstanceId'))\\r\",\"        transformedData = transformedData.withColumn('ticket', col('ticket'))\\r\",\"        transformedData = transformedData.withColumn('server', col('server'))\\r\",\"        \\r\",\"        data = transformedData.select('user','ji_group','ticket','api_url','execution_type','rundeck_group','job_name','server','start_time','predicted_status','running_job_instance_id')\\r\",\"       \\r\",\"        #Get token\\r\",\"        def get_token():\\r\",\"            keycloakurl ='https://myzul02o:8443/auth/realms/IAMP/protocol/openid-connect/token'\\r\",\"            tokenHeaders = {'Content-Type':'application/x-www-form-urlencoded','Accept':'*/*'}\\r\",\"            data={'grant_type': 'password','username': 'admin','password': 'infy@123','scope': 'openid','client_id': 'ang-app','client_secret': ''}\\r\",\"            requests.packages.urllib3.disable_warnings()\\r\",\"            keycloakResponse = requests.post(keycloakurl, headers=tokenHeaders ,data=data,verify=False,proxies={'http':'','https':''})\\r\",\"            jwttoken = keycloakResponse.json()['access_token']\\r\",\"            #print(keycloakResponse.json())\\r\",\"            return jwttoken\\r\",\"        \\r\",\"        JWTToken = get_token()\\r\",\"        head = {'Authorization': 'Bearer {}'.format(JWTToken)}\\r\",\"        params=transformedData.select('id','job_name','start_time','predicted_status').toJSON().map(lambda j: json.loads(j)).collect()\\r\",\"        for j in params:\\r\",\"            j['prediction'] = {'predicted_status':j['predicted_status']}\\r\",\"            \\r\",\"        url= 'https://leap/api/event/trigger/update_forecast?org=NextGenAcme&param='+str({'params':params})\\r\",\"        print(url)\\r\",\"        try:\\r\",\"            resp = requests.get(url=url,headers=head,verify=False,proxies={'http':'','https':''})\\r\",\"            if(resp.status_code!=200):\\r\",\"                print(resp.status_code)\\r\",\"                exit()\\r\",\"        except Exception as e:\\r\",\"            print(e)\\r\",\"       \\r\",\"        return data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},{\"modelName\":\"JobStatusPrediction\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"hNPFF\",\"alias\":\"Model  Source\",\"name\":\"Model  Source\",\"classname\":\"ModelSourceConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"modelName\":\"JobStatusPrediction\"},\"position_x\":\"450\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"gFJOt\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"tBouL\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"modelName\":\"text\"},\"context\":[{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"gFJOt\",\"alias\":\"Post  Processing  Script\",\"name\":\"Post  Processing  Script\",\"classname\":\"PostProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"from pyspark.ml.feature import IndexToString\\r\",\"import requests\\r\",\"import urllib\\r\",\"import json\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, model,dataset):\\r\",\"       \\r\",\"        indexer = model.stages[8]\\r\",\"        \\r\",\"        labeler = IndexToString(inputCol='prediction', outputCol='predicted_status', labels=indexer.labels)\\r\",\"        transformedData = labeler.transform(dataset)\\r\",\"       \\r\",\"        transformedData = transformedData.withColumn('ji_group', col('jiGroup'))\\r\",\"        transformedData = transformedData.withColumn('api_url', col('apiUrl'))\\r\",\"        transformedData = transformedData.withColumn('execution_type', col('executionType'))\\r\",\"        transformedData = transformedData.withColumn('rundeck_group', col('rundeckGroup'))\\r\",\"        transformedData = transformedData.withColumn('job_name', col('jobName'))\\r\",\"        transformedData = transformedData.withColumn('start_time', col('startTime'))\\r\",\"        # transformedData = transformedData.withColumn('end_time', col('endTime'))\\r\",\"        transformedData = transformedData.withColumn('running_job_instance_id', col('jobInstanceId'))\\r\",\"        transformedData = transformedData.withColumn('ticket', col('ticket'))\\r\",\"        transformedData = transformedData.withColumn('server', col('server'))\\r\",\"        \\r\",\"        data = transformedData.select('user','ji_group','ticket','api_url','execution_type','rundeck_group','job_name','server','start_time','predicted_status','running_job_instance_id')\\r\",\"       \\r\",\"        #Get token\\r\",\"        def get_token():\\r\",\"            keycloakurl ='https://myzul02o:8443/auth/realms/IAMP/protocol/openid-connect/token'\\r\",\"            tokenHeaders = {'Content-Type':'application/x-www-form-urlencoded','Accept':'*/*'}\\r\",\"            data={'grant_type': 'password','username': 'admin','password': 'infy@123','scope': 'openid','client_id': 'ang-app','client_secret': ''}\\r\",\"            requests.packages.urllib3.disable_warnings()\\r\",\"            keycloakResponse = requests.post(keycloakurl, headers=tokenHeaders ,data=data,verify=False,proxies={'http':'','https':''})\\r\",\"            jwttoken = keycloakResponse.json()['access_token']\\r\",\"            #print(keycloakResponse.json())\\r\",\"            return jwttoken\\r\",\"        \\r\",\"        JWTToken = get_token()\\r\",\"        head = {'Authorization': 'Bearer {}'.format(JWTToken)}\\r\",\"        params=transformedData.select('id','job_name','start_time','predicted_status').toJSON().map(lambda j: json.loads(j)).collect()\\r\",\"        for j in params:\\r\",\"            j['prediction'] = {'predicted_status':j['predicted_status']}\\r\",\"            \\r\",\"        url= 'https://leap/api/event/trigger/update_forecast?org=NextGenAcme&param='+str({'params':params})\\r\",\"        print(url)\\r\",\"        try:\\r\",\"            resp = requests.get(url=url,headers=head,verify=False,proxies={'http':'','https':''})\\r\",\"            if(resp.status_code!=200):\\r\",\"                print(resp.status_code)\\r\",\"                exit()\\r\",\"        except Exception as e:\\r\",\"            print(e)\\r\",\"       \\r\",\"        return data\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"]},\"position_x\":\"670\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"hNPFF\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"spKJZ\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\",\"out2\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\"},\"context\":[{\"modelName\":\"JobStatusPrediction\"},{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"tBouL\",\"alias\":\"Pre  Processing  Script\",\"name\":\"Pre  Processing  Script\",\"classname\":\"PreProcessingScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"params\":\"\",\"script\":[\"import logging as logger\\r\",\"from pyspark.sql.functions import *\\r\",\"from pyspark.sql.types import *\\r\",\"class CustomPythonClass():\\r\",\"    def __main__(self, dataset):\\r\",\"    \\r\",\"        print(dataset.show())\\r\",\"        \\r\",\"        dataset= dataset.withColumn('year_start', year(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('month_start', month(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('date_start', dayofmonth(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('dayofweek_start', dayofweek(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('hour_start', hour(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('minute_start', minute(dataset.startTime))\\r\",\"        dataset = dataset.withColumn('second_start', second(dataset.startTime))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('year_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('month_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('date_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('dayofweek_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('hour_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('minute_end', lit(None).cast(IntegerType()))\\r\",\"        dataset = dataset.withColumn('second_end', lit(None).cast(IntegerType()))\\r\",\"        \\r\",\"        dataset = dataset.withColumn('jiGroup', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('user', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('executionType', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('ticket', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('jobInstanceId', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('apiUrl', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('status', lit(None).cast(StringType()))\\r\",\"        dataset = dataset.withColumn('sourceName', lit(None).cast(StringType()))\\r\",\"        \\r\",\"        \\r\",\"        print('Seperator2')\\r\",\"        \\r\",\"        \\r\",\"        \\r\",\"        return dataset\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\"],\"outputCols\":\"id,jobName,server,startTime,year_start,month_start,date_start,dayofweek_start,hour_start,minute_start,second_start,user,execuionType,rundeckGroup,ticket,apiUrl,jobInstanceId,year_end,month_end,date_end,dayofweek_end,hour_end,minute_end,second_end,status\"},\"position_x\":\"240\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"hNPFF\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"VlAMK\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"params\":\"textarea\",\"script\":\"textarea\",\"outputCols\":\"text\"},\"context\":[{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false}]},{\"id\":\"VlAMK\",\"alias\":\"Dataset  Extractor\",\"name\":\"Dataset  Extractor\",\"classname\":\"DatasetExtractorConfig\",\"category\":\"ExtractorConfig\",\"attributes\":{\"dataset\":{\"name\":\"ForecastDataForStatusPrediction\",\"description\":\"ForecastDataForStatusPrediction\",\"schema\":{\"id\":10094,\"name\":\"bjm\",\"schemavalue\":\"[]\",\"organization\":\"NextGenAcme\"},\"type\":\"r\",\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT id, job_name as 'jobName',status,scheduled_time AS 'startTime',server,group_name AS 'rundeckGroup' FROM bjm_job_forecast WHERE predicted_status IS NULL LIMIT 10\",\"Cacheable\":\"\",\"isStreaming\":\"false\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"\",\"defaultValues\":\"\",\"uniqueIdentifier\":\"\"},\"expStatus\":0,\"backingDataset\":\"\",\"organization\":\"NextGenAcme\",\"alias\":\"ForecastDataForStatusPrediction\",\"groups\":[]},\"isValidation\":\"\",\"samplingRatio\":\"\",\"applySchema\":false},\"position_x\":\"40\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"tBouL\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[\"out\"],\"requiredJars\":[],\"formats\":{\"dataset\":\"dropdown\",\"isValidation\":\"checkbox\",\"samplingRatio\":\"text\",\"applySchema\":\"checkbox\"},\"context\":[]}]}","admin","Job Status Train","2023-10-16T10:41:01","LEOJBSTS71894","leo1311","DragAndDrop","NULL","NULL","pipeline","NULL"
"admin","2023-10-17T07:09:43.335","false","Use SageMaker  to train a DeepAR model and deploy it for forecasting","NULL","{\"elements\":[{\"id\":\"HxyGW\",\"alias\":\"PrepareData\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas==1.4.3,zipfile,boto3,matplotlib,sagemaker\",\"params\":[],\"script\":[\"\\rimport zipfile\\rimport json\\rimport random\\rimport os\\rimport boto3\\r#import s3fs\\rimport sagemaker\\rimport numpy as np\\rimport pandas as pd\\rimport matplotlib.pyplot as plt\\rfrom datetime import timedelta\\r\\r\\rdef prepare_data( ):    #python-script Data\\r    np.random.seed(42)\\r    random.seed(42)\\r    session = boto3.Session( aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    sagemaker_session =  sagemaker.Session(boto_session=session)\\r    \\r    role = 'aiplat'\\r    # role = sagemaker.get_execution_role()\\r    \\r    region = sagemaker_session.boto_region_name\\r    \\r    DATA_HOST = f'sagemaker-example-files-prod-{region}'\\r    DATA_PATH = 'datasets/timeseries/uci_electricity/'\\r    ARCHIVE_NAME = 'LD2011_2014.txt.zip'\\r    FILE_NAME = ARCHIVE_NAME[:-4]\\r    \\r    s3_client = boto3.client('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r            aws_secret_access_key='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',\\r            region_name = 'us-east-1')\\r    \\r    if not os.path.isfile(FILE_NAME):\\r        print('downloading dataset (258MB), can take a few minutes depending on your connection')\\r        s3_client.download_file(DATA_HOST, DATA_PATH + ARCHIVE_NAME, ARCHIVE_NAME)\\r    \\r        print('extracting data archive')\\r        zip_ref = zipfile.ZipFile(ARCHIVE_NAME, 'r')\\r        zip_ref.extractall('./')\\r        zip_ref.close()\\r    else:\\r        print('File found skipping download')\\r    \\r    data = pd.read_csv(FILE_NAME, sep=';', index_col=0, parse_dates=True, decimal=',')\\r    num_timeseries = data.shape[1]\\r    data_kw = data.resample('2H').sum() / 8\\r    timeseries = []\\r    for i in range(num_timeseries):\\r        timeseries.append(np.trim_zeros(data_kw.iloc[:, i], trim='f'))\\r    \\r    fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\\r    axx = axs.ravel()\\r    for i in range(0, 10):\\r        timeseries[i].loc['2014-01-01':'2014-01-14'].plot(ax=axx[i])\\r        axx[i].set_xlabel('date')\\r        axx[i].set_ylabel('kW consumption')\\r        axx[i].grid(which='minor', axis='x')\\r\\r    return timeseries\"]},\"position_x\":\"39\",\"position_y\":\"174\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"DDVIc\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"DDVIc\",\"alias\":\"Split Data\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"split\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef split( timeseries):    #python-script Data\\r    s3_bucket = 'aiplatdata1'  # replace with an existing bucket if needed\\r    s3_prefix = 'deepar-ticketsvolume-demo-notebook'  # prefix used for all data stored within the bucket\\r    \\r    freq = '2H'\\r    \\r    # we predict for 7 days\\r    prediction_length = 7 * 12\\r    \\r    # we also use 7 days as context length, this is the number of state updates accomplished before making predictions\\r    context_length = 7 * 12\\r    \\r    start_dataset = pd.Timestamp('2014-01-01 00:00:00', freq=freq)\\r    end_training = pd.Timestamp('2014-09-01 00:00:00', freq=freq)\\r    \\r    s3_data_path = 's3://{}/{}/data'.format(s3_bucket, s3_prefix)\\r    s3_output_path = 's3://{}/{}/output'.format(s3_bucket, s3_prefix)\\r    \\r    training_data = [\\r        {\\r            'start': str(start_dataset),\\r            'target': ts[\\r                start_dataset : end_training - timedelta(days=1)\\r            ].tolist(),  # We use -1, because pandas indexing includes the upper bound\\r        }\\r        for ts in timeseries\\r    ]\\r    print(len(training_data))\\r    \\r    num_test_windows = 4\\r    \\r    test_data = [\\r        {\\r            'start': str(start_dataset),\\r            'target': ts[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\\r        }\\r        for k in range(1, num_test_windows + 1)\\r        for ts in timeseries\\r    ]\\r    print(len(test_data))\\r    \\r    def write_dicts_to_file(path, data):\\r        with open(path, 'wb') as fp:\\r            for d in data:\\r                fp.write(json.dumps(d).encode('utf-8'))\\r                fp.write(''.encode('utf-8'))\\r    \\r    write_dicts_to_file('train.json', training_data)\\r    write_dicts_to_file('test.json', test_data)\\r    \\r    s3 = boto3.resource('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    \\r    def copy_to_s3(local_file, s3_path, override=False):\\r        assert s3_path.startswith('s3://')\\r        split = s3_path.split('/')\\r        bucket = split[2]\\r        path = '/'.join(split[3:])\\r        buk = s3.Bucket(bucket)\\r    \\r        if len(list(buk.objects.filter(Prefix=path))) > 0:\\r            if not override:\\r                print(\\r                    'File s3://{}/{} already exists.Set override to upload anyway.'.format(\\r                        s3_bucket, s3_path\\r                    )\\r                )\\r                return\\r            else:\\r                print('Overwriting existing file')\\r        with open(local_file, 'rb') as data:\\r            print('Uploading file to {}'.format(s3_path))\\r            buk.put_object(Key=path, Body=data)\\r    \\r    copy_to_s3('train.json', s3_data_path + '/train/train.json')\\r    copy_to_s3('test.json', s3_data_path + '/test/test.json')\\r    \\r    s3_sample = s3.Object(s3_bucket, s3_prefix + '/data/train/train.json').get()['Body'].read()\\r    StringVariable = s3_sample.decode('UTF-8', 'ignore')\\r    lines = StringVariable.split('\\r')\\r    print(lines[0][:100] + '...')\\r\\r    return 'Saved'\"]},\"position_x\":\"260\",\"position_y\":\"174\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"HxyGW\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"pokIb\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas==1.4.3,zipfile,boto3,matplotlib,sagemaker\",\"params\":[],\"script\":[\"\\rimport zipfile\\rimport json\\rimport random\\rimport os\\rimport boto3\\r#import s3fs\\rimport sagemaker\\rimport numpy as np\\rimport pandas as pd\\rimport matplotlib.pyplot as plt\\rfrom datetime import timedelta\\r\\r\\rdef prepare_data( ):    #python-script Data\\r    np.random.seed(42)\\r    random.seed(42)\\r    session = boto3.Session( aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    sagemaker_session =  sagemaker.Session(boto_session=session)\\r    \\r    role = 'aiplat'\\r    # role = sagemaker.get_execution_role()\\r    \\r    region = sagemaker_session.boto_region_name\\r    \\r    DATA_HOST = f'sagemaker-example-files-prod-{region}'\\r    DATA_PATH = 'datasets/timeseries/uci_electricity/'\\r    ARCHIVE_NAME = 'LD2011_2014.txt.zip'\\r    FILE_NAME = ARCHIVE_NAME[:-4]\\r    \\r    s3_client = boto3.client('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r            aws_secret_access_key='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',\\r            region_name = 'us-east-1')\\r    \\r    if not os.path.isfile(FILE_NAME):\\r        print('downloading dataset (258MB), can take a few minutes depending on your connection')\\r        s3_client.download_file(DATA_HOST, DATA_PATH + ARCHIVE_NAME, ARCHIVE_NAME)\\r    \\r        print('extracting data archive')\\r        zip_ref = zipfile.ZipFile(ARCHIVE_NAME, 'r')\\r        zip_ref.extractall('./')\\r        zip_ref.close()\\r    else:\\r        print('File found skipping download')\\r    \\r    data = pd.read_csv(FILE_NAME, sep=';', index_col=0, parse_dates=True, decimal=',')\\r    num_timeseries = data.shape[1]\\r    data_kw = data.resample('2H').sum() / 8\\r    timeseries = []\\r    for i in range(num_timeseries):\\r        timeseries.append(np.trim_zeros(data_kw.iloc[:, i], trim='f'))\\r    \\r    fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\\r    axx = axs.ravel()\\r    for i in range(0, 10):\\r        timeseries[i].loc['2014-01-01':'2014-01-14'].plot(ax=axx[i])\\r        axx[i].set_xlabel('date')\\r        axx[i].set_ylabel('kW consumption')\\r        axx[i].grid(which='minor', axis='x')\\r\\r    return timeseries\"]}]},{\"id\":\"pokIb\",\"alias\":\"Train  Model\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"train\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef train( dataset):    #python-script Data\\r    session = boto3.Session(aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r                            aws_secret_access_key='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',\\r                            region_name='us-east-1')\\r    sagemaker_session =  sagemaker.Session(boto_session=session)\\r    region = sagemaker_session.boto_region_name\\r    role = 'aiplat'\\r    image_name = sagemaker.image_uris.retrieve('forecasting-deepar', region)\\r    s3_bucket = 'aiplatdata1'  # replace with an existing bucket if needed\\r    s3_prefix = 'deepar-ticketsvolume-demo-notebook'  # prefix used for all data stored within the bucket\\r    s3_data_path = 's3://{}/{}/data'.format(s3_bucket, s3_prefix)\\r    s3_output_path = 's3://{}/{}/output'.format(s3_bucket, s3_prefix)\\r    estimator = sagemaker.estimator.Estimator(\\r        image_uri=image_name,\\r        sagemaker_session=sagemaker_session,\\r        role=role,\\r        train_instance_count=1,\\r        train_instance_type='ml.c4.2xlarge',\\r        base_job_name='deepar-ticketsvolume-demo',\\r        output_path=s3_output_path,\\r    )\\r    freq = '2H'\\r    # we predict for 7 days\\r    prediction_length = 7 * 12\\r    # we also use 7 days as context length, this is the number of state updates accomplished before making predictions\\r    context_length = 7 * 12\\r    hyperparameters = {\\r        'time_freq': freq,\\r        'epochs': '400',\\r        'early_stopping_patience': '40',\\r        'mini_batch_size': '64',\\r        'learning_rate': '5E-4',\\r        'context_length': str(context_length),\\r        'prediction_length': str(prediction_length),\\r    }\\r    \\r    estimator.set_hyperparameters(**hyperparameters)\\r    \\r    data_channels = {'train': '{}/train/'.format(s3_data_path), 'test': '{}/test/'.format(s3_data_path)}\\r    \\r    estimator.fit(inputs=data_channels, wait=True)\\r\\r    return estimator\"]},\"position_x\":\"455\",\"position_y\":\"173\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"DDVIc\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"pcayl\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"split\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef split( timeseries):    #python-script Data\\r    s3_bucket = 'aiplatdata1'  # replace with an existing bucket if needed\\r    s3_prefix = 'deepar-ticketsvolume-demo-notebook'  # prefix used for all data stored within the bucket\\r    \\r    freq = '2H'\\r    \\r    # we predict for 7 days\\r    prediction_length = 7 * 12\\r    \\r    # we also use 7 days as context length, this is the number of state updates accomplished before making predictions\\r    context_length = 7 * 12\\r    \\r    start_dataset = pd.Timestamp('2014-01-01 00:00:00', freq=freq)\\r    end_training = pd.Timestamp('2014-09-01 00:00:00', freq=freq)\\r    \\r    s3_data_path = 's3://{}/{}/data'.format(s3_bucket, s3_prefix)\\r    s3_output_path = 's3://{}/{}/output'.format(s3_bucket, s3_prefix)\\r    \\r    training_data = [\\r        {\\r            'start': str(start_dataset),\\r            'target': ts[\\r                start_dataset : end_training - timedelta(days=1)\\r            ].tolist(),  # We use -1, because pandas indexing includes the upper bound\\r        }\\r        for ts in timeseries\\r    ]\\r    print(len(training_data))\\r    \\r    num_test_windows = 4\\r    \\r    test_data = [\\r        {\\r            'start': str(start_dataset),\\r            'target': ts[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\\r        }\\r        for k in range(1, num_test_windows + 1)\\r        for ts in timeseries\\r    ]\\r    print(len(test_data))\\r    \\r    def write_dicts_to_file(path, data):\\r        with open(path, 'wb') as fp:\\r            for d in data:\\r                fp.write(json.dumps(d).encode('utf-8'))\\r                fp.write(''.encode('utf-8'))\\r    \\r    write_dicts_to_file('train.json', training_data)\\r    write_dicts_to_file('test.json', test_data)\\r    \\r    s3 = boto3.resource('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    \\r    def copy_to_s3(local_file, s3_path, override=False):\\r        assert s3_path.startswith('s3://')\\r        split = s3_path.split('/')\\r        bucket = split[2]\\r        path = '/'.join(split[3:])\\r        buk = s3.Bucket(bucket)\\r    \\r        if len(list(buk.objects.filter(Prefix=path))) > 0:\\r            if not override:\\r                print(\\r                    'File s3://{}/{} already exists.Set override to upload anyway.'.format(\\r                        s3_bucket, s3_path\\r                    )\\r                )\\r                return\\r            else:\\r                print('Overwriting existing file')\\r        with open(local_file, 'rb') as data:\\r            print('Uploading file to {}'.format(s3_path))\\r            buk.put_object(Key=path, Body=data)\\r    \\r    copy_to_s3('train.json', s3_data_path + '/train/train.json')\\r    copy_to_s3('test.json', s3_data_path + '/test/test.json')\\r    \\r    s3_sample = s3.Object(s3_bucket, s3_prefix + '/data/train/train.json').get()['Body'].read()\\r    StringVariable = s3_sample.decode('UTF-8', 'ignore')\\r    lines = StringVariable.split('\\r')\\r    print(lines[0][:100] + '...')\\r\\r    return 'Saved'\"]},{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas==1.4.3,zipfile,boto3,matplotlib,sagemaker\",\"params\":[],\"script\":[\"\\rimport zipfile\\rimport json\\rimport random\\rimport os\\rimport boto3\\r#import s3fs\\rimport sagemaker\\rimport numpy as np\\rimport pandas as pd\\rimport matplotlib.pyplot as plt\\rfrom datetime import timedelta\\r\\r\\rdef prepare_data( ):    #python-script Data\\r    np.random.seed(42)\\r    random.seed(42)\\r    session = boto3.Session( aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    sagemaker_session =  sagemaker.Session(boto_session=session)\\r    \\r    role = 'aiplat'\\r    # role = sagemaker.get_execution_role()\\r    \\r    region = sagemaker_session.boto_region_name\\r    \\r    DATA_HOST = f'sagemaker-example-files-prod-{region}'\\r    DATA_PATH = 'datasets/timeseries/uci_electricity/'\\r    ARCHIVE_NAME = 'LD2011_2014.txt.zip'\\r    FILE_NAME = ARCHIVE_NAME[:-4]\\r    \\r    s3_client = boto3.client('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r            aws_secret_access_key='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',\\r            region_name = 'us-east-1')\\r    \\r    if not os.path.isfile(FILE_NAME):\\r        print('downloading dataset (258MB), can take a few minutes depending on your connection')\\r        s3_client.download_file(DATA_HOST, DATA_PATH + ARCHIVE_NAME, ARCHIVE_NAME)\\r    \\r        print('extracting data archive')\\r        zip_ref = zipfile.ZipFile(ARCHIVE_NAME, 'r')\\r        zip_ref.extractall('./')\\r        zip_ref.close()\\r    else:\\r        print('File found skipping download')\\r    \\r    data = pd.read_csv(FILE_NAME, sep=';', index_col=0, parse_dates=True, decimal=',')\\r    num_timeseries = data.shape[1]\\r    data_kw = data.resample('2H').sum() / 8\\r    timeseries = []\\r    for i in range(num_timeseries):\\r        timeseries.append(np.trim_zeros(data_kw.iloc[:, i], trim='f'))\\r    \\r    fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\\r    axx = axs.ravel()\\r    for i in range(0, 10):\\r        timeseries[i].loc['2014-01-01':'2014-01-14'].plot(ax=axx[i])\\r        axx[i].set_xlabel('date')\\r        axx[i].set_ylabel('kW consumption')\\r        axx[i].grid(which='minor', axis='x')\\r\\r    return timeseries\"]}]},{\"id\":\"pcayl\",\"alias\":\"Deploy\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"deploy\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef deploy( estimator):    #python-script Data\\r    from sagemaker.serializers import IdentitySerializer\\r    \\r    class DeepARPredictor(sagemaker.predictor.Predictor):\\r        def __init__(self, *args, **kwargs):\\r            super().__init__(\\r                *args,\\r                # serializer=JSONSerializer(),\\r                serializer=IdentitySerializer(content_type='application/json'),\\r                **kwargs,\\r            )\\r    \\r        def predict(\\r            self,\\r            ts,\\r            cat=None,\\r            dynamic_feat=None,\\r            num_samples=100,\\r            return_samples=False,\\r            quantiles=['0.1', '0.5', '0.9'],\\r        ):\\r            '''Requests the prediction of for the time series listed in `ts`, each with the (optional)\\r            corresponding category listed in `cat`.\\r    \\r            ts -- `pandas.Series` object, the time series to predict\\r            cat -- integer, the group associated to the time series (default: None)\\r            num_samples -- integer, number of samples to compute at prediction time (default: 100)\\r            return_samples -- boolean indicating whether to include samples in the response (default: False)\\r            quantiles -- list of strings specifying the quantiles to compute (default: ['0.1', '0.5', '0.9'])\\r    \\r            Return value: list of `pandas.DataFrame` objects, each containing the predictions\\r            '''\\r            prediction_time = ts.index[-1] + ts.index.freq\\r            quantiles = [str(q) for q in quantiles]\\r            req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\\r            res = super(DeepARPredictor, self).predict(req)\\r            return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\\r    \\r        def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\\r            instance = series_to_dict(\\r                ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\\r            )\\r    \\r            configuration = {\\r                'num_samples': num_samples,\\r                'output_types': ['quantiles', 'samples'] if return_samples else ['quantiles'],\\r                'quantiles': quantiles,\\r            }\\r    \\r            http_request_data = {'instances': [instance], 'configuration': configuration}\\r    \\r            return json.dumps(http_request_data).encode('utf-8')\\r    \\r        def __decode_response(self, response, freq, prediction_time, return_samples):\\r            # we only sent one time series so we only receive one in return\\r            # however, if possible one will pass multiple time series as predictions will then be faster\\r            predictions = json.loads(response.decode('utf-8'))['predictions'][0]\\r            prediction_length = len(next(iter(predictions['quantiles'].values())))\\r            prediction_index = pd.date_range(\\r                start=prediction_time, freq=freq, periods=prediction_length\\r            )\\r            if return_samples:\\r                dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\\r            else:\\r                dict_of_samples = {}\\r            return pd.DataFrame(\\r                data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index\\r            )\\r    \\r        def set_frequency(self, freq):\\r            self.freq = freq\\r    \\r    \\r    def encode_target(ts):\\r        return [x if np.isfinite(x) else 'NaN' for x in ts]\\r    \\r    \\r    def series_to_dict(ts, cat=None, dynamic_feat=None):\\r        '''Given a pandas.Series object, returns a dictionary encoding the time series.\\r    \\r        ts -- a pands.Series object with the target time series\\r        cat -- an integer indicating the time series category\\r    \\r        Return value: a dictionary\\r        '''\\r        obj = {'start': str(ts.index[0]), 'target': encode_target(ts)}\\r        if cat is not None:\\r            obj['cat'] = cat\\r        if dynamic_feat is not None:\\r            obj['dynamic_feat'] = dynamic_feat\\r        return obj\\r    \\r    predictor = estimator.deploy(\\r        initial_instance_count=1, instance_type='ml.m5.large', predictor_cls=DeepARPredictor\\r    )\\r    print(predictor.endpoint_name)\\r    \"]},\"position_x\":\"645\",\"position_y\":\"174\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"pokIb\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"train\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef train( dataset):    #python-script Data\\r    session = boto3.Session(aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r                            aws_secret_access_key='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',\\r                            region_name='us-east-1')\\r    sagemaker_session =  sagemaker.Session(boto_session=session)\\r    region = sagemaker_session.boto_region_name\\r    role = 'aiplat'\\r    image_name = sagemaker.image_uris.retrieve('forecasting-deepar', region)\\r    s3_bucket = 'aiplatdata1'  # replace with an existing bucket if needed\\r    s3_prefix = 'deepar-ticketsvolume-demo-notebook'  # prefix used for all data stored within the bucket\\r    s3_data_path = 's3://{}/{}/data'.format(s3_bucket, s3_prefix)\\r    s3_output_path = 's3://{}/{}/output'.format(s3_bucket, s3_prefix)\\r    estimator = sagemaker.estimator.Estimator(\\r        image_uri=image_name,\\r        sagemaker_session=sagemaker_session,\\r        role=role,\\r        train_instance_count=1,\\r        train_instance_type='ml.c4.2xlarge',\\r        base_job_name='deepar-ticketsvolume-demo',\\r        output_path=s3_output_path,\\r    )\\r    freq = '2H'\\r    # we predict for 7 days\\r    prediction_length = 7 * 12\\r    # we also use 7 days as context length, this is the number of state updates accomplished before making predictions\\r    context_length = 7 * 12\\r    hyperparameters = {\\r        'time_freq': freq,\\r        'epochs': '400',\\r        'early_stopping_patience': '40',\\r        'mini_batch_size': '64',\\r        'learning_rate': '5E-4',\\r        'context_length': str(context_length),\\r        'prediction_length': str(prediction_length),\\r    }\\r    \\r    estimator.set_hyperparameters(**hyperparameters)\\r    \\r    data_channels = {'train': '{}/train/'.format(s3_data_path), 'test': '{}/test/'.format(s3_data_path)}\\r    \\r    estimator.fit(inputs=data_channels, wait=True)\\r\\r    return estimator\"]},{\"FunctionName\":\"split\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef split( timeseries):    #python-script Data\\r    s3_bucket = 'aiplatdata1'  # replace with an existing bucket if needed\\r    s3_prefix = 'deepar-ticketsvolume-demo-notebook'  # prefix used for all data stored within the bucket\\r    \\r    freq = '2H'\\r    \\r    # we predict for 7 days\\r    prediction_length = 7 * 12\\r    \\r    # we also use 7 days as context length, this is the number of state updates accomplished before making predictions\\r    context_length = 7 * 12\\r    \\r    start_dataset = pd.Timestamp('2014-01-01 00:00:00', freq=freq)\\r    end_training = pd.Timestamp('2014-09-01 00:00:00', freq=freq)\\r    \\r    s3_data_path = 's3://{}/{}/data'.format(s3_bucket, s3_prefix)\\r    s3_output_path = 's3://{}/{}/output'.format(s3_bucket, s3_prefix)\\r    \\r    training_data = [\\r        {\\r            'start': str(start_dataset),\\r            'target': ts[\\r                start_dataset : end_training - timedelta(days=1)\\r            ].tolist(),  # We use -1, because pandas indexing includes the upper bound\\r        }\\r        for ts in timeseries\\r    ]\\r    print(len(training_data))\\r    \\r    num_test_windows = 4\\r    \\r    test_data = [\\r        {\\r            'start': str(start_dataset),\\r            'target': ts[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\\r        }\\r        for k in range(1, num_test_windows + 1)\\r        for ts in timeseries\\r    ]\\r    print(len(test_data))\\r    \\r    def write_dicts_to_file(path, data):\\r        with open(path, 'wb') as fp:\\r            for d in data:\\r                fp.write(json.dumps(d).encode('utf-8'))\\r                fp.write(''.encode('utf-8'))\\r    \\r    write_dicts_to_file('train.json', training_data)\\r    write_dicts_to_file('test.json', test_data)\\r    \\r    s3 = boto3.resource('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    \\r    def copy_to_s3(local_file, s3_path, override=False):\\r        assert s3_path.startswith('s3://')\\r        split = s3_path.split('/')\\r        bucket = split[2]\\r        path = '/'.join(split[3:])\\r        buk = s3.Bucket(bucket)\\r    \\r        if len(list(buk.objects.filter(Prefix=path))) > 0:\\r            if not override:\\r                print(\\r                    'File s3://{}/{} already exists.Set override to upload anyway.'.format(\\r                        s3_bucket, s3_path\\r                    )\\r                )\\r                return\\r            else:\\r                print('Overwriting existing file')\\r        with open(local_file, 'rb') as data:\\r            print('Uploading file to {}'.format(s3_path))\\r            buk.put_object(Key=path, Body=data)\\r    \\r    copy_to_s3('train.json', s3_data_path + '/train/train.json')\\r    copy_to_s3('test.json', s3_data_path + '/test/test.json')\\r    \\r    s3_sample = s3.Object(s3_bucket, s3_prefix + '/data/train/train.json').get()['Body'].read()\\r    StringVariable = s3_sample.decode('UTF-8', 'ignore')\\r    lines = StringVariable.split('\\r')\\r    print(lines[0][:100] + '...')\\r\\r    return 'Saved'\"]},{\"FunctionName\":\"prepare_data\",\"requirements\":\"pandas==1.4.3,zipfile,boto3,matplotlib,sagemaker\",\"params\":[],\"script\":[\"\\rimport zipfile\\rimport json\\rimport random\\rimport os\\rimport boto3\\r#import s3fs\\rimport sagemaker\\rimport numpy as np\\rimport pandas as pd\\rimport matplotlib.pyplot as plt\\rfrom datetime import timedelta\\r\\r\\rdef prepare_data( ):    #python-script Data\\r    np.random.seed(42)\\r    random.seed(42)\\r    session = boto3.Session( aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r        aws_secret_access_key='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',\\r        region_name = 'us-east-1')\\r    \\r    sagemaker_session =  sagemaker.Session(boto_session=session)\\r    \\r    role = 'aiplat'\\r    # role = sagemaker.get_execution_role()\\r    \\r    region = sagemaker_session.boto_region_name\\r    \\r    DATA_HOST = f'sagemaker-example-files-prod-{region}'\\r    DATA_PATH = 'datasets/timeseries/uci_electricity/'\\r    ARCHIVE_NAME = 'LD2011_2014.txt.zip'\\r    FILE_NAME = ARCHIVE_NAME[:-4]\\r    \\r    s3_client = boto3.client('s3',aws_access_key_id='AKIAWSEIAMU6H5SKF2E2',\\r            aws_secret_access_key='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',\\r            region_name = 'us-east-1')\\r    \\r    if not os.path.isfile(FILE_NAME):\\r        print('downloading dataset (258MB), can take a few minutes depending on your connection')\\r        s3_client.download_file(DATA_HOST, DATA_PATH + ARCHIVE_NAME, ARCHIVE_NAME)\\r    \\r        print('extracting data archive')\\r        zip_ref = zipfile.ZipFile(ARCHIVE_NAME, 'r')\\r        zip_ref.extractall('./')\\r        zip_ref.close()\\r    else:\\r        print('File found skipping download')\\r    \\r    data = pd.read_csv(FILE_NAME, sep=';', index_col=0, parse_dates=True, decimal=',')\\r    num_timeseries = data.shape[1]\\r    data_kw = data.resample('2H').sum() / 8\\r    timeseries = []\\r    for i in range(num_timeseries):\\r        timeseries.append(np.trim_zeros(data_kw.iloc[:, i], trim='f'))\\r    \\r    fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\\r    axx = axs.ravel()\\r    for i in range(0, 10):\\r        timeseries[i].loc['2014-01-01':'2014-01-14'].plot(ax=axx[i])\\r        axx[i].set_xlabel('date')\\r        axx[i].set_ylabel('kW consumption')\\r        axx[i].grid(which='minor', axis='x')\\r\\r    return timeseries\"]}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}]}","admin","Forecasting-DeepAR Training","2023-10-18T04:51:57","LEOFRCST52331","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"61\":{\"taskId\":\"c9934cb5-cf19-4d90-9605-f6f3d9f0ffe6\"}}"
"admin","2023-10-17T09:39:01.313","false","Predict using trained deep AR model","NULL","{\"elements\":[{\"id\":\"TzZZi\",\"alias\":\"PrepareData\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"prepare_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef prepare_data( ):    #python-script Data\\r\\r    return dataset\"]},\"position_x\":\"219\",\"position_y\":\"128\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"qwPbz\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"qwPbz\",\"alias\":\"Predict\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"predict\",\"requirements\":\"\",\"params\":[{\"name\":\"EndpointName\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"}],\"script\":[\"\\rdef predict( dataset):    #python-script Data\\r    predictor = Predictor('deepar-electricity-demo-2023-10-17-09-36-50-558', sagemaker_session=sagemaker_session)\\r    print(dataset[120])\\r    inference_response = predictor.predict(ts=dataset[120], quantiles=[0.10, 0.5, 0.90])\\r    return dataset\"]},\"position_x\":\"489\",\"position_y\":\"124\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"TzZZi\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"}}]}","admin","Forecast- DeepAR Predict","2023-10-17T12:28:43","LEOFRCST30361","leo1311","DragNDropLite","NULL","NULL","pipeline","NULL"
"admin","2023-10-18T05:17:49.038","false","","NULL","{\"elements\":[{\"id\":\"MUXac\",\"alias\":\"SetupDataset\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"SetupDataset\",\"requirements\":\"boto3\",\"params\":[],\"script\":[\"import os\\r\\\\n\",\"import shutil\\r\\\\n\",\"import zipfile\\r\\\\n\",\"import boto3\\r\\\\n\",\"import sys\\r\\\\n\",\"import time\\r\\\\n\",\"\\r\\\\n\",\"role_name = 'aiplat'\\r\\\\n\",\"role_arn = 'arn:aws:iam::451256804668:role/aiplat'\\r\\\\n\",\"region = 'us-east-1'\\r\\\\n\",\"bucket_name == 'aiplatdata1'\\r\\\\n\",\"\\r\\\\n\",\"def SetupDataset():\\r\\\\n\",\"    return True\\r\\\\n\"]},\"position_x\":\"32\",\"position_y\":\"64\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"zLmkX\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"zLmkX\",\"alias\":\"anomalydetector\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"anomalydetector\",\"requirements\":\"\",\"params\":[{\"name\":\"project_name\",\"value\":\"initial-lookoutmetrics-backtesting-test1\",\"type\":\"Text\",\"alias\":\"initial-lookoutmetrics-backtesting-test1\",\"index\":\"1\"},{\"name\":\"frequency\",\"value\":\"PT1H\",\"type\":\"Text\",\"alias\":\"PT1H\",\"index\":\"2\"}],\"script\":[\"def anomalydetector(data,project_name_param,frequency_param):\\r\\\\n\",\"    global anomaly_detector_arn,L4M\\r\\\\n\",\"    anomaly_detector_arn = None\\r\\\\n\",\"    bucket = 'lookoutmetrics'\\r\\\\n\",\"    frequency = 'PT1H'\\r\\\\n\",\"    L4M = boto3.client(bucket, aws_access_key_id='AKIAWSEIAMU6H5SKF2E2', aws_secret_access_key='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',region_name = 'us-east-1')\\r\\\\n\",\"    project = project_name_param\\r\\\\n\",\"    if project_name_param == 'initial-lookoutmetrics-backtesting-test1':\\r\\\\n\",\"        anomaly_detector_arn = 'arn:aws:lookoutmetrics:us-east-1:451256804668:AnomalyDetector:initial-lookoutmetrics-backtesting-test-1-detector'\\r\\\\n\",\"    else:\\r\\\\n\",\"        frequency = 'PT1H' # one of 'P1D', 'PT1H', 'PT10M' and 'PT5M', this one means every one hour\\r\\\\n\",\"        response = L4M.create_anomaly_detector( \\r\\\\n\",\"        AnomalyDetectorName = project + '-detector',\\r\\\\n\",\"        AnomalyDetectorDescription = 'My Detector',\\r\\\\n\",\"        AnomalyDetectorConfig = {\\r\\\\n\",\"            'AnomalyDetectorFrequency' : 'PT1H',\\r\\\\n\",\"        },)\\r\\\\n\",\"        anomaly_detector_arn = response['AnomalyDetectorArn']\\r\\\\n\",\"\\r\\\\n\",\"    print('anomaly_detector_arn = ',anomaly_detector_arn)\\r\\\\n\",\"    metrics = defineMetrics(anomaly_detector_arn,project_name_param,frequency)\\r\\\\n\",\"    return metrics\"]},\"position_x\":\"236\",\"position_y\":\"64\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"MUXac\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"Ozfbi\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"SetupDataset\",\"requirements\":\"boto3\",\"params\":[],\"script\":[\"import os\\r\\\\n\",\"import shutil\\r\\\\n\",\"import zipfile\\r\\\\n\",\"import boto3\\r\\\\n\",\"import sys\\r\\\\n\",\"import time\\r\\\\n\",\"\\r\\\\n\",\"role_name = 'aiplat'\\r\\\\n\",\"role_arn = 'arn:aws:iam::451256804668:role/aiplat'\\r\\\\n\",\"region = 'us-east-1'\\r\\\\n\",\"bucket_name == 'aiplatdata1'\\r\\\\n\",\"\\r\\\\n\",\"def SetupDataset():\\r\\\\n\",\"    return True\\r\\\\n\"]}]},{\"id\":\"Ozfbi\",\"alias\":\"defineMetrics\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"defineMetrics\",\"requirements\":\"\",\"params\":[],\"script\":[\"def defineMetrics(metrics,project_name,frequency):\\r\\\\n\",\"    s3_path_backtest = 's3://'+ bucket_name + '/ecommerce/backtest/'\\r\\\\n\",\"    print('s3_path_backtest',s3_path_backtest)\\r\\\\n\",\"    params = {\\r\\\\n\",\"    'AnomalyDetectorArn': anomaly_detector_arn,\\r\\\\n\",\"    'MetricSetName' : project_name + '-metric-set-1',\\r\\\\n\",\"    'MetricList' : [\\r\\\\n\",\"        {\\r\\\\n\",\"            'MetricName' : 'views',\\r\\\\n\",\"            'AggregationFunction' : 'SUM',\\r\\\\n\",\"        },\\r\\\\n\",\"        {\\r\\\\n\",\"            'MetricName' : 'revenue',\\r\\\\n\",\"            'AggregationFunction' : 'SUM',\\r\\\\n\",\"        },\\r\\\\n\",\"    ],\\r\\\\n\",\"\\r\\\\n\",\"    'DimensionList' : [ 'platform', 'marketplace' ],\\r\\\\n\",\"\\r\\\\n\",\"    'TimestampColumn' : {\\r\\\\n\",\"        'ColumnName' : 'timestamp',\\r\\\\n\",\"        'ColumnFormat' : 'yyyy-MM-dd HH:mm:ss',\\r\\\\n\",\"    },\\r\\\\n\",\"    'MetricSetFrequency' : frequency,\\r\\\\n\",\"\\r\\\\n\",\"    'MetricSource' : {\\r\\\\n\",\"        'S3SourceConfig': {\\r\\\\n\",\"            'RoleArn' : role_arn,\\r\\\\n\",\"            'HistoricalDataPathList': [\\r\\\\n\",\"                s3_path_backtest,\\r\\\\n\",\"            ],\\r\\\\n\",\"            'FileFormatDescriptor' : {\\r\\\\n\",\"                'CsvFormatDescriptor' : {\\r\\\\n\",\"                    'FileCompression' : 'NONE',\\r\\\\n\",\"                    'Charset' : 'UTF-8',\\r\\\\n\",\"                    'ContainsHeader' : True,\\r\\\\n\",\"                    'Delimiter' : ',',\\r\\\\n\",\"                    'QuoteSymbol' : ''\\r\\\\n\",\"                },\\r\\\\n\",\"            }\\r\\\\n\",\"        }\\r\\\\n\",\"    },}\\r\\\\n\",\"    \\r\\\\n\",\"    response = L4M.create_metric_set( ** params )\\r\\\\n\",\"    print(response)\\r\\\\n\",\"    metric_set_arn = response['MetricSetArn']\\r\\\\n\",\"    return metric_set_arn\"]},\"position_x\":\"448\",\"position_y\":\"64\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"zLmkX\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"NYWBY\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"anomalydetector\",\"requirements\":\"\",\"params\":[{\"name\":\"project_name\",\"value\":\"initial-lookoutmetrics-backtesting-test1\",\"type\":\"Text\",\"alias\":\"initial-lookoutmetrics-backtesting-test1\",\"index\":\"1\"},{\"name\":\"frequency\",\"value\":\"PT1H\",\"type\":\"Text\",\"alias\":\"PT1H\",\"index\":\"2\"}],\"script\":[\"def anomalydetector(data,project_name_param,frequency_param):\\r\\\\n\",\"    global anomaly_detector_arn,L4M\\r\\\\n\",\"    anomaly_detector_arn = None\\r\\\\n\",\"    bucket = 'lookoutmetrics'\\r\\\\n\",\"    frequency = 'PT1H'\\r\\\\n\",\"    L4M = boto3.client(bucket, aws_access_key_id='AKIAWSEIAMU6H5SKF2E2', aws_secret_access_key='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',region_name = 'us-east-1')\\r\\\\n\",\"    project = project_name_param\\r\\\\n\",\"    if project_name_param == 'initial-lookoutmetrics-backtesting-test1':\\r\\\\n\",\"        anomaly_detector_arn = 'arn:aws:lookoutmetrics:us-east-1:451256804668:AnomalyDetector:initial-lookoutmetrics-backtesting-test-1-detector'\\r\\\\n\",\"    else:\\r\\\\n\",\"        frequency = 'PT1H' # one of 'P1D', 'PT1H', 'PT10M' and 'PT5M', this one means every one hour\\r\\\\n\",\"        response = L4M.create_anomaly_detector( \\r\\\\n\",\"        AnomalyDetectorName = project + '-detector',\\r\\\\n\",\"        AnomalyDetectorDescription = 'My Detector',\\r\\\\n\",\"        AnomalyDetectorConfig = {\\r\\\\n\",\"            'AnomalyDetectorFrequency' : 'PT1H',\\r\\\\n\",\"        },)\\r\\\\n\",\"        anomaly_detector_arn = response['AnomalyDetectorArn']\\r\\\\n\",\"\\r\\\\n\",\"    print('anomaly_detector_arn = ',anomaly_detector_arn)\\r\\\\n\",\"    metrics = defineMetrics(anomaly_detector_arn,project_name_param,frequency)\\r\\\\n\",\"    return metrics\"]},{\"FunctionName\":\"SetupDataset\",\"requirements\":\"boto3\",\"params\":[],\"script\":[\"import os\\r\\\\n\",\"import shutil\\r\\\\n\",\"import zipfile\\r\\\\n\",\"import boto3\\r\\\\n\",\"import sys\\r\\\\n\",\"import time\\r\\\\n\",\"\\r\\\\n\",\"role_name = 'aiplat'\\r\\\\n\",\"role_arn = 'arn:aws:iam::451256804668:role/aiplat'\\r\\\\n\",\"region = 'us-east-1'\\r\\\\n\",\"bucket_name == 'aiplatdata1'\\r\\\\n\",\"\\r\\\\n\",\"def SetupDataset():\\r\\\\n\",\"    return True\\r\\\\n\"]}]},{\"id\":\"NYWBY\",\"alias\":\"activateDetector\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"activateDetector\",\"requirements\":\"\",\"params\":[],\"script\":[\"def activateDetector(metric_set_arn):\\r\\\\n\",\"    L4M.back_test_anomaly_detector(AnomalyDetectorArn = anomaly_detector_arn)\\r\\\\n\",\"    prev_status = None\\r\\\\n\",\"    while True:\\r\\\\n\",\"\\r\\\\n\",\"        response = L4M.describe_anomaly_detector( AnomalyDetectorArn = anomaly_detector_arn )\\r\\\\n\",\"        status = response['Status']\\r\\\\n\",\"\\r\\\\n\",\"        if status != prev_status:\\r\\\\n\",\"            if prev_status:\\r\\\\n\",\"                sys.stdout.write('')\\r\\\\n\",\"            sys.stdout.write( status + ' ' )\\r\\\\n\",\"            sys.stdout.flush()\\r\\\\n\",\"            prev_status = status\\r\\\\n\",\"\\r\\\\n\",\"        if status in ( 'ACTIVATING', 'BACK_TEST_ACTIVATING', 'BACK_TEST_ACTIVE' ):\\r\\\\n\",\"            sys.stdout.write('.')\\r\\\\n\",\"            sys.stdout.flush()\\r\\\\n\",\"            time.sleep(5)\\r\\\\n\",\"            continue\\r\\\\n\",\"\\r\\\\n\",\"        break\\r\\\\n\",\"\\r\\\\n\",\"    return response\"]},\"position_x\":\"678\",\"position_y\":\"64\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"Ozfbi\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"NJwpY\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"defineMetrics\",\"requirements\":\"\",\"params\":[],\"script\":[\"def defineMetrics(metrics,project_name,frequency):\\r\\\\n\",\"    s3_path_backtest = 's3://'+ bucket_name + '/ecommerce/backtest/'\\r\\\\n\",\"    print('s3_path_backtest',s3_path_backtest)\\r\\\\n\",\"    params = {\\r\\\\n\",\"    'AnomalyDetectorArn': anomaly_detector_arn,\\r\\\\n\",\"    'MetricSetName' : project_name + '-metric-set-1',\\r\\\\n\",\"    'MetricList' : [\\r\\\\n\",\"        {\\r\\\\n\",\"            'MetricName' : 'views',\\r\\\\n\",\"            'AggregationFunction' : 'SUM',\\r\\\\n\",\"        },\\r\\\\n\",\"        {\\r\\\\n\",\"            'MetricName' : 'revenue',\\r\\\\n\",\"            'AggregationFunction' : 'SUM',\\r\\\\n\",\"        },\\r\\\\n\",\"    ],\\r\\\\n\",\"\\r\\\\n\",\"    'DimensionList' : [ 'platform', 'marketplace' ],\\r\\\\n\",\"\\r\\\\n\",\"    'TimestampColumn' : {\\r\\\\n\",\"        'ColumnName' : 'timestamp',\\r\\\\n\",\"        'ColumnFormat' : 'yyyy-MM-dd HH:mm:ss',\\r\\\\n\",\"    },\\r\\\\n\",\"    'MetricSetFrequency' : frequency,\\r\\\\n\",\"\\r\\\\n\",\"    'MetricSource' : {\\r\\\\n\",\"        'S3SourceConfig': {\\r\\\\n\",\"            'RoleArn' : role_arn,\\r\\\\n\",\"            'HistoricalDataPathList': [\\r\\\\n\",\"                s3_path_backtest,\\r\\\\n\",\"            ],\\r\\\\n\",\"            'FileFormatDescriptor' : {\\r\\\\n\",\"                'CsvFormatDescriptor' : {\\r\\\\n\",\"                    'FileCompression' : 'NONE',\\r\\\\n\",\"                    'Charset' : 'UTF-8',\\r\\\\n\",\"                    'ContainsHeader' : True,\\r\\\\n\",\"                    'Delimiter' : ',',\\r\\\\n\",\"                    'QuoteSymbol' : ''\\r\\\\n\",\"                },\\r\\\\n\",\"            }\\r\\\\n\",\"        }\\r\\\\n\",\"    },}\\r\\\\n\",\"    \\r\\\\n\",\"    response = L4M.create_metric_set( ** params )\\r\\\\n\",\"    print(response)\\r\\\\n\",\"    metric_set_arn = response['MetricSetArn']\\r\\\\n\",\"    return metric_set_arn\"]},{\"FunctionName\":\"anomalydetector\",\"requirements\":\"\",\"params\":[{\"name\":\"project_name\",\"value\":\"initial-lookoutmetrics-backtesting-test1\",\"type\":\"Text\",\"alias\":\"initial-lookoutmetrics-backtesting-test1\",\"index\":\"1\"},{\"name\":\"frequency\",\"value\":\"PT1H\",\"type\":\"Text\",\"alias\":\"PT1H\",\"index\":\"2\"}],\"script\":[\"def anomalydetector(data,project_name_param,frequency_param):\\r\\\\n\",\"    global anomaly_detector_arn,L4M\\r\\\\n\",\"    anomaly_detector_arn = None\\r\\\\n\",\"    bucket = 'lookoutmetrics'\\r\\\\n\",\"    frequency = 'PT1H'\\r\\\\n\",\"    L4M = boto3.client(bucket, aws_access_key_id='AKIAWSEIAMU6H5SKF2E2', aws_secret_access_key='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',region_name = 'us-east-1')\\r\\\\n\",\"    project = project_name_param\\r\\\\n\",\"    if project_name_param == 'initial-lookoutmetrics-backtesting-test1':\\r\\\\n\",\"        anomaly_detector_arn = 'arn:aws:lookoutmetrics:us-east-1:451256804668:AnomalyDetector:initial-lookoutmetrics-backtesting-test-1-detector'\\r\\\\n\",\"    else:\\r\\\\n\",\"        frequency = 'PT1H' # one of 'P1D', 'PT1H', 'PT10M' and 'PT5M', this one means every one hour\\r\\\\n\",\"        response = L4M.create_anomaly_detector( \\r\\\\n\",\"        AnomalyDetectorName = project + '-detector',\\r\\\\n\",\"        AnomalyDetectorDescription = 'My Detector',\\r\\\\n\",\"        AnomalyDetectorConfig = {\\r\\\\n\",\"            'AnomalyDetectorFrequency' : 'PT1H',\\r\\\\n\",\"        },)\\r\\\\n\",\"        anomaly_detector_arn = response['AnomalyDetectorArn']\\r\\\\n\",\"\\r\\\\n\",\"    print('anomaly_detector_arn = ',anomaly_detector_arn)\\r\\\\n\",\"    metrics = defineMetrics(anomaly_detector_arn,project_name_param,frequency)\\r\\\\n\",\"    return metrics\"]},{\"FunctionName\":\"SetupDataset\",\"requirements\":\"boto3\",\"params\":[],\"script\":[\"import os\\r\\\\n\",\"import shutil\\r\\\\n\",\"import zipfile\\r\\\\n\",\"import boto3\\r\\\\n\",\"import sys\\r\\\\n\",\"import time\\r\\\\n\",\"\\r\\\\n\",\"role_name = 'aiplat'\\r\\\\n\",\"role_arn = 'arn:aws:iam::451256804668:role/aiplat'\\r\\\\n\",\"region = 'us-east-1'\\r\\\\n\",\"bucket_name == 'aiplatdata1'\\r\\\\n\",\"\\r\\\\n\",\"def SetupDataset():\\r\\\\n\",\"    return True\\r\\\\n\"]}]},{\"id\":\"NJwpY\",\"alias\":\"validateResult\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"validateResult\",\"requirements\":\"\",\"params\":[],\"script\":[\"def validateResult(data):\\r\\\\n\",\"    print('In Validation function')\\r\\\\n\",\"    anomaly_groups = []\\r\\\\n\",\"    first_response = None\\r\\\\n\",\"    next_token = None\\r\\\\n\",\"    while True:\\r\\\\n\",\"        params = {\\r\\\\n\",\"            'AnomalyDetectorArn' : anomaly_detector_arn,\\r\\\\n\",\"            'SensitivityThreshold' : 50,\\r\\\\n\",\"            'MaxResults' : 100,\\r\\\\n\",\"        }\\r\\\\n\",\"        \\r\\\\n\",\"        if next_token:\\r\\\\n\",\"            params['NextToken'] = next_token\\r\\\\n\",\"        \\r\\\\n\",\"        response = L4M.list_anomaly_group_summaries( **params )\\r\\\\n\",\"        if first_response is None:\\r\\\\n\",\"            first_response = response\\r\\\\n\",\"        \\r\\\\n\",\"        anomaly_groups += response['AnomalyGroupSummaryList']\\r\\\\n\",\"        \\r\\\\n\",\"        if 'NextToken' in response:\\r\\\\n\",\"            next_token = response['NextToken']\\r\\\\n\",\"            continue\\r\\\\n\",\"        break\\r\\\\n\",\"    return first_response\"]},\"position_x\":\"870\",\"position_y\":\"64\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"NYWBY\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"activateDetector\",\"requirements\":\"\",\"params\":[],\"script\":[\"def activateDetector(metric_set_arn):\\r\\\\n\",\"    L4M.back_test_anomaly_detector(AnomalyDetectorArn = anomaly_detector_arn)\\r\\\\n\",\"    prev_status = None\\r\\\\n\",\"    while True:\\r\\\\n\",\"\\r\\\\n\",\"        response = L4M.describe_anomaly_detector( AnomalyDetectorArn = anomaly_detector_arn )\\r\\\\n\",\"        status = response['Status']\\r\\\\n\",\"\\r\\\\n\",\"        if status != prev_status:\\r\\\\n\",\"            if prev_status:\\r\\\\n\",\"                sys.stdout.write('')\\r\\\\n\",\"            sys.stdout.write( status + ' ' )\\r\\\\n\",\"            sys.stdout.flush()\\r\\\\n\",\"            prev_status = status\\r\\\\n\",\"\\r\\\\n\",\"        if status in ( 'ACTIVATING', 'BACK_TEST_ACTIVATING', 'BACK_TEST_ACTIVE' ):\\r\\\\n\",\"            sys.stdout.write('.')\\r\\\\n\",\"            sys.stdout.flush()\\r\\\\n\",\"            time.sleep(5)\\r\\\\n\",\"            continue\\r\\\\n\",\"\\r\\\\n\",\"        break\\r\\\\n\",\"\\r\\\\n\",\"    return response\"]},{\"FunctionName\":\"defineMetrics\",\"requirements\":\"\",\"params\":[],\"script\":[\"def defineMetrics(metrics,project_name,frequency):\\r\\\\n\",\"    s3_path_backtest = 's3://'+ bucket_name + '/ecommerce/backtest/'\\r\\\\n\",\"    print('s3_path_backtest',s3_path_backtest)\\r\\\\n\",\"    params = {\\r\\\\n\",\"    'AnomalyDetectorArn': anomaly_detector_arn,\\r\\\\n\",\"    'MetricSetName' : project_name + '-metric-set-1',\\r\\\\n\",\"    'MetricList' : [\\r\\\\n\",\"        {\\r\\\\n\",\"            'MetricName' : 'views',\\r\\\\n\",\"            'AggregationFunction' : 'SUM',\\r\\\\n\",\"        },\\r\\\\n\",\"        {\\r\\\\n\",\"            'MetricName' : 'revenue',\\r\\\\n\",\"            'AggregationFunction' : 'SUM',\\r\\\\n\",\"        },\\r\\\\n\",\"    ],\\r\\\\n\",\"\\r\\\\n\",\"    'DimensionList' : [ 'platform', 'marketplace' ],\\r\\\\n\",\"\\r\\\\n\",\"    'TimestampColumn' : {\\r\\\\n\",\"        'ColumnName' : 'timestamp',\\r\\\\n\",\"        'ColumnFormat' : 'yyyy-MM-dd HH:mm:ss',\\r\\\\n\",\"    },\\r\\\\n\",\"    'MetricSetFrequency' : frequency,\\r\\\\n\",\"\\r\\\\n\",\"    'MetricSource' : {\\r\\\\n\",\"        'S3SourceConfig': {\\r\\\\n\",\"            'RoleArn' : role_arn,\\r\\\\n\",\"            'HistoricalDataPathList': [\\r\\\\n\",\"                s3_path_backtest,\\r\\\\n\",\"            ],\\r\\\\n\",\"            'FileFormatDescriptor' : {\\r\\\\n\",\"                'CsvFormatDescriptor' : {\\r\\\\n\",\"                    'FileCompression' : 'NONE',\\r\\\\n\",\"                    'Charset' : 'UTF-8',\\r\\\\n\",\"                    'ContainsHeader' : True,\\r\\\\n\",\"                    'Delimiter' : ',',\\r\\\\n\",\"                    'QuoteSymbol' : ''\\r\\\\n\",\"                },\\r\\\\n\",\"            }\\r\\\\n\",\"        }\\r\\\\n\",\"    },}\\r\\\\n\",\"    \\r\\\\n\",\"    response = L4M.create_metric_set( ** params )\\r\\\\n\",\"    print(response)\\r\\\\n\",\"    metric_set_arn = response['MetricSetArn']\\r\\\\n\",\"    return metric_set_arn\"]},{\"FunctionName\":\"anomalydetector\",\"requirements\":\"\",\"params\":[{\"name\":\"project_name\",\"value\":\"initial-lookoutmetrics-backtesting-test1\",\"type\":\"Text\",\"alias\":\"initial-lookoutmetrics-backtesting-test1\",\"index\":\"1\"},{\"name\":\"frequency\",\"value\":\"PT1H\",\"type\":\"Text\",\"alias\":\"PT1H\",\"index\":\"2\"}],\"script\":[\"def anomalydetector(data,project_name_param,frequency_param):\\r\\\\n\",\"    global anomaly_detector_arn,L4M\\r\\\\n\",\"    anomaly_detector_arn = None\\r\\\\n\",\"    bucket = 'lookoutmetrics'\\r\\\\n\",\"    frequency = 'PT1H'\\r\\\\n\",\"    L4M = boto3.client(bucket, aws_access_key_id='AKIAWSEIAMU6H5SKF2E2', aws_secret_access_key='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',region_name = 'us-east-1')\\r\\\\n\",\"    project = project_name_param\\r\\\\n\",\"    if project_name_param == 'initial-lookoutmetrics-backtesting-test1':\\r\\\\n\",\"        anomaly_detector_arn = 'arn:aws:lookoutmetrics:us-east-1:451256804668:AnomalyDetector:initial-lookoutmetrics-backtesting-test-1-detector'\\r\\\\n\",\"    else:\\r\\\\n\",\"        frequency = 'PT1H' # one of 'P1D', 'PT1H', 'PT10M' and 'PT5M', this one means every one hour\\r\\\\n\",\"        response = L4M.create_anomaly_detector( \\r\\\\n\",\"        AnomalyDetectorName = project + '-detector',\\r\\\\n\",\"        AnomalyDetectorDescription = 'My Detector',\\r\\\\n\",\"        AnomalyDetectorConfig = {\\r\\\\n\",\"            'AnomalyDetectorFrequency' : 'PT1H',\\r\\\\n\",\"        },)\\r\\\\n\",\"        anomaly_detector_arn = response['AnomalyDetectorArn']\\r\\\\n\",\"\\r\\\\n\",\"    print('anomaly_detector_arn = ',anomaly_detector_arn)\\r\\\\n\",\"    metrics = defineMetrics(anomaly_detector_arn,project_name_param,frequency)\\r\\\\n\",\"    return metrics\"]},{\"FunctionName\":\"SetupDataset\",\"requirements\":\"boto3\",\"params\":[],\"script\":[\"import os\\r\\\\n\",\"import shutil\\r\\\\n\",\"import zipfile\\r\\\\n\",\"import boto3\\r\\\\n\",\"import sys\\r\\\\n\",\"import time\\r\\\\n\",\"\\r\\\\n\",\"role_name = 'aiplat'\\r\\\\n\",\"role_arn = 'arn:aws:iam::451256804668:role/aiplat'\\r\\\\n\",\"region = 'us-east-1'\\r\\\\n\",\"bucket_name == 'aiplatdata1'\\r\\\\n\",\"\\r\\\\n\",\"def SetupDataset():\\r\\\\n\",\"    return True\\r\\\\n\"]}]}],\"pipeline_attributes\":[]}","admin","Lookout_AnomalyDetection_HistoricalData","2023-10-18T06:26:42","LEOLKT_N33178","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"51\":{\"taskId\":\"f604ee24-162c-4edd-8530-1c563048087b\"}}"
"admin","2023-10-23T07:09:17.943","false","","NULL","{\"elements\":[{\"id\":\"wpxvm\",\"alias\":\"Shift Assignment\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"shift_assignment\",\"requirements\":\"\",\"params\":[],\"script\":[\"from datetime import datetime\\rdef shift_assignment(UnAssignedTickets, current_shift_emp):\\r    #print(UnAssignedTickets)\\r    if len(UnAssignedTickets) == 0:\\r        print('No Tickets to assign')\\r        print('Completed')\\r        exit()\\r    current_time = datetime.now().time().strftime('%H:%M:%S')\\r    print('CurrentTime',current_time)\\r    print('current_Shift', current_shift_emp)\\r    techList= list(set([ sub['assignmentgroup'] for sub in current_shift_emp ]))\\r    print('AssignmentGroup',techList)\\r    assignedTickets= []\\r    for item in techList:\\r        print('Assigning for', item)\\r        techTickets= [e for e in UnAssignedTickets if e['assignmentgroup']== item]\\r        techEmpList = [e for e in current_shift_emp if e['assignmentgroup']== item]\\r        print('Available Employees', techEmpList)\\r        if len(techEmpList)>0:\\r            for ticket in techTickets:\\r                #print(techEmpList)\\r                \\r                employee= min(techEmpList, key= lambda x:x['AssignedWeight']) \\r                techEmployee= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                e= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                techEmployee['AssignedWeight'] = techEmployee['AssignedWeight']+ticket['weight']\\r                ticket['assignedTo']= employee['Name']\\r                assignedTickets.append({'number': ticket['number'], 'predicted_assignee': ticket['assignedTo'], 'last_updated':datetime.now()})\\r                #print('Ticket {0} assigned to {1}. Current Weight {2}'.format(ticket['number'],ticket['assignedTo'],techEmployee['AssignedWeight']))\\r                \\r    if len(assignedTickets)>0:\\r        print('Assignment completed')\\r        print(assignedTickets)\\r        return assignedTickets\\r    else:\\r        print('No Tickets Assigned')\\r        print('Completed')\\r        exit()\"]},\"position_x\":\"503\",\"position_y\":\"84\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"uUOtw\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"PTgyc\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"geKuH\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"User Availability\",\"id\":806,\"name\":\"LEORSTRD13362\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-17 16:43:33\",\"alias\":\"User Availability\",\"id\":36,\"name\":\"User Availability\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"Name\\\",\\\"recordcolumndisplayname\\\":\\\"Name\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\":\\\"assignmentgroup\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Shift\\\",\\\"recordcolumndisplayname\\\":\\\"Shift\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Date\\\",\\\"recordcolumndisplayname\\\":\\\"Date\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_useravailability\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_useravailability\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"Unassigned Tickets\",\"id\":805,\"name\":\"LEOUNSGN96999\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"PTgyc\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"User Availability\",\"id\":806,\"name\":\"LEORSTRD13362\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-17 16:43:33\",\"alias\":\"User Availability\",\"id\":36,\"name\":\"User Availability\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"Name\\\",\\\"recordcolumndisplayname\\\":\\\"Name\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\":\\\"assignmentgroup\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Shift\\\",\\\"recordcolumndisplayname\\\":\\\"Shift\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Date\\\",\\\"recordcolumndisplayname\\\":\\\"Date\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_useravailability\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_useravailability\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"162\",\"position_y\":\"42\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wpxvm\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"geKuH\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"Unassigned Tickets\",\"id\":805,\"name\":\"LEOUNSGN96999\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"211\",\"position_y\":\"149\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wpxvm\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"uUOtw\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"736\",\"position_y\":\"88\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"wpxvm\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"shift_assignment\",\"requirements\":\"\",\"params\":[],\"script\":[\"from datetime import datetime\\rdef shift_assignment(UnAssignedTickets, current_shift_emp):\\r    #print(UnAssignedTickets)\\r    if len(UnAssignedTickets) == 0:\\r        print('No Tickets to assign')\\r        print('Completed')\\r        exit()\\r    current_time = datetime.now().time().strftime('%H:%M:%S')\\r    print('CurrentTime',current_time)\\r    print('current_Shift', current_shift_emp)\\r    techList= list(set([ sub['assignmentgroup'] for sub in current_shift_emp ]))\\r    print('AssignmentGroup',techList)\\r    assignedTickets= []\\r    for item in techList:\\r        print('Assigning for', item)\\r        techTickets= [e for e in UnAssignedTickets if e['assignmentgroup']== item]\\r        techEmpList = [e for e in current_shift_emp if e['assignmentgroup']== item]\\r        print('Available Employees', techEmpList)\\r        if len(techEmpList)>0:\\r            for ticket in techTickets:\\r                #print(techEmpList)\\r                \\r                employee= min(techEmpList, key= lambda x:x['AssignedWeight']) \\r                techEmployee= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                e= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                techEmployee['AssignedWeight'] = techEmployee['AssignedWeight']+ticket['weight']\\r                ticket['assignedTo']= employee['Name']\\r                assignedTickets.append({'number': ticket['number'], 'predicted_assignee': ticket['assignedTo'], 'last_updated':datetime.now()})\\r                #print('Ticket {0} assigned to {1}. Current Weight {2}'.format(ticket['number'],ticket['assignedTo'],techEmployee['AssignedWeight']))\\r                \\r    if len(assignedTickets)>0:\\r        print('Assignment completed')\\r        print(assignedTickets)\\r        return assignedTickets\\r    else:\\r        print('No Tickets Assigned')\\r        print('Completed')\\r        exit()\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"User Availability\",\"id\":806,\"name\":\"LEORSTRD13362\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-17 16:43:33\",\"alias\":\"User Availability\",\"id\":36,\"name\":\"User Availability\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"Name\\\",\\\"recordcolumndisplayname\\\":\\\"Name\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\":\\\"assignmentgroup\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Shift\\\",\\\"recordcolumndisplayname\\\":\\\"Shift\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Date\\\",\\\"recordcolumndisplayname\\\":\\\"Date\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_useravailability\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_useravailability\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"Unassigned Tickets\",\"id\":805,\"name\":\"LEOUNSGN96999\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Current Shift Assignment","2023-12-19T11:58:03","LEOCRNTS51008","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"6\":{\"taskId\":\"f9eb5077-43a4-42ce-9434-685efd424c28\"}}"
"admin","2023-10-26T08:57:40.810","false","","NULL","{\"elements\":[{\"id\":\"wpxvm\",\"alias\":\"Shift Assignment\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"shift_assignment\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef shift_assignment(UnAssignedTickets, current_shift_emp):\\r    if len(UnAssignedTickets) == 0:\\r        print('No Tickets to assign')\\r        print('Completed')\\r        exit()\\r    current_time = datetime.now().time().strftime('%H:%M:%S')\\r    print('CurrentTime',current_time)\\r    print('current_Shift', current_shift_emp)\\r    techList= list(set([ sub['assignmentgroup'] for sub in current_shift_emp ]))\\r    print('AssignmentGroup',techList)\\r    assignedTickets= []\\r    for item in techList:\\r        print('Assigning for', item)\\r        techTickets= [e for e in UnAssignedTickets if e['assignmentgroup']== item]\\r        techEmpList = [e for e in current_shift_emp if e['assignmentgroup']== item]\\r        print('Available Employees', techEmpList)\\r        if len(techEmpList)>0:\\r            for ticket in techTickets:\\r                #print(techEmpList)\\r                \\r                employee= min(techEmpList, key= lambda x:x['AssignedWeight']) \\r                techEmployee= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                e= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                techEmployee['AssignedWeight'] = techEmployee['AssignedWeight']+ticket['weight']\\r                ticket['assignedTo']= employee['Name']\\r                assignedTickets.append({'number': ticket['number'], 'predicted_assignee': ticket['assignedTo'], 'last_updated':datetime.now()})\\r                print('Ticket {0} assigned to {1}. Current Weight {2}'.format(ticket['number'],ticket['assignedTo'],techEmployee['AssignedWeight']))\\r                \\r    if len(assignedTickets)>0:\\r        print('Assignment completed')\\r        print(assignedTickets)\\r        return assignedTickets\\r    else:\\r        print('No Tickets Assigned')\\r        print('Completed')\\r        exit()\"]},\"position_x\":\"503\",\"position_y\":\"84\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"uUOtw\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"PTgyc\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"geKuH\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"User Availability\",\"id\":806,\"name\":\"LEORSTRD13362\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-17 16:43:33\",\"alias\":\"User Availability\",\"id\":36,\"name\":\"User Availability\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"Name\\\",\\\"recordcolumndisplayname\\\":\\\"Name\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\":\\\"assignmentgroup\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Shift\\\",\\\"recordcolumndisplayname\\\":\\\"Shift\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Date\\\",\\\"recordcolumndisplayname\\\":\\\"Date\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_useravailability\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_useravailability\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"Unassigned Tickets\",\"id\":805,\"name\":\"LEOUNSGN96999\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"PTgyc\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"User Availability\",\"id\":806,\"name\":\"LEORSTRD13362\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-17 16:43:33\",\"alias\":\"User Availability\",\"id\":36,\"name\":\"User Availability\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"Name\\\",\\\"recordcolumndisplayname\\\":\\\"Name\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\":\\\"assignmentgroup\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Shift\\\",\\\"recordcolumndisplayname\\\":\\\"Shift\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Date\\\",\\\"recordcolumndisplayname\\\":\\\"Date\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_useravailability\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_useravailability\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"162\",\"position_y\":\"42\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wpxvm\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"geKuH\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"Unassigned Tickets\",\"id\":805,\"name\":\"LEOUNSGN96999\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"211\",\"position_y\":\"149\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wpxvm\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"uUOtw\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"736\",\"position_y\":\"88\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"wpxvm\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"shift_assignment\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef shift_assignment(UnAssignedTickets, current_shift_emp):\\r    if len(UnAssignedTickets) == 0:\\r        print('No Tickets to assign')\\r        print('Completed')\\r        exit()\\r    current_time = datetime.now().time().strftime('%H:%M:%S')\\r    print('CurrentTime',current_time)\\r    print('current_Shift', current_shift_emp)\\r    techList= list(set([ sub['assignmentgroup'] for sub in current_shift_emp ]))\\r    print('AssignmentGroup',techList)\\r    assignedTickets= []\\r    for item in techList:\\r        print('Assigning for', item)\\r        techTickets= [e for e in UnAssignedTickets if e['assignmentgroup']== item]\\r        techEmpList = [e for e in current_shift_emp if e['assignmentgroup']== item]\\r        print('Available Employees', techEmpList)\\r        if len(techEmpList)>0:\\r            for ticket in techTickets:\\r                #print(techEmpList)\\r                \\r                employee= min(techEmpList, key= lambda x:x['AssignedWeight']) \\r                techEmployee= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                e= next(x for x in techEmpList if x['Name']== employee['Name'])\\r                techEmployee['AssignedWeight'] = techEmployee['AssignedWeight']+ticket['weight']\\r                ticket['assignedTo']= employee['Name']\\r                assignedTickets.append({'number': ticket['number'], 'predicted_assignee': ticket['assignedTo'], 'last_updated':datetime.now()})\\r                print('Ticket {0} assigned to {1}. Current Weight {2}'.format(ticket['number'],ticket['assignedTo'],techEmployee['AssignedWeight']))\\r                \\r    if len(assignedTickets)>0:\\r        print('Assignment completed')\\r        print(assignedTickets)\\r        return assignedTickets\\r    else:\\r        print('No Tickets Assigned')\\r        print('Completed')\\r        exit()\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"User Availability\",\"id\":806,\"name\":\"LEORSTRD13362\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-17 16:43:33\",\"alias\":\"User Availability\",\"id\":36,\"name\":\"User Availability\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"Name\\\",\\\"recordcolumndisplayname\\\":\\\"Name\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"assignmentgroup\\\",\\\"recordcolumndisplayname\\\":\\\"assignmentgroup\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Shift\\\",\\\"recordcolumndisplayname\\\":\\\"Shift\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Date\\\",\\\"recordcolumndisplayname\\\":\\\"Date\\\",\\\"isunique\\\":false,\\\"isrequired\\\":false,\\\"isencrypted\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_useravailability\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_useravailability\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2022-06-20 07:49:22\",\"alias\":\"Unassigned Tickets\",\"id\":805,\"name\":\"LEOUNSGN96999\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"dashboard\":null,\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-19 05:32:59\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encUInC2zwosfWh6ubpDeWFsCO/oJA66sWX\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"leapuser\\\",\\\"url\\\":\\\"jdbc:mysql://Cvictsecst1:3306/300_leapmaster_ref_data\\\"}\",\"salt\":\"IL2pNsDxQ/1ZTkLsCZFa19COPkYVj+7APhrc0NsNrhCTxBzaFw8xAHOlmlXf3wCaMJ+EMG8hsfLfb/lSr1tRtA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4a35d09ed8e6babb1e0c29c9b791d0a264626e1123921df98235cb8245931126\",\"activetime\":\"2023-10-19 05:32:58\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}]}","admin","Next Shift Assignment","2023-10-26T09:17:25","LEONXTSH64071","leo1311","DragNDropLite","NULL","NULL","pipeline","NULL"
"admin","2023-10-26T09:13:16.158","false","","NULL","{\"elements\":[{\"id\":\"AzWwh\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"resolver\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentid\",\"value\":\"INC0020765\",\"type\":\"Text\",\"alias\":\"INC0020765\",\"index\":\"1\"},{\"name\":\"workflowname\",\"value\":\"FacebookInsights\",\"type\":\"Text\",\"alias\":\"FacebookInsights\",\"index\":\"2\"}],\"script\":[\"\\rimport pandas as pd\\r\\rdef resolver(dataset,incidentid_param='',workflowname_param=''):\\r    dataset = pd.DataFrame(dataset)\\r    incidentid=incidentid_param\\r    workflow=workflowname_param\\r    print('Running for incident '+ incidentid)\\r    print('workflow' + workflow)\\r    if incidentid !='':\\r        dataset = dataset[dataset['number'] == incidentid]\\r        dataset['workflow_name'] = workflow\\r        return dataset\\r\\r\"]},\"position_x\":\"496\",\"position_y\":\"160\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"PelRo\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"sfVmW\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 11:50:49\",\"alias\":\"Tickets\",\"id\":882,\"name\":\"LEOTCKTS64682\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\\"CLOSED\\\\\\\", \\\\\\\"RESOLVED\\\\\\\") \\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"PelRo\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"Executeworkflow\",\"requirements\":\"\",\"params\":[],\"script\":[\"from leaputils import Security\\rimport requests\\rimport json\\rimport os\\rimport pandas\\rfrom datetime import datetime\\r\\rdef Executeworkflow (dataset):\\r    def parentresolve(ticket,workflow,sysid):\\r        out = []\\r        ticket,workflow,sysid = ticket.tolist(),workflow.tolist(),sysid.tolist()\\r        for i in range(len(ticket)):\\r            value = resolve(ticket[i],workflow[i],sysid[i])\\r            if value is None:\\r                value = ''\\r            out.append(value)\\r        return out\\r    def resolve(ticket,workflow,sysid):\\r        if workflow == 'FacebookInsights':\\r            try:\\r                ####----Congigure rundeck api  URL/params/headers corresponding to workflow and return response from api\\r                url= 'http://myzul02u:4440/api/28/job/1a0ccbeb-887d-4c6c-9a7e-7bc3193bf545/executions?authtoken=MQqGyiIIRyUynjavzHWVgovg96qi3C3K' \\r                data={'argString': '-IncidentSystemID '+sysid }\\r                #Headers required by rundeck Api\\r                headers={'Accept':'application/json'}\\r                response=requests.post(url,data=data,headers=headers,verify=False)\\r                #return response.text.encode('utf8')\\r                return 'Workflow Executed for IncidentId:' + ticket + ' ' +str(response.text.encode('utf8'))\\r            except:\\r                return 'workflow execution error'+ ticket\\r        elif workflow == 'CancelShipment':\\r            try:\\r                url= 'http://victadpst-10:6032/api/workflow/saveworkflowinputdata'\\r                InputJson = [{'IncidentId': ticket}]\\r                data={\\r                    'ScheduleID': 1,\\r                        'WorkFlowID': '2',\\r                        'WorkFlowName': 'Shipment Cancel',\\r                        'ServerName': 'victadpst-10',\\r                        'AgentList': [5],\\r                        'InputJson': json.dumps(InputJson)\\r                    }\\r\\r            #Headers required by rundeck Api\\r                headers={'Content-Type':'application/json' }\\r                response=requests.post(url,auth=('admin','ZTLiPW70rwTT/6VoFmw0zg=='), data=json.dumps(data),headers=headers)\\r                return 'Workflow Executed for IncidentId:' + ticket + ' ' +str(response.text.encode('utf8'))\\r            except:\\r                return 'Workflow execution error'\\r        elif workflow == 'ProcessInvoice':\\r            try:\\r                url= 'https://infosysq3dev1.service-now.com/api/now/table/incident/'+sysid \\r                proxyDict = {\\r                'http'  : os.environ['HTTP_PROXY'],\\r                'https' : os.environ['HTTPS_PROXY']            }\\r                data=json.dumps({'state':'7','close_code':'Closed by LEAP','caller_id':'6816f79cc0a8016401c5a33be04be441'})\\r                #Headers required by rundeck Api\\r                headers={'Content-Type':'application/json'}\\r                pwd=Security.decrypt('enc36lBo7boFl9A7QUQ86Tq/YWPxM4/+6Oj', 'ay0lj4Po/1FsrtKnrODl+Pk3yfkG7gLDzeCOtJRwpXRnIbnLtrHoDE7ktm3mmgq3MjGSJZ+ojMxtynt4BqHJwg==') \\r                pwd='qwer1234'\\r                response=requests.patch(url,auth=('ICSP_icap_user', pwd),proxies=proxyDict,data=data,headers=headers,verify=False)\\r                #return response.text.encode('utf8')\\r                return 'Workflow Executed for IncidentId:' + ticket + ' ' +str(response.text.encode('utf8'))\\r            except:\\r                return 'Workflow execution error'\\r        else:\\r            return 'No API configured for workflow'\\r        \\r    \\r    # dfresolve = dataset.apply(resolve).astype(str)\\r    # dataset['apiResponse']=dataset.apply(lambda row:dfresolve(row['number'],row['workflow_name'],row['sysId']),axis=1)\\r    # dataset['apiResponse']=dataset.apply(lambda row: resolve(row['number'],row['workflow_name'],row['sysId']))\\r    #dataset['apiResponse']=dataset.apply(lambda row: resolve(row['number'],row['workflow_name'],row['sysId']))\\r    dataset = dataset.assign(apiResponse = lambda row: parentresolve(row['number'],row['workflow_name'],row['sysId']))\\r    #dataset = dataset.assign(apiResponse = lambda row: resolve(row['number'],row['workflow_name'],row['sysId']))\\r    dataset['run_timestamp'] = pd.to_datetime(datetime.now())\\r    dataset.head()\\r    \\r    return dataset\\r\\r\\r\"]},\"position_x\":\"709\",\"position_y\":\"160\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"AzWwh\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"resolver\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentid\",\"value\":\"INC0020765\",\"type\":\"Text\",\"alias\":\"INC0020765\",\"index\":\"1\"},{\"name\":\"workflowname\",\"value\":\"FacebookInsights\",\"type\":\"Text\",\"alias\":\"FacebookInsights\",\"index\":\"2\"}],\"script\":[\"\\rimport pandas as pd\\r\\rdef resolver(dataset,incidentid_param='',workflowname_param=''):\\r    dataset = pd.DataFrame(dataset)\\r    incidentid=incidentid_param\\r    workflow=workflowname_param\\r    print('Running for incident '+ incidentid)\\r    print('workflow' + workflow)\\r    if incidentid !='':\\r        dataset = dataset[dataset['number'] == incidentid]\\r        dataset['workflow_name'] = workflow\\r        return dataset\\r\\r\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 11:50:49\",\"alias\":\"Tickets\",\"id\":882,\"name\":\"LEOTCKTS64682\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\\"CLOSED\\\\\\\", \\\\\\\"RESOLVED\\\\\\\") \\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"sfVmW\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 11:50:49\",\"alias\":\"Tickets\",\"id\":882,\"name\":\"LEOTCKTS64682\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT te.number, te.sop, te.workflow as workflow_name,  sysId FROM leo1311_tickets_enriched te JOIN leo1311_tickets tkt ON te.number = tkt.number  where  tkt.state NOT IN (\\\\\\\"CLOSED\\\\\\\", \\\\\\\"RESOLVED\\\\\\\") \\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"213\",\"position_y\":\"195\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"AzWwh\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Generic Workflow Resolver","2023-12-19T11:51:27","LEOGNRCW45804","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"13\":{\"taskId\":\"30270762-9d1b-46a9-9dae-5bf9a791c47f\"}}"
"admin","2023-10-25T10:13:40.998","false","Generate clusters for tickets","NULL","{\"elements\":[{\"id\":\"lOPzI\",\"alias\":\"Tickets\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"0\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ErwuN\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"XSWOG\",\"alias\":\"Clean text\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"476\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"YvhqP\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"ErwuN\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"YvhqP\",\"alias\":\"StopwordRemover\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"666\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"XSWOG\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"MWXZY\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"MWXZY\",\"alias\":\"Lemmetizer\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"856\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"YvhqP\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"qyeEc\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"wvMSR\",\"alias\":\"Soundex\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"467\",\"position_y\":\"131\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"JFNBn\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"qyeEc\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"JFNBn\",\"alias\":\"Ngram\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"ngram\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram(dataset):\\r    logger.info('Generating Ngram clusters...')\\r    try:\\r        txt1 =[]\\r        for index, row in dataset.iterrows():\\r            txt1.append(row['clean_text'])\\r       \\r        # Getting trigrams \\r        vectorizer = CountVectorizer(ngram_range = (3,3))\\r        X1 = vectorizer.fit_transform(txt1) \\r        features = (vectorizer.get_feature_names_out())\\r        # Applying TFIDF\\r        vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r        X2 = vectorizer.fit_transform(txt1)\\r        scores = (X2.toarray())\\r        # Getting top ranking features\\r        logging.info('Getting Top 50 Grams')\\r        sums = X2.sum(axis = 0)\\r        data1 = []\\r        for col, term in enumerate(features):\\r            data1.append( (term, sums[0,col] ))\\r        ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r        words = (ranking.sort_values('rank', ascending = False))\\r        top50df = words.nlargest(50,'rank')\\r        top50 = words['term'].tolist()\\r        \\r        distinctText = list(set(txt1))\\r        count=0\\r        totalRecords = len(distinctText)\\r        logging.info('Mapping Text to Gram')\\r        logging.info('Total Unique Values: {0}'.format(totalRecords))\\r        ngramDict = {}\\r        for item in distinctText:\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r            if len(matchingGrams) >0:\\r                ngramDict[item] = matchingGrams[0]\\r            count = count+1\\r            if count%1000 ==0:\\r                logging.info('{0} rows mapped'.format(count))\\r    \\r        dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    except Exception as ex:\\r        logger.error('Error in Ngram')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    \\r    return dataset\"]},\"position_x\":\"688\",\"position_y\":\"131\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"wvMSR\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"omSub\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"omSub\",\"alias\":\"Extract Phrases\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logging.info(ex)\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in Extract Phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},\"position_x\":\"870\",\"position_y\":\"131\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"JFNBn\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"lDCSS\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"ngram\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram(dataset):\\r    logger.info('Generating Ngram clusters...')\\r    try:\\r        txt1 =[]\\r        for index, row in dataset.iterrows():\\r            txt1.append(row['clean_text'])\\r       \\r        # Getting trigrams \\r        vectorizer = CountVectorizer(ngram_range = (3,3))\\r        X1 = vectorizer.fit_transform(txt1) \\r        features = (vectorizer.get_feature_names_out())\\r        # Applying TFIDF\\r        vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r        X2 = vectorizer.fit_transform(txt1)\\r        scores = (X2.toarray())\\r        # Getting top ranking features\\r        logging.info('Getting Top 50 Grams')\\r        sums = X2.sum(axis = 0)\\r        data1 = []\\r        for col, term in enumerate(features):\\r            data1.append( (term, sums[0,col] ))\\r        ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r        words = (ranking.sort_values('rank', ascending = False))\\r        top50df = words.nlargest(50,'rank')\\r        top50 = words['term'].tolist()\\r        \\r        distinctText = list(set(txt1))\\r        count=0\\r        totalRecords = len(distinctText)\\r        logging.info('Mapping Text to Gram')\\r        logging.info('Total Unique Values: {0}'.format(totalRecords))\\r        ngramDict = {}\\r        for item in distinctText:\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r            if len(matchingGrams) >0:\\r                ngramDict[item] = matchingGrams[0]\\r            count = count+1\\r            if count%1000 ==0:\\r                logging.info('{0} rows mapped'.format(count))\\r    \\r        dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    except Exception as ex:\\r        logger.error('Error in Ngram')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    \\r    return dataset\"]},{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"lDCSS\",\"alias\":\"Map Phrases\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    logger.info('Mapping extracted phrases to key phrases....')\\r    try:\\r        keywords = []\\r        for item in ease:\\r            word = item['Key_Word']\\r            if word not in keywords:\\r                keywords.append(word)\\r        phrases = []\\r        for item in dataset:\\r            phrase = item['extracted_phrase']\\r            if phrase not in phrases:\\r                phrases.append(phrase)\\r        \\r            \\r        def getSimilar(sentences, keywords):\\r            corpus = sentences + keywords\\r            keywordstartIndex = len(sentences)\\r            tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r            pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r            arr = pairwise_similarity.toarray()\\r            np.fill_diagonal(arr, np.nan)\\r            results = {}\\r            for s in sentences:\\r                input_idx = sentences.index(s)\\r                result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r                match = arr[input_idx][keywordstartIndex + result_idx]\\r                r = keywords[result_idx]\\r                if match > 0:\\r                    results[s] = r + ':' + str(match)\\r                else:\\r                    results[s] = 'NO MATCH:0'\\r            return results\\r\\r    \\r        def getEASE(phrases,keywords):\\r            try:\\r                results = getSimilar(phrases, keywords)\\r                mappings = {}\\r                for pattern in results.keys():\\r                    kw = results[pattern].split(':')[0]\\r                    score = results[pattern].split(':')[-1]\\r                    if kw != 'NO MATCH':\\r                        mappings[pattern] = {'keyword':kw, 'score':score}\\r                return mappings\\r            except Exception as ex:\\r                logger.warning(ex)\\r                return False\\r            \\r        totallen = len(phrases)\\r        logger.info('Total Records: {0}'.format(totallen))\\r        start = 0\\r        step = 1000\\r        results = []\\r        for i in range(start, totallen, step):\\r            stop = i + step\\r            if (stop > totallen):\\r                stop = totallen\\r            mappedphrases = getEASE(phrases[i:stop],keywords)\\r            results.append(mappedphrases)\\r            for row in dataset:\\r                phrase =row['extracted_phrase']\\r                if mappedphrases.get(phrase,'') !='':\\r                    row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round( float(mappedphrases[phrase]['score']),4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r    except Exception as ex:\\r        logger.error('error in Map phrase')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"280\",\"position_y\":\"264\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"omSub\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"yVBoI\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"OVsqJ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logging.info(ex)\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in Extract Phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"ngram\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram(dataset):\\r    logger.info('Generating Ngram clusters...')\\r    try:\\r        txt1 =[]\\r        for index, row in dataset.iterrows():\\r            txt1.append(row['clean_text'])\\r       \\r        # Getting trigrams \\r        vectorizer = CountVectorizer(ngram_range = (3,3))\\r        X1 = vectorizer.fit_transform(txt1) \\r        features = (vectorizer.get_feature_names_out())\\r        # Applying TFIDF\\r        vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r        X2 = vectorizer.fit_transform(txt1)\\r        scores = (X2.toarray())\\r        # Getting top ranking features\\r        logging.info('Getting Top 50 Grams')\\r        sums = X2.sum(axis = 0)\\r        data1 = []\\r        for col, term in enumerate(features):\\r            data1.append( (term, sums[0,col] ))\\r        ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r        words = (ranking.sort_values('rank', ascending = False))\\r        top50df = words.nlargest(50,'rank')\\r        top50 = words['term'].tolist()\\r        \\r        distinctText = list(set(txt1))\\r        count=0\\r        totalRecords = len(distinctText)\\r        logging.info('Mapping Text to Gram')\\r        logging.info('Total Unique Values: {0}'.format(totalRecords))\\r        ngramDict = {}\\r        for item in distinctText:\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r            if len(matchingGrams) >0:\\r                ngramDict[item] = matchingGrams[0]\\r            count = count+1\\r            if count%1000 ==0:\\r                logging.info('{0} rows mapped'.format(count))\\r    \\r        dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    except Exception as ex:\\r        logger.error('Error in Ngram')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    \\r    return dataset\"]},{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"OVsqJ\",\"alias\":\"Ease Mappings\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"0\",\"position_y\":\"186\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"lDCSS\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"yVBoI\",\"alias\":\"LDA\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"lda\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\r\\\\n\",\"\\r\\\\n\",\"def lda(dataset, clustercount_param=5, uniqueidcolumn_param=''):\\r\\\\n\",\"    logger.info('Generating LDA clusters...')\\r\\\\n\",\"    try:\\r\\\\n\",\"        def cv(dataset, input_col_param):\\r\\\\n\",\"            from sklearn.feature_extraction.text import CountVectorizer\\r\\\\n\",\"            count_vectorizer = CountVectorizer(stop_words='english')\\r\\\\n\",\"            count_data = count_vectorizer.fit_transform(dataset['clean_text'].to_list())\\r\\\\n\",\"            words = count_vectorizer.get_feature_names_out()\\r\\\\n\",\"            return {'data': count_data, 'words': words}\\r\\\\n\",\"        \\r\\\\n\",\"        dataset = pd.DataFrame(dataset)\\r\\\\n\",\"        originaldataset = dataset\\r\\\\n\",\"        result = {}\\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            result[name] = cv(dataset, 'clean_text')\\r\\\\n\",\"            result[name]['number'] = dataset['number'].to_list()\\r\\\\n\",\"    \\r\\\\n\",\"        def LDARunner(dataset, clustercount_param):\\r\\\\n\",\"            from sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\\\n\",\"            import numpy as np\\r\\\\n\",\"            number_topics =  clustercount_param\\r\\\\n\",\"            count_data, words, number = dataset['data'], dataset['words'], dataset['number']\\r\\\\n\",\"             \\r\\\\n\",\"            lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\\\n\",\"            lda.fit(count_data)\\r\\\\n\",\"            documents = lda.transform(count_data)\\r\\\\n\",\"            \\r\\\\n\",\"    \\r\\\\n\",\"            argmax_values = np.argmax(documents, axis=1)\\r\\\\n\",\"    \\r\\\\n\",\"            newdocuments = pd.DataFrame({'cluster_Id':argmax_values, 'number':number})\\r\\\\n\",\"    \\r\\\\n\",\"            def wordsWithWeights(termIndices, termWeights, index):\\r\\\\n\",\"                terms = [words[i] for i in termIndices]\\r\\\\n\",\"                topic = [index]*len(terms)\\r\\\\n\",\"                return list(zip(terms, topic, termIndices, termWeights))\\r\\\\n\",\"            \\r\\\\n\",\"            topics = [ wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) for index, topic in enumerate(lda.components_)]\\r\\\\n\",\"            topicWords = []\\r\\\\n\",\"            \\r\\\\n\",\"            for topic in topics:\\r\\\\n\",\"                topicWords.extend(topic)\\r\\\\n\",\"            finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\\\n\",\"            finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\\\n\",\"            finalTopic.columns = ['topic', 'word', 'weight']\\r\\\\n\",\"            \\r\\\\n\",\"            # newdocuments = newdocuments.merge(finalTopic, left_on='cluster_Id', right_index=True).drop(columns='cluster_Id')\\r\\\\n\",\"            \\r\\\\n\",\"            topicwordsdf = finalTopic.groupby('topic')['word'].apply(list).reset_index()\\r\\\\n\",\"            topicwordsdf.rename(columns={'word': 'cluster_Name'}, inplace=True)\\r\\\\n\",\"            topicwordsdf = topicwordsdf[['topic', 'cluster_Name']]\\r\\\\n\",\"            newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\\\n\",\"            newdocuments = newdocuments.drop(columns='topic')\\r\\\\n\",\"            return newdocuments, finalTopic\\r\\\\n\",\"            \\r\\\\n\",\"        newdocuments_result = {}\\r\\\\n\",\"        finalTopic_result = {}\\r\\\\n\",\"        for name, datasets in result.items(): # dataset->datasets\\r\\\\n\",\"            logger.info('Running LDA for '+name)\\r\\\\n\",\"            newdocuments, finalTopic = LDARunner(datasets, clustercount_param)\\r\\\\n\",\"            finalTopic['group_by_field'] = name\\r\\\\n\",\"            newdocuments['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\\\n\",\"            newdocuments_result[name] = newdocuments\\r\\\\n\",\"            finalTopic_result[name] = finalTopic\\r\\\\n\",\"    \\r\\\\n\",\"        \\r\\\\n\",\"        resultsdf = pd.concat(newdocuments_result.values(), ignore_index=True)\\r\\\\n\",\"        topicsdf = pd.concat(finalTopic_result.values(), ignore_index=True)\\r\\\\n\",\"        topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({'word': list, 'weight': list}).reset_index()\\r\\\\n\",\"        topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"        topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(list(map(lambda y: str(y), x))))\\r\\\\n\",\"        topicsdf['alias'] = topicsdf['words']\\r\\\\n\",\"        topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r\\\\n\",\"    \\r\\\\n\",\"        \\r\\\\n\",\"        resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"        resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r\\\\n\",\"        resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']]\\r\\\\n\",\"      \\r\\\\n\",\"    \\r\\\\n\",\"        dataset = pd.merge(originaldataset,resultsdf, on='number', how='left')\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in LDA')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},\"position_x\":\"476\",\"position_y\":\"264\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"lDCSS\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"YCnzZ\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    logger.info('Mapping extracted phrases to key phrases....')\\r    try:\\r        keywords = []\\r        for item in ease:\\r            word = item['Key_Word']\\r            if word not in keywords:\\r                keywords.append(word)\\r        phrases = []\\r        for item in dataset:\\r            phrase = item['extracted_phrase']\\r            if phrase not in phrases:\\r                phrases.append(phrase)\\r        \\r            \\r        def getSimilar(sentences, keywords):\\r            corpus = sentences + keywords\\r            keywordstartIndex = len(sentences)\\r            tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r            pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r            arr = pairwise_similarity.toarray()\\r            np.fill_diagonal(arr, np.nan)\\r            results = {}\\r            for s in sentences:\\r                input_idx = sentences.index(s)\\r                result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r                match = arr[input_idx][keywordstartIndex + result_idx]\\r                r = keywords[result_idx]\\r                if match > 0:\\r                    results[s] = r + ':' + str(match)\\r                else:\\r                    results[s] = 'NO MATCH:0'\\r            return results\\r\\r    \\r        def getEASE(phrases,keywords):\\r            try:\\r                results = getSimilar(phrases, keywords)\\r                mappings = {}\\r                for pattern in results.keys():\\r                    kw = results[pattern].split(':')[0]\\r                    score = results[pattern].split(':')[-1]\\r                    if kw != 'NO MATCH':\\r                        mappings[pattern] = {'keyword':kw, 'score':score}\\r                return mappings\\r            except Exception as ex:\\r                logger.warning(ex)\\r                return False\\r            \\r        totallen = len(phrases)\\r        logger.info('Total Records: {0}'.format(totallen))\\r        start = 0\\r        step = 1000\\r        results = []\\r        for i in range(start, totallen, step):\\r            stop = i + step\\r            if (stop > totallen):\\r                stop = totallen\\r            mappedphrases = getEASE(phrases[i:stop],keywords)\\r            results.append(mappedphrases)\\r            for row in dataset:\\r                phrase =row['extracted_phrase']\\r                if mappedphrases.get(phrase,'') !='':\\r                    row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round( float(mappedphrases[phrase]['score']),4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r    except Exception as ex:\\r        logger.error('error in Map phrase')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logging.info(ex)\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in Extract Phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"ngram\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram(dataset):\\r    logger.info('Generating Ngram clusters...')\\r    try:\\r        txt1 =[]\\r        for index, row in dataset.iterrows():\\r            txt1.append(row['clean_text'])\\r       \\r        # Getting trigrams \\r        vectorizer = CountVectorizer(ngram_range = (3,3))\\r        X1 = vectorizer.fit_transform(txt1) \\r        features = (vectorizer.get_feature_names_out())\\r        # Applying TFIDF\\r        vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r        X2 = vectorizer.fit_transform(txt1)\\r        scores = (X2.toarray())\\r        # Getting top ranking features\\r        logging.info('Getting Top 50 Grams')\\r        sums = X2.sum(axis = 0)\\r        data1 = []\\r        for col, term in enumerate(features):\\r            data1.append( (term, sums[0,col] ))\\r        ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r        words = (ranking.sort_values('rank', ascending = False))\\r        top50df = words.nlargest(50,'rank')\\r        top50 = words['term'].tolist()\\r        \\r        distinctText = list(set(txt1))\\r        count=0\\r        totalRecords = len(distinctText)\\r        logging.info('Mapping Text to Gram')\\r        logging.info('Total Unique Values: {0}'.format(totalRecords))\\r        ngramDict = {}\\r        for item in distinctText:\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r            if len(matchingGrams) >0:\\r                ngramDict[item] = matchingGrams[0]\\r            count = count+1\\r            if count%1000 ==0:\\r                logging.info('{0} rows mapped'.format(count))\\r    \\r        dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    except Exception as ex:\\r        logger.error('Error in Ngram')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    \\r    return dataset\"]},{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"qyeEc\",\"alias\":\"Tokenize\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},\"position_x\":\"272\",\"position_y\":\"131\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"MWXZY\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wvMSR\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"ErwuN\",\"alias\":\"Filter\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"259\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"lOPzI\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"XSWOG\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"YCnzZ\",\"alias\":\"Cluster Prioritization\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"prioritization\",\"requirements\":\"\",\"params\":[],\"script\":[\"def prioritization(dataset):\\r    def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r        cluster = ''\\r        if tags is not None and tags != '':\\r            cluster = tags\\r        elif ngram is not None and ngram != '':\\r            cluster = ngram\\r        elif soundex_cluster is not None and soundex_cluster != '':\\r            cluster = soundex_cluster\\r        elif mapped_phrase is not None and mapped_phrase != '':\\r            cluster = mapped_phrase\\r        elif lda_cluster is not None and lda_cluster != '':\\r            cluster = lda_cluster\\r        return cluster\\r    logger.info('Prioritizing cluster for tickets..')\\r    try:\\r        dataset = dataset.replace(np.nan,'')\\r        dataset = dataset.to_dict('records')\\r        \\r        for row in dataset:\\r            row['post_ranking_cluster'] = getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r    \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        dataset = dataset[['number','clean_text','ngram','soundex_cluster','lda_cluster','extracted_phrase','mapped_phrase','mapped_phrase_confidennce','last_updated','post_ranking_cluster']]\\r        dataset = dataset.to_dict('records')\\r        logger.info('Total tickets after clustering {0}'.format(len(dataset)))\\r    except Exception as ex:\\r        logger.error('error in cluster Prioritizing')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    logger.info('Saving Data')\\r    return dataset\\r\"]},\"position_x\":\"679\",\"position_y\":\"264\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"yVBoI\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"DimsH\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"lda\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\r\\\\n\",\"\\r\\\\n\",\"def lda(dataset, clustercount_param=5, uniqueidcolumn_param=''):\\r\\\\n\",\"    logger.info('Generating LDA clusters...')\\r\\\\n\",\"    try:\\r\\\\n\",\"        def cv(dataset, input_col_param):\\r\\\\n\",\"            from sklearn.feature_extraction.text import CountVectorizer\\r\\\\n\",\"            count_vectorizer = CountVectorizer(stop_words='english')\\r\\\\n\",\"            count_data = count_vectorizer.fit_transform(dataset['clean_text'].to_list())\\r\\\\n\",\"            words = count_vectorizer.get_feature_names_out()\\r\\\\n\",\"            return {'data': count_data, 'words': words}\\r\\\\n\",\"        \\r\\\\n\",\"        dataset = pd.DataFrame(dataset)\\r\\\\n\",\"        originaldataset = dataset\\r\\\\n\",\"        result = {}\\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            result[name] = cv(dataset, 'clean_text')\\r\\\\n\",\"            result[name]['number'] = dataset['number'].to_list()\\r\\\\n\",\"    \\r\\\\n\",\"        def LDARunner(dataset, clustercount_param):\\r\\\\n\",\"            from sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\\\n\",\"            import numpy as np\\r\\\\n\",\"            number_topics =  clustercount_param\\r\\\\n\",\"            count_data, words, number = dataset['data'], dataset['words'], dataset['number']\\r\\\\n\",\"             \\r\\\\n\",\"            lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\\\n\",\"            lda.fit(count_data)\\r\\\\n\",\"            documents = lda.transform(count_data)\\r\\\\n\",\"            \\r\\\\n\",\"    \\r\\\\n\",\"            argmax_values = np.argmax(documents, axis=1)\\r\\\\n\",\"    \\r\\\\n\",\"            newdocuments = pd.DataFrame({'cluster_Id':argmax_values, 'number':number})\\r\\\\n\",\"    \\r\\\\n\",\"            def wordsWithWeights(termIndices, termWeights, index):\\r\\\\n\",\"                terms = [words[i] for i in termIndices]\\r\\\\n\",\"                topic = [index]*len(terms)\\r\\\\n\",\"                return list(zip(terms, topic, termIndices, termWeights))\\r\\\\n\",\"            \\r\\\\n\",\"            topics = [ wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) for index, topic in enumerate(lda.components_)]\\r\\\\n\",\"            topicWords = []\\r\\\\n\",\"            \\r\\\\n\",\"            for topic in topics:\\r\\\\n\",\"                topicWords.extend(topic)\\r\\\\n\",\"            finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\\\n\",\"            finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\\\n\",\"            finalTopic.columns = ['topic', 'word', 'weight']\\r\\\\n\",\"            \\r\\\\n\",\"            # newdocuments = newdocuments.merge(finalTopic, left_on='cluster_Id', right_index=True).drop(columns='cluster_Id')\\r\\\\n\",\"            \\r\\\\n\",\"            topicwordsdf = finalTopic.groupby('topic')['word'].apply(list).reset_index()\\r\\\\n\",\"            topicwordsdf.rename(columns={'word': 'cluster_Name'}, inplace=True)\\r\\\\n\",\"            topicwordsdf = topicwordsdf[['topic', 'cluster_Name']]\\r\\\\n\",\"            newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\\\n\",\"            newdocuments = newdocuments.drop(columns='topic')\\r\\\\n\",\"            return newdocuments, finalTopic\\r\\\\n\",\"            \\r\\\\n\",\"        newdocuments_result = {}\\r\\\\n\",\"        finalTopic_result = {}\\r\\\\n\",\"        for name, datasets in result.items(): # dataset->datasets\\r\\\\n\",\"            logger.info('Running LDA for '+name)\\r\\\\n\",\"            newdocuments, finalTopic = LDARunner(datasets, clustercount_param)\\r\\\\n\",\"            finalTopic['group_by_field'] = name\\r\\\\n\",\"            newdocuments['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\\\n\",\"            newdocuments_result[name] = newdocuments\\r\\\\n\",\"            finalTopic_result[name] = finalTopic\\r\\\\n\",\"    \\r\\\\n\",\"        \\r\\\\n\",\"        resultsdf = pd.concat(newdocuments_result.values(), ignore_index=True)\\r\\\\n\",\"        topicsdf = pd.concat(finalTopic_result.values(), ignore_index=True)\\r\\\\n\",\"        topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({'word': list, 'weight': list}).reset_index()\\r\\\\n\",\"        topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"        topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(list(map(lambda y: str(y), x))))\\r\\\\n\",\"        topicsdf['alias'] = topicsdf['words']\\r\\\\n\",\"        topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r\\\\n\",\"    \\r\\\\n\",\"        \\r\\\\n\",\"        resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"        resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r\\\\n\",\"        resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']]\\r\\\\n\",\"      \\r\\\\n\",\"    \\r\\\\n\",\"        dataset = pd.merge(originaldataset,resultsdf, on='number', how='left')\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in LDA')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    logger.info('Mapping extracted phrases to key phrases....')\\r    try:\\r        keywords = []\\r        for item in ease:\\r            word = item['Key_Word']\\r            if word not in keywords:\\r                keywords.append(word)\\r        phrases = []\\r        for item in dataset:\\r            phrase = item['extracted_phrase']\\r            if phrase not in phrases:\\r                phrases.append(phrase)\\r        \\r            \\r        def getSimilar(sentences, keywords):\\r            corpus = sentences + keywords\\r            keywordstartIndex = len(sentences)\\r            tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r            pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r            arr = pairwise_similarity.toarray()\\r            np.fill_diagonal(arr, np.nan)\\r            results = {}\\r            for s in sentences:\\r                input_idx = sentences.index(s)\\r                result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r                match = arr[input_idx][keywordstartIndex + result_idx]\\r                r = keywords[result_idx]\\r                if match > 0:\\r                    results[s] = r + ':' + str(match)\\r                else:\\r                    results[s] = 'NO MATCH:0'\\r            return results\\r\\r    \\r        def getEASE(phrases,keywords):\\r            try:\\r                results = getSimilar(phrases, keywords)\\r                mappings = {}\\r                for pattern in results.keys():\\r                    kw = results[pattern].split(':')[0]\\r                    score = results[pattern].split(':')[-1]\\r                    if kw != 'NO MATCH':\\r                        mappings[pattern] = {'keyword':kw, 'score':score}\\r                return mappings\\r            except Exception as ex:\\r                logger.warning(ex)\\r                return False\\r            \\r        totallen = len(phrases)\\r        logger.info('Total Records: {0}'.format(totallen))\\r        start = 0\\r        step = 1000\\r        results = []\\r        for i in range(start, totallen, step):\\r            stop = i + step\\r            if (stop > totallen):\\r                stop = totallen\\r            mappedphrases = getEASE(phrases[i:stop],keywords)\\r            results.append(mappedphrases)\\r            for row in dataset:\\r                phrase =row['extracted_phrase']\\r                if mappedphrases.get(phrase,'') !='':\\r                    row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round( float(mappedphrases[phrase]['score']),4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r    except Exception as ex:\\r        logger.error('error in Map phrase')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logging.info(ex)\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in Extract Phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"ngram\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram(dataset):\\r    logger.info('Generating Ngram clusters...')\\r    try:\\r        txt1 =[]\\r        for index, row in dataset.iterrows():\\r            txt1.append(row['clean_text'])\\r       \\r        # Getting trigrams \\r        vectorizer = CountVectorizer(ngram_range = (3,3))\\r        X1 = vectorizer.fit_transform(txt1) \\r        features = (vectorizer.get_feature_names_out())\\r        # Applying TFIDF\\r        vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r        X2 = vectorizer.fit_transform(txt1)\\r        scores = (X2.toarray())\\r        # Getting top ranking features\\r        logging.info('Getting Top 50 Grams')\\r        sums = X2.sum(axis = 0)\\r        data1 = []\\r        for col, term in enumerate(features):\\r            data1.append( (term, sums[0,col] ))\\r        ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r        words = (ranking.sort_values('rank', ascending = False))\\r        top50df = words.nlargest(50,'rank')\\r        top50 = words['term'].tolist()\\r        \\r        distinctText = list(set(txt1))\\r        count=0\\r        totalRecords = len(distinctText)\\r        logging.info('Mapping Text to Gram')\\r        logging.info('Total Unique Values: {0}'.format(totalRecords))\\r        ngramDict = {}\\r        for item in distinctText:\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r            if len(matchingGrams) >0:\\r                ngramDict[item] = matchingGrams[0]\\r            count = count+1\\r            if count%1000 ==0:\\r                logging.info('{0} rows mapped'.format(count))\\r    \\r        dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    except Exception as ex:\\r        logger.error('Error in Ngram')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    \\r    return dataset\"]},{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"DimsH\",\"alias\":\"Clusters\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-12 07:27:13\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-05-26 13:02:13\",\"alias\":\"Tickets Enriched\",\"id\":7,\"name\":\"ACMTCKTS40780\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"number\\\",\\\"recordcolumndisplayname\\\":\\\"number\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"clean_text\\\",\\\"recordcolumndisplayname\\\":\\\"clean_text\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"group_by_field\\\",\\\"recordcolumndisplayname\\\":\\\"group_by_field\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"ngram\\\",\\\"recordcolumndisplayname\\\":\\\"ngram\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"soundex_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"soundex_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"lda_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"lda_cluster_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"lda_cluster_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"extracted_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"extracted_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"mapped_phrase\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"mapped_phrase_confidennce\\\",\\\"recordcolumndisplayname\\\":\\\"mapped_phrase_confidennce\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"post_ranking_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"post_ranking_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":12,\\\"recordcolumnname\\\":\\\"cluster_classification_label\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_label\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":13,\\\"recordcolumnname\\\":\\\"cluster_classification_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"cluster_classification_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":14,\\\"recordcolumnname\\\":\\\"resolution_steps_cluster\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_steps_cluster\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":15,\\\"recordcolumnname\\\":\\\"resolution_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":16,\\\"recordcolumnname\\\":\\\"resolution_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"resolution_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":17,\\\"recordcolumnname\\\":\\\"sop\\\",\\\"recordcolumndisplayname\\\":\\\"sop\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":18,\\\"recordcolumnname\\\":\\\"sop_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"sop_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":19,\\\"recordcolumnname\\\":\\\"workflow\\\",\\\"recordcolumndisplayname\\\":\\\"workflow\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":20,\\\"recordcolumnname\\\":\\\"workflow_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"workflow_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":21,\\\"recordcolumnname\\\":\\\"predicted_assignment_group\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignment_group\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":22,\\\"recordcolumnname\\\":\\\"predicted_assignee\\\",\\\"recordcolumndisplayname\\\":\\\"predicted_assignee\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":23,\\\"recordcolumnname\\\":\\\"last_updated\\\",\\\"recordcolumndisplayname\\\":\\\"last_updated\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"datetime\\\",\\\"columnorder\\\":24,\\\"recordcolumnname\\\":\\\"response_SLA\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"string\\\",\\\"columnorder\\\":25,\\\"recordcolumnname\\\":\\\"response_SLA_confidence\\\",\\\"recordcolumndisplayname\\\":\\\"response_SLA_confidence\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"null\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"870\",\"position_y\":\"264\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"YCnzZ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"prioritization\",\"requirements\":\"\",\"params\":[],\"script\":[\"def prioritization(dataset):\\r    def getCluster(tags,ngram,soundex_cluster,lda_cluster,mapped_phrase):\\r        cluster = ''\\r        if tags is not None and tags != '':\\r            cluster = tags\\r        elif ngram is not None and ngram != '':\\r            cluster = ngram\\r        elif soundex_cluster is not None and soundex_cluster != '':\\r            cluster = soundex_cluster\\r        elif mapped_phrase is not None and mapped_phrase != '':\\r            cluster = mapped_phrase\\r        elif lda_cluster is not None and lda_cluster != '':\\r            cluster = lda_cluster\\r        return cluster\\r    logger.info('Prioritizing cluster for tickets..')\\r    try:\\r        dataset = dataset.replace(np.nan,'')\\r        dataset = dataset.to_dict('records')\\r        \\r        for row in dataset:\\r            row['post_ranking_cluster'] = getCluster(row['tags'],row['ngram'],row['soundex_cluster'],row['lda_cluster'],row['mapped_phrase'])\\r    \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        dataset = dataset[['number','clean_text','ngram','soundex_cluster','lda_cluster','extracted_phrase','mapped_phrase','mapped_phrase_confidennce','last_updated','post_ranking_cluster']]\\r        dataset = dataset.to_dict('records')\\r        logger.info('Total tickets after clustering {0}'.format(len(dataset)))\\r    except Exception as ex:\\r        logger.error('error in cluster Prioritizing')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    logger.info('Saving Data')\\r    return dataset\\r\"]},{\"FunctionName\":\"lda\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\r\\\\n\",\"\\r\\\\n\",\"def lda(dataset, clustercount_param=5, uniqueidcolumn_param=''):\\r\\\\n\",\"    logger.info('Generating LDA clusters...')\\r\\\\n\",\"    try:\\r\\\\n\",\"        def cv(dataset, input_col_param):\\r\\\\n\",\"            from sklearn.feature_extraction.text import CountVectorizer\\r\\\\n\",\"            count_vectorizer = CountVectorizer(stop_words='english')\\r\\\\n\",\"            count_data = count_vectorizer.fit_transform(dataset['clean_text'].to_list())\\r\\\\n\",\"            words = count_vectorizer.get_feature_names_out()\\r\\\\n\",\"            return {'data': count_data, 'words': words}\\r\\\\n\",\"        \\r\\\\n\",\"        dataset = pd.DataFrame(dataset)\\r\\\\n\",\"        originaldataset = dataset\\r\\\\n\",\"        result = {}\\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            result[name] = cv(dataset, 'clean_text')\\r\\\\n\",\"            result[name]['number'] = dataset['number'].to_list()\\r\\\\n\",\"    \\r\\\\n\",\"        def LDARunner(dataset, clustercount_param):\\r\\\\n\",\"            from sklearn.decomposition import LatentDirichletAllocation as LDA\\r\\\\n\",\"            import numpy as np\\r\\\\n\",\"            number_topics =  clustercount_param\\r\\\\n\",\"            count_data, words, number = dataset['data'], dataset['words'], dataset['number']\\r\\\\n\",\"             \\r\\\\n\",\"            lda = LDA(n_components=number_topics, n_jobs=-1)\\r\\\\n\",\"            lda.fit(count_data)\\r\\\\n\",\"            documents = lda.transform(count_data)\\r\\\\n\",\"            \\r\\\\n\",\"    \\r\\\\n\",\"            argmax_values = np.argmax(documents, axis=1)\\r\\\\n\",\"    \\r\\\\n\",\"            newdocuments = pd.DataFrame({'cluster_Id':argmax_values, 'number':number})\\r\\\\n\",\"    \\r\\\\n\",\"            def wordsWithWeights(termIndices, termWeights, index):\\r\\\\n\",\"                terms = [words[i] for i in termIndices]\\r\\\\n\",\"                topic = [index]*len(terms)\\r\\\\n\",\"                return list(zip(terms, topic, termIndices, termWeights))\\r\\\\n\",\"            \\r\\\\n\",\"            topics = [ wordsWithWeights(topic.argsort()[:-10:-1], topic[topic.argsort()[:-10:-1]], index) for index, topic in enumerate(lda.components_)]\\r\\\\n\",\"            topicWords = []\\r\\\\n\",\"            \\r\\\\n\",\"            for topic in topics:\\r\\\\n\",\"                topicWords.extend(topic)\\r\\\\n\",\"            finalTopic = pd.DataFrame(topicWords, columns=['topicWords', 'topic', 'termIndices', 'termWeights'])\\r\\\\n\",\"            finalTopic = finalTopic[['topic', 'topicWords', 'termWeights']]\\r\\\\n\",\"            finalTopic.columns = ['topic', 'word', 'weight']\\r\\\\n\",\"            \\r\\\\n\",\"            # newdocuments = newdocuments.merge(finalTopic, left_on='cluster_Id', right_index=True).drop(columns='cluster_Id')\\r\\\\n\",\"            \\r\\\\n\",\"            topicwordsdf = finalTopic.groupby('topic')['word'].apply(list).reset_index()\\r\\\\n\",\"            topicwordsdf.rename(columns={'word': 'cluster_Name'}, inplace=True)\\r\\\\n\",\"            topicwordsdf = topicwordsdf[['topic', 'cluster_Name']]\\r\\\\n\",\"            newdocuments = newdocuments.merge(topicwordsdf, left_on='cluster_Id', right_on='topic', how='inner')\\r\\\\n\",\"            newdocuments = newdocuments.drop(columns='topic')\\r\\\\n\",\"            return newdocuments, finalTopic\\r\\\\n\",\"            \\r\\\\n\",\"        newdocuments_result = {}\\r\\\\n\",\"        finalTopic_result = {}\\r\\\\n\",\"        for name, datasets in result.items(): # dataset->datasets\\r\\\\n\",\"            logger.info('Running LDA for '+name)\\r\\\\n\",\"            newdocuments, finalTopic = LDARunner(datasets, clustercount_param)\\r\\\\n\",\"            finalTopic['group_by_field'] = name\\r\\\\n\",\"            newdocuments['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r\\\\n\",\"            newdocuments_result[name] = newdocuments\\r\\\\n\",\"            finalTopic_result[name] = finalTopic\\r\\\\n\",\"    \\r\\\\n\",\"        \\r\\\\n\",\"        resultsdf = pd.concat(newdocuments_result.values(), ignore_index=True)\\r\\\\n\",\"        topicsdf = pd.concat(finalTopic_result.values(), ignore_index=True)\\r\\\\n\",\"        topicsdf = topicsdf.groupby(['group_by_field', 'topic']).agg({'word': list, 'weight': list}).reset_index()\\r\\\\n\",\"        topicsdf['words'] = topicsdf['word'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"        topicsdf['weights'] = topicsdf['weight'].apply(lambda x: ', '.join(list(map(lambda y: str(y), x))))\\r\\\\n\",\"        topicsdf['alias'] = topicsdf['words']\\r\\\\n\",\"        topicsdf.drop(columns=['word', 'weight'], inplace=True)\\r\\\\n\",\"    \\r\\\\n\",\"        \\r\\\\n\",\"        resultsdf['lda_cluster'] = resultsdf['cluster_Name'].apply(lambda x: ', '.join(x))\\r\\\\n\",\"        resultsdf.drop(columns=['cluster_Name'], inplace=True)\\r\\\\n\",\"        resultsdf = resultsdf[['number', 'lda_cluster', 'last_updated']]\\r\\\\n\",\"      \\r\\\\n\",\"    \\r\\\\n\",\"        dataset = pd.merge(originaldataset,resultsdf, on='number', how='left')\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in LDA')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    logger.info('Mapping extracted phrases to key phrases....')\\r    try:\\r        keywords = []\\r        for item in ease:\\r            word = item['Key_Word']\\r            if word not in keywords:\\r                keywords.append(word)\\r        phrases = []\\r        for item in dataset:\\r            phrase = item['extracted_phrase']\\r            if phrase not in phrases:\\r                phrases.append(phrase)\\r        \\r            \\r        def getSimilar(sentences, keywords):\\r            corpus = sentences + keywords\\r            keywordstartIndex = len(sentences)\\r            tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r            pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r            arr = pairwise_similarity.toarray()\\r            np.fill_diagonal(arr, np.nan)\\r            results = {}\\r            for s in sentences:\\r                input_idx = sentences.index(s)\\r                result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r                match = arr[input_idx][keywordstartIndex + result_idx]\\r                r = keywords[result_idx]\\r                if match > 0:\\r                    results[s] = r + ':' + str(match)\\r                else:\\r                    results[s] = 'NO MATCH:0'\\r            return results\\r\\r    \\r        def getEASE(phrases,keywords):\\r            try:\\r                results = getSimilar(phrases, keywords)\\r                mappings = {}\\r                for pattern in results.keys():\\r                    kw = results[pattern].split(':')[0]\\r                    score = results[pattern].split(':')[-1]\\r                    if kw != 'NO MATCH':\\r                        mappings[pattern] = {'keyword':kw, 'score':score}\\r                return mappings\\r            except Exception as ex:\\r                logger.warning(ex)\\r                return False\\r            \\r        totallen = len(phrases)\\r        logger.info('Total Records: {0}'.format(totallen))\\r        start = 0\\r        step = 1000\\r        results = []\\r        for i in range(start, totallen, step):\\r            stop = i + step\\r            if (stop > totallen):\\r                stop = totallen\\r            mappedphrases = getEASE(phrases[i:stop],keywords)\\r            results.append(mappedphrases)\\r            for row in dataset:\\r                phrase =row['extracted_phrase']\\r                if mappedphrases.get(phrase,'') !='':\\r                    row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                    row['mapped_phrase_confidennce'] = str(round( float(mappedphrases[phrase]['score']),4))\\r                else:\\r                    row['mapped_phrase'] = ''\\r                    row['mapped_phrase_confidennce'] = '0.0'\\r    except Exception as ex:\\r        logger.error('error in Map phrase')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logging.info(ex)\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in Extract Phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"ngram\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rimport sklearn\\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\rimport pandas as pd\\rimport numpy as np\\rfrom datetime import datetime\\r\\rdef ngram(dataset):\\r    logger.info('Generating Ngram clusters...')\\r    try:\\r        txt1 =[]\\r        for index, row in dataset.iterrows():\\r            txt1.append(row['clean_text'])\\r       \\r        # Getting trigrams \\r        vectorizer = CountVectorizer(ngram_range = (3,3))\\r        X1 = vectorizer.fit_transform(txt1) \\r        features = (vectorizer.get_feature_names_out())\\r        # Applying TFIDF\\r        vectorizer = TfidfVectorizer(ngram_range = (3,3))\\r        X2 = vectorizer.fit_transform(txt1)\\r        scores = (X2.toarray())\\r        # Getting top ranking features\\r        logging.info('Getting Top 50 Grams')\\r        sums = X2.sum(axis = 0)\\r        data1 = []\\r        for col, term in enumerate(features):\\r            data1.append( (term, sums[0,col] ))\\r        ranking = pd.DataFrame(data1, columns = ['term','rank'])\\r        words = (ranking.sort_values('rank', ascending = False))\\r        top50df = words.nlargest(50,'rank')\\r        top50 = words['term'].tolist()\\r        \\r        distinctText = list(set(txt1))\\r        count=0\\r        totalRecords = len(distinctText)\\r        logging.info('Mapping Text to Gram')\\r        logging.info('Total Unique Values: {0}'.format(totalRecords))\\r        ngramDict = {}\\r        for item in distinctText:\\r            matchingGrams = [gram for gram in top50 if gram in item]\\r            if len(matchingGrams) >0:\\r                ngramDict[item] = matchingGrams[0]\\r            count = count+1\\r            if count%1000 ==0:\\r                logging.info('{0} rows mapped'.format(count))\\r    \\r        dataset['ngram'] = dataset['clean_text'].apply(lambda x: ngramDict.get(x,''))\\r    except Exception as ex:\\r        logger.error('Error in Ngram')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    \\r    return dataset\"]},{\"FunctionName\":\"soundex\",\"requirements\":\"\",\"params\":[],\"script\":[\"def soundex_generator(token):\\r    if token == '':\\r        return ''\\r    # Convert the word to upper \\r    # case for uniformity\\r    token = token.upper()\\r    soundex = ''\\r    # Retain the First Letter\\r    soundex += token[0]\\r\\r    dictionary = {'BFPV': '1', 'CGJKQSXZ': '2', \\r                  'DT': '3',\\r                  'L': '4', 'MN': '5', 'R': '6',\\r                  'AEIOUHWY': '.'}\\r \\r    # Enode as per the dictionary\\r    for char in token[1:]:\\r        for key in dictionary.keys():\\r            if char in key:\\r                code = dictionary[key]\\r                if code != '.':\\r                    if code != soundex[-1]:\\r                        soundex += code\\r \\r    return soundex\\rdef soundex( dataset):    #python-script Data\\r    logger.info('Generating soundex clusters...')\\r    try:\\r        dataset['sound'] = dataset['clean_text'].apply(soundex_generator)\\r        sound_Df = dataset.groupby(['group_by_field','sound']).agg(\\r        numberList = pd.NamedAgg(column='number',aggfunc=list),\\r            textList = pd.NamedAgg(column='clean_text',aggfunc=list)\\r        ).reset_index()\\r        sound_Df['numberListSize'] = sound_Df['numberList'].apply(len)\\r        sound_Df = sound_Df[sound_Df['numberList'].apply(lambda x : len(x) >= 5)]\\r        sound_Df['cluster'] = sound_Df['textList'].apply(lambda x: x[0])\\r        sound_Df = sound_Df.drop(columns=['textList'])\\r        sound_Df = sound_Df.explode('numberList').reset_index(drop=True)\\r        sound_Df = sound_Df.rename(columns={'numberList':'number','cluster': 'soundex_cluster'})\\r        dataset = pd.merge(dataset, sound_Df,on=['number','group_by_field','sound'], how='left')\\r        logger.info('Total tickets after soundex {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in soundex')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"tokenize\",\"requirements\":\"\",\"params\":[],\"script\":[\"def tokenize(dataset):\\r\\\\n\",\"    try:\\r\\\\n\",\"        from nltk.tokenize import word_tokenize\\r\\\\n\",\"      \\r\\\\n\",\"        grouped = dataset.groupby('group_by_field')\\r\\\\n\",\"        grouped_df = {}\\r\\\\n\",\"        for name, group in grouped:\\r\\\\n\",\"            grouped_df[name] = group\\r\\\\n\",\"    \\r\\\\n\",\"        for name, dataset in grouped_df.items():\\r\\\\n\",\"            dataset['tokens'] = dataset['clean_text'].apply(lambda input: word_tokenize(input))\\r\\\\n\",\"        dataset = pd.concat(grouped_df.values(), ignore_index=True)\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('error in tokenizer')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test,id\",\"type\":\"Text\",\"alias\":\"test,id\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in stop word remover')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"import traceback\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head())\\r        \\r        logger.info('Filtering Tickets... CIs with less than 10 tickets will be ignored for clustering')\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        logger.info('Filtering Tickets with empty shortdescription')\\r        filteredCIs = dataset.groupby('group_by_field').size().reset_index(name='count')\\r        ciList = filteredCIs[filteredCIs['count'] >=10]['group_by_field'].tolist()\\r        filtereddf = dataset['group_by_field'].isin(ciList)\\r        dataset['include'] = filtereddf\\r        dataset = dataset[dataset['include'] == True]\\r        logger.info('Tickets  for clustering {0}'.format(len(dataset.index)))\\r    except Exception as ex:\\r        logger.error('Error in Filter Data')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-10-20 09:32:46\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"number\"},\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"sumit.dobal@infosys.com\",\"lastmodifieddate\":\"2023-11-15 08:02:41\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"encG3LoaV6CZ0ouDWBTBBHLaUj38IE5CwPA\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"root\\\",\\\"url\\\":\\\"jdbc:mysql://10.85.12.143:32261/300_leapmaster_ref_data\\\"}\",\"salt\":\"MHNLZ7yorsU8VquHzuH+kCpSteHz+A8S438ny2uIEEq/Kn9/aEGI/y5LK0P2lQH5yJQdpugHOuEv4uwt+Vo1lQ==\",\"organization\":\"leo1311\",\"dshashcode\":\"ba558f3349cb118be357aa2a49787b80a2f1e371d76bb66001815abef42dd027\",\"activetime\":\"2023-11-15 08:02:40\",\"category\":\"SQL\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}]}","shreya_bansal@infosys.com","Clustering","2023-11-20T12:58:40","LEOCLSTR73042","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"36\":{\"taskId\":\"b0b106d0-e97d-46db-b136-dbdc7e1ed7a0\"}}"
"admin","2023-12-08T04:52:41.935","false","Anomaly Detection using Isolation Forest","NULL","{\"elements\":[{\"id\":\"hPzjt\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"anomaly_function\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\r\\\\n\",\"import numpy as np\\r\\\\n\",\"import matplotlib.pyplot as plt\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"from sklearn.ensemble import IsolationForest\\r\\\\n\",\"\\r\\\\n\",\"def anomaly_function(dataset):\\r\\\\n\",\"  data=pd.DataFrame(dataset)\\r\\\\n\",\"  data['run_timestamp']=pd.to_datetime(data['run_timestamp'])\\r\\\\n\",\"  data=data.set_index('run_timestamp').resample('H').mean().reset_index()\\r\\\\n\",\"  model=IsolationForest(contamination=0.01)\\r\\\\n\",\"  model.fit(data[['value']])\\r\\\\n\",\"  data['anomaly']=pd.Series(model.predict(data[['value']])).apply(lambda x:'yes' if x==-1 else 'no')\\r\\\\n\",\"  data['anomaly_scores'] = model.decision_function(data[['value']])\\r\\\\n\",\"  data = data.to_dict('records')\\r\\\\n\",\"  return data\"]},\"position_x\":\"536\",\"position_y\":\"73\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"Bymji\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"COJVu\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-08 05:32:11\",\"alias\":\"CPU_Percent\",\"id\":92,\"name\":\"CPU_Percent\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT run_timestamp, value FROM @projectname_anomaly_cpu  ORDER BY run_timestamp\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"COJVu\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-08 05:32:11\",\"alias\":\"CPU_Percent\",\"id\":92,\"name\":\"CPU_Percent\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT run_timestamp, value FROM @projectname_anomaly_cpu  ORDER BY run_timestamp\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"232\",\"position_y\":\"73\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"hPzjt\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"Bymji\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 09:48:53\",\"alias\":\"CPU_Percentageanomaly_ML\",\"id\":864,\"name\":\"LEOCP_PR87493\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_anomaly_detection\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"overwrite\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_anomaly_detection\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"747\",\"position_y\":\"73\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"hPzjt\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"anomaly_function\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\r\\\\n\",\"import numpy as np\\r\\\\n\",\"import matplotlib.pyplot as plt\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"from sklearn.ensemble import IsolationForest\\r\\\\n\",\"\\r\\\\n\",\"def anomaly_function(dataset):\\r\\\\n\",\"  data=pd.DataFrame(dataset)\\r\\\\n\",\"  data['run_timestamp']=pd.to_datetime(data['run_timestamp'])\\r\\\\n\",\"  data=data.set_index('run_timestamp').resample('H').mean().reset_index()\\r\\\\n\",\"  model=IsolationForest(contamination=0.01)\\r\\\\n\",\"  model.fit(data[['value']])\\r\\\\n\",\"  data['anomaly']=pd.Series(model.predict(data[['value']])).apply(lambda x:'yes' if x==-1 else 'no')\\r\\\\n\",\"  data['anomaly_scores'] = model.decision_function(data[['value']])\\r\\\\n\",\"  data = data.to_dict('records')\\r\\\\n\",\"  return data\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-08 05:32:11\",\"alias\":\"CPU_Percent\",\"id\":92,\"name\":\"CPU_Percent\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT run_timestamp, value FROM @projectname_anomaly_cpu  ORDER BY run_timestamp\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Anomaly_Detection","2023-12-19T09:48:57","LEOANMLY94383","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"16\":{\"taskId\":\"0ce52ba8-26d6-4eca-b283-ec7b48758306\"}}"
"admin","2023-10-26T08:50:53.205","false","","NULL","{\"elements\":[{\"id\":\"lOPzI\",\"alias\":\"Tickets\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"36\",\"position_y\":\"19\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"ErwuN\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"XSWOG\",\"alias\":\"Clean text\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rimport traceback\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('Error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"476\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"YvhqP\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"ErwuN\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"YvhqP\",\"alias\":\"StopwordRemover\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in Remove Stopwords')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"667\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"XSWOG\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"MWXZY\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rimport traceback\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('Error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"MWXZY\",\"alias\":\"Lemmetizer\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('Error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"446\",\"position_y\":\"129\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"YvhqP\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"omSub\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in Remove Stopwords')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rimport traceback\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('Error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"omSub\",\"alias\":\"Extract Phrases\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    \\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logger.warn(traceback.format_exc())\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('Error in Extract phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},\"position_x\":\"870\",\"position_y\":\"131\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"MWXZY\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"lDCSS\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('Error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in Remove Stopwords')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rimport traceback\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('Error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"lDCSS\",\"alias\":\"Map Phrases\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    keywords = []\\r    for item in ease:\\r        word = item['Key_Word']\\r        if word not in keywords:\\r            keywords.append(word)\\r    phrases = []\\r    for item in dataset:\\r        phrase = item['extracted_phrase']\\r        if phrase not in phrases:\\r            phrases.append(phrase)\\r    \\r        \\r    def getSimilar(sentences, keywords):\\r        corpus = sentences + keywords\\r        keywordstartIndex = len(sentences)\\r        tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r        pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r        arr = pairwise_similarity.toarray()\\r        np.fill_diagonal(arr, np.nan)\\r        results = {}\\r        for s in sentences:\\r            input_idx = sentences.index(s)\\r            result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r            match = arr[input_idx][keywordstartIndex + result_idx]\\r            r = keywords[result_idx]\\r            if match > 0:\\r                results[s] = r + ':' + str(match)\\r            else:\\r                results[s] = 'NO MATCH:0'\\r        return results\\r\\r\\r    def getEASE(phrases,keywords):\\r        try:\\r            results = getSimilar(phrases, keywords)\\r            mappings = {}\\r            for pattern in results.keys():\\r                kw = results[pattern].split(':')[0]\\r                score = results[pattern].split(':')[-1]\\r                if kw != 'NO MATCH':\\r                    mappings[pattern] = {'keyword':kw, 'score':score}\\r            return mappings\\r        except Exception as ex:\\r            logger.warning(ex)\\r            return False\\r    \\r    totallen = len(phrases)\\r    logger.info('Total Records: {0}'.format(totallen))\\r    start = 0\\r    step = 1000\\r    results = []\\r    for i in range(start, totallen, step):\\r        stop = i + step\\r        if (stop > totallen):\\r            stop = totallen\\r        mappedphrases = getEASE(phrases[i:stop],keywords)\\r       \\r        for row in dataset:\\r            phrase =row['extracted_phrase']\\r            row['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r            if mappedphrases.get(phrase,'') !='':\\r                row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                row['mapped_phrase_confidennce'] =  mappedphrases[phrase]['score']\\r            else:\\r                row['mapped_phrase'] =''\\r                row['mapped_phrase_confidennce'] = '0.0'\\r            results.append(row)\\r    print(results[0:10])\\r    return results\"]},\"position_x\":\"280\",\"position_y\":\"264\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"omSub\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"xiAMI\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"OVsqJ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    \\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logger.warn(traceback.format_exc())\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('Error in Extract phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('Error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in Remove Stopwords')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rimport traceback\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('Error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"OVsqJ\",\"alias\":\"Ease Mappings\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"65\",\"position_y\":\"192\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"lDCSS\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"ErwuN\",\"alias\":\"Filter\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},\"position_x\":\"259\",\"position_y\":\"14\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"lOPzI\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"XSWOG\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"xiAMI\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:57:13\",\"alias\":\"Extracted Phrases\",\"id\":873,\"name\":\"LEOEXTRC41937\",\"description\":\"Phrases extracted from tickets\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select number, extracted_phrase, mapped_phrase, mapped_phrase_confidence, last_updated from @projectname_phrase_extraction\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"overwrite\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_phrase_extraction\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"\\\"\\\"\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"532\",\"position_y\":\"262\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"lDCSS\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"map_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"from sklearn.feature_extraction.text import TfidfVectorizer\\rimport numpy as np\\rdef map_phrases( dataset, ease):    #python-script Data\\r    keywords = []\\r    for item in ease:\\r        word = item['Key_Word']\\r        if word not in keywords:\\r            keywords.append(word)\\r    phrases = []\\r    for item in dataset:\\r        phrase = item['extracted_phrase']\\r        if phrase not in phrases:\\r            phrases.append(phrase)\\r    \\r        \\r    def getSimilar(sentences, keywords):\\r        corpus = sentences + keywords\\r        keywordstartIndex = len(sentences)\\r        tfidfCorpus = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(corpus)\\r        pairwise_similarity = tfidfCorpus * tfidfCorpus.T\\r        arr = pairwise_similarity.toarray()\\r        np.fill_diagonal(arr, np.nan)\\r        results = {}\\r        for s in sentences:\\r            input_idx = sentences.index(s)\\r            result_idx = np.nanargmax(arr[input_idx][keywordstartIndex:])\\r            match = arr[input_idx][keywordstartIndex + result_idx]\\r            r = keywords[result_idx]\\r            if match > 0:\\r                results[s] = r + ':' + str(match)\\r            else:\\r                results[s] = 'NO MATCH:0'\\r        return results\\r\\r\\r    def getEASE(phrases,keywords):\\r        try:\\r            results = getSimilar(phrases, keywords)\\r            mappings = {}\\r            for pattern in results.keys():\\r                kw = results[pattern].split(':')[0]\\r                score = results[pattern].split(':')[-1]\\r                if kw != 'NO MATCH':\\r                    mappings[pattern] = {'keyword':kw, 'score':score}\\r            return mappings\\r        except Exception as ex:\\r            logger.warning(ex)\\r            return False\\r    \\r    totallen = len(phrases)\\r    logger.info('Total Records: {0}'.format(totallen))\\r    start = 0\\r    step = 1000\\r    results = []\\r    for i in range(start, totallen, step):\\r        stop = i + step\\r        if (stop > totallen):\\r            stop = totallen\\r        mappedphrases = getEASE(phrases[i:stop],keywords)\\r       \\r        for row in dataset:\\r            phrase =row['extracted_phrase']\\r            row['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\r            if mappedphrases.get(phrase,'') !='':\\r                row['mapped_phrase'] = mappedphrases[phrase]['keyword']\\r                row['mapped_phrase_confidennce'] =  mappedphrases[phrase]['score']\\r            else:\\r                row['mapped_phrase'] =''\\r                row['mapped_phrase_confidennce'] = '0.0'\\r            results.append(row)\\r    print(results[0:10])\\r    return results\"]},{\"FunctionName\":\"extract_phrases\",\"requirements\":\"\",\"params\":[],\"script\":[\"import logging\\r\\\\n\",\"import spacy\\r\\\\n\",\"import pytextrank\\r\\\\n\",\"from datetime import datetime\\r\\\\n\",\"def extract_phrases(dataset):\\r\\\\n\",\"    logger.info('Extracting phrases....')\\r\\\\n\",\"    \\r\\\\n\",\"    try:\\r\\\\n\",\"        dataset = dataset.to_dict('records')\\r\\\\n\",\"        nlp = spacy.load('en_core_web_sm')\\r\\\\n\",\"        nlp.add_pipe('textrank')\\r\\\\n\",\"        timenow = datetime.now()\\r\\\\n\",\"        totalRecords = len(dataset)\\r\\\\n\",\"        count =0\\r\\\\n\",\"        textPhraseMappings = {}\\r\\\\n\",\"        for row in dataset:\\r\\\\n\",\"            try:\\r\\\\n\",\"                text = row['clean_text']\\r\\\\n\",\"                if textPhraseMappings.get(text,'') != '':\\r\\\\n\",\"                    row['extracted_phrase'] = textPhraseMappings[text]\\r\\\\n\",\"                    break\\r\\\\n\",\"                doc = nlp(text)\\r\\\\n\",\"                phrase =''\\r\\\\n\",\"                if len(doc._.phrases)>0:\\r\\\\n\",\"                    for item in doc._.phrases:\\r\\\\n\",\"                        withoutSpace = item.text.replace(' ' ,'')\\r\\\\n\",\"                        if not withoutSpace.isdigit() and len(item.text.split(' ')) >1:\\r\\\\n\",\"                            phrase = item.text\\r\\\\n\",\"                            break\\r\\\\n\",\"                if phrase != '':\\r\\\\n\",\"                    row['extracted_phrase'] = phrase\\r\\\\n\",\"                else:\\r\\\\n\",\"                    row['extracted_phrase'] = text\\r\\\\n\",\"            except Exception as ex:\\r\\\\n\",\"                logger.warn(traceback.format_exc())\\r\\\\n\",\"                row['extracted_phrase'] = text\\r\\\\n\",\"    except Exception as ex:\\r\\\\n\",\"        logger.error('Error in Extract phrases')\\r\\\\n\",\"        logger.error(traceback.format_exc())\\r\\\\n\",\"        exit()\\r\\\\n\",\"        \\r\\\\n\",\"    return dataset\"]},{\"FunctionName\":\"lemmetize_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"def lematize(text):\\r    w_tokenizer = word_tokenize(text)\\r    lemmatizer = nltk.stem.WordNetLemmatizer()\\r    words=[lemmatizer.lemmatize(token, pos='v') for token in w_tokenizer]\\r    return ' '.join(words)\\rdef lemmetize_text( dataset):    #python-script Data\\r    logger.info('Lemmetizing text in Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(lematize)\\r    except Exception as ex:\\r        logger.error('Error in lemmatizer')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"remove_stopwords\",\"requirements\":\"\",\"params\":[{\"name\":\"custom_stopwords\",\"value\":\"test\",\"type\":\"Text\",\"alias\":\"test\",\"index\":\"1\"}],\"script\":[\"import nltk\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rdef stopword_remover(tokens,custom_stopwords_param=''):\\r    custom_stopwords_param = custom_stopwords_param.split(',')\\r    stopwords_nltk = set(stopwords.words('english'))\\r    stop_words=stopwords_nltk.union(custom_stopwords_param)\\r    word = word_tokenize(tokens)\\r    words = [token for token in word if token.lower() not in stop_words]\\r    return ' '.join(words)\\rdef remove_stopwords( dataset,custom_stopwords_param=''):    #python-script Data\\r    logger.info('Removing stopwords.... All english stopwords like is,a,the, etc. + custom stop words will be removed.')\\r    try:\\r        dataset['clean_text'] = dataset['clean_text'].apply(stopword_remover,custom_stopwords_param=custom_stopwords_param)\\r    except Exception as ex:\\r        logger.error('Error in Remove Stopwords')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"clean_text\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rimport traceback\\rdef alphaNum(text):   \\r        alphanumeric = ''    \\r        for character in text:\\r            if character.isalnum():            \\r                alphanumeric += character        \\r            else:            \\r                alphanumeric += ' '    \\r        finalTokens = [t for t in alphanumeric.split(' ') if not t.isnumeric()]\\r        return ' '.join(finalTokens) \\rdef clean_text( dataset):  \\r    logger.info('Cleaning Tickets...')\\r    try:\\r        dataset['clean_text'] = dataset['shortdescription'].apply(alphaNum)\\r    except Exception as ex:\\r        logger.error('Error in Clean Text')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"FunctionName\":\"filter_data\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef filter_data( dataset):    #python-script Data\\r    logger.info('Fetched {0} tickets'.format(len(dataset)))\\r    try:\\r  \\r        dataset = pd.DataFrame(dataset)\\r        logger.info(dataset.head(5))\\r        \\r        dataset = dataset[['number',  'shortdescription', 'configurationItem','tags']]\\r        dataset['shortdescription'].replace('', np.nan, inplace=True)\\r        dataset.dropna(subset=['shortdescription'], inplace=True)\\r        dataset = dataset.rename(columns={'configurationItem':'group_by_field'})\\r        \\r        logger.info('Tickets  for EASE {0}'.format(len(dataset.index)))\\r    \\r    except Exception as ex:\\r        logger.error('Error in Filter Tickets')\\r        logger.error(traceback.format_exc())\\r        exit()\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:55:40\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT * from @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":\"[]\",\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:55:16\",\"alias\":\"EASE Mapping\",\"id\":288,\"name\":\"ACMESMPN85731\",\"description\":\"\",\"schema\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-06-14 11:49:12\",\"alias\":\"EASE Mapping\",\"id\":14,\"name\":\"ACMESMPN94605\",\"description\":null,\"schemavalue\":\"[{\\\"columntype\\\":\\\"int\\\",\\\"columnorder\\\":1,\\\"recordcolumnname\\\":\\\"ID\\\",\\\"recordcolumndisplayname\\\":\\\"ID\\\",\\\"isunique\\\":true,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":2,\\\"recordcolumnname\\\":\\\"Key_Word\\\",\\\"recordcolumndisplayname\\\":\\\"Key_Word\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":3,\\\"recordcolumnname\\\":\\\"Category\\\",\\\"recordcolumndisplayname\\\":\\\"Category\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":4,\\\"recordcolumnname\\\":\\\"Ease\\\",\\\"recordcolumndisplayname\\\":\\\"Ease\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":5,\\\"recordcolumnname\\\":\\\"Support_Level\\\",\\\"recordcolumndisplayname\\\":\\\"Support_Level\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":6,\\\"recordcolumnname\\\":\\\"Ranks\\\",\\\"recordcolumndisplayname\\\":\\\"Ranks\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":true},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":7,\\\"recordcolumnname\\\":\\\"Business_Area\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Area\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":8,\\\"recordcolumnname\\\":\\\"Typical_Resolution\\\",\\\"recordcolumndisplayname\\\":\\\"Typical_Resolution\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"text\\\",\\\"columnorder\\\":9,\\\"recordcolumnname\\\":\\\"Business_Impact\\\",\\\"recordcolumndisplayname\\\":\\\"Business_Impact\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":10,\\\"recordcolumnname\\\":\\\"Account\\\",\\\"recordcolumndisplayname\\\":\\\"Account\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false},{\\\"columntype\\\":\\\"varchar\\\",\\\"columnorder\\\":11,\\\"recordcolumnname\\\":\\\"BotName\\\",\\\"recordcolumndisplayname\\\":\\\"BotName\\\",\\\"isprimarykey\\\":false,\\\"isunique\\\":false,\\\"isrequired\\\":false}]\",\"organization\":\"leo1311\"},\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_EASEMapping\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_EASEMapping\\\",\\\"uniqueIdentifier\\\":\\\"ID\\\"}\",\"dashboard\":null,\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":null,\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}],\"environment\":[],\"default_runtime\":\"{\\\"dsAlias\\\":\\\"LocalCluster\\\",\\\"dsName\\\":\\\"LEALCLCL12132\\\",\\\"type\\\":\\\"REMOTE\\\"}\"}","admin","EASE Analytics","2023-12-15T05:52:31","LEOESNLY85902","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"16\":{\"taskId\":\"17866b52-fa8c-4a9a-82e5-037aa7450c55\"}}"
"admin","2023-12-11T10:27:53.563","false","","NULL","{\"elements\":[{\"id\":\"aXkUZ\",\"alias\":\"Assignment config\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:09:18\",\"alias\":\"Assignment Config\",\"id\":267,\"name\":\"ACMASGNM88206\",\"description\":null,\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentGroup FROM @projectname_assignment_config\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_assignment_config\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"86\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wCLyn\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"blqER\",\"alias\":\"Unassigned Tickets\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 10:46:24\",\"alias\":\"tickets unassigned \",\"id\":867,\"name\":\"LEOTCKTS57088\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, configurationItem FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"101\",\"position_y\":\"92\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wCLyn\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"wCLyn\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"PreProcessing\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentId\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"}],\"script\":[\"import logging\\rimport pandas as pd\\rdef PreProcessing(dataset1,dataset2,incidentid_param=''):\\r    incidentId=incidentid_param\\r    print('Running for incident '+ incidentId)\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    if incidentId !='':\\r        dataset1=dataset1[(dataset1['number']==incidentId)]\\r    dataset = pd.merge(dataset1,dataset2,on ='configurationItem')\\r    print(dataset)\\r    # print(dataset.isnull().sum())\\r    return dataset\"]},\"position_x\":\"323\",\"position_y\":\"95\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"aXkUZ\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"blqER\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"EarHI\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:09:18\",\"alias\":\"Assignment Config\",\"id\":267,\"name\":\"ACMASGNM88206\",\"description\":null,\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentGroup FROM @projectname_assignment_config\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_assignment_config\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 10:46:24\",\"alias\":\"tickets unassigned \",\"id\":867,\"name\":\"LEOTCKTS57088\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, configurationItem FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"EarHI\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef PythonScript( dataset):    \\r    dataset = dataset[['number','assignmentGroup']].rename(columns = {'assignmentGroup':'predicted_assignment_group'})\\r    dataset = dataset.to_dict('records')\\r    print(dataset)\\r    return dataset\"]},\"position_x\":\"538\",\"position_y\":\"91\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"wCLyn\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"lJFHL\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"PreProcessing\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentId\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"}],\"script\":[\"import logging\\rimport pandas as pd\\rdef PreProcessing(dataset1,dataset2,incidentid_param=''):\\r    incidentId=incidentid_param\\r    print('Running for incident '+ incidentId)\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    if incidentId !='':\\r        dataset1=dataset1[(dataset1['number']==incidentId)]\\r    dataset = pd.merge(dataset1,dataset2,on ='configurationItem')\\r    print(dataset)\\r    # print(dataset.isnull().sum())\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:09:18\",\"alias\":\"Assignment Config\",\"id\":267,\"name\":\"ACMASGNM88206\",\"description\":null,\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentGroup FROM @projectname_assignment_config\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_assignment_config\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 10:46:24\",\"alias\":\"tickets unassigned \",\"id\":867,\"name\":\"LEOTCKTS57088\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, configurationItem FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"lJFHL\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:47:45\",\"alias\":\"Assigned tickets\",\"id\":868,\"name\":\"LEOASGND10555\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select number, predicted_assignment_group from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"730\",\"position_y\":\"90\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"EarHI\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"\\rdef PythonScript( dataset):    \\r    dataset = dataset[['number','assignmentGroup']].rename(columns = {'assignmentGroup':'predicted_assignment_group'})\\r    dataset = dataset.to_dict('records')\\r    print(dataset)\\r    return dataset\"]},{\"FunctionName\":\"PreProcessing\",\"requirements\":\"\",\"params\":[{\"name\":\"incidentId\",\"value\":\"\",\"type\":\"Text\",\"alias\":\"\",\"index\":\"1\"}],\"script\":[\"import logging\\rimport pandas as pd\\rdef PreProcessing(dataset1,dataset2,incidentid_param=''):\\r    incidentId=incidentid_param\\r    print('Running for incident '+ incidentId)\\r    dataset1 = pd.DataFrame(dataset1)\\r    dataset2 = pd.DataFrame(dataset2)\\r    if incidentId !='':\\r        dataset1=dataset1[(dataset1['number']==incidentId)]\\r    dataset = pd.merge(dataset1,dataset2,on ='configurationItem')\\r    print(dataset)\\r    # print(dataset.isnull().sum())\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 11:09:18\",\"alias\":\"Assignment Config\",\"id\":267,\"name\":\"ACMASGNM88206\",\"description\":null,\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"SELECT DISTINCT configurationItem, assignmentGroup FROM @projectname_assignment_config\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"append\",\"params\":\"{}\",\"tableName\":\"@projectname_assignment_config\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-14 10:46:24\",\"alias\":\"tickets unassigned \",\"id\":867,\"name\":\"LEOTCKTS57088\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT number, configurationItem FROM @projectname_tickets\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Auto Assignment","2023-12-15T09:47:48","LEOATSGN80502","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"298\":{\"taskId\":\"76c0fa50-e25b-4ae9-ac15-77666a70f0d7\"}}"
"admin","2023-12-11T09:13:21.966","false","","NULL","{\"elements\":[{\"id\":\"VworF\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:49:23\",\"alias\":\"SLA_tickets\",\"id\":859,\"name\":\"LEOSL_TC31517\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT t.number, t.priority, t.createdDate FROM @projectname_tickets t LEFT JOIN @projectname_tickets_enriched e ON t.number = e.number WHERE t.createdDate IS NOT NULL AND e.number IS NULL OR e.resolution_SLA IS NULL\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"180\",\"position_y\":\"51\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"WDPKJ\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"context\":[],\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"}},{\"id\":\"ZXcJw\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:49:35\",\"alias\":\"SLA Configuration\",\"id\":266,\"name\":\"ACMSLCNF38699\",\"description\":null,\"schema\":\"ACMSLCNF56467\",\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_sla_configuration\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_sla_configuration\\\",\\\"uniqueIdentifier\\\":\\\"id\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"180\",\"position_y\":\"156\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"WDPKJ\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"context\":[],\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"}},{\"id\":\"WDPKJ\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"slaCalculation\",\"requirements\":\"\",\"params\":[],\"script\":[\"from datetime import datetime,timedelta\\rimport pandas as pd\\rimport logging as logger\\r\\rdef slaCalculation(dataset1,dataset2):\\r    dataset1=pd.DataFrame(dataset1)\\r    dataset2=pd.DataFrame(dataset2)\\r    logger.info('Merging dataset')\\r    dataset=pd.merge(dataset1,dataset2,on='priority',how='left')\\r    logger.info(dataset.shape[0])\\r\\r    dataset['responseSLA']=dataset['responseSLA']*3600.0\\r    dataset['resolutionSLA']=dataset['resolutionSLA']*3600.0\\r    dataset=dataset[['id','priority','number','responseSLA','resolutionSLA','createdDate']]\\r    \\r    logger.info('Calculating response SLA')\\r    dataset['createdDate']=pd.to_datetime(dataset['createdDate'])\\r    dataset['responseSLA']=pd.to_timedelta(dataset['responseSLA'],unit='s')\\r    dataset['response_SLA']=dataset['createdDate']+dataset['responseSLA']\\r    \\r    logger.info('Calculating resolution SLA')\\r    dataset['resolutionSLA']=pd.to_timedelta(dataset['resolutionSLA'],unit='s')\\r    dataset['resolution_SLA']=dataset['createdDate']+dataset['resolutionSLA']\\r\\r    dataset['last_updated']= datetime.now()\\r    dataset['response_SLA_confidence']= 100\\r    dataset['resolution_SLA_confidence']= 100\\r    \\r    dataset= dataset[['number','response_SLA','resolution_SLA','response_SLA_confidence','resolution_SLA_confidence','last_updated']]\\r    print(dataset)\\r    dataset = dataset.fillna('nan')\\r    logger.info(dataset.shape[0])\\r    dataset=dataset.to_dict('records')\\r    logger.info('Saving data')\\r    return dataset\"]},\"position_x\":\"452\",\"position_y\":\"156\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"VworF\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"ZXcJw\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"aEkTW\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:49:23\",\"alias\":\"SLA_tickets\",\"id\":859,\"name\":\"LEOSL_TC31517\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT t.number, t.priority, t.createdDate FROM @projectname_tickets t LEFT JOIN @projectname_tickets_enriched e ON t.number = e.number WHERE t.createdDate IS NOT NULL AND e.number IS NULL OR e.resolution_SLA IS NULL\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:49:35\",\"alias\":\"SLA Configuration\",\"id\":266,\"name\":\"ACMSLCNF38699\",\"description\":null,\"schema\":\"ACMSLCNF56467\",\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_sla_configuration\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_sla_configuration\\\",\\\"uniqueIdentifier\\\":\\\"id\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}],\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"}},{\"id\":\"aEkTW\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 11:47:30\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"672\",\"position_y\":\"156\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"WDPKJ\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"slaCalculation\",\"requirements\":\"\",\"params\":[],\"script\":[\"from datetime import datetime,timedelta\\rimport pandas as pd\\rimport logging as logger\\r\\rdef slaCalculation(dataset1,dataset2):\\r    dataset1=pd.DataFrame(dataset1)\\r    dataset2=pd.DataFrame(dataset2)\\r    logger.info('Merging dataset')\\r    dataset=pd.merge(dataset1,dataset2,on='priority',how='left')\\r    logger.info(dataset.shape[0])\\r\\r    dataset['responseSLA']=dataset['responseSLA']*3600.0\\r    dataset['resolutionSLA']=dataset['resolutionSLA']*3600.0\\r    dataset=dataset[['id','priority','number','responseSLA','resolutionSLA','createdDate']]\\r    \\r    logger.info('Calculating response SLA')\\r    dataset['createdDate']=pd.to_datetime(dataset['createdDate'])\\r    dataset['responseSLA']=pd.to_timedelta(dataset['responseSLA'],unit='s')\\r    dataset['response_SLA']=dataset['createdDate']+dataset['responseSLA']\\r    \\r    logger.info('Calculating resolution SLA')\\r    dataset['resolutionSLA']=pd.to_timedelta(dataset['resolutionSLA'],unit='s')\\r    dataset['resolution_SLA']=dataset['createdDate']+dataset['resolutionSLA']\\r\\r    dataset['last_updated']= datetime.now()\\r    dataset['response_SLA_confidence']= 100\\r    dataset['resolution_SLA_confidence']= 100\\r    \\r    dataset= dataset[['number','response_SLA','resolution_SLA','response_SLA_confidence','resolution_SLA_confidence','last_updated']]\\r    print(dataset)\\r    dataset = dataset.fillna('nan')\\r    logger.info(dataset.shape[0])\\r    dataset=dataset.to_dict('records')\\r    logger.info('Saving data')\\r    return dataset\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:49:23\",\"alias\":\"SLA_tickets\",\"id\":859,\"name\":\"LEOSL_TC31517\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT t.number, t.priority, t.createdDate FROM @projectname_tickets t LEFT JOIN @projectname_tickets_enriched e ON t.number = e.number WHERE t.createdDate IS NOT NULL AND e.number IS NULL OR e.resolution_SLA IS NULL\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-15 09:49:35\",\"alias\":\"SLA Configuration\",\"id\":266,\"name\":\"ACMSLCNF38699\",\"description\":null,\"schema\":\"ACMSLCNF56467\",\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_sla_configuration\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_sla_configuration\\\",\\\"uniqueIdentifier\\\":\\\"id\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Sla Calculation","2023-12-15T11:48:16","LEOSLCLC71901","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"135\":{\"taskId\":\"c5f6a7fc-8d46-449c-a94e-99b7eb3e11f5\"}}"
"admin","2023-12-08T04:45:23.982","false","Volume Forecasting using Auto ARIMA","NULL","{\"elements\":[{\"id\":\"JdypT\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"volume_forecast_arima\",\"requirements\":\"pmdarima\",\"params\":[],\"script\":[\"import pandas as pd\\rimport numpy as np\\rimport matplotlib.pyplot as plt\\rfrom sklearn.model_selection import TimeSeriesSplit\\rfrom pmdarima import auto_arima\\rfrom sklearn.metrics import mean_squared_error\\rfrom math import sqrt\\rfrom statsmodels.tsa.stattools import adfuller\\r\\rdef volume_forecast_arima(dataset):\\r  data=pd.DataFrame(dataset)\\r  data['Dates'] = pd.to_datetime(data['CreatedDate']).dt.date\\r  data['Time'] = pd.to_datetime(data['CreatedDate']).dt.time\\r  cnt = data.groupby('Dates').size().rename('Count')\\r  result = data.drop_duplicates(subset='Dates').merge(cnt, left_on='Dates', right_index=True)\\r  result=result.sort_values('Dates').set_index('Dates')\\r  tss = TimeSeriesSplit(n_splits = 3)\\r  for train_index, test_index in tss.split(result):\\r    X_train, X_test = result.iloc[train_index, :], result.iloc[test_index,:]\\r  del(X_train['CreatedDate'],X_train['Time'],X_test['CreatedDate'],X_test['Time'])\\r  X_train.dropna()\\r  \\r  X_test.dropna()\\r  test_array = X_test['Count']\\r  counter = 0\\r  while True:\\r     diff = test_array.diff().dropna()\\r\\r     p = adfuller(diff.dropna())[1]\\r     print('p-value',p)\\r     if p < 0.05:\\r         break\\r\\r     train_array = diff\\r     counter += 1\\r  print('Counter:',counter)\\r  model = auto_arima(X_train,d=counter,trace=True, error_action='ignore',suppress_warnings=True)\\r  model.fit(X_train)\\r  forecast_pred = model.predict(n_periods=len(X_test))\\r  forecast_pred.index = X_test.index\\r  forecast=pd.DataFrame(forecast_pred)\\r  forecast.columns=['Count_Prediction']\\r  plt.plot(X_train,label='Train')\\r  plt.plot(X_test, label='Test')\\r  plt.plot(forecast, label='Prediction')\\r  plt.legend(['Train','Test','Prediction'])\\r  plt.show()\\r  forecast['Count_Prediction']=forecast['Count_Prediction'].apply(np.int64)\\r  rmse = sqrt(mean_squared_error(X_test['Count'], forecast['Count_Prediction']))\\r  print('---------------------------------------------')\\r  print('Test RMSE: %.3f' % rmse)\\r  forecast=forecast.reset_index()\\r  print(forecast.head())\\r  forecast =forecast.to_dict('records')\\r  return forecast\"]},\"position_x\":\"476\",\"position_y\":\"98\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"wsEXB\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"rSxiL\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 11:17:49\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT CreatedDate from @projectname_tickets\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"rSxiL\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 11:17:49\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT CreatedDate from @projectname_tickets\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"252\",\"position_y\":\"98\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"JdypT\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"wsEXB\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-08 12:31:56\",\"alias\":\"Volume_Forecast\",\"id\":862,\"name\":\"LEOVLM_F21892\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select Dates,* from @projectname_volumeforecast\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"overwrite\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_volumeforecast\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"707\",\"position_y\":\"98\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"JdypT\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"volume_forecast_arima\",\"requirements\":\"pmdarima\",\"params\":[],\"script\":[\"import pandas as pd\\rimport numpy as np\\rimport matplotlib.pyplot as plt\\rfrom sklearn.model_selection import TimeSeriesSplit\\rfrom pmdarima import auto_arima\\rfrom sklearn.metrics import mean_squared_error\\rfrom math import sqrt\\rfrom statsmodels.tsa.stattools import adfuller\\r\\rdef volume_forecast_arima(dataset):\\r  data=pd.DataFrame(dataset)\\r  data['Dates'] = pd.to_datetime(data['CreatedDate']).dt.date\\r  data['Time'] = pd.to_datetime(data['CreatedDate']).dt.time\\r  cnt = data.groupby('Dates').size().rename('Count')\\r  result = data.drop_duplicates(subset='Dates').merge(cnt, left_on='Dates', right_index=True)\\r  result=result.sort_values('Dates').set_index('Dates')\\r  tss = TimeSeriesSplit(n_splits = 3)\\r  for train_index, test_index in tss.split(result):\\r    X_train, X_test = result.iloc[train_index, :], result.iloc[test_index,:]\\r  del(X_train['CreatedDate'],X_train['Time'],X_test['CreatedDate'],X_test['Time'])\\r  X_train.dropna()\\r  \\r  X_test.dropna()\\r  test_array = X_test['Count']\\r  counter = 0\\r  while True:\\r     diff = test_array.diff().dropna()\\r\\r     p = adfuller(diff.dropna())[1]\\r     print('p-value',p)\\r     if p < 0.05:\\r         break\\r\\r     train_array = diff\\r     counter += 1\\r  print('Counter:',counter)\\r  model = auto_arima(X_train,d=counter,trace=True, error_action='ignore',suppress_warnings=True)\\r  model.fit(X_train)\\r  forecast_pred = model.predict(n_periods=len(X_test))\\r  forecast_pred.index = X_test.index\\r  forecast=pd.DataFrame(forecast_pred)\\r  forecast.columns=['Count_Prediction']\\r  plt.plot(X_train,label='Train')\\r  plt.plot(X_test, label='Test')\\r  plt.plot(forecast, label='Prediction')\\r  plt.legend(['Train','Test','Prediction'])\\r  plt.show()\\r  forecast['Count_Prediction']=forecast['Count_Prediction'].apply(np.int64)\\r  rmse = sqrt(mean_squared_error(X_test['Count'], forecast['Count_Prediction']))\\r  print('---------------------------------------------')\\r  print('Test RMSE: %.3f' % rmse)\\r  forecast=forecast.reset_index()\\r  print(forecast.head())\\r  forecast =forecast.to_dict('records')\\r  return forecast\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 11:17:49\",\"alias\":\"Tickets\",\"id\":1,\"name\":\"Tickets\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"SELECT CreatedDate from @projectname_tickets\\\",\\\"Cacheable\\\":\\\"false\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":4,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]}],\"pipeline_attributes\":[],\"environment\":[]}","admin","Volume_Forecast","2023-12-19T09:50:53","LEOL131_21262","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"213\":{\"taskId\":\"696901c8-daa2-4b3b-bbc2-a7b29fb09f23\"}}"
"admin","2023-12-13T05:20:02.725","false","","NULL","{\"elements\":[{\"id\":\"IAYmV\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"snow_get_api\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"snowTable\",\"value\":\"incident\",\"type\":\"Text\",\"alias\":\"incident\",\"index\":\"3\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"4\"},{\"name\":\"fullLoad\",\"value\":\"False\",\"type\":\"Text\",\"alias\":\"False\",\"index\":\"5\"},{\"name\":\"TimeDeltahrs\",\"value\":\"0.3\",\"type\":\"Text\",\"alias\":\"0.3\",\"index\":\"6\"},{\"name\":\"limit\",\"value\":\"1000\",\"type\":\"Text\",\"alias\":\"1000\",\"index\":\"7\"},{\"name\":\"offset\",\"value\":\"0\",\"type\":\"Text\",\"alias\":\"0\",\"index\":\"8\"}],\"script\":[\"import requests\\rimport os\\rimport json\\rfrom leaputils import Security\\rimport logging as logger\\rimport datetime\\rimport numpy as np\\rimport pandas as pd\\rfrom urllib.parse import *\\r\\ros.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\ros.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\rdef snow_get_api(snow_data_source, api_param='', \\r                    params_param='', snowtable_param='',\\r                    setproxy_param='', fullload_param='',\\r                    timedeltahrs_param='',\\r                    limit_param='', offset_param=''):\\r  \\r\\r    # Set the request parameters\\r    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r    url = connectiondetails['Url']\\r    user = connectiondetails['AuthDetails']['username']\\r    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r    api = api_param\\r    params = params_param\\r    snowtable= snowtable_param\\r    limit = limit_param\\r    offset = offset_param\\r    resultCount = int(limit)\\r\\r    # Set proper headers\\r    headers = {'Content-Type':'application/json','Accept':'application/json'}\\r    #add filter to get records updated in last 24  hours\\r    if fullload_param == 'True':\\r        fromDate = datetime.datetime(2017,1,1)\\r    else:\\r        try:\\r            timedelta = float(timedeltahrs_param)\\r        except:\\r            timedelta = 1 # default 1 hour\\r        print('Fetching records for last {0} hrs'.format(timedelta))\\r        fromDate =  datetime.datetime.utcnow() -  datetime.timedelta(hours = timedelta)\\r    timestampParam = 'sys_updated_on>='+fromDate.strftime('%Y-%m-%d %H:%M:%S')\\r    params = params + '&sysparm_query=' + quote_plus(timestampParam)\\r\\r    out_df = []\\r    # Do the HTTP request\\r    while(resultCount==int(limit)):\\r        url = url+api+snowtable+'?'+params+'&sysparm_limit='+str(limit)+'&sysparm_offset='+str(offset)\\r        \\r        logger.info(url)\\r        proxyDict ={}\\r        if setproxy_param == 'True':\\r            proxyDict = {\\r                        'http'  : os.environ['HTTP_PROXY'],\\r                        'https' : os.environ['HTTPS_PROXY']      \\r                        }\\r                        \\r        response = requests.get(url, auth=(user, pwd), headers=headers ,proxies=proxyDict)\\r        # Check for HTTP codes other than 200\\r        if response.status_code != 200:\\r            logger.info('Status:'+response.status_code)\\r            exit()\\r        # Decode the JSON response into a dictionary and use the data\\r        response = response.json()\\r        logger.info('Records Fetched {0}'.format( len(response['result'])))\\r        resultCount = len(response['result'])\\r\\r        #convert json to spark dataframe\\r        data =[]\\r        for i in response['result']:\\r            data.append(i)\\r        \\r            \\r        df = pd.DataFrame(data)\\r        \\r        #Update default columns \\r        columnList = list(df.columns)\\r        columns_to_be_filledna = []\\r        \\r        if 'shortdescription' in columnList:\\r            columns_to_be_filledna.append('shortdescription')\\r        else:\\r            columnList.append('shortdescription')\\r            df['shortdescription'] = ''\\r\\r        if 'category' in columnList:\\r            columns_to_be_filledna.append('category')\\r        else:\\r            columnList.append('category')\\r            df['category'] = ''\\r\\r        if 'configurationItem' in columnList:\\r            columns_to_be_filledna.append('configurationItem')\\r        else:\\r            columnList.append('configurationItem')\\r            df['configurationItem'] = ''\\r\\r        if 'resolutionCategory' in columnList:\\r            columns_to_be_filledna.append('resolutionCategory')\\r        else:\\r            columnList.append('resolutionCategory')\\r            df['resolutionCategory'] = ''\\r\\r        if len(columns_to_be_filledna) > 0:\\r            df[columns_to_be_filledna] = df[columns_to_be_filledna].fillna('')\\r\\r       \\r        if(resultCount == 0):\\r            break\\r        \\r        def recordType(i):\\r            switcher = {\\r                'incident': 'Incident',\\r                'change_request': 'ChangeRequest-Normal',\\r                'incident_task': 'Task',\\r                'sc_request': 'ServiceRequest'\\r            }\\r            return switcher.get(i, 'Incident')\\r        df['type'] = recordType(snowtable.lower())\\r        \\r        \\r        snowToIcmColumnMapping = {'number':'number','short_description':'shortdescription','priority':'priority','state':'state','description':'description',\\r                                'sys_id':'sysId','opened_at':'openedDate','sys_created_on':'createdDate','sys_updated_on':'updatedDate',\\r                                'sla_due':'sladueDate','closed_at':'closedDate','due_date':'duedate','sys_created_by':'createdby',\\r                                'reopened_time':'reopenedDate','resolved_at':'resolvedDate','category':'category',\\r                                'close_code':'closecode','close_code':'resolutionCategory','impact':'impact','requested_for.display_value':'requested_for',\\r                                'assignment_group.display_value':'assignmentgroup','caller_id.display_value':'caller','assigned_to.display_value':'assignedto',\\r                                'resolved_by.display_value':'resolvedby','closed_by.display_value':'closedby','cmdb_ci.display_value':'configurationItem',\\r                                'close_notes':'closenotes','close_notes':'resolution_steps','location.display_value':'location','request_state':'request_state','price':'price',\\r                                'special_instructions':'special_instructions','approval':'approval','business_service':'business_service',\\r                                'risk':'risk','type':'type','requested_by.display_value':'requested_by','incident.display_value':'parent_Incident',\\r                                'urgency':'severity' }\\r        #mapColumns\\r        records = df.to_dict('records')\\r        for row in records:\\r            \\r            payload = {}\\r            for key in snowToIcmColumnMapping.keys():\\r                jv = row\\r                snowColumn = key.split('.')\\r                icmColumn = snowToIcmColumnMapping[key]\\r                for item in snowColumn:\\r                    try:\\r                        jv = jv[item]\\r                        if jv =='':\\r                            payload[icmColumn] = None\\r                        else:\\r                            payload[icmColumn] = jv\\r                    except:\\r                        a = 'No mapping'\\r            \\r        \\r            payload['source'] = 'SNOW'\\r            payload['lastUpdated'] = datetime.datetime.now()\\r            if payload.get('number','')!='':\\r                out_df.append(payload)\\r            \\r    return out_df\\r\\r\\r\"]},\"position_x\":\"421\",\"position_y\":\"127\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"QiDFC\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"qLIWC\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:41:20\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"encvA9rI2x6Jy7juYioii4Do9Ys4zDzuvYR\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"TcAnpEvBcFl4Ui6BJWWuZ+mTgf5Kg7R1OFpVuHU/bLycsfqFaRnLsNPqM7ug36+cH2Wwqj29ttEW5viwjDKYkw==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:41:20\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"QiDFC\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-13 08:39:51\",\"alias\":\"SNOW_GET_API \",\"id\":869,\"name\":\"LEOSNW_G39177\",\"description\":null,\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"select * from @projectname_tickets\",\"Cacheable\":false,\"isStreaming\":\"false\",\"Headers\":\"\",\"defaultValues\":\"\",\"QueryParams\":\"\",\"writeMode\":\"update\",\"params\":\"{}\",\"tableName\":\"@projectname_tickets\",\"uniqueIdentifier\":\"\"},\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"743\",\"position_y\":\"127\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"IAYmV\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"snow_get_api\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"snowTable\",\"value\":\"incident\",\"type\":\"Text\",\"alias\":\"incident\",\"index\":\"3\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"4\"},{\"name\":\"fullLoad\",\"value\":\"False\",\"type\":\"Text\",\"alias\":\"False\",\"index\":\"5\"},{\"name\":\"TimeDeltahrs\",\"value\":\"0.3\",\"type\":\"Text\",\"alias\":\"0.3\",\"index\":\"6\"},{\"name\":\"limit\",\"value\":\"1000\",\"type\":\"Text\",\"alias\":\"1000\",\"index\":\"7\"},{\"name\":\"offset\",\"value\":\"0\",\"type\":\"Text\",\"alias\":\"0\",\"index\":\"8\"}],\"script\":[\"import requests\\rimport os\\rimport json\\rfrom leaputils import Security\\rimport logging as logger\\rimport datetime\\rimport numpy as np\\rimport pandas as pd\\rfrom urllib.parse import *\\r\\ros.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\ros.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\rdef snow_get_api(snow_data_source, api_param='', \\r                    params_param='', snowtable_param='',\\r                    setproxy_param='', fullload_param='',\\r                    timedeltahrs_param='',\\r                    limit_param='', offset_param=''):\\r  \\r\\r    # Set the request parameters\\r    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r    url = connectiondetails['Url']\\r    user = connectiondetails['AuthDetails']['username']\\r    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r    api = api_param\\r    params = params_param\\r    snowtable= snowtable_param\\r    limit = limit_param\\r    offset = offset_param\\r    resultCount = int(limit)\\r\\r    # Set proper headers\\r    headers = {'Content-Type':'application/json','Accept':'application/json'}\\r    #add filter to get records updated in last 24  hours\\r    if fullload_param == 'True':\\r        fromDate = datetime.datetime(2017,1,1)\\r    else:\\r        try:\\r            timedelta = float(timedeltahrs_param)\\r        except:\\r            timedelta = 1 # default 1 hour\\r        print('Fetching records for last {0} hrs'.format(timedelta))\\r        fromDate =  datetime.datetime.utcnow() -  datetime.timedelta(hours = timedelta)\\r    timestampParam = 'sys_updated_on>='+fromDate.strftime('%Y-%m-%d %H:%M:%S')\\r    params = params + '&sysparm_query=' + quote_plus(timestampParam)\\r\\r    out_df = []\\r    # Do the HTTP request\\r    while(resultCount==int(limit)):\\r        url = url+api+snowtable+'?'+params+'&sysparm_limit='+str(limit)+'&sysparm_offset='+str(offset)\\r        \\r        logger.info(url)\\r        proxyDict ={}\\r        if setproxy_param == 'True':\\r            proxyDict = {\\r                        'http'  : os.environ['HTTP_PROXY'],\\r                        'https' : os.environ['HTTPS_PROXY']      \\r                        }\\r                        \\r        response = requests.get(url, auth=(user, pwd), headers=headers ,proxies=proxyDict)\\r        # Check for HTTP codes other than 200\\r        if response.status_code != 200:\\r            logger.info('Status:'+response.status_code)\\r            exit()\\r        # Decode the JSON response into a dictionary and use the data\\r        response = response.json()\\r        logger.info('Records Fetched {0}'.format( len(response['result'])))\\r        resultCount = len(response['result'])\\r\\r        #convert json to spark dataframe\\r        data =[]\\r        for i in response['result']:\\r            data.append(i)\\r        \\r            \\r        df = pd.DataFrame(data)\\r        \\r        #Update default columns \\r        columnList = list(df.columns)\\r        columns_to_be_filledna = []\\r        \\r        if 'shortdescription' in columnList:\\r            columns_to_be_filledna.append('shortdescription')\\r        else:\\r            columnList.append('shortdescription')\\r            df['shortdescription'] = ''\\r\\r        if 'category' in columnList:\\r            columns_to_be_filledna.append('category')\\r        else:\\r            columnList.append('category')\\r            df['category'] = ''\\r\\r        if 'configurationItem' in columnList:\\r            columns_to_be_filledna.append('configurationItem')\\r        else:\\r            columnList.append('configurationItem')\\r            df['configurationItem'] = ''\\r\\r        if 'resolutionCategory' in columnList:\\r            columns_to_be_filledna.append('resolutionCategory')\\r        else:\\r            columnList.append('resolutionCategory')\\r            df['resolutionCategory'] = ''\\r\\r        if len(columns_to_be_filledna) > 0:\\r            df[columns_to_be_filledna] = df[columns_to_be_filledna].fillna('')\\r\\r       \\r        if(resultCount == 0):\\r            break\\r        \\r        def recordType(i):\\r            switcher = {\\r                'incident': 'Incident',\\r                'change_request': 'ChangeRequest-Normal',\\r                'incident_task': 'Task',\\r                'sc_request': 'ServiceRequest'\\r            }\\r            return switcher.get(i, 'Incident')\\r        df['type'] = recordType(snowtable.lower())\\r        \\r        \\r        snowToIcmColumnMapping = {'number':'number','short_description':'shortdescription','priority':'priority','state':'state','description':'description',\\r                                'sys_id':'sysId','opened_at':'openedDate','sys_created_on':'createdDate','sys_updated_on':'updatedDate',\\r                                'sla_due':'sladueDate','closed_at':'closedDate','due_date':'duedate','sys_created_by':'createdby',\\r                                'reopened_time':'reopenedDate','resolved_at':'resolvedDate','category':'category',\\r                                'close_code':'closecode','close_code':'resolutionCategory','impact':'impact','requested_for.display_value':'requested_for',\\r                                'assignment_group.display_value':'assignmentgroup','caller_id.display_value':'caller','assigned_to.display_value':'assignedto',\\r                                'resolved_by.display_value':'resolvedby','closed_by.display_value':'closedby','cmdb_ci.display_value':'configurationItem',\\r                                'close_notes':'closenotes','close_notes':'resolution_steps','location.display_value':'location','request_state':'request_state','price':'price',\\r                                'special_instructions':'special_instructions','approval':'approval','business_service':'business_service',\\r                                'risk':'risk','type':'type','requested_by.display_value':'requested_by','incident.display_value':'parent_Incident',\\r                                'urgency':'severity' }\\r        #mapColumns\\r        records = df.to_dict('records')\\r        for row in records:\\r            \\r            payload = {}\\r            for key in snowToIcmColumnMapping.keys():\\r                jv = row\\r                snowColumn = key.split('.')\\r                icmColumn = snowToIcmColumnMapping[key]\\r                for item in snowColumn:\\r                    try:\\r                        jv = jv[item]\\r                        if jv =='':\\r                            payload[icmColumn] = None\\r                        else:\\r                            payload[icmColumn] = jv\\r                    except:\\r                        a = 'No mapping'\\r            \\r        \\r            payload['source'] = 'SNOW'\\r            payload['lastUpdated'] = datetime.datetime.now()\\r            if payload.get('number','')!='':\\r                out_df.append(payload)\\r            \\r    return out_df\\r\\r\\r\"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:41:20\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"encvA9rI2x6Jy7juYioii4Do9Ys4zDzuvYR\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"TcAnpEvBcFl4Ui6BJWWuZ+mTgf5Kg7R1OFpVuHU/bLycsfqFaRnLsNPqM7ug36+cH2Wwqj29ttEW5viwjDKYkw==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:41:20\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"qLIWC\",\"alias\":\"Connection\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:41:20\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"encvA9rI2x6Jy7juYioii4Do9Ys4zDzuvYR\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"TcAnpEvBcFl4Ui6BJWWuZ+mTgf5Kg7R1OFpVuHU/bLycsfqFaRnLsNPqM7ug36+cH2Wwqj29ttEW5viwjDKYkw==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:41:20\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"191\",\"position_y\":\"147\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"IAYmV\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"context\":[]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Snow Get API","2023-12-18T08:28:25","LEOSNWGT79954","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"70\":{\"taskId\":\"92c5dfbb-e786-4091-bd93-dae3c3aaf723\"}}"
"admin","2023-11-07T12:29:36.544","false","","NULL","{\"elements\":[{\"id\":\"BUNAd\",\"alias\":\"Create ticket \",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"snow_create_ticket\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"index\":\"4\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"alias\":\"Incident\",\"index\":\"5\"}],\"script\":[\"\\r\\\\n\",\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"def snow_create_ticket(snow_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',\\r\\\\n\",\"                       incidentpayload_param='', tickettype_param=''):\\r\\\\n\",\"   \\r\\\\n\",\"  \\r\\\\n\",\"\\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"   \\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    ticketType = tickettype_param\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"    if (ticketType.lower() == 'incident'):\\r\\\\n\",\"        snowtable = 'incident'\\r\\\\n\",\"    elif (ticketType.lower() == 'changerequest-normal'):\\r\\\\n\",\"        snowtable = 'change_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'servicerequest'):\\r\\\\n\",\"        snowtable = 'sc_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'incidenttask'):\\r\\\\n\",\"        snowtable = 'incident_task'\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = incidentpayload_param\\r\\\\n\",\"    icmPayload = json.loads(icmPayload)\\r\\\\n\",\"    icmPayloadMapping = {'number': 'number', 'shortdescription': 'short_description', 'priority.systemId': 'priority',\\r\\\\n\",\"                         'state.systemId': 'state', 'description': 'description',\\r\\\\n\",\"                         'sysId': 'sys_id', 'category.systemId': 'category', 'impact.systemId': 'impact',\\r\\\\n\",\"                         'assignmentgroup.systemId': 'assignment_group', 'assignedto.systemId': 'assigned_to',\\r\\\\n\",\"                         'configurationitem.systemId': 'cmdb_ci', 'urgency.systemId': 'urgency'}\\r\\\\n\",\"\\r\\\\n\",\"    snowPayload = {}\\r\\\\n\",\"    for key in icmPayloadMapping.keys():\\r\\\\n\",\"        jv = icmPayload\\r\\\\n\",\"        icmColumn = key.split('.')\\r\\\\n\",\"        for item in icmColumn:\\r\\\\n\",\"            try:\\r\\\\n\",\"                jv = jv[item]\\r\\\\n\",\"                snowPayload[icmPayloadMapping[key]] = jv\\r\\\\n\",\"            except:\\r\\\\n\",\"                a = 'No mapping'\\r\\\\n\",\"\\r\\\\n\",\"    snowPayload = json.dumps(snowPayload)\\r\\\\n\",\"\\r\\\\n\",\"    # setproxy\\r\\\\n\",\"    proxyDict = {}\\r\\\\n\",\"    if setproxy_param == 'True':\\r\\\\n\",\"        proxyDict = {\\r\\\\n\",\"            'http': os.environ['HTTP_PROXY'],\\r\\\\n\",\"            'https': os.environ['HTTPS_PROXY']\\r\\\\n\",\"        }\\r\\\\n\",\"    # set headers\\r\\\\n\",\"    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"    url = url + api + snowtable + '?' + params\\r\\\\n\",\"    # Do the HTTP request\\r\\\\n\",\"    response = requests.post(url, auth=(user, pwd), headers=headers, data=snowPayload, proxies=proxyDict, verify=False)\\r\\\\n\",\"    \\r\\\\n\",\"    # Check for HTTP codes other than 201\\r\\\\n\",\"    if response.status_code != 201:\\r\\\\n\",\"        logger.error('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:', response.json())\\r\\\\n\",\"        exit()\\r\\\\n\",\"\\r\\\\n\",\"    # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"    response = response.json()\\r\\\\n\",\"    \\r\\\\n\",\"    data = response['result']\\r\\\\n\",\"    \\r\\\\n\",\"    def recordType(i):\\r\\\\n\",\"        switcher = {\\r\\\\n\",\"            'incident': 'Incident',\\r\\\\n\",\"            'change_request': 'ChangeRequest-Normal',\\r\\\\n\",\"            'incident_task': 'Task',\\r\\\\n\",\"            'sc_request': 'ServiceRequest'\\r\\\\n\",\"        }\\r\\\\n\",\"        return switcher.get(i, 'Incident')\\r\\\\n\",\"    \\r\\\\n\",\"    data['type'] = recordType(snowtable.lower())\\r\\\\n\",\"    logger.info(\\\"Ticket Created:\\\" + data[\\\"number\\\"])\\r\\\\n\",\"    return data\"]},\"position_x\":\"455\",\"position_y\":\"147\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"sxRjc\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"lxPrP\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-12 12:03:29\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enczw6NP1ZEtcZUZ+yM5j3VXtx3pQFuxRGn\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"ZCiQih6gf+pU1u6BusPq8jPjeUo3Gx39sMegbOnUb7c9u6owriXHxaH1olGj6Q03o2edzUdwykOmswgGIC15tA==\",\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":null,\"category\":\"REST\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"sxRjc\",\"alias\":\"Post Processing\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"post_processing\",\"requirements\":\"\",\"params\":[],\"script\":[\"import datetime\\rdef post_processing( data):    #python-script Data\\r    \\r    row= data\\r    snowToIcmColumnMapping = {'number':'number','short_description':'shortdescription','priority':'priority','state':'state','description':'description',\\r                            'sys_id':'sysId','opened_at':'openedDate','sys_created_on':'createdDate','sys_updated_on':'updatedDate',\\r                            'sla_due':'sladueDate','closed_at':'closedDate','due_date':'duedate','sys_created_by':'createdby',\\r                            'reopened_time':'reopenedDate','resolved_at':'resolvedDate','category':'category',\\r                            'close_code':'closecode','close_code':'resolutionCategory','impact':'impact','requested_for.display_value':'requested_for',\\r                            'assignment_group.display_value':'assignmentgroup','caller_id.display_value':'caller','assigned_to.display_value':'assignedto',\\r                            'resolved_by.display_value':'resolvedby','closed_by.display_value':'closedby','cmdb_ci.display_value':'configurationItem',\\r                            'close_notes':'closenotes','close_notes':'resolution_steps','location.display_value':'location','request_state':'request_state','price':'price',\\r                            'special_instructions':'special_instructions','approval':'approval','business_service':'business_service',\\r                            'risk':'risk','type':'type','requested_by.display_value':'requested_by','incident.display_value':'parent_Incident',\\r                            'urgency':'severity' }\\r    #mapColumns\\r\\r    icmPayload = {}\\r    for key in snowToIcmColumnMapping.keys():\\r        jv = row\\r        snowColumn = key.split('.')\\r        icmColumn = snowToIcmColumnMapping[key]\\r        for item in snowColumn:\\r            try:\\r                jv = jv[item]\\r                if jv =='':\\r                    icmPayload[icmColumn] = None\\r                else:\\r                    icmPayload[icmColumn] = jv\\r            except:\\r                a = 'No mapping'\\r    logger.error(icmPayload)\\r    \\r    icmPayload['source'] = 'SNOW'\\r    icmPayload['lastUpdated'] = datetime.datetime.now()\\r\\r    logger.error(icmPayload)\\r    dataset = []\\r    dataset.append(icmPayload)\\r\\r    return dataset\"]},\"position_x\":\"639\",\"position_y\":\"194\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"BUNAd\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"LwllA\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"snow_create_ticket\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"index\":\"4\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"alias\":\"Incident\",\"index\":\"5\"}],\"script\":[\"\\r\\\\n\",\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"def snow_create_ticket(snow_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',\\r\\\\n\",\"                       incidentpayload_param='', tickettype_param=''):\\r\\\\n\",\"   \\r\\\\n\",\"  \\r\\\\n\",\"\\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"   \\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    ticketType = tickettype_param\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"    if (ticketType.lower() == 'incident'):\\r\\\\n\",\"        snowtable = 'incident'\\r\\\\n\",\"    elif (ticketType.lower() == 'changerequest-normal'):\\r\\\\n\",\"        snowtable = 'change_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'servicerequest'):\\r\\\\n\",\"        snowtable = 'sc_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'incidenttask'):\\r\\\\n\",\"        snowtable = 'incident_task'\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = incidentpayload_param\\r\\\\n\",\"    icmPayload = json.loads(icmPayload)\\r\\\\n\",\"    icmPayloadMapping = {'number': 'number', 'shortdescription': 'short_description', 'priority.systemId': 'priority',\\r\\\\n\",\"                         'state.systemId': 'state', 'description': 'description',\\r\\\\n\",\"                         'sysId': 'sys_id', 'category.systemId': 'category', 'impact.systemId': 'impact',\\r\\\\n\",\"                         'assignmentgroup.systemId': 'assignment_group', 'assignedto.systemId': 'assigned_to',\\r\\\\n\",\"                         'configurationitem.systemId': 'cmdb_ci', 'urgency.systemId': 'urgency'}\\r\\\\n\",\"\\r\\\\n\",\"    snowPayload = {}\\r\\\\n\",\"    for key in icmPayloadMapping.keys():\\r\\\\n\",\"        jv = icmPayload\\r\\\\n\",\"        icmColumn = key.split('.')\\r\\\\n\",\"        for item in icmColumn:\\r\\\\n\",\"            try:\\r\\\\n\",\"                jv = jv[item]\\r\\\\n\",\"                snowPayload[icmPayloadMapping[key]] = jv\\r\\\\n\",\"            except:\\r\\\\n\",\"                a = 'No mapping'\\r\\\\n\",\"\\r\\\\n\",\"    snowPayload = json.dumps(snowPayload)\\r\\\\n\",\"\\r\\\\n\",\"    # setproxy\\r\\\\n\",\"    proxyDict = {}\\r\\\\n\",\"    if setproxy_param == 'True':\\r\\\\n\",\"        proxyDict = {\\r\\\\n\",\"            'http': os.environ['HTTP_PROXY'],\\r\\\\n\",\"            'https': os.environ['HTTPS_PROXY']\\r\\\\n\",\"        }\\r\\\\n\",\"    # set headers\\r\\\\n\",\"    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"    url = url + api + snowtable + '?' + params\\r\\\\n\",\"    # Do the HTTP request\\r\\\\n\",\"    response = requests.post(url, auth=(user, pwd), headers=headers, data=snowPayload, proxies=proxyDict, verify=False)\\r\\\\n\",\"    \\r\\\\n\",\"    # Check for HTTP codes other than 201\\r\\\\n\",\"    if response.status_code != 201:\\r\\\\n\",\"        logger.error('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:', response.json())\\r\\\\n\",\"        exit()\\r\\\\n\",\"\\r\\\\n\",\"    # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"    response = response.json()\\r\\\\n\",\"    \\r\\\\n\",\"    data = response['result']\\r\\\\n\",\"    \\r\\\\n\",\"    def recordType(i):\\r\\\\n\",\"        switcher = {\\r\\\\n\",\"            'incident': 'Incident',\\r\\\\n\",\"            'change_request': 'ChangeRequest-Normal',\\r\\\\n\",\"            'incident_task': 'Task',\\r\\\\n\",\"            'sc_request': 'ServiceRequest'\\r\\\\n\",\"        }\\r\\\\n\",\"        return switcher.get(i, 'Incident')\\r\\\\n\",\"    \\r\\\\n\",\"    data['type'] = recordType(snowtable.lower())\\r\\\\n\",\"    logger.info(\\\"Ticket Created:\\\" + data[\\\"number\\\"])\\r\\\\n\",\"    return data\"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-12 12:03:29\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enczw6NP1ZEtcZUZ+yM5j3VXtx3pQFuxRGn\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"ZCiQih6gf+pU1u6BusPq8jPjeUo3Gx39sMegbOnUb7c9u6owriXHxaH1olGj6Q03o2edzUdwykOmswgGIC15tA==\",\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":null,\"category\":\"REST\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"LwllA\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-11-17 10:36:08\",\"alias\":\"Tickets\",\"id\":860,\"name\":\"LEOTCKTS35663\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select * from @projectname_tickets order by lastUpdated desc\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"862\",\"position_y\":\"225\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"sxRjc\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"post_processing\",\"requirements\":\"\",\"params\":[],\"script\":[\"import datetime\\rdef post_processing( data):    #python-script Data\\r    \\r    row= data\\r    snowToIcmColumnMapping = {'number':'number','short_description':'shortdescription','priority':'priority','state':'state','description':'description',\\r                            'sys_id':'sysId','opened_at':'openedDate','sys_created_on':'createdDate','sys_updated_on':'updatedDate',\\r                            'sla_due':'sladueDate','closed_at':'closedDate','due_date':'duedate','sys_created_by':'createdby',\\r                            'reopened_time':'reopenedDate','resolved_at':'resolvedDate','category':'category',\\r                            'close_code':'closecode','close_code':'resolutionCategory','impact':'impact','requested_for.display_value':'requested_for',\\r                            'assignment_group.display_value':'assignmentgroup','caller_id.display_value':'caller','assigned_to.display_value':'assignedto',\\r                            'resolved_by.display_value':'resolvedby','closed_by.display_value':'closedby','cmdb_ci.display_value':'configurationItem',\\r                            'close_notes':'closenotes','close_notes':'resolution_steps','location.display_value':'location','request_state':'request_state','price':'price',\\r                            'special_instructions':'special_instructions','approval':'approval','business_service':'business_service',\\r                            'risk':'risk','type':'type','requested_by.display_value':'requested_by','incident.display_value':'parent_Incident',\\r                            'urgency':'severity' }\\r    #mapColumns\\r\\r    icmPayload = {}\\r    for key in snowToIcmColumnMapping.keys():\\r        jv = row\\r        snowColumn = key.split('.')\\r        icmColumn = snowToIcmColumnMapping[key]\\r        for item in snowColumn:\\r            try:\\r                jv = jv[item]\\r                if jv =='':\\r                    icmPayload[icmColumn] = None\\r                else:\\r                    icmPayload[icmColumn] = jv\\r            except:\\r                a = 'No mapping'\\r    logger.error(icmPayload)\\r    \\r    icmPayload['source'] = 'SNOW'\\r    icmPayload['lastUpdated'] = datetime.datetime.now()\\r\\r    logger.error(icmPayload)\\r    dataset = []\\r    dataset.append(icmPayload)\\r\\r    return dataset\"]},{\"FunctionName\":\"snow_create_ticket\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"shortdescription\\\": \\\"VPLBM01 FACEBOOK INSIGHTS LOG ERROR DETECTED\\\", \\\"priority\\\": \\\"5\\\", \\\"state\\\": \\\"1\\\"}\",\"index\":\"4\"},{\"name\":\"ticketType\",\"value\":\"Incident\",\"type\":\"Text\",\"alias\":\"Incident\",\"index\":\"5\"}],\"script\":[\"\\r\\\\n\",\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"def snow_create_ticket(snow_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',\\r\\\\n\",\"                       incidentpayload_param='', tickettype_param=''):\\r\\\\n\",\"   \\r\\\\n\",\"  \\r\\\\n\",\"\\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"   \\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    ticketType = tickettype_param\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"    if (ticketType.lower() == 'incident'):\\r\\\\n\",\"        snowtable = 'incident'\\r\\\\n\",\"    elif (ticketType.lower() == 'changerequest-normal'):\\r\\\\n\",\"        snowtable = 'change_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'servicerequest'):\\r\\\\n\",\"        snowtable = 'sc_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'incidenttask'):\\r\\\\n\",\"        snowtable = 'incident_task'\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = incidentpayload_param\\r\\\\n\",\"    icmPayload = json.loads(icmPayload)\\r\\\\n\",\"    icmPayloadMapping = {'number': 'number', 'shortdescription': 'short_description', 'priority.systemId': 'priority',\\r\\\\n\",\"                         'state.systemId': 'state', 'description': 'description',\\r\\\\n\",\"                         'sysId': 'sys_id', 'category.systemId': 'category', 'impact.systemId': 'impact',\\r\\\\n\",\"                         'assignmentgroup.systemId': 'assignment_group', 'assignedto.systemId': 'assigned_to',\\r\\\\n\",\"                         'configurationitem.systemId': 'cmdb_ci', 'urgency.systemId': 'urgency'}\\r\\\\n\",\"\\r\\\\n\",\"    snowPayload = {}\\r\\\\n\",\"    for key in icmPayloadMapping.keys():\\r\\\\n\",\"        jv = icmPayload\\r\\\\n\",\"        icmColumn = key.split('.')\\r\\\\n\",\"        for item in icmColumn:\\r\\\\n\",\"            try:\\r\\\\n\",\"                jv = jv[item]\\r\\\\n\",\"                snowPayload[icmPayloadMapping[key]] = jv\\r\\\\n\",\"            except:\\r\\\\n\",\"                a = 'No mapping'\\r\\\\n\",\"\\r\\\\n\",\"    snowPayload = json.dumps(snowPayload)\\r\\\\n\",\"\\r\\\\n\",\"    # setproxy\\r\\\\n\",\"    proxyDict = {}\\r\\\\n\",\"    if setproxy_param == 'True':\\r\\\\n\",\"        proxyDict = {\\r\\\\n\",\"            'http': os.environ['HTTP_PROXY'],\\r\\\\n\",\"            'https': os.environ['HTTPS_PROXY']\\r\\\\n\",\"        }\\r\\\\n\",\"    # set headers\\r\\\\n\",\"    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"    url = url + api + snowtable + '?' + params\\r\\\\n\",\"    # Do the HTTP request\\r\\\\n\",\"    response = requests.post(url, auth=(user, pwd), headers=headers, data=snowPayload, proxies=proxyDict, verify=False)\\r\\\\n\",\"    \\r\\\\n\",\"    # Check for HTTP codes other than 201\\r\\\\n\",\"    if response.status_code != 201:\\r\\\\n\",\"        logger.error('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:', response.json())\\r\\\\n\",\"        exit()\\r\\\\n\",\"\\r\\\\n\",\"    # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"    response = response.json()\\r\\\\n\",\"    \\r\\\\n\",\"    data = response['result']\\r\\\\n\",\"    \\r\\\\n\",\"    def recordType(i):\\r\\\\n\",\"        switcher = {\\r\\\\n\",\"            'incident': 'Incident',\\r\\\\n\",\"            'change_request': 'ChangeRequest-Normal',\\r\\\\n\",\"            'incident_task': 'Task',\\r\\\\n\",\"            'sc_request': 'ServiceRequest'\\r\\\\n\",\"        }\\r\\\\n\",\"        return switcher.get(i, 'Incident')\\r\\\\n\",\"    \\r\\\\n\",\"    data['type'] = recordType(snowtable.lower())\\r\\\\n\",\"    logger.info(\\\"Ticket Created:\\\" + data[\\\"number\\\"])\\r\\\\n\",\"    return data\"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-12 12:03:29\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enczw6NP1ZEtcZUZ+yM5j3VXtx3pQFuxRGn\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"ZCiQih6gf+pU1u6BusPq8jPjeUo3Gx39sMegbOnUb7c9u6owriXHxaH1olGj6Q03o2edzUdwykOmswgGIC15tA==\",\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":null,\"category\":\"REST\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"lxPrP\",\"alias\":\"Connection\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"220\",\"position_y\":\"152\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"BUNAd\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}}],\"pipeline_attributes\":[{\"key\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Snow Create API","2023-12-18T12:25:32","LEOSNWCR43373","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"19\":{\"taskId\":\"20c7b2a8-4aa2-4ca7-a056-4ee5cbfbfce1\"}}"
"admin","2023-12-18T09:58:06.988","false","","NULL","{\"elements\":[{\"id\":\"PCqhf\",\"alias\":\"SNOW Connection\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"161\",\"position_y\":\"85\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"BRDrb\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"BRDrb\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"snow_update\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"number\\\":\\\"INC0058811\\\",\\\"shortdescription\\\":\\\"test update\\\",\\\"sysId\\\":\\\"52f08ab38747b110a79462ce8bbb35a4\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"number\\\":\\\"INC0058811\\\",\\\"shortdescription\\\":\\\"test update\\\",\\\"sysId\\\":\\\"52f08ab38747b110a79462ce8bbb35a4\\\"}\",\"index\":\"4\"},{\"name\":\"ticketType\",\"value\":\"incident\",\"type\":\"Text\",\"alias\":\"incident\",\"index\":\"5\"},{\"name\":\"tablename\",\"value\":\"leo1311_tickets\",\"type\":\"Text\",\"alias\":\"leo1311_tickets\",\"index\":\"6\"}],\"script\":[\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"import mysql.connector\\r\\\\n\",\"import datetime\\r\\\\n\",\"from urllib.parse import quote_plus, urlparse\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"def snow_update(snow_data_source,leap_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',\\r\\\\n\",\"                       incidentpayload_param='', tickettype_param='',tablename_param=''):\\r\\\\n\",\"                           \\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"    \\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    ticketType = tickettype_param\\r\\\\n\",\"    \\r\\\\n\",\"    if (ticketType.lower() == 'incident'):\\r\\\\n\",\"        snowtable = 'incident'\\r\\\\n\",\"    elif (ticketType.lower() == 'changerequest-normal'):\\r\\\\n\",\"        snowtable = 'change_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'servicerequest'):\\r\\\\n\",\"        snowtable = 'sc_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'incidenttask'):\\r\\\\n\",\"        snowtable = 'incident_task'\\r\\\\n\",\"    \\r\\\\n\",\"    # LEAP Datasource\\r\\\\n\",\"    leapds = leap_data_source\\r\\\\n\",\"    leapDSdict = json.loads(leap_data_source[\\\"connectionDetails\\\"])\\r\\\\n\",\"    dbusername = leapDSdict['userName']\\r\\\\n\",\"    dbpassword =  Security.decrypt(leapDSdict['password'], leap_data_source['salt'])\\r\\\\n\",\"    dbhost = urlparse(leapDSdict['url'][5:]).hostname\\r\\\\n\",\"    dbport = urlparse(leapDSdict['url'][5:]).port\\r\\\\n\",\"    database = urlparse(leapDSdict['url'][5:]).path.rsplit('/', 1)[1]\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = incidentpayload_param\\r\\\\n\",\"    icmPayload = json.loads(icmPayload)\\r\\\\n\",\"    \\r\\\\n\",\"    sysId = icmPayload.get('sysId', '')\\r\\\\n\",\"    number = icmPayload.get('number', '')\\r\\\\n\",\"    logger.info(number)\\r\\\\n\",\"    if number is not None and (sysId is None or sysId == ''):\\r\\\\n\",\"        number = icmPayload['number']\\r\\\\n\",\"        query = '''SELECT sysId FROM {0} where number = '{1}' '''.format(tablename, number)\\r\\\\n\",\"        cnx = mysql.connector.connect(user=dbusername, password=dbpassword, host=dbhost, port=dbport, database=database)\\r\\\\n\",\"        mycursor = cnx.cursor()\\r\\\\n\",\"        mycursor.execute(query)\\r\\\\n\",\"        myresult = mycursor.fetchone()\\r\\\\n\",\"        if myresult is not None and len(myresult) == 1:\\r\\\\n\",\"            sysId = myresult[0]\\r\\\\n\",\"        mycursor.close()\\r\\\\n\",\"        cnx.close()\\r\\\\n\",\"    icmPayloadMapping = {'number': 'number', 'shortdescription': 'short_description', 'priority.systemId': 'priority',\\r\\\\n\",\"                     'state.systemId': 'state', 'description': 'description',\\r\\\\n\",\"                     'sysId': 'sys_id', 'category.systemId': 'category', 'impact.systemId': 'impact',\\r\\\\n\",\"                     'assignmentgroup.systemId': 'assignment_group', 'assignedto.systemId': 'assigned_to',\\r\\\\n\",\"                     'configurationitem.systemId': 'cmdb_ci', 'urgency.systemId': 'urgency', 'sop': 'sop',\\r\\\\n\",\"                     'workflow': 'workflow'}\\r\\\\n\",\"                     \\r\\\\n\",\"    snowPayload = {}\\r\\\\n\",\"    for key in icmPayloadMapping.keys():\\r\\\\n\",\"        jv = icmPayload\\r\\\\n\",\"        icmColumn = key.split('.')\\r\\\\n\",\"        for item in icmColumn:\\r\\\\n\",\"            try:\\r\\\\n\",\"                jv = jv[item]\\r\\\\n\",\"                snowPayload[icmPayloadMapping[key]] = jv\\r\\\\n\",\"            except:\\r\\\\n\",\"                a = 'No mapping'\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = snowPayload\\r\\\\n\",\"    # setproxy\\r\\\\n\",\"    proxyDict = {}\\r\\\\n\",\"    if setproxy_param == 'True':\\r\\\\n\",\"        proxyDict = {\\r\\\\n\",\"            'http': os.environ['HTTP_PROXY'],\\r\\\\n\",\"            'https': os.environ['HTTPS_PROXY']\\r\\\\n\",\"        }\\r\\\\n\",\"    # set headers\\r\\\\n\",\"    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"    # Do the HTTP request\\r\\\\n\",\"    def recordType(i):\\r\\\\n\",\"        switcher = {\\r\\\\n\",\"            'incident': 'Incident',\\r\\\\n\",\"            'change_request': 'ChangeRequest-Normal',\\r\\\\n\",\"            'incident_task': 'Task',\\r\\\\n\",\"            'sc_request': 'ServiceRequest'\\r\\\\n\",\"        }\\r\\\\n\",\"        return switcher.get(i, 'Incident')\\r\\\\n\",\"    def tupleToStr(tuple_obj_list):\\r\\\\n\",\"        res = [''.join(i) for i in tuple_obj_list]\\r\\\\n\",\"        return ''.join(res)\\r\\\\n\",\"    cnx = mysql.connector.connect(user=dbusername, password=dbpassword, host=dbhost, port=dbport, database=database)\\r\\\\n\",\"    mycursor = cnx.cursor(dictionary=True, buffered=True)\\r\\\\n\",\"    \\r\\\\n\",\"    tickets = []\\r\\\\n\",\"    if(len(icmPayload.get('number',''))==0):\\r\\\\n\",\"        query = 'SELECT * FROM leo1311_tickets where (`createdDate` > DATE_SUB(now(), INTERVAL 1 HOUR)) and sysId is not null;'\\r\\\\n\",\"        mycursor.execute(query)\\r\\\\n\",\"        tickets = mycursor.fetchall()\\r\\\\n\",\"    else:\\r\\\\n\",\"        if(icmPayload.get('sysId', '') is None or icmPayload.get('sysId', '')==''):\\r\\\\n\",\"            icmPayload['sysId']=sysId\\r\\\\n\",\"        tickets.append(icmPayload)\\r\\\\n\",\"    dataset = []\\r\\\\n\",\"    for ticket in tickets:\\r\\\\n\",\"        url = url+api+'incident/'\\r\\\\n\",\"        if('assignmentgroup' not in ticket.keys()):\\r\\\\n\",\"            ticket['assignmentgroup'] = None\\r\\\\n\",\"        predicted_assignment_group=ticket['assignmentgroup']\\r\\\\n\",\"        if(ticket['assignmentgroup'] is None or len(ticket['assignmentgroup'])==0):\\r\\\\n\",\"            query = 'SELECT predicted_assignment_group FROM leo1311_tickets_enriched where (`number` = %s);'\\r\\\\n\",\"            mycursor.execute(query, (ticket['number'],))\\r\\\\n\",\"            results = mycursor.fetchone()\\r\\\\n\",\"            if results is not None:\\r\\\\n\",\"                predicted_assignment_group = results['predicted_assignment_group']\\r\\\\n\",\"        query = 'SELECT systemId FROM leo1311_tool_metadata where (`displayValue` = %s);'\\r\\\\n\",\"        print(predicted_assignment_group)\\r\\\\n\",\"        mycursor.execute(query, (predicted_assignment_group,))\\r\\\\n\",\"        SystemIdGroup = mycursor.fetchone()\\r\\\\n\",\"        if SystemIdGroup is not None:\\r\\\\n\",\"            SystemIdGroup = SystemIdGroup['systemId']\\r\\\\n\",\"            ticket['assignment_group']=SystemIdGroup\\r\\\\n\",\"        if('assignedto' not in ticket.keys()):\\r\\\\n\",\"            ticket['assignedto'] = None\\r\\\\n\",\"        predicted_assignee=ticket['assignedto']\\r\\\\n\",\"        if(ticket['assignedto'] is None or len(ticket['assignedto'])==0):\\r\\\\n\",\"            query = 'SELECT predicted_assignee FROM leo1311_tickets_enriched where (`number` = %s);'\\r\\\\n\",\"            mycursor.execute(query, (ticket['number'],))\\r\\\\n\",\"            predicted_assignee = mycursor.fetchone()\\r\\\\n\",\"            if predicted_assignee is not None:\\r\\\\n\",\"                predicted_assignee = predicted_assignee['predicted_assignee']\\r\\\\n\",\"        query = 'SELECT systemId FROM leo1311_tool_metadata where (`displayValue` = %s);'\\r\\\\n\",\"        mycursor.execute(query, (predicted_assignee,))\\r\\\\n\",\"        SystemIdAssignee = mycursor.fetchone()\\r\\\\n\",\"        if SystemIdAssignee is not None:\\r\\\\n\",\"            SystemIdAssignee = SystemIdAssignee['systemId']\\r\\\\n\",\"            ticket['assigned_to']=SystemIdAssignee\\r\\\\n\",\"        query = 'SELECT systemId FROM leo1311_tool_metadata where (`displayValue` = %s);'\\r\\\\n\",\"        SystemIdconfigurationitem = None\\r\\\\n\",\"        if 'configurationitem' in ticket.keys():\\r\\\\n\",\"            mycursor.execute(query, (ticket['configurationitem'],))\\r\\\\n\",\"            SystemIdconfigurationitem = mycursor.fetchone()\\r\\\\n\",\"        if SystemIdconfigurationitem is not None:\\r\\\\n\",\"            SystemIdconfigurationitem = SystemIdconfigurationitem['systemId']\\r\\\\n\",\"            ticket['cmdb_ci']=SystemIdconfigurationitem\\r\\\\n\",\"\\r\\\\n\",\"        url = url  + ticket['sysId'] + '?' + params\\r\\\\n\",\"        print('SnowUrl: {0}'.format(url))\\r\\\\n\",\"        snowPayload = ticket\\r\\\\n\",\"        if 'closedDate' in snowPayload:\\r\\\\n\",\"            del snowPayload['closedDate']\\r\\\\n\",\"        if 'createdDate' in snowPayload: \\r\\\\n\",\"            del snowPayload['createdDate']\\r\\\\n\",\"        if 'lastUpdated' in snowPayload:\\r\\\\n\",\"            del snowPayload['lastUpdated']\\r\\\\n\",\"        if 'updatedDate' in snowPayload:\\r\\\\n\",\"            del snowPayload['updatedDate']\\r\\\\n\",\"        if 'openedDate' in snowPayload:\\r\\\\n\",\"            del snowPayload['openedDate']\\r\\\\n\",\"        if 'reopenedDate' in snowPayload:    \\r\\\\n\",\"            del snowPayload['reopenedDate']\\r\\\\n\",\"        snowPayload = json.dumps(snowPayload)\\r\\\\n\",\"\\r\\\\n\",\"    \\r\\\\n\",\"        logger.info('SnowPayload:'+snowPayload)\\r\\\\n\",\"        response = requests.patch(url, auth=(user, pwd), headers=headers, data=snowPayload, proxies=proxyDict)\\r\\\\n\",\"        logger.info(\\r\\\\n\",\"            'Status:{0}, Headers:{1}, Error Response:{2}'.format(response.status_code, response.headers, response.json()))\\r\\\\n\",\"        # Check for HTTP codes other than 200\\r\\\\n\",\"        if response.status_code != 200:\\r\\\\n\",\"            exit()\\r\\\\n\",\"        # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"        data = response.json()\\r\\\\n\",\"        logger.info('Snow Response:{0}'.format(data))\\r\\\\n\",\"        row = data['result']\\r\\\\n\",\"        \\r\\\\n\",\"        if(icmPayload.get('sop', '') is not None and icmPayload.get('sop', '')!=''):\\r\\\\n\",\"            row['sop'] = icmPayload.get('sop', '')\\r\\\\n\",\"        if(icmPayload.get('workflow', '') is not None and icmPayload.get('workflow', '')!=''):\\r\\\\n\",\"            row['workflow'] = icmPayload.get('workflow', '')\\r\\\\n\",\"       \\r\\\\n\",\"        \\r\\\\n\",\"        \\r\\\\n\",\"        snowToIcmColumnMapping = {'number': 'number', 'short_description': 'shortDescription', 'priority': 'priority',\\r\\\\n\",\"                                  'state': 'state', 'description': 'description',\\r\\\\n\",\"                                  'sys_id': 'sysId', 'opened_at': 'openedDate', 'sys_created_on': 'createdDate',\\r\\\\n\",\"                                  'sys_updated_on': 'updatedDate',\\r\\\\n\",\"                                  'sla_due': 'sladueDate', 'closed_at': 'closedDate', 'due_date': 'duedate',\\r\\\\n\",\"                                  'sys_created_by': 'createdBy',\\r\\\\n\",\"                                  'reopened_time': 'reopenedDate', 'resolved_at': 'resolvedDate', 'category': 'category',\\r\\\\n\",\"                                  'close_code': 'closecode', 'close_code': 'resolutionCategory', 'impact': 'impact',\\r\\\\n\",\"                                  'requested_for.display_value': 'requested_for',\\r\\\\n\",\"                                  'assignment_group.display_value': 'assignmentGroup', 'caller_id.display_value': 'caller',\\r\\\\n\",\"                                  'assigned_to.display_value': 'assignedTo',\\r\\\\n\",\"                                  'resolved_by.display_value': 'resolvedBy', 'closed_by.display_value': 'closedBy',\\r\\\\n\",\"                                  'cmdb_ci.display_value': 'configurationItem',\\r\\\\n\",\"                                  'close_notes': 'closeNotes', 'close_notes': 'resolution_steps',\\r\\\\n\",\"                                  'location.display_value': 'location', 'request_state': 'request_state', 'price': 'price',\\r\\\\n\",\"                                  'special_instructions': 'special_instructions', 'approval': 'approval',\\r\\\\n\",\"                                  'business_service': 'business_service',\\r\\\\n\",\"                                  'risk': 'risk', 'type': 'type', 'requested_by.display_value': 'requested_by',\\r\\\\n\",\"                                  'incident.display_value': 'parent_Incident',\\r\\\\n\",\"                                  'urgency': 'severity', 'workflow': 'workflow', 'sop': 'sop'}\\r\\\\n\",\"    \\r\\\\n\",\"        #mapColumns\\r\\\\n\",\"        icmPayload = {}\\r\\\\n\",\"        \\r\\\\n\",\"        for key in snowToIcmColumnMapping.keys():\\r\\\\n\",\"            jv = row\\r\\\\n\",\"            snowColumn = key.split('.')\\r\\\\n\",\"            icmColumn = snowToIcmColumnMapping[key]\\r\\\\n\",\"            for item in snowColumn:\\r\\\\n\",\"                try:\\r\\\\n\",\"                    jv = jv[item]\\r\\\\n\",\"                    if jv =='':\\r\\\\n\",\"                        icmPayload[icmColumn] = None\\r\\\\n\",\"                    else:\\r\\\\n\",\"                        icmPayload[icmColumn] = jv\\r\\\\n\",\"                except:\\r\\\\n\",\"                    a = 'No mapping'\\r\\\\n\",\"    \\r\\\\n\",\"    \\r\\\\n\",\"        icmPayload['source'] = 'SNOW'\\r\\\\n\",\"        icmPayload['type'] = recordType(snowtable.lower())\\r\\\\n\",\"        icmPayload['lastUpdated'] = datetime.datetime.now()\\r\\\\n\",\"\\r\\\\n\",\"        \\r\\\\n\",\"        \\r\\\\n\",\"        dataset.append(icmPayload)\\r\\\\n\",\"        print(dataset)\\r\\\\n\",\"    return dataset\\r\\\\n\",\"    \\r\\\\n\"]},\"position_x\":\"414\",\"position_y\":\"82\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"PCqhf\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"XkJSs\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"owkBb\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"owkBb\",\"alias\":\"Leap Connection\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"274\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"BRDrb\",\"elementPosition\":\"TopCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"XkJSs\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 12:20:45\",\"alias\":\"ticket\",\"id\":880,\"name\":\"LEOTCKTW91170\",\"description\":\"\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets\\\",\\\"uniqueIdentifier\\\":\\\"\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"673\",\"position_y\":\"92\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"BRDrb\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"snow_update\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true&sysparm_fields=number%2Cshort_description%2Cpriority%2Cstate%2Cdescription%2Csys_id%2Copened_at%2Csys_created_on%2Csys_updated_on%2Cclosed_at%2Cdue_date%2Csys_created_by%2Creopened_time%2Cresolved_at%2Ccategory%2Cclose_code%2Cimpact%2Curgency%2Crequested_for%2Cassignment_group%2Ccaller_id%2Cassigned_to%2Cresolved_by%2Cclosed_by%2Ccmdb_ci%2Cclose_notes%2Clocation%2Crequest_state%2Cprice%2Cspecial_instructions%2Capproval%2Cbusiness_service%2Crisk%2Ctype%2Crequested_by%2Cincident\",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"},{\"name\":\"incidentPayload\",\"value\":\"{\\\"number\\\":\\\"INC0058811\\\",\\\"shortdescription\\\":\\\"test update\\\",\\\"sysId\\\":\\\"52f08ab38747b110a79462ce8bbb35a4\\\"}\",\"type\":\"Text\",\"alias\":\"{\\\"number\\\":\\\"INC0058811\\\",\\\"shortdescription\\\":\\\"test update\\\",\\\"sysId\\\":\\\"52f08ab38747b110a79462ce8bbb35a4\\\"}\",\"index\":\"4\"},{\"name\":\"ticketType\",\"value\":\"incident\",\"type\":\"Text\",\"alias\":\"incident\",\"index\":\"5\"},{\"name\":\"tablename\",\"value\":\"leo1311_tickets\",\"type\":\"Text\",\"alias\":\"leo1311_tickets\",\"index\":\"6\"}],\"script\":[\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"import mysql.connector\\r\\\\n\",\"import datetime\\r\\\\n\",\"from urllib.parse import quote_plus, urlparse\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"def snow_update(snow_data_source,leap_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',\\r\\\\n\",\"                       incidentpayload_param='', tickettype_param='',tablename_param=''):\\r\\\\n\",\"                           \\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"    \\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    ticketType = tickettype_param\\r\\\\n\",\"    \\r\\\\n\",\"    if (ticketType.lower() == 'incident'):\\r\\\\n\",\"        snowtable = 'incident'\\r\\\\n\",\"    elif (ticketType.lower() == 'changerequest-normal'):\\r\\\\n\",\"        snowtable = 'change_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'servicerequest'):\\r\\\\n\",\"        snowtable = 'sc_request'\\r\\\\n\",\"    elif (ticketType.lower() == 'incidenttask'):\\r\\\\n\",\"        snowtable = 'incident_task'\\r\\\\n\",\"    \\r\\\\n\",\"    # LEAP Datasource\\r\\\\n\",\"    leapds = leap_data_source\\r\\\\n\",\"    leapDSdict = json.loads(leap_data_source[\\\"connectionDetails\\\"])\\r\\\\n\",\"    dbusername = leapDSdict['userName']\\r\\\\n\",\"    dbpassword =  Security.decrypt(leapDSdict['password'], leap_data_source['salt'])\\r\\\\n\",\"    dbhost = urlparse(leapDSdict['url'][5:]).hostname\\r\\\\n\",\"    dbport = urlparse(leapDSdict['url'][5:]).port\\r\\\\n\",\"    database = urlparse(leapDSdict['url'][5:]).path.rsplit('/', 1)[1]\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = incidentpayload_param\\r\\\\n\",\"    icmPayload = json.loads(icmPayload)\\r\\\\n\",\"    \\r\\\\n\",\"    sysId = icmPayload.get('sysId', '')\\r\\\\n\",\"    number = icmPayload.get('number', '')\\r\\\\n\",\"    logger.info(number)\\r\\\\n\",\"    if number is not None and (sysId is None or sysId == ''):\\r\\\\n\",\"        number = icmPayload['number']\\r\\\\n\",\"        query = '''SELECT sysId FROM {0} where number = '{1}' '''.format(tablename, number)\\r\\\\n\",\"        cnx = mysql.connector.connect(user=dbusername, password=dbpassword, host=dbhost, port=dbport, database=database)\\r\\\\n\",\"        mycursor = cnx.cursor()\\r\\\\n\",\"        mycursor.execute(query)\\r\\\\n\",\"        myresult = mycursor.fetchone()\\r\\\\n\",\"        if myresult is not None and len(myresult) == 1:\\r\\\\n\",\"            sysId = myresult[0]\\r\\\\n\",\"        mycursor.close()\\r\\\\n\",\"        cnx.close()\\r\\\\n\",\"    icmPayloadMapping = {'number': 'number', 'shortdescription': 'short_description', 'priority.systemId': 'priority',\\r\\\\n\",\"                     'state.systemId': 'state', 'description': 'description',\\r\\\\n\",\"                     'sysId': 'sys_id', 'category.systemId': 'category', 'impact.systemId': 'impact',\\r\\\\n\",\"                     'assignmentgroup.systemId': 'assignment_group', 'assignedto.systemId': 'assigned_to',\\r\\\\n\",\"                     'configurationitem.systemId': 'cmdb_ci', 'urgency.systemId': 'urgency', 'sop': 'sop',\\r\\\\n\",\"                     'workflow': 'workflow'}\\r\\\\n\",\"                     \\r\\\\n\",\"    snowPayload = {}\\r\\\\n\",\"    for key in icmPayloadMapping.keys():\\r\\\\n\",\"        jv = icmPayload\\r\\\\n\",\"        icmColumn = key.split('.')\\r\\\\n\",\"        for item in icmColumn:\\r\\\\n\",\"            try:\\r\\\\n\",\"                jv = jv[item]\\r\\\\n\",\"                snowPayload[icmPayloadMapping[key]] = jv\\r\\\\n\",\"            except:\\r\\\\n\",\"                a = 'No mapping'\\r\\\\n\",\"\\r\\\\n\",\"    icmPayload = snowPayload\\r\\\\n\",\"    # setproxy\\r\\\\n\",\"    proxyDict = {}\\r\\\\n\",\"    if setproxy_param == 'True':\\r\\\\n\",\"        proxyDict = {\\r\\\\n\",\"            'http': os.environ['HTTP_PROXY'],\\r\\\\n\",\"            'https': os.environ['HTTPS_PROXY']\\r\\\\n\",\"        }\\r\\\\n\",\"    # set headers\\r\\\\n\",\"    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"    # Do the HTTP request\\r\\\\n\",\"    def recordType(i):\\r\\\\n\",\"        switcher = {\\r\\\\n\",\"            'incident': 'Incident',\\r\\\\n\",\"            'change_request': 'ChangeRequest-Normal',\\r\\\\n\",\"            'incident_task': 'Task',\\r\\\\n\",\"            'sc_request': 'ServiceRequest'\\r\\\\n\",\"        }\\r\\\\n\",\"        return switcher.get(i, 'Incident')\\r\\\\n\",\"    def tupleToStr(tuple_obj_list):\\r\\\\n\",\"        res = [''.join(i) for i in tuple_obj_list]\\r\\\\n\",\"        return ''.join(res)\\r\\\\n\",\"    cnx = mysql.connector.connect(user=dbusername, password=dbpassword, host=dbhost, port=dbport, database=database)\\r\\\\n\",\"    mycursor = cnx.cursor(dictionary=True, buffered=True)\\r\\\\n\",\"    \\r\\\\n\",\"    tickets = []\\r\\\\n\",\"    if(len(icmPayload.get('number',''))==0):\\r\\\\n\",\"        query = 'SELECT * FROM leo1311_tickets where (`createdDate` > DATE_SUB(now(), INTERVAL 1 HOUR)) and sysId is not null;'\\r\\\\n\",\"        mycursor.execute(query)\\r\\\\n\",\"        tickets = mycursor.fetchall()\\r\\\\n\",\"    else:\\r\\\\n\",\"        if(icmPayload.get('sysId', '') is None or icmPayload.get('sysId', '')==''):\\r\\\\n\",\"            icmPayload['sysId']=sysId\\r\\\\n\",\"        tickets.append(icmPayload)\\r\\\\n\",\"    dataset = []\\r\\\\n\",\"    for ticket in tickets:\\r\\\\n\",\"        url = url+api+'incident/'\\r\\\\n\",\"        if('assignmentgroup' not in ticket.keys()):\\r\\\\n\",\"            ticket['assignmentgroup'] = None\\r\\\\n\",\"        predicted_assignment_group=ticket['assignmentgroup']\\r\\\\n\",\"        if(ticket['assignmentgroup'] is None or len(ticket['assignmentgroup'])==0):\\r\\\\n\",\"            query = 'SELECT predicted_assignment_group FROM leo1311_tickets_enriched where (`number` = %s);'\\r\\\\n\",\"            mycursor.execute(query, (ticket['number'],))\\r\\\\n\",\"            results = mycursor.fetchone()\\r\\\\n\",\"            if results is not None:\\r\\\\n\",\"                predicted_assignment_group = results['predicted_assignment_group']\\r\\\\n\",\"        query = 'SELECT systemId FROM leo1311_tool_metadata where (`displayValue` = %s);'\\r\\\\n\",\"        print(predicted_assignment_group)\\r\\\\n\",\"        mycursor.execute(query, (predicted_assignment_group,))\\r\\\\n\",\"        SystemIdGroup = mycursor.fetchone()\\r\\\\n\",\"        if SystemIdGroup is not None:\\r\\\\n\",\"            SystemIdGroup = SystemIdGroup['systemId']\\r\\\\n\",\"            ticket['assignment_group']=SystemIdGroup\\r\\\\n\",\"        if('assignedto' not in ticket.keys()):\\r\\\\n\",\"            ticket['assignedto'] = None\\r\\\\n\",\"        predicted_assignee=ticket['assignedto']\\r\\\\n\",\"        if(ticket['assignedto'] is None or len(ticket['assignedto'])==0):\\r\\\\n\",\"            query = 'SELECT predicted_assignee FROM leo1311_tickets_enriched where (`number` = %s);'\\r\\\\n\",\"            mycursor.execute(query, (ticket['number'],))\\r\\\\n\",\"            predicted_assignee = mycursor.fetchone()\\r\\\\n\",\"            if predicted_assignee is not None:\\r\\\\n\",\"                predicted_assignee = predicted_assignee['predicted_assignee']\\r\\\\n\",\"        query = 'SELECT systemId FROM leo1311_tool_metadata where (`displayValue` = %s);'\\r\\\\n\",\"        mycursor.execute(query, (predicted_assignee,))\\r\\\\n\",\"        SystemIdAssignee = mycursor.fetchone()\\r\\\\n\",\"        if SystemIdAssignee is not None:\\r\\\\n\",\"            SystemIdAssignee = SystemIdAssignee['systemId']\\r\\\\n\",\"            ticket['assigned_to']=SystemIdAssignee\\r\\\\n\",\"        query = 'SELECT systemId FROM leo1311_tool_metadata where (`displayValue` = %s);'\\r\\\\n\",\"        SystemIdconfigurationitem = None\\r\\\\n\",\"        if 'configurationitem' in ticket.keys():\\r\\\\n\",\"            mycursor.execute(query, (ticket['configurationitem'],))\\r\\\\n\",\"            SystemIdconfigurationitem = mycursor.fetchone()\\r\\\\n\",\"        if SystemIdconfigurationitem is not None:\\r\\\\n\",\"            SystemIdconfigurationitem = SystemIdconfigurationitem['systemId']\\r\\\\n\",\"            ticket['cmdb_ci']=SystemIdconfigurationitem\\r\\\\n\",\"\\r\\\\n\",\"        url = url  + ticket['sysId'] + '?' + params\\r\\\\n\",\"        print('SnowUrl: {0}'.format(url))\\r\\\\n\",\"        snowPayload = ticket\\r\\\\n\",\"        if 'closedDate' in snowPayload:\\r\\\\n\",\"            del snowPayload['closedDate']\\r\\\\n\",\"        if 'createdDate' in snowPayload: \\r\\\\n\",\"            del snowPayload['createdDate']\\r\\\\n\",\"        if 'lastUpdated' in snowPayload:\\r\\\\n\",\"            del snowPayload['lastUpdated']\\r\\\\n\",\"        if 'updatedDate' in snowPayload:\\r\\\\n\",\"            del snowPayload['updatedDate']\\r\\\\n\",\"        if 'openedDate' in snowPayload:\\r\\\\n\",\"            del snowPayload['openedDate']\\r\\\\n\",\"        if 'reopenedDate' in snowPayload:    \\r\\\\n\",\"            del snowPayload['reopenedDate']\\r\\\\n\",\"        snowPayload = json.dumps(snowPayload)\\r\\\\n\",\"\\r\\\\n\",\"    \\r\\\\n\",\"        logger.info('SnowPayload:'+snowPayload)\\r\\\\n\",\"        response = requests.patch(url, auth=(user, pwd), headers=headers, data=snowPayload, proxies=proxyDict)\\r\\\\n\",\"        logger.info(\\r\\\\n\",\"            'Status:{0}, Headers:{1}, Error Response:{2}'.format(response.status_code, response.headers, response.json()))\\r\\\\n\",\"        # Check for HTTP codes other than 200\\r\\\\n\",\"        if response.status_code != 200:\\r\\\\n\",\"            exit()\\r\\\\n\",\"        # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"        data = response.json()\\r\\\\n\",\"        logger.info('Snow Response:{0}'.format(data))\\r\\\\n\",\"        row = data['result']\\r\\\\n\",\"        \\r\\\\n\",\"        if(icmPayload.get('sop', '') is not None and icmPayload.get('sop', '')!=''):\\r\\\\n\",\"            row['sop'] = icmPayload.get('sop', '')\\r\\\\n\",\"        if(icmPayload.get('workflow', '') is not None and icmPayload.get('workflow', '')!=''):\\r\\\\n\",\"            row['workflow'] = icmPayload.get('workflow', '')\\r\\\\n\",\"       \\r\\\\n\",\"        \\r\\\\n\",\"        \\r\\\\n\",\"        snowToIcmColumnMapping = {'number': 'number', 'short_description': 'shortDescription', 'priority': 'priority',\\r\\\\n\",\"                                  'state': 'state', 'description': 'description',\\r\\\\n\",\"                                  'sys_id': 'sysId', 'opened_at': 'openedDate', 'sys_created_on': 'createdDate',\\r\\\\n\",\"                                  'sys_updated_on': 'updatedDate',\\r\\\\n\",\"                                  'sla_due': 'sladueDate', 'closed_at': 'closedDate', 'due_date': 'duedate',\\r\\\\n\",\"                                  'sys_created_by': 'createdBy',\\r\\\\n\",\"                                  'reopened_time': 'reopenedDate', 'resolved_at': 'resolvedDate', 'category': 'category',\\r\\\\n\",\"                                  'close_code': 'closecode', 'close_code': 'resolutionCategory', 'impact': 'impact',\\r\\\\n\",\"                                  'requested_for.display_value': 'requested_for',\\r\\\\n\",\"                                  'assignment_group.display_value': 'assignmentGroup', 'caller_id.display_value': 'caller',\\r\\\\n\",\"                                  'assigned_to.display_value': 'assignedTo',\\r\\\\n\",\"                                  'resolved_by.display_value': 'resolvedBy', 'closed_by.display_value': 'closedBy',\\r\\\\n\",\"                                  'cmdb_ci.display_value': 'configurationItem',\\r\\\\n\",\"                                  'close_notes': 'closeNotes', 'close_notes': 'resolution_steps',\\r\\\\n\",\"                                  'location.display_value': 'location', 'request_state': 'request_state', 'price': 'price',\\r\\\\n\",\"                                  'special_instructions': 'special_instructions', 'approval': 'approval',\\r\\\\n\",\"                                  'business_service': 'business_service',\\r\\\\n\",\"                                  'risk': 'risk', 'type': 'type', 'requested_by.display_value': 'requested_by',\\r\\\\n\",\"                                  'incident.display_value': 'parent_Incident',\\r\\\\n\",\"                                  'urgency': 'severity', 'workflow': 'workflow', 'sop': 'sop'}\\r\\\\n\",\"    \\r\\\\n\",\"        #mapColumns\\r\\\\n\",\"        icmPayload = {}\\r\\\\n\",\"        \\r\\\\n\",\"        for key in snowToIcmColumnMapping.keys():\\r\\\\n\",\"            jv = row\\r\\\\n\",\"            snowColumn = key.split('.')\\r\\\\n\",\"            icmColumn = snowToIcmColumnMapping[key]\\r\\\\n\",\"            for item in snowColumn:\\r\\\\n\",\"                try:\\r\\\\n\",\"                    jv = jv[item]\\r\\\\n\",\"                    if jv =='':\\r\\\\n\",\"                        icmPayload[icmColumn] = None\\r\\\\n\",\"                    else:\\r\\\\n\",\"                        icmPayload[icmColumn] = jv\\r\\\\n\",\"                except:\\r\\\\n\",\"                    a = 'No mapping'\\r\\\\n\",\"    \\r\\\\n\",\"    \\r\\\\n\",\"        icmPayload['source'] = 'SNOW'\\r\\\\n\",\"        icmPayload['type'] = recordType(snowtable.lower())\\r\\\\n\",\"        icmPayload['lastUpdated'] = datetime.datetime.now()\\r\\\\n\",\"\\r\\\\n\",\"        \\r\\\\n\",\"        \\r\\\\n\",\"        dataset.append(icmPayload)\\r\\\\n\",\"        print(dataset)\\r\\\\n\",\"    return dataset\\r\\\\n\",\"    \\r\\\\n\"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]}],\"pipeline_attributes\":[{\"name\":\"storageType\",\"value\":\"s3\"}],\"environment\":[]}","admin","Snow Update","2023-12-18T12:33:18","LEOSNWPD76824","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"55\":{\"taskId\":\"678b9c05-f1c0-4ddc-89f0-3214c58e8663\"}}"
"admin","2023-12-13T05:38:20.427","false","","NULL","{\"elements\":[{\"id\":\"VodDw\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"snow_get_metadata_api\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true \",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true \",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"}],\"script\":[\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"import datetime\\r\\\\n\",\"\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"def snow_get_metadata_api(snow_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',datatable_param=''):\\r\\\\n\",\"    \\r\\\\n\",\"    \\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    tablename = datatable_param\\r\\\\n\",\"    configs = ['configurationitem','assignee','caller','assignmentgroup','priority','state','urgency','category','impact']\\r\\\\n\",\"    dataset=[]\\r\\\\n\",\"    for config in configs:\\r\\\\n\",\"        logger.info('fetching {0} from SNOW'.format(config))\\r\\\\n\",\"        #configurationItem\\r\\\\n\",\"        if config.lower() == 'configurationitem':\\r\\\\n\",\"            snowtable='cmdb_ci'\\r\\\\n\",\"            configtype = 'configurationItem'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'assignee':\\r\\\\n\",\"        #Assignee\\r\\\\n\",\"            snowtable='sys_user'\\r\\\\n\",\"            configtype = 'assignee'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'caller':\\r\\\\n\",\"        #Caller\\r\\\\n\",\"            snowtable='sys_user'\\r\\\\n\",\"            configtype = 'caller'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'assignmentgroup': \\r\\\\n\",\"        #AssignmentGroup\\r\\\\n\",\"            snowtable='sys_user_group'\\r\\\\n\",\"            configtype = 'assignmentGroup'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'priority':    \\r\\\\n\",\"        #priority\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'priority'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=priority'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower()=='state':\\r\\\\n\",\"        # #state\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'state'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=state'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower() =='urgency':\\r\\\\n\",\"        #urgency\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'urgency'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=urgency'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'category':  \\r\\\\n\",\"        #Category\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'category'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=category'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'impact':\\r\\\\n\",\"        # #impact\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'impact'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=impact'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        else:\\r\\\\n\",\"            logger.info('Configuration Type not found')\\r\\\\n\",\"            exit()\\r\\\\n\",\"    \\r\\\\n\",\"        #setproxy\\r\\\\n\",\"        proxyDict ={}\\r\\\\n\",\"        if setproxy_param == 'True':\\r\\\\n\",\"            proxyDict = {\\r\\\\n\",\"                        'http'  : os.environ['HTTP_PROXY'],\\r\\\\n\",\"                        'https' : os.environ['HTTPS_PROXY']      \\r\\\\n\",\"                        }\\r\\\\n\",\"    \\r\\\\n\",\"        # Do the HTTP request\\r\\\\n\",\"        headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"        url = url+api+snowtable+'?'+params  + customParams\\r\\\\n\",\"    \\r\\\\n\",\"        response = requests.get(url, auth=(user, pwd), headers=headers ,proxies=proxyDict)\\r\\\\n\",\"        # Check for HTTP codes other than 200\\r\\\\n\",\"        if response.status_code != 200:\\r\\\\n\",\"            logger.info('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:',response.json())\\r\\\\n\",\"            exit()\\r\\\\n\",\"    \\r\\\\n\",\"        # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"        data1 = response.json()\\r\\\\n\",\"        data = data1['result']\\r\\\\n\",\"    \\r\\\\n\",\"    \\r\\\\n\",\"        #mapColumns\\r\\\\n\",\"        \\r\\\\n\",\"        for row in data:\\r\\\\n\",\"            icmPayload = {}\\r\\\\n\",\"            for key in snowToIcmColumnMapping.keys():\\r\\\\n\",\"                jv = row\\r\\\\n\",\"                snowColumn = key.split('.')\\r\\\\n\",\"                icmColumn = snowToIcmColumnMapping[key]\\r\\\\n\",\"                for item in snowColumn:\\r\\\\n\",\"                    try:\\r\\\\n\",\"                        jv = jv[item]\\r\\\\n\",\"                        if jv =='':\\r\\\\n\",\"                            icmPayload[icmColumn] = None\\r\\\\n\",\"                        else:\\r\\\\n\",\"                            icmPayload[icmColumn] = jv\\r\\\\n\",\"                    except:\\r\\\\n\",\"                        a = 'No mapping'\\r\\\\n\",\"            \\r\\\\n\",\"        \\r\\\\n\",\"            icmPayload['source'] = 'SNOW'\\r\\\\n\",\"            icmPayload['lastUpdated'] = datetime.datetime.now()\\r\\\\n\",\"            icmPayload['type'] = config\\r\\\\n\",\"            \\r\\\\n\",\"            \\r\\\\n\",\"            dataset.append(icmPayload)\\r\\\\n\",\"    \\r\\\\n\",\"    return dataset\\r\\\\n\",\"\\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \"]},\"position_x\":\"410\",\"position_y\":\"110\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"lYVIA\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"BvKyd\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"context\":[{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}],\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"}},{\"id\":\"BvKyd\",\"alias\":\"Connection\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"138\",\"position_y\":\"112\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"VodDw\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"lYVIA\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 08:37:12\",\"alias\":\"Metadata\",\"id\":877,\"name\":\"LEOMTDTP69578\",\"description\":\"\",\"schema\":null,\"attributes\":{\"filter\":\"\",\"mode\":\"query\",\"Query\":\"Select * from @projectname_tool_metadata\",\"isStreaming\":\"false\",\"defaultValues\":\"\",\"writeMode\":\"overwrite\",\"params\":\"{}\",\"tableName\":\"@projectname_tool_metadata\",\"uniqueIdentifier\":\"\"},\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":\"\",\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"653\",\"position_y\":\"101\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"VodDw\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"snow_get_metadata_api\",\"requirements\":\"\",\"params\":[{\"name\":\"api\",\"value\":\"/api/now/table/\",\"type\":\"Text\",\"alias\":\"/api/now/table/\",\"index\":\"1\"},{\"name\":\"params\",\"value\":\"sysparm_display_value=true \",\"type\":\"Text\",\"alias\":\"sysparm_display_value=true \",\"index\":\"2\"},{\"name\":\"setProxy\",\"value\":\"True\",\"type\":\"Text\",\"alias\":\"True\",\"index\":\"3\"}],\"script\":[\"import requests\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"from leaputils import Security\\r\\\\n\",\"import logging as logger\\r\\\\n\",\"import datetime\\r\\\\n\",\"\\r\\\\n\",\"os.environ['HTTP_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"os.environ['HTTPS_PROXY'] = 'http://blrproxy.ad.infosys.com:443'\\r\\\\n\",\"\\r\\\\n\",\"def snow_get_metadata_api(snow_data_source, api_param='',\\r\\\\n\",\"                       params_param='', setproxy_param='',datatable_param=''):\\r\\\\n\",\"    \\r\\\\n\",\"    \\r\\\\n\",\"    # Set the request parameters\\r\\\\n\",\"    connectiondetails = json.loads(snow_data_source['connectionDetails'])\\r\\\\n\",\"    url = connectiondetails['Url']\\r\\\\n\",\"    user = connectiondetails['AuthDetails']['username']\\r\\\\n\",\"    pwd = Security.decrypt(connectiondetails['AuthDetails']['password'], snow_data_source['salt'])\\r\\\\n\",\"    api = api_param\\r\\\\n\",\"    params = params_param\\r\\\\n\",\"    tablename = datatable_param\\r\\\\n\",\"    configs = ['configurationitem','assignee','caller','assignmentgroup','priority','state','urgency','category','impact']\\r\\\\n\",\"    dataset=[]\\r\\\\n\",\"    for config in configs:\\r\\\\n\",\"        logger.info('fetching {0} from SNOW'.format(config))\\r\\\\n\",\"        #configurationItem\\r\\\\n\",\"        if config.lower() == 'configurationitem':\\r\\\\n\",\"            snowtable='cmdb_ci'\\r\\\\n\",\"            configtype = 'configurationItem'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'assignee':\\r\\\\n\",\"        #Assignee\\r\\\\n\",\"            snowtable='sys_user'\\r\\\\n\",\"            configtype = 'assignee'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'caller':\\r\\\\n\",\"        #Caller\\r\\\\n\",\"            snowtable='sys_user'\\r\\\\n\",\"            configtype = 'caller'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'assignmentgroup': \\r\\\\n\",\"        #AssignmentGroup\\r\\\\n\",\"            snowtable='sys_user_group'\\r\\\\n\",\"            configtype = 'assignmentGroup'\\r\\\\n\",\"            customParams = '&sysparm_fields=sys_id%2Cname'\\r\\\\n\",\"            snowToIcmColumnMapping ={'name':'displayValue', 'sys_id':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'priority':    \\r\\\\n\",\"        #priority\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'priority'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=priority'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower()=='state':\\r\\\\n\",\"        # #state\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'state'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=state'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower() =='urgency':\\r\\\\n\",\"        #urgency\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'urgency'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=urgency'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'category':  \\r\\\\n\",\"        #Category\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'category'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=category'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        elif config.lower() == 'impact':\\r\\\\n\",\"        # #impact\\r\\\\n\",\"            snowtable='sys_choice'\\r\\\\n\",\"            configtype = 'impact'\\r\\\\n\",\"            customParams = '&sysparm_query=name=incident^element=impact'\\r\\\\n\",\"            snowToIcmColumnMapping ={'label':'displayValue', 'value':'systemId'}\\r\\\\n\",\"        else:\\r\\\\n\",\"            logger.info('Configuration Type not found')\\r\\\\n\",\"            exit()\\r\\\\n\",\"    \\r\\\\n\",\"        #setproxy\\r\\\\n\",\"        proxyDict ={}\\r\\\\n\",\"        if setproxy_param == 'True':\\r\\\\n\",\"            proxyDict = {\\r\\\\n\",\"                        'http'  : os.environ['HTTP_PROXY'],\\r\\\\n\",\"                        'https' : os.environ['HTTPS_PROXY']      \\r\\\\n\",\"                        }\\r\\\\n\",\"    \\r\\\\n\",\"        # Do the HTTP request\\r\\\\n\",\"        headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\\r\\\\n\",\"        url = url+api+snowtable+'?'+params  + customParams\\r\\\\n\",\"    \\r\\\\n\",\"        response = requests.get(url, auth=(user, pwd), headers=headers ,proxies=proxyDict)\\r\\\\n\",\"        # Check for HTTP codes other than 200\\r\\\\n\",\"        if response.status_code != 200:\\r\\\\n\",\"            logger.info('Status:', response.status_code, 'Headers:', response.headers, 'Error Response:',response.json())\\r\\\\n\",\"            exit()\\r\\\\n\",\"    \\r\\\\n\",\"        # Decode the JSON response into a dictionary and use the data\\r\\\\n\",\"        data1 = response.json()\\r\\\\n\",\"        data = data1['result']\\r\\\\n\",\"    \\r\\\\n\",\"    \\r\\\\n\",\"        #mapColumns\\r\\\\n\",\"        \\r\\\\n\",\"        for row in data:\\r\\\\n\",\"            icmPayload = {}\\r\\\\n\",\"            for key in snowToIcmColumnMapping.keys():\\r\\\\n\",\"                jv = row\\r\\\\n\",\"                snowColumn = key.split('.')\\r\\\\n\",\"                icmColumn = snowToIcmColumnMapping[key]\\r\\\\n\",\"                for item in snowColumn:\\r\\\\n\",\"                    try:\\r\\\\n\",\"                        jv = jv[item]\\r\\\\n\",\"                        if jv =='':\\r\\\\n\",\"                            icmPayload[icmColumn] = None\\r\\\\n\",\"                        else:\\r\\\\n\",\"                            icmPayload[icmColumn] = jv\\r\\\\n\",\"                    except:\\r\\\\n\",\"                        a = 'No mapping'\\r\\\\n\",\"            \\r\\\\n\",\"        \\r\\\\n\",\"            icmPayload['source'] = 'SNOW'\\r\\\\n\",\"            icmPayload['lastUpdated'] = datetime.datetime.now()\\r\\\\n\",\"            icmPayload['type'] = config\\r\\\\n\",\"            \\r\\\\n\",\"            \\r\\\\n\",\"            dataset.append(icmPayload)\\r\\\\n\",\"    \\r\\\\n\",\"    return dataset\\r\\\\n\",\"\\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \\r\\\\n\",\"                      \"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-18 07:45:06\",\"alias\":\"SNOW\",\"id\":4,\"name\":\"ACMSNWQB82627\",\"description\":\"Service Now\",\"type\":\"SERVICENOW\",\"connectionDetails\":\"{\\\"NoProxy\\\":\\\"false\\\",\\\"ConnectionType\\\":\\\"ApiRequest\\\",\\\"testDataset\\\":{\\\"name\\\":\\\"\\\",\\\"attributes\\\":{\\\"bodyType\\\":\\\"raw\\\",\\\"Endpoint\\\":\\\"\\\",\\\"RequestMethod\\\":\\\"GET\\\",\\\"Headers\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"Body\\\":\\\"\\\"}},\\\"AuthDetails\\\":{\\\"password\\\":\\\"enc9giJ/cd1mEmrPyO5NUOfKmTvs/im88j/\\\",\\\"authParams\\\":{\\\"grant_type\\\":\\\"\\\",\\\"client_secret\\\":\\\"\\\",\\\"client_id\\\":\\\"\\\"},\\\"username\\\":\\\"ICSP_icap_user\\\"},\\\"AuthType\\\":\\\"BasicAuth\\\",\\\"Url\\\":\\\"https://infosysq3dev1.service-now.com\\\"}\",\"salt\":\"6JVGAk93wQBFBmGG4iwVF3tuoN9dph1WJrP3BekdeD1yQJl7xYia/KVBYG7yGZyKBvvU5XdJnaCRWnkkcFdhgA==\",\"organization\":\"leo1311\",\"dshashcode\":\"4726251e4022841d6078fd68ec70e429bfdb754f79652c75b5c88c667defecf7\",\"activetime\":\"2023-12-18 07:45:06\",\"category\":\"REST\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]}],\"pipeline_attributes\":[],\"environment\":[]}","admin","Snow Get Metadata API","2023-12-18T08:45:40","LEOSNWGT92755","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"37\":{\"taskId\":\"8c83a8de-30f1-41df-b449-c67ba809f08f\"}}"
"admin","2023-12-18T10:50:45.206","false","Training ML model on Tickets data to predict cluster ","NULL","{\"elements\":[{\"id\":\"eRLzH\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rfrom nltk.corpus import stopwords\\rimport numpy as np\\rfrom nltk.tokenize import word_tokenize\\rfrom sklearn.preprocessing import LabelEncoder\\rimport regex as re\\rimport string\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem import WordNetLemmatizer\\rimport matplotlib.pyplot as plt\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\rfrom sklearn.model_selection import train_test_split\\rfrom sklearn.naive_bayes import MultinomialNB\\rfrom sklearn.model_selection import GridSearchCV\\rfrom sklearn.metrics import accuracy_score\\rimport pickle\\rimport tempfile\\r\\rdef PythonScript( dataset):\\r    data=pd.DataFrame(dataset)\\r    data=data.dropna()\\r    data.head()\\r    categories=data['post_ranking_cluster'].unique()\\r    encoder = LabelEncoder()\\r    encoded_categories = encoder.fit_transform(categories)\\r    vectorizer = TfidfVectorizer(stop_words='english')\\r    x = vectorizer.fit_transform(data['shortdescription'])\\r    y=encoder.fit_transform(data['post_ranking_cluster'])\\r    indices = np.argsort(vectorizer.idf_)[::-1]\\r    features = vectorizer.get_feature_names_out()\\r    X=pd.DataFrame(x.toarray(), columns=features)\\r    finalmodel=MultinomialNB()\\r    finalmodel.fit(X,y)\\r    \\r\\r    pickle_data = {\\\"model\\\": finalmodel,\\\"vectorizer\\\": vectorizer,\\\"encoder\\\": encoder}\\r\\r    with open(\\\"model_with_components.pkl\\\", \\\"wb\\\") as f:\\r        pickle.dump(pickle_data, f)\\r    # Get the file pointer to the beginning of the temporary file\\r        \\r\\r    # Use this file pointer for S3 upload\\r        f.close()\\r    return 'model_with_components.pkl'\"]},\"position_x\":\"522\",\"position_y\":\"66\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"FUuHi\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"WtBCw\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 12:38:38\",\"alias\":\"Tickets_with_pr_cluster\",\"id\":874,\"name\":\"LEOTCKTS63858\",\"description\":\"Enriched Tickets\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select pe.number,pt.shortdescription,pe.post_ranking_cluster from @projectname_tickets_enriched as pe join @projectname_tickets as pt on pe.number=pt.number where pe.post_ranking_cluster is not null and pe.post_ranking_cluster <> ''\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"WtBCw\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 12:38:38\",\"alias\":\"Tickets_with_pr_cluster\",\"id\":874,\"name\":\"LEOTCKTS63858\",\"description\":\"Enriched Tickets\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select pe.number,pt.shortdescription,pe.post_ranking_cluster from @projectname_tickets_enriched as pe join @projectname_tickets as pt on pe.number=pt.number where pe.post_ranking_cluster is not null and pe.post_ranking_cluster <> ''\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"259\",\"position_y\":\"68\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"eRLzH\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"eVCFH\",\"alias\":\"Nutanix\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-07-24 06:49:15\",\"alias\":\"Nutanix\",\"id\":15,\"name\":\"AIPNTNXH56520\",\"description\":\"\",\"type\":\"S3\",\"connectionDetails\":\"{\\\"secretKey\\\":\\\"g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\\\",\\\"accessKey\\\":\\\"GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\\\",\\\"Region\\\":\\\"\\\",\\\"url\\\":\\\"https://10.82.53.110/\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2023-07-24 06:49:14\",\"category\":\"S3\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"556\",\"position_y\":\"191\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"FUuHi\",\"elementPosition\":\"BottomCenter\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"context\":[]},{\"id\":\"FUuHi\",\"alias\":\"Model Upload\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"upload\",\"requirements\":\"\",\"params\":[{\"name\":\"bucket\",\"value\":\"aicloudprd\",\"type\":\"Text\",\"alias\":\"aicloudprd\",\"index\":\"1\"},{\"name\":\"object_key\",\"value\":\"leapdatasets/modelnb.pkl\",\"type\":\"Text\",\"alias\":\"leapdatasets/modelnb.pkl\",\"index\":\"2\"}],\"script\":[\"import sys\\r\\\\n\",\"import threading\\r\\\\n\",\"import pickle\\r\\\\n\",\"import boto3\\r\\\\n\",\"from boto3.s3.transfer import TransferConfig\\r\\\\n\",\"import pathlib\\r\\\\n\",\"import tempfile\\r\\\\n\",\"import os\\r\\\\n\",\"import json\\r\\\\n\",\"\\r\\\\n\",\"if not os.path.exists(\\\"my_directory\\\"):\\r\\\\n\",\"    os.makedirs(\\\"my_directory\\\")\\r\\\\n\",\"MB = 1024 * 1024\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"local_path='model_with_components.pkl'\\r\\\\n\",\"def upload(local_path,s3_connection,bucket_param,object_key_param,file_size_mb=5, metadata=None):\\r\\\\n\",\"    connectiondetails = json.loads(s3_connection['connectionDetails'])\\r\\\\n\",\"    s3 = boto3.resource('s3',endpoint_url=connectiondetails['url'],\\r\\\\n\",\"                        aws_access_key_id=connectiondetails['accessKey'],\\r\\\\n\",\"                        aws_secret_access_key=connectiondetails['secretKey'],\\r\\\\n\",\"                        verify=False)\\r\\\\n\",\"    \\r\\\\n\",\"    config = TransferConfig(multipart_chunksize=5 * MB)\\r\\\\n\",\"    extra_args = {'Metadata': metadata} if metadata else None\\r\\\\n\",\"    s3.Bucket(bucket_param).upload_file(local_path,object_key_param,\\r\\\\n\",\"        Config=config,ExtraArgs=extra_args)\\r\\\\n\",\"    \\r\\\\n\",\"    print(f\\\"Model saved to S3: s3://{bucket_param}/{object_key_param}\\\")\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\",\"\\r\\\\n\"]},\"position_x\":\"793\",\"position_y\":\"66\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"eRLzH\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset3\",\"position\":\"BottomCenter\",\"elementId\":\"eVCFH\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\rfrom nltk.corpus import stopwords\\rimport numpy as np\\rfrom nltk.tokenize import word_tokenize\\rfrom sklearn.preprocessing import LabelEncoder\\rimport regex as re\\rimport string\\rfrom nltk.corpus import stopwords\\rfrom nltk.tokenize import word_tokenize\\rfrom nltk.stem import WordNetLemmatizer\\rimport matplotlib.pyplot as plt\\rfrom sklearn.feature_extraction.text import TfidfVectorizer\\rfrom sklearn.model_selection import train_test_split\\rfrom sklearn.naive_bayes import MultinomialNB\\rfrom sklearn.model_selection import GridSearchCV\\rfrom sklearn.metrics import accuracy_score\\rimport pickle\\rimport tempfile\\r\\rdef PythonScript( dataset):\\r    data=pd.DataFrame(dataset)\\r    data=data.dropna()\\r    data.head()\\r    categories=data['post_ranking_cluster'].unique()\\r    encoder = LabelEncoder()\\r    encoded_categories = encoder.fit_transform(categories)\\r    vectorizer = TfidfVectorizer(stop_words='english')\\r    x = vectorizer.fit_transform(data['shortdescription'])\\r    y=encoder.fit_transform(data['post_ranking_cluster'])\\r    indices = np.argsort(vectorizer.idf_)[::-1]\\r    features = vectorizer.get_feature_names_out()\\r    X=pd.DataFrame(x.toarray(), columns=features)\\r    finalmodel=MultinomialNB()\\r    finalmodel.fit(X,y)\\r    \\r\\r    pickle_data = {\\\"model\\\": finalmodel,\\\"vectorizer\\\": vectorizer,\\\"encoder\\\": encoder}\\r\\r    with open(\\\"model_with_components.pkl\\\", \\\"wb\\\") as f:\\r        pickle.dump(pickle_data, f)\\r    # Get the file pointer to the beginning of the temporary file\\r        \\r\\r    # Use this file pointer for S3 upload\\r        f.close()\\r    return 'model_with_components.pkl'\"]},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 12:38:38\",\"alias\":\"Tickets_with_pr_cluster\",\"id\":874,\"name\":\"LEOTCKTS63858\",\"description\":\"Enriched Tickets\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select pe.number,pt.shortdescription,pe.post_ranking_cluster from @projectname_tickets_enriched as pe join @projectname_tickets as pt on pe.number=pt.number where pe.post_ranking_cluster is not null and pe.post_ranking_cluster <> ''\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-07-24 06:49:15\",\"alias\":\"Nutanix\",\"id\":15,\"name\":\"AIPNTNXH56520\",\"description\":\"\",\"type\":\"S3\",\"connectionDetails\":\"{\\\"secretKey\\\":\\\"g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\\\",\\\"accessKey\\\":\\\"GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\\\",\\\"Region\\\":\\\"\\\",\\\"url\\\":\\\"https://10.82.53.110/\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2023-07-24 06:49:14\",\"category\":\"S3\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]}],\"pipeline_attributes\":[],\"environment\":[]}","admin","Cluster Training","2023-12-19T15:03:23","LEOCLSTR33592","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"332\":{\"taskId\":\"ad68beb5-3c2c-4410-a9b7-719dbfe68aa8\"}}"
"admin","2023-12-19T05:33:51.556","false","Cluster Prediction on Tickets Data","NULL","{\"elements\":[{\"id\":\"TfLkr\",\"alias\":\"Python Script\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\r\\\\n\",\"from nltk.corpus import stopwords\\r\\\\n\",\"import numpy as np\\r\\\\n\",\"from nltk.tokenize import word_tokenize\\r\\\\n\",\"from sklearn.preprocessing import LabelEncoder\\r\\\\n\",\"import regex as re\\r\\\\n\",\"import string\\r\\\\n\",\"from nltk.corpus import stopwords\\r\\\\n\",\"from nltk.tokenize import word_tokenize\\r\\\\n\",\"from nltk.stem import WordNetLemmatizer\\r\\\\n\",\"import matplotlib.pyplot as plt\\r\\\\n\",\"from sklearn.feature_extraction.text import TfidfVectorizer\\r\\\\n\",\"from sklearn.model_selection import train_test_split\\r\\\\n\",\"from sklearn.naive_bayes import MultinomialNB\\r\\\\n\",\"from sklearn.model_selection import GridSearchCV\\r\\\\n\",\"from sklearn.metrics import accuracy_score\\r\\\\n\",\"import pickle\\r\\\\n\",\"\\r\\\\n\",\"def PythonScript( dataset,object_save_path):\\r\\\\n\",\"    data=pd.DataFrame(dataset)\\r\\\\n\",\"    data = data.dropna(subset=['shortdescription'])\\r\\\\n\",\"\\r\\\\n\",\"    print(data.head())\\r\\\\n\",\"    with open(object_save_path, \\\"rb\\\") as f:\\r\\\\n\",\"        loaded_data = pickle.load(f)\\r\\\\n\",\"    finalmodel = loaded_data[\\\"model\\\"]\\r\\\\n\",\"    vectorizer = loaded_data[\\\"vectorizer\\\"]\\r\\\\n\",\"    encoder = loaded_data[\\\"encoder\\\"]\\r\\\\n\",\"    x = vectorizer.transform(data['shortdescription'])\\r\\\\n\",\"    features = vectorizer.get_feature_names_out()\\r\\\\n\",\"    X=pd.DataFrame(x.toarray(), columns=features)\\r\\\\n\",\"    y_hat=finalmodel.predict(X)\\r\\\\n\",\"    predicted_y=encoder.inverse_transform(y_hat)\\r\\\\n\",\"    final_data={'number':data['number'],'cluster_classification_label':predicted_y}\\r\\\\n\",\"    final_df=pd.DataFrame(final_data)\\r\\\\n\",\"    print(final_df.head())\\r\\\\n\",\"    final_df=final_df.to_dict('records')\\r\\\\n\",\"    return final_df\"]},\"position_x\":\"529\",\"position_y\":\"88\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"mTXUL\",\"elementPosition\":\"LeftMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset2\",\"position\":\"TopCenter\",\"elementId\":\"KSOgG\",\"elementPosition\":\"RightMiddle\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"YPXje\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"FunctionName\":\"download\",\"requirements\":\"\",\"params\":[{\"name\":\"bucket\",\"value\":\"aicloudprd\",\"type\":\"Text\",\"alias\":\"aicloudprd\",\"index\":\"1\"},{\"name\":\"s3_path\",\"value\":\"leapdatasets/modelnb.pkl\",\"type\":\"Text\",\"alias\":\"leapdatasets/modelnb.pkl\",\"index\":\"2\"}],\"script\":[\"import sys\\r\\\\n\",\"import threading\\r\\\\n\",\"import boto3\\r\\\\n\",\"from boto3.s3.transfer import TransferConfig\\r\\\\n\",\"import pathlib\\r\\\\n\",\"MB = 1024 * 1024\\r\\\\n\",\"\\r\\\\n\",\"                    \\r\\\\n\",\"def download(s3_connection,bucket_param,s3_path_param):\\r\\\\n\",\"    connectiondetails = json.loads(s3_connection['connectionDetails'])\\r\\\\n\",\"    s3 = boto3.resource('s3',endpoint_url=connectiondetails['url'],\\r\\\\n\",\"                        aws_access_key_id=connectiondetails['accessKey'],\\r\\\\n\",\"                        aws_secret_access_key=connectiondetails['secretKey'],\\r\\\\n\",\"                        verify=False)\\r\\\\n\",\"    bucket_object = s3.Bucket(bucket_param)\\r\\\\n\",\"    local_path='/data'\\r\\\\n\",\"    print(bucket_object)\\r\\\\n\",\"    for my_bucket_object in bucket_object.objects.filter(Prefix=s3_path_param):\\r\\\\n\",\"           object_save_path = (f\\\"{local_path}/{pathlib.Path(my_bucket_object.key).name}\\\")\\r\\\\n\",\"           bucket_object.download_file(my_bucket_object.key, object_save_path)\\r\\\\n\",\"    return object_save_path\\r\\\\n\",\"   \"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-07-24 06:49:15\",\"alias\":\"Nutanix\",\"id\":15,\"name\":\"AIPNTNXH56520\",\"description\":\"\",\"type\":\"S3\",\"connectionDetails\":\"{\\\"secretKey\\\":\\\"g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\\\",\\\"accessKey\\\":\\\"GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\\\",\\\"Region\\\":\\\"\\\",\\\"url\\\":\\\"https://10.82.53.110/\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2023-07-24 06:49:14\",\"category\":\"S3\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 12:44:16\",\"alias\":\"Tickets_without_prcluster\",\"id\":886,\"name\":\"LEOTCKTS32420\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select pe.number,pt.shortdescription,pe.post_ranking_cluster from @projectname_tickets_enriched as pe join @projectname_tickets as pt on pt.number=pe.number where pe.post_ranking_cluster is null or pe.post_ranking_cluster =''\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"KSOgG\",\"alias\":\"Model Download\",\"name\":\"Python Script\",\"classname\":\"PythonScriptConfig\",\"category\":\"BaseConfig\",\"attributes\":{\"FunctionName\":\"download\",\"requirements\":\"\",\"params\":[{\"name\":\"bucket\",\"value\":\"aicloudprd\",\"type\":\"Text\",\"alias\":\"aicloudprd\",\"index\":\"1\"},{\"name\":\"s3_path\",\"value\":\"leapdatasets/modelnb.pkl\",\"type\":\"Text\",\"alias\":\"leapdatasets/modelnb.pkl\",\"index\":\"2\"}],\"script\":[\"import sys\\r\\\\n\",\"import threading\\r\\\\n\",\"import boto3\\r\\\\n\",\"from boto3.s3.transfer import TransferConfig\\r\\\\n\",\"import pathlib\\r\\\\n\",\"MB = 1024 * 1024\\r\\\\n\",\"\\r\\\\n\",\"                    \\r\\\\n\",\"def download(s3_connection,bucket_param,s3_path_param):\\r\\\\n\",\"    connectiondetails = json.loads(s3_connection['connectionDetails'])\\r\\\\n\",\"    s3 = boto3.resource('s3',endpoint_url=connectiondetails['url'],\\r\\\\n\",\"                        aws_access_key_id=connectiondetails['accessKey'],\\r\\\\n\",\"                        aws_secret_access_key=connectiondetails['secretKey'],\\r\\\\n\",\"                        verify=False)\\r\\\\n\",\"    bucket_object = s3.Bucket(bucket_param)\\r\\\\n\",\"    local_path='/data'\\r\\\\n\",\"    print(bucket_object)\\r\\\\n\",\"    for my_bucket_object in bucket_object.objects.filter(Prefix=s3_path_param):\\r\\\\n\",\"           object_save_path = (f\\\"{local_path}/{pathlib.Path(my_bucket_object.key).name}\\\")\\r\\\\n\",\"           bucket_object.download_file(my_bucket_object.key, object_save_path)\\r\\\\n\",\"    return object_save_path\\r\\\\n\",\"   \"]},\"position_x\":\"416\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"TfLkr\",\"elementPosition\":\"TopCenter\"},{\"type\":\"target\",\"endpoint\":\"dataset1\",\"position\":\"LeftMiddle\",\"elementId\":\"YksGc\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"dataset1\",\"dataset2\",\"dataset3\"],\"outputEndpoints\":[\"out\"],\"formats\":{\"FunctionName\":\"text\",\"requirements\":\"textarea\",\"params\":\"list\",\"script\":\"textarea\"},\"codeGeneration\":{\"imports\":[],\"script\":\"\\n\\n\\n\\n\\n\\n\"},\"context\":[{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-07-24 06:49:15\",\"alias\":\"Nutanix\",\"id\":15,\"name\":\"AIPNTNXH56520\",\"description\":\"\",\"type\":\"S3\",\"connectionDetails\":\"{\\\"secretKey\\\":\\\"g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\\\",\\\"accessKey\\\":\\\"GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\\\",\\\"Region\\\":\\\"\\\",\\\"url\\\":\\\"https://10.82.53.110/\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2023-07-24 06:49:14\",\"category\":\"S3\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}}]},{\"id\":\"mTXUL\",\"alias\":\"Dataset Loader\",\"name\":\"Dataset Loader\",\"classname\":\"DatasetLoader\",\"category\":\"Loader\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 08:43:25\",\"alias\":\"TicketsEnriched\",\"id\":276,\"name\":\"ACMTCKTS76661\",\"description\":null,\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"select * from @projectname_tickets_enriched\\\",\\\"Cacheable\\\":false,\\\"isStreaming\\\":\\\"false\\\",\\\"Headers\\\":\\\"\\\",\\\"defaultValues\\\":\\\"\\\",\\\"QueryParams\\\":\\\"\\\",\\\"writeMode\\\":\\\"update\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"@projectname_tickets_enriched\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"rw\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":0,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":false,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"808\",\"position_y\":\"88\",\"connectors\":[{\"type\":\"target\",\"endpoint\":\"in\",\"position\":\"LeftMiddle\",\"elementId\":\"TfLkr\",\"elementPosition\":\"RightMiddle\"}],\"inputEndpoints\":[\"in\"],\"outputEndpoints\":[],\"formats\":{\"dataset\":\"dropdown\"},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"imports\":[],\"MYSQL\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetLoader_<id>(dataset,dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    loadertype = dataset_param['datasource'].get('type','')\\r    if loadertype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Loader datasource mapping')\\r    logger.info('Loading Dataset - {0} of type {1}'.format(datasetName, loadertype))\\r    datasetAttributes = dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt', '')\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    loader = ''\\r\\r    # load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('EXTRA_PLUGINS_PATH not a valid Path. Please update icip.environment - EXTRA_PLUGINS_PATH constant')\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/loaders/' + loadertype  # ask user - filePath\\r    logger.info('Loading plugin from path {0}'.format(file_path))\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Loader', fp, pathname, description);\\r    class_name = loadertype  # ask user - className\\r    loader = getattr(module, class_name)\\r    loader = loader(datasourceAttributes, datasetAttributes)\\r    if loader == '':\\r        logger.error('No loader configured for type {0}'.format(loadertype))\\r    \\r    loader.loadData(dataset)\\r    print('Data Saved')\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[{\"FunctionName\":\"PythonScript\",\"requirements\":\"\",\"params\":[],\"script\":[\"import pandas as pd\\r\\\\n\",\"from nltk.corpus import stopwords\\r\\\\n\",\"import numpy as np\\r\\\\n\",\"from nltk.tokenize import word_tokenize\\r\\\\n\",\"from sklearn.preprocessing import LabelEncoder\\r\\\\n\",\"import regex as re\\r\\\\n\",\"import string\\r\\\\n\",\"from nltk.corpus import stopwords\\r\\\\n\",\"from nltk.tokenize import word_tokenize\\r\\\\n\",\"from nltk.stem import WordNetLemmatizer\\r\\\\n\",\"import matplotlib.pyplot as plt\\r\\\\n\",\"from sklearn.feature_extraction.text import TfidfVectorizer\\r\\\\n\",\"from sklearn.model_selection import train_test_split\\r\\\\n\",\"from sklearn.naive_bayes import MultinomialNB\\r\\\\n\",\"from sklearn.model_selection import GridSearchCV\\r\\\\n\",\"from sklearn.metrics import accuracy_score\\r\\\\n\",\"import pickle\\r\\\\n\",\"\\r\\\\n\",\"def PythonScript( dataset,object_save_path):\\r\\\\n\",\"    data=pd.DataFrame(dataset)\\r\\\\n\",\"    data = data.dropna(subset=['shortdescription'])\\r\\\\n\",\"\\r\\\\n\",\"    print(data.head())\\r\\\\n\",\"    with open(object_save_path, \\\"rb\\\") as f:\\r\\\\n\",\"        loaded_data = pickle.load(f)\\r\\\\n\",\"    finalmodel = loaded_data[\\\"model\\\"]\\r\\\\n\",\"    vectorizer = loaded_data[\\\"vectorizer\\\"]\\r\\\\n\",\"    encoder = loaded_data[\\\"encoder\\\"]\\r\\\\n\",\"    x = vectorizer.transform(data['shortdescription'])\\r\\\\n\",\"    features = vectorizer.get_feature_names_out()\\r\\\\n\",\"    X=pd.DataFrame(x.toarray(), columns=features)\\r\\\\n\",\"    y_hat=finalmodel.predict(X)\\r\\\\n\",\"    predicted_y=encoder.inverse_transform(y_hat)\\r\\\\n\",\"    final_data={'number':data['number'],'cluster_classification_label':predicted_y}\\r\\\\n\",\"    final_df=pd.DataFrame(final_data)\\r\\\\n\",\"    print(final_df.head())\\r\\\\n\",\"    final_df=final_df.to_dict('records')\\r\\\\n\",\"    return final_df\"]},{\"FunctionName\":\"download\",\"requirements\":\"\",\"params\":[{\"name\":\"bucket\",\"value\":\"aicloudprd\",\"type\":\"Text\",\"alias\":\"aicloudprd\",\"index\":\"1\"},{\"name\":\"s3_path\",\"value\":\"leapdatasets/modelnb.pkl\",\"type\":\"Text\",\"alias\":\"leapdatasets/modelnb.pkl\",\"index\":\"2\"}],\"script\":[\"import sys\\r\\\\n\",\"import threading\\r\\\\n\",\"import boto3\\r\\\\n\",\"from boto3.s3.transfer import TransferConfig\\r\\\\n\",\"import pathlib\\r\\\\n\",\"MB = 1024 * 1024\\r\\\\n\",\"\\r\\\\n\",\"                    \\r\\\\n\",\"def download(s3_connection,bucket_param,s3_path_param):\\r\\\\n\",\"    connectiondetails = json.loads(s3_connection['connectionDetails'])\\r\\\\n\",\"    s3 = boto3.resource('s3',endpoint_url=connectiondetails['url'],\\r\\\\n\",\"                        aws_access_key_id=connectiondetails['accessKey'],\\r\\\\n\",\"                        aws_secret_access_key=connectiondetails['secretKey'],\\r\\\\n\",\"                        verify=False)\\r\\\\n\",\"    bucket_object = s3.Bucket(bucket_param)\\r\\\\n\",\"    local_path='/data'\\r\\\\n\",\"    print(bucket_object)\\r\\\\n\",\"    for my_bucket_object in bucket_object.objects.filter(Prefix=s3_path_param):\\r\\\\n\",\"           object_save_path = (f\\\"{local_path}/{pathlib.Path(my_bucket_object.key).name}\\\")\\r\\\\n\",\"           bucket_object.download_file(my_bucket_object.key, object_save_path)\\r\\\\n\",\"    return object_save_path\\r\\\\n\",\"   \"]},{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-07-24 06:49:15\",\"alias\":\"Nutanix\",\"id\":15,\"name\":\"AIPNTNXH56520\",\"description\":\"\",\"type\":\"S3\",\"connectionDetails\":\"{\\\"secretKey\\\":\\\"g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\\\",\\\"accessKey\\\":\\\"GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\\\",\\\"Region\\\":\\\"\\\",\\\"url\\\":\\\"https://10.82.53.110/\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2023-07-24 06:49:14\",\"category\":\"S3\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 12:44:16\",\"alias\":\"Tickets_without_prcluster\",\"id\":886,\"name\":\"LEOTCKTS32420\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select pe.number,pt.shortdescription,pe.post_ranking_cluster from @projectname_tickets_enriched as pe join @projectname_tickets as pt on pt.number=pe.number where pe.post_ranking_cluster is null or pe.post_ranking_cluster =''\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}}]},{\"id\":\"YPXje\",\"alias\":\"Dataset Extractor\",\"name\":\"Dataset Extractor\",\"classname\":\"DatasetExtractor\",\"category\":\"Extractor\",\"attributes\":{\"dataset\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-12-19 12:44:16\",\"alias\":\"Tickets_without_prcluster\",\"id\":886,\"name\":\"LEOTCKTS32420\",\"description\":\"Tickets data\",\"schema\":null,\"attributes\":\"{\\\"filter\\\":\\\"\\\",\\\"mode\\\":\\\"query\\\",\\\"Query\\\":\\\"Select pe.number,pt.shortdescription,pe.post_ranking_cluster from @projectname_tickets_enriched as pe join @projectname_tickets as pt on pt.number=pe.number where pe.post_ranking_cluster is null or pe.post_ranking_cluster =''\\\",\\\"isStreaming\\\":\\\"false\\\",\\\"defaultValues\\\":\\\"\\\",\\\"writeMode\\\":\\\"append\\\",\\\"params\\\":\\\"{}\\\",\\\"tableName\\\":\\\"\\\",\\\"uniqueIdentifier\\\":\\\"number\\\"}\",\"type\":\"r\",\"datasource\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2021-04-09 07:03:18\",\"alias\":\"leo1311\",\"id\":1,\"name\":\"leo1311\",\"description\":\"Local data for setup\",\"type\":\"MYSQL\",\"connectionDetails\":\"{\\\"password\\\":\\\"@datasourcepass\\\",\\\"datasource\\\":\\\"\\\",\\\"userName\\\":\\\"@datasourceuser\\\",\\\"url\\\":\\\"@datasourceurl/leapmaster_ref_data\\\"}\",\"salt\":\"E1uP5fNKX66G/tyeT1Vz304MwyOa8urMohnCakE/bOnYf47zC+u9Q9+8tmjhlUagunL858NTWkxGo3bqHg9tEw==\",\"organization\":\"leo1311\",\"dshashcode\":\"2b057d5ee2557413a31224216f8d9e39708e21676559b1290e6a545f46d59dc8\",\"activetime\":null,\"category\":\"SQL\",\"extras\":null,\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false},\"backingDataset\":null,\"organization\":\"leo1311\",\"expStatus\":null,\"views\":\"Table View\",\"context\":null,\"archivalConfig\":null,\"isArchivalEnabled\":null,\"isApprovalRequired\":false,\"isPermissionManaged\":false,\"isAuditRequired\":false,\"isInboxRequired\":false,\"dashboard\":null,\"metadata\":null,\"modeltype\":null,\"taskdetails\":null,\"tags\":\"[]\",\"interfacetype\":null,\"adaptername\":null,\"isadapteractive\":null}},\"position_x\":\"139\",\"position_y\":\"88\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"TfLkr\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"dataset\":[\"dropdown\"]},\"codeGeneration\":{\"REST\":{},\"requirements\":[],\"servicenow\":{},\"imports\":[],\"MYSQL\":{},\"w\":{},\"H2\":{},\"MSSQL\":{},\"AWS\":{},\"POSTGRESQL\":{},\"script\":\"def DatasetExtractor_<id>(dataset_param={}):\\r    datasetName = dataset_param.get('alias',dataset_param.get('name'))\\r    extractortype = dataset_param['datasource'].get('type','')\\r    if extractortype == '':\\r        logger.error('Datasource Type mapping not found. Validate Dataset Extractor datasource mapping')\\r    logger.info('Extracting Dataset - {0} of type {1}'.format(datasetName, extractortype))\\r    datasetAttributes= dataset_param['attributes']\\r    if type(datasetAttributes) is str:\\r        datasetAttributes = json.loads(datasetAttributes)\\r    datasource = dataset_param['datasource']\\r    datasourceAttributes = json.loads(datasource['connectionDetails'])\\r    datasourceAttributes['salt'] = datasource.get('salt','')\\r    for item in datasourceAttributes.keys():\\r        if '_vault' not in item:\\r            from leaputils import Vault\\r            try:\\r                isvault=datasourceAttributes[item+'_vault']\\r                if isvault:\\r                    value = Vault.getPassword(datasourceAttributes[item+'_vault'])\\r                    datasourceAttributes[item] = value\\r            except:\\r                a=1\\r    datasetAttributes['schema'] = dataset_param.get('schema','')\\r    datasetAttributes['applySchema'] = False\\r\\r    extractor = ''\\r\\r    #load from plugins path\\r    EXTRA_PLUGINS_PATH = os.environ.get('EXTRA_PLUGINS_PATH','')\\r    if not os.path.exists(EXTRA_PLUGINS_PATH) or EXTRA_PLUGINS_PATH =='':\\r        EXTRA_PLUGINS_PATH = '/root/plugins'\\r        logger.error('Please update environment variable - EXTRA_PLUGINS_PATH ')\\r\\r    file_path = EXTRA_PLUGINS_PATH.replace('\\\\\\\\','/') + '/extractors/' + extractortype  # ask user - filePath\\r    fp, pathname, description = imp.find_module(file_path);\\r    module = imp.load_module('Extractor', fp, pathname, description);\\r    class_name = extractortype  # ask user - className\\r    extractor = getattr(module, class_name)\\r    extractor = extractor(datasourceAttributes, datasetAttributes)\\r    if extractor == '':\\r        logger.error('No extractor configured for type {0}'.format(extractortype))\\r    dataset = extractor.getData()\\r    return dataset\\r\\r\\r\\r\\r\\r\\r\\r\\n\"},\"context\":[]},{\"id\":\"YksGc\",\"alias\":\"Nutanix\",\"name\":\"Connection\",\"classname\":\"Connection\",\"category\":\"Connection\",\"attributes\":{\"connections\":{\"lastmodifiedby\":\"admin\",\"lastmodifieddate\":\"2023-07-24 06:49:15\",\"alias\":\"Nutanix\",\"id\":15,\"name\":\"AIPNTNXH56520\",\"description\":\"\",\"type\":\"S3\",\"connectionDetails\":\"{\\\"secretKey\\\":\\\"g2d4nVxehagjOkCkZ4WrCMOzrfTrFiI0\\\",\\\"accessKey\\\":\\\"GISeSU7xd6WBnXrU-QbffBee7WsCxaE2\\\",\\\"Region\\\":\\\"\\\",\\\"url\\\":\\\"https://10.82.53.110/\\\"}\",\"salt\":null,\"organization\":\"leo1311\",\"dshashcode\":null,\"activetime\":\"2023-07-24 06:49:14\",\"category\":\"S3\",\"extras\":\"{\\\"apispec\\\":{},\\\"apispectemplate\\\":{}}\",\"interfacetype\":null,\"fordataset\":false,\"forruntime\":false,\"foradapter\":false,\"formodel\":false}},\"position_x\":\"127\",\"position_y\":\"0\",\"connectors\":[{\"type\":\"source\",\"endpoint\":\"out\",\"position\":\"RightMiddle\",\"elementId\":\"KSOgG\",\"elementPosition\":\"LeftMiddle\"}],\"inputEndpoints\":[],\"outputEndpoints\":[\"out\"],\"formats\":{\"connections\":\"\"},\"codeGeneration\":{\"requirements\":[],\"imports\":[],\"script\":\"def Connection_<id>(connections_param={}):\\n    \\n    return connections_param\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"context\":[]}],\"pipeline_attributes\":[],\"environment\":[]}","admin","Cluster Prediction","2023-12-19T15:00:26","LEOCLSTR80390","leo1311","DragNDropLite","NULL","NULL","pipeline","{\"195\":{\"taskId\":\"8bb68515-109d-440a-8f54-5cdb16a699a9\"}}"
"admin","2023-12-15T12:05:03.747","false","","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"LEOITSMB35657_leo1311.py\"],\"arguments\":[{\"name\":\"App\",\"value\":\"ITSM Buddy\"}],\"dataset\":[]}}]}","admin","ITSM Buddy","2023-12-15T12:05:04","LEOITSMB35657","leo1311","App","0","NULL","App","NULL"
"admin","2023-12-15T12:06:58.236","false","","NULL","{\"elements\":[{\"attributes\":{\"filetype\":\"Python3\",\"files\":[\"LEOVDKNW76064_leo1311.py\"],\"arguments\":[{\"name\":\"App\",\"value\":\"Video Knowledge Search\"}],\"dataset\":[]}}]}","admin","Video Knowledge Search","2023-12-15T12:07:15","LEOVDKNW76064","leo1311","App","1","NULL","App","NULL"